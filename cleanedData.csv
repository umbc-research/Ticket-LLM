"TicketID","CreatedDate","SubjectNoHTML","TransactionContent"
"3280927","2025-09-25 16:56:10","HPC Slurm/Software Issue: GPU Job QoS not Permitted","Hi [USER], I see that your job is currently running without issue. Did you get that output from squeue? If so, you can see that the job status is 'PD' which means pending. I am not 100% sure why it is showing this exact error, but it seems the nodes were allocated shortly after the start of the job. Are there any actual errors with the job itself? We are looking into the cause of this message, however it does not seem to actually be causing any errors. #!/bin/bash #SBATCH --job-name=[JOBNAME] #SBATCH --output=klog/collect_%A_%a.out #SBATCH --error=klog/collect_%A_%a.err #SBATCH --mem=64G #SBATCH --time=72:00:00 #SBATCH --constraint=rtx_6000 #SBATCH --gres=gpu:4 #SBATCH --array=6,7,12,13,20 #SBATCH --mail-user=[EMAIL] #SBATCH --mail-type=END,FAIL #SBATCH --partition=gpu-general"
"3281158","2025-09-30 17:10:20","HPC User Account: [ID] in Student Group","I confirm [STAFF] is also an iHARP student besides his current group setup. Thanks! Hello, We just need the written permission from Drs. [STAFF] and [STAFF]. They are cc'd. Then I can add you to the iHarp group as usual. No rush, Elliot Gobbert"
"3281243","2025-09-26 00:00:06","HPC Other Issue: Nodes on partition pi_[ID] being used in partition 2024","I was unaware of the 10 minute grace period. Many apologies. My calculations did run. I appreciate your help and information on this. Thank you! [USER] On Thu, Sep 25, 2025 at 4:41PM [STAFF] via RT <[EMAIL]> wrote: Ticket Last Update From Ticket: Hello [USER], That is correct - nodes in the pi_bennettj partition will preempt jobs from users that are not in the pi_bennettj group after a 10 minute grace period. It looks like your jobs were submitted at 3:48pm EST and started ~10 minutes later at 3:58pm EST. Can you confirm that your job is running now? Thank you, [STAFF] On Thu Sep 25 15:55:41 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster Hello, I'm trying to run jobs on the pi_bennettj partition, however, 4 of our nodes are being used on partition 2024. I was under the impression that the pi researchers would preempt anyone using our nodes. I've attached a list of the nodes being used. Thank you, [USER] Attachment 1: nodes.txt"
"3281293","2025-09-29 16:34:21","HPC Other Issue: jobs getting killed on 2024 queue","Yes, had much better success over the weekend, Thanks, [USER] On Mon, Sep 29, 2025 at 12:03 PM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3281293> Last Update From Ticket: I see. Is everything working as intended now? -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Fri Sep 26 16:41:53 2025, [USER] wrote: Whoops, sorry, was submitting the batch file designed for taki ... -[USER] On Fri, Sep 26, 2025 at 4:38 PM [USER] <[EMAIL]> wrote: Hi [STAFF], Thanks for the info I tried this just now #SBATCH --job-name=3DFIT_TILE_TRENDS ## name #SBATCH -N1 ## number of job step is to be allocated per instance of matlab #SBATCH --cpus-per-task 1 ## tasks per node/number of cores per matlab session will be #SBATCH --partition=2024 ## desired partition #SBATCH --partition=match ## desired partition #SBATCH --cluster=chip-cpu ## desired cluster #SBATCH --account=pi_strow #SBATCH --qos=shared ## qos to get as many cpu2024 as possible, else put pi_strow and get bin/rm: cannot remove '*~': No such file or directory /bin/rm: cannot remove 'slurm*.err': No such file or directory sbatch: error: Missing: '--gres' sbatch: error: You must specify a Generic RESource to use in your job. sbatch: error: See this webpage for more details: https://hpcf.umbc.edu/compute/overview/. sbatch: error: Batch job submission failed: Unspecified error which according to the webpage you pointed me to, should be used for gpu processors? Thanks [USER] On Fri, Sep 26, 2025 at 9:52 AM [STAFF] via RT <[EMAIL]> wrote: If you agree your issue is resolved, please give us feedback on your experience by completing a brief satisfaction survey: https://umbc.us2.qualtrics.com/SE/?SID=SV_etfDUq3MTISF6Ly&customeremail=[EMAIL]&groupid=EIS&ticketid=3281293&ticketowner=[STAFF]%40umbc.edu&ticketsubject=HPC+Other+Issue%3A+jobs+getting+killed+on+2024+queue If you believe your issue has not been resolved, please respond to this message, which will reopen your ticket. Note: A full record of your request can be found at: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3281293> Thank You _______________________________________ R e s o l u t i o n: Hi [USER], First off, let me elaborate on the slurm job ID's you are seeing. When submitting an array job, there is the main job ID (which is 402067 in this case), along with the array tasks for each (which are like, 402067_90). However, each individual array task also gets its own actual job ID. For example, the actual job ID for array task 402067_90 is 402157. Next, your job was likely canceled due to someone in pi_bennettj attempting to run a job on their nodes. Nodes c24-[01-10] are all nodes contributed from pi_bennettj, therefore they have priority access and the ability to preempt other users not in their group. If you want to run your jobs on 2024 nodes, but do not want to risk preemption, use the 'match' partition. If you would like more information on which partitions allow/disallow preemption, check out this wiki page: https://umbc.atlassian.net/wiki/spaces/faq/pages/1249509377/chip+Partitions+and+Usage#Partitions Let me know if that helps! Have a nice day! -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Thu Sep 25 16:56:36 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster For some reason many of my jobs are getting killed/preempted (slurmsteps). I thought I had an issue with the code but seems fine to me. Haven't had that before Is there a way to limit the jobs only to Larrabee's computer? Wierd this is suppose the slurm job ID is 402067, I keep getting told eg 402166 has been killed?? makes no sense? [USER]@chip-login1 AI_RTA]$ grep -in slurmst slurm* slurm-402067_90.out:17:.........+slurmstepd: error: *** JOB 402157 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** slurm-402067_91.out:17:.........+slurmstepd: error: *** JOB 402158 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** slurm-402067_92.out:17:.........slurmstepd: error: *** JOB 402159 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***"
"3281491","2025-10-03 15:01:47","Srun not working for a parallel task","I apologize for the late reply. I wanted to suggest trying to add --mpi=pmix to srun commands (srun --mpi=pmix -n 8 /path/to/your/test --verbose). I think srun might be defaulting to a single rank, which could explain why you're seeing the 'Insufficient processes' error even when requesting multiple tasks and srun working when task is set to 1. pmix (Process Management Interface for Exascale) is the plugin that enables Slurm to correctly launch and manage MPI tasks across multiple ranks. Without specifying it, srun may not properly initialize the MPI environment. Let me know if that doesn’t resolve the issue. On Fri Sep 26 12:00:58 2025, [STAFF] wrote: Hi, I have listed the 6 files (SLURM scripts, job outputs, executable outputs) in the next section. In short, the 'mpirun' launches my program successfully, while 'srun' leads to several invalid memory access issues. Both SLURM scripts launch 31 executables at the same time (on 1 CPU node with 8 CPU cores). The only difference is the MPI launcher 'srun' v.s. 'mpirun'. 'srun' version: - Work directory for the 'srun' version: /umbc/rs/[STAFF]/[USER]/[DATE]-genex-test-srun/ - 'srun' job script: /umbc/rs/[STAFF]/[USER]/[DATE]-genex-test-srun/job.sh - 'srun' job output: /umbc/rs/[STAFF]/[USER]/[DATE]-genex-test-srun/[ID].genex-test-srun.out - Actual 'srun' commands and output: /umbc/rs/[STAFF]/[USER]/MPCDF/phoenix/genex/build-[DATE]/src/Testing/Temporary/LastTest.log 'mpirun' version: - Work directory for the 'mpirun' version: /umbc/rs/[STAFF]/[USER]/[DATE]-genex-test-mpirun/ - 'mpirun' job script: /umbc/rs/[STAFF]/[USER]/[DATE]-genex-test-mpirun/job.sh - 'mpirun' job output: /umbc/rs/[STAFF]/[USER]/[DATE]-genex-test-mpirun/[ID].genex-test-mpirun.out - Actual 'mpirun' commands and output: /umbc/rs/[STAFF]/[USER]/MPCDF/phoenix/genex/build-[DATE]/src/Testing/Temporary/LastTest.log The 'mpirun' command I use for this test is from a Spack build of OpenMPI with the following spec: -- linux-rhel9-cascadelake / %c,cxx,fortran=gcc@13.3.0 ---------- openmpi@5.0.8+atomics~cuda~debug+fortran~gpfs~internal-hwloc~internal-libevent~internal-pmix~ipv6~java~lustre~memchecker~openshmem~rocm~romio+rsh~static~two_level_namespace+vt+wrapper-rpath build_system=autotools fabrics:=none romio-filesystem:=none schedulers:=none Best, [USER] On Sep 26, 2025, at 10:38 AM, via RT <[EMAIL]> wrote: Greetings, This message has been automatically generated in response to the creation of a ticket regarding: Subject: 'Srun not working for a parallel task ' Message: [STAFF] had an office hour with [STAFF] and [STAFF]. We weren’t able to provide an immediate solution. [USER], please add the errors you get, your slurm script, and the working directory. Your ticket has been assigned an ID of [Research Computing #[ID]] or you can go there directly by clicking the link below. Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=[ID]> You can login to view your open tickets at any time by visiting https://my.umbc.edu and clicking on 'Help' and 'Request Help'. Alternately you can click on https://my.umbc.edu/help Thank you -- Best, [STAFF]"
"3281714","2025-09-26 18:20:20","HPC Slurm/Software Issue: Cannot create slurm job","Thank you On Fri Sep 26 2025 at 1 55 PM [STAFF] via RT [EMAIL] wrote Ticket Last Update From Ticket Hi [USER] Chip is not down That being said I did notice that the configuration was acting strange and it turns out that it was trying to submit jobs to the gpu-contrib partition which is non-submittable partition Ive changed that the gpu partition is back to being the default partition and should work In the future you can always check to see if chip is really down by using the sinfo command Additionally you can also specify the partition using the --partition=${gpu_partition} Let me know if you have more questions about this Ill leave the ticket open for a few days On Fri Sep 26 13 36 48 2025 [USER] wrote Request Type High Performance Cluster Hello I attempted to run a slurm job srun --cluster=chip-gpu --account=[ACCOUNT] --mem=20000 --time=12 00 00 --gres=gpu 1 --pty $SHELL Recieved srun Requested partition configuration not available now I am pretty sure chip is down would it be possible to recieve an update when chip is available again Thank you"
"3282078","2025-10-31 13:11:06","Cloud: Google Vision","Waiting for info from [STAFF] to get GCP billing in place -- [STAFF], Associate Director of Research and Enterprise Computing, Enterprise Infrastructure Solutions, UMBC - DoIT"
"3282188","2025-11-04 16:49:54","HPC Slurm/Software Issue: Please install newer Matlab","The text you provided appears to be a lengthy email thread related to an issue with installing MATLAB R2025b on a high-performance cluster (HPC) system. The conversation involves Matthias Gobbert, a professor of mathematics and statistics, and Max Breitmeyer, the DOIT HPC System Administrator.  Here's a brief summary:  * Matthias Gobbert requests that MATLAB R2025b be installed on the HPC system, as the current version (R2023b) is two years old. * Max Breitmeyer responds that he has added MATLAB 2025b to some partitions and is working on installing it on others. * Matthias Gobbert reports an issue with running MATLAB 2025b on the HPC system, providing error messages from the slurm job output files. * Max Breitmeyer investigates the issue and eventually resolves it by updating the license server configuration.  The conversation also mentions a survey about the HPC system's usage and performance."
"3282261","2025-09-29 16:59:49","HPC User Account: [ID] in Pi_ksolaima","Hi [USER], Your account ([USER]) has been added to the pi_[STAFF] group on chip.rs.umbc.edu . Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/). Kind regards, [STAFF] DoIT Unix Infra Student Worker On Mon Sep 29 05:51:37 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              High Performance Cluster Create/Modify account in existing PI group Existing PI Email:    [EMAIL] Existing Group:       Pi_[STAFF] Project Title:        Mutimodal information retriever Project Abstract:     This project focuses on the development of a robust system for anomaly detection in multivariate irregularly-sampled time-series data with a application to identifying electricity theft from the Smart Grid Generated Data (SGCC) dataset. The primary methodology involves the implementation and training of Neural Controlled Differential Equations a deep learning architecture designed to capture complex temporal dependencies. I am developing and training a series of deep learning models for my time-series analysis research. On my current resources the training process is prohibitively slow with each epoch taking approximately 40-45 minutes to complete. This makes iterative development debugging and essential hyperparameter tuning impractical. Access to the HPC cluster would significantly accelerate this research by enabling me to run multiple experiments in parallel and iterate on model architectures and feature engineering more efficiently."
"3282435","2025-09-30 15:09:21","Need to create account in the chip cluster under [STAFF]'s group","Correct request was submitted: https://rt.umbc.edu/Ticket/Display.html?id=[TICKETNUMBER] -- [STAFF], [DEPARTMENT]"
"3282455","2025-10-03 13:48:18","HPC Other Issue: client_loop: send disconnect: Broken pipe happens every few minutes","Hi [USER], We've made a change to the ssh config that we are hoping will solve the issue. Could you keep an eye on it let us know if you notice a difference? On Mon Sep 29 13:24:40 2025, [STAFF] wrote: Hi [USER], we've received reports from other researchers who are experiencing something similar and we are currently investigating the issue. A little more information might help us: - First, can you give us exact times and dates for some of the disconnects you are seeing? This will help us look at the connection logs to find where it happened. - Second, what kind of connection do you have? Wired or wireless? Are you on the vpn when connecting to [SERVER]? - Third, are you experiencing it on one of the login nodes more than another? Let me know the above and we can go from there. If you experience anymore of these disconnects please write down the exact date and time you experienced it so we can look into it further. On Mon Sep 29 11:46:36 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster Hello, since last week I have been disconnected repeatedly after logging into [SERVER] after only a few minutes of waiting for jobs to progress. I am using the standard 'ssh [USER]@[SERVER]' command that has been working for months, but it is disconnecting me too frequently. Why is the Connection to [SERVER] closed by remote host closed so much while I'm working? This is interrupting our research. Is there a different way to login now of which I am unaware? Thank you!"
"3282508","2025-09-30 20:11:03","HPC Other Issue: Need to install a module for WRF Simulation.","Hello [USER], good afternoon. Thank you very much for providing this information. Actually, I tried to find the module using 'module spider gfortran' to check if it is available or not. I will try to load the module using the way you mentioned and complete my work. If not I will ask for further help. Regards, [USER]. On Mon Sep 29 12:57:26 2025, [STAFF] wrote: Hi [USER], What happens when you attempt to use gfortran? Without loading any modules, there is a library for gfortran located in /usr/bin/gfortran, which should be on your PATH. Additionally, you can load more specific versions of gfortran through other modules. For example, if you load GCCcore with 'module load GCCcore', you can see that the gfortran library is also present, now located at '/usr/ebuild/installs/software/GCCcore/13.3.0/bin/gfortran' (you can verify yourself with 'which gfortran' to show you the location of the gfortran library). Let me know if this helps! -- Kind regards, [STAFF] On Mon Sep 29 12:33:11 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: High Performance Cluster Hello, I need to run the WRF climate simulation model to generate datasets for my project. To run this simulation model, I need to use 'gfortran' library, but this is not available in the CHIP cluster. It would be very helpful if you could install the library. Please check the given link if you need further information about running the WRF model. WRF Tutorial Link: https://www2.mmm.ucar.edu/wrf/OnLineTutorial/compilation_tutorial.php#STEP1"
"3282986","2025-10-27 22:20:16","Migrating Research Storage Volume to Ceph Cluster","Hello [USER], We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note: The path has changed, and is now available under /umbc/rs/pi_[USER]. The alias used to reach the volume is now pi_[USER]_common and pi_[USER]_user. Your new volume has a quota of 25TB. When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good? Thank you, [STAFF]. On Mon Oct 27 13:21:50 2025, [STAFF] wrote: Hello [USER], This is a reminder that we will be performing your group's migration to the Ceph storage cluster today. During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated. We will provide an update once completed. Best, [STAFF]. On Fri Oct 17 11:52:31 2025, [STAFF] wrote: [USER]"
"3282987","2025-10-23 14:16:12","Migrating Research Storage Volume to Ceph Cluster - [USER]","Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3282989","2025-11-04 18:28:42","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Hello [USER], We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note: The path has changed, and is now available under /umbc/rs/pi_[STAFF]. The alias used to reach the volume is now pi_[STAFF]_common and pi_[STAFF]_user. Your new volume has a quota of 25TB. When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good? Thank you, [STAFF]. On Tue Nov 04 12:04:40 2025, [EMAIL] wrote: Hello [USER], This is a reminder that we will be performing your group's migration to the Ceph storage cluster today. During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated. We will provide an update once completed. Best, [STAFF]. On Fri Oct 17 13:09:09 2025, [EMAIL] wrote: Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 4th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]."
"3282995","2025-10-30 16:55:51","Migrating Research Storage Volume to Ceph Cluster - [ID]","Hello [USER], It took longer than expected, but we have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note: The path has changed, and is now available under /umbc/rs/pi_bsol. The alias used to reach the volume is now pi_bsol_common and pi_bsol_user. Your new volume has a quota of 10TB. When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good? I apologize if the extended length of the migration caused any issues for you or your group. Thank you, [STAFF]. On Wed Oct 29 13:43:53 2025, [EMAIL] wrote: Hello , This is just a reminder that we are starting your migration today. During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated. We will provide an update once completed. On Fri Oct 17 12:16:06 2025, [EMAIL] wrote: Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of October 29th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]."
"3282997","2025-10-28 19:36:46","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Hello [USER], We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note: The path has changed, and is now available under /umbc/rs/[STAFF]. The alias used to reach the volume is now [ALIAS]_common and [ALIAS]_user. Your new volume has a quota of 10TB. When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good? Thank you, [STAFF]. On Tue Oct 28 10:49:25 2025, [EMAIL] wrote: Hello [USER], This is a reminder that we will be performing your group's migration to the Ceph storage cluster today. During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated. We will provide an update once completed. Best, [STAFF]. On Fri Oct 17 12:04:45 2025, [EMAIL] wrote: Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of October 28th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]."
"3283002","2025-10-31 13:53:06","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Hello We have finished migrating your volume to the Ceph cluster. As far as we can tell everything seems to have gone smoothly. There are a few things to note: The path has changed and is now available under /umbc/rs/[STAFF]. The alias used to reach the volume is now [ALIAS]_common and [ALIAS]_user. Your new volume has a quota of 10TB. When you have a chance could you try running some jobs on Chip using the new volume to verify everything looks good? Thank you On Fri Oct 17 12:35:08 2025 [EMAIL] wrote: Dear [USER] As per our previous communications since you did not schedule a date by October 15 we will be going with Option 2 randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of October 31st for your migration. Let us know if there is a better day for you and within reason we can reschedule that date. You will be notified when the migration begins and completes. Thank you [STAFF]"
"3283005","2025-10-22 14:01:34","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Hello. We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note: The path has changed, and is now available under /umbc/rs/pi_[USER]. The alias used to reach the volume is now pi_[USER]_common and pi_[USER]_user. Your new volume has a quota of 25TB. When you have a chance, please try running some jobs on Chip using the new volume to verify everything looks good. On Wed Oct 22 09:30:55 2025, [STAFF] wrote: Hello This is just a reminder that we are starting your migration today. During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated. We will provide an update once completed. On Fri Oct 10 13:37:41 2025, [STAFF] wrote: Hello [USER], Sounds good, I've put your group on our schedule for Wednesday October 22nd. We will send you an email alert via this RT ticket when we begin the migration process, and again once it has completed. Please let us know if you have any questions or concerns. Best, [STAFF] On Fri Oct 10 11:17:29 2025, [USER] wrote: Good Morning, It is fine with me if you schedule the transfer of /umbc/rs/[USER] for an arbitrary date 10/16 - 11/15. On Fri, Oct 10, 2025 at 10:26 AM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3283005 > Last Update From Ticket: Hello [USER], This is a reminder email, in case you missed the first. We need a response by October 15th, or we'll be forced to go with Option 2, randomly scheduling a time. As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server. To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at '/umbc/rs/[USER]'. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume. Below we've listed two options for handling this data migration - please let us know which of these you'd prefer. Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day. Option 2: If you don't respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes. Note: After this process has completed, the new storage volume will have a new name. For example, group 'pi_doit' will find its data under '/umbc/rs/pi_doit', or in your group's case you will find your volume under '/umbc/rs/pi_[USER]'. Thank you, [STAFF]."
"3283011","2025-10-30 23:26:51","Migrating Research Storage Volume to Ceph Cluster - [ID]","Hello [USER], We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note: The path has changed, and is now available under /umbc/rs/[SERVER]. The alias used to reach the volume is now [SERVER]_common and [SERVER]_user. Your new volume has a quota of 10TB. When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good? Thank you, [STAFF] On Thu Oct 30 10:33:42 2025, [STAFF] wrote: Hello [USER], This is a reminder that we will be performing your group's migration to the Ceph storage cluster today. During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated. We will provide an update once completed. Best, [STAFF] On Sat Oct 18 07:28:24 2025, [STAFF] wrote: Hi, No worries, you don't need to do anything except NOT use the cluster on Oct. 30th. To transfer the files, we make the old directory read only, so just don't use the cluster while we transfer things. We will email you in this ticket when we start and finish the migration. Best, [STAFF]"
"3283013","2025-10-30 23:25:03","Migrating Research Storage Volume to Ceph Cluster","Hello [USER], We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note: The path has changed, and is now available under /umbc/rs/pi_[USER]. The alias used to reach the volume is now pi_[USER]_common and pi_[USER]_user. Your new volume has a quota of 10TB. When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good? Thank you, [STAFF]. On Thu Oct 30 10:32:46 2025, wrote: Hello [USER], This is a reminder that we will be performing your group's migration to the Ceph storage cluster today. During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated. We will provide an update once completed. Best, [STAFF]. On Fri Oct 17 12:30:41 2025, wrote: Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of October 30th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]."
"3283015","2025-10-31 18:20:42","Migrating Research Storage Volume to Ceph Cluster - [ID]","Hello [USER], We have finished migrating your volume to the Ceph cluster. As far as we can tell everything seems to have gone smoothly. There are a few things to note: The path has changed, and is now available under /umbc/rs/[STAFF]. The alias used to reach the volume is now [ALIAS]_common and [ALIAS]_user. Your new volume has a quota of 25TB. When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good? Thank you, [STAFF]. On Fri Oct 31 10:42:00 2025, [STAFF] wrote: Good morning [USER], This is a reminder that we will be performing your group's migration to the Ceph storage cluster today. During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated. We will provide an update once completed. Best, [STAFF]. On Fri Oct 17 12:48:06 2025, [STAFF] wrote: Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of October 31st for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]."
"3283018","2025-10-23 13:53:20","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Dear [STAFF], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3283023","2025-11-04 15:03:43","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Excellent, cheers! On Mon, Nov 3, 2025 at 8:26 PM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3283023 > Last Update From Ticket: Hello [USER], We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note: * The path has changed, and is now available under /umbc/rs/pi_[USER]. * The alias used to reach the volume is now pi_[USER]_common and pi_[USER]_user. * Your new volume has a quota of 10TB. When you have a chance, could you try running some jobs on [SERVER] using the new volume to verify everything looks good? Thank you, [STAFF] On Mon Nov 03 11:41:24 2025, [USER] wrote: Hello [USER], This is a reminder that we will be performing your group's migration to the Ceph storage cluster today. During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated. We will provide an update once completed. Best, [STAFF] On Fri Oct 17 12:52:22 2025, [USER] wrote: Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 3rd for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3283027","2025-10-11 03:05:56","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Hello [USER], Thanks for getting back to us I put your group's migration on our schedule for Wednesday October 29th We will send you an email alert via this RT ticket when we begin the migration and again once it has completed Please let us know if you have any questions or concerns Best [STAFF] On Fri Oct 10 17:36:42 2025 [USER] wrote Hi [STAFF] Thank you for reaching out to me I prefer 10 29 30 31 Thanks [USER] On Fri Oct 10 2025 at PM [STAFF] via RT [EMAIL] wrote Ticket Display html id=32 83027 Last Update From Ticket Hello [USER] This is a reminder email in case you missed the first We need a response by October 15th or we'll be forced to go with Option 2 randomly scheduling a time As per the communication via myUMBC earlier this summer https //my3 my umbc edu/groups/hpcf/posts/150838 DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster Your group is using 0GB of a 500GB quota on the old storage server To perform these migrations we need to take individual storage volumes offline while we migrate them to the Ceph cluster Thus we are reaching out to schedule a date where we can migrate your volume located at /umbc/rs/[USER] During the migration we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume Below we've listed two options for handling this data migration - please let us know which of these you'd prefer Option 1 Schedule a group-wide downtime date during standard business hours which can be done by responding to this email with your preferred date(s) to perform the migration During this time DoIT staff will work to migrate your volume to the Ceph storage cluster DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed For most storage volumes this process should take less than a business day Option 2 If you don't respond to this email by October 15th DoIT staff will assign a day over the following month October 16th through November 15th to migrate your volume The day chosen will be random and will occur during business hours You will be notified of the date chosen to perform the migration and will be notified when the migration begins and completes Note After this process has completed the new storage volume will have a new name For example group pi_doit will find its data under /umbc/rs/pi_doit or in your group's case you will find your volume under /umbc/rs/[USER] Thank you [STAFF]"
"3283030","2025-10-17 14:14:08","Migrating Research Storage Volume to Ceph Cluster","Hello We have finished migrating your volume to the Ceph cluster As far as we can tell everything seems to have gone smoothly There are a few things to note The path has changed and is now available under /umbc/rs/pi_dli The alias used to reach the volume is now pi_dli_common and pi_dli_user Your new volume has a quota of 25TB When you have a chance please try running some jobs on Chip using the new volume to verify everything looks good Thank you On Fri Oct 17 09:43:32 2025 [STAFF] wrote Hello This is just a reminder that we are starting your migration today During this time please ensure there are not any jobs being run in your research group otherwise these may be terminated We will provide an update once completed On Thu Oct 02 10:17:53 2025 [STAFF] wrote Good morning [USER] I've put your group's migration on our schedule for October 17th We will send you a notification via this RT ticket when we begin the migration and again once the migration has completed Please let me know if you have any questions or concerns Best [STAFF] On Thu Oct 02 09:34:10 2025 [USER] wrote Hi [STAFF] Thank you for reaching out Any day before October 30th works for me for migrations Best [USER] On Mon Sep 29 2025 at 4:28PM via RT [EMAIL] wrote Greetings This message has been automatically generated in response to the creation of a ticket regarding Migrating Research Storage Volume to Ceph Cluster - pi_dli Message Hello [USER] As per the communication via myUMBC earlier this summer DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster Your group is using 0GB of a 250GB quota on the old storage server To perform these migrations we need to take individual storage volumes offline while we migrate them to the Ceph cluster Thus we are reaching out to schedule a date where we can migrate your volume located at /umbc/rs/dli During the migration we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume Below we've listed two options for handling this data migration - please let us know which of these you'd prefer Option 1 Schedule a group-wide downtime date during standard business hours which can be done by responding to this email with your preferred date(s) to perform the migration During this time DoIT staff will work to migrate your volume to the Ceph storage cluster DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed For most storage volumes this process should take less than a business day Option 2 If you don't respond to this email by October 15th DoIT staff will assign a day over the following month October 16th through November 15th to migrate your volume The day chosen will be random and will occur during business hours You will be notified of the date chosen to perform the migration and will be notified when the migration begins and completes Note After this process has completed the new storage volume will have a new name For example group pi_doit will find its data under /umbc/rs/pi_doit or in your group's case you will find your volume under /umbc/rs/pi_dli Thank you [STAFF] -- There is no need to reply to this message right now Your ticket has been assigned an ID of [Research Computing #3283030] or you can go there directly by clicking the link below Ticket https //rt umbc edu/Ticket/Display html?id=3283030 You can login to view your open tickets at any time by visiting http //my umbe edu and clicking on Help and Request Help Alternately you can click on http //my umbe edu/help -- [USER] Assistant Professor Department of Computer Science and Electrical Engineering University of Maryland Baltimore County Homepage https leetton github io Schedule a meeting Google Calendar -- [STAFF] System Administrator for Research and Enterprise Computing UMBC - DoIT -- Best [STAFF] DOIT Unix infra Graduate Assistant"
"3283033","2025-10-31 18:20:05","Migrating Research Storage Volume to Ceph Cluster - [ID]","Hello [USER], We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note: The path has changed, and is now available under /umbc/rs/[SERVER]. The alias used to reach the volume is now [ALIAS]_common and [ALIAS]_user. Your new volume has a quota of 25TB. When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good? Thank you, [STAFF]. On Fri Oct 31 10:42:33 2025, [STAFF] wrote: Good morning [USER], This is a reminder that we will be performing your group's migration to the Ceph storage cluster today. During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated. We will provide an update once completed. Best, [STAFF]. On Fri Oct 17 12:50:43 2025, [STAFF] wrote: Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of October 31st for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]."
"3283034","2025-10-22 18:02:33","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note: The path has changed and is now available under /umbc/rs/pi_[USER]. The alias used to reach the volume is now pi_[USER]_common and pi_[USER]_user. Your new volume has a quota of 10TB. When you have a chance please try running some jobs on Chip using the new volume to verify everything looks good. On Wed Oct 22 09:29:50 2025 [STAFF] wrote: Hello This is just a reminder that we are starting your migration today. During this time please ensure there are not any jobs being run in your research group otherwise these may be terminated. We will provide an update once completed. On Fri Oct 10 13:48:10 2025 [USER] wrote: Great thank you! On Fri Oct 10 10:40:46 2025 [USER] wrote: Thank you Elliot I am fine with it whenever it gets migrated as I have yet to store data on the server. Thanks [USER]."
"3283037","2025-10-01 18:36:19","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Hello [USER], We have finished migrating your volume to the Ceph cluster. As far as we can tell everything seems to have gone smoothly. There are a few things to note: The path has changed, and is now available under /umbc/rs/pi_[USER]. The alias used to reach the volume is now pi_[USER]_common and pi_[USER]_user. Your new volume has a quota of 10TB. We know you're not actively using it right now, but please don't hesitate to reach out if you hit any snags once you start working with the new volume. Best, [STAFF] On Wed Oct 01 10:38:55 2025, [USER] wrote: Good morning [USER], Understood. We will send you an email later today with the details for your storage volume after the migration. Best, [STAFF] On Tue Sep 30 15:20:57 2025, [USER] wrote: Hi [STAFF], We don't currently have any data stored yet, so you are welcome to do any migration at any time-- it won't impact us at the moment. Thanks, [USER] On Mon, Sep 29, 2025 at 4:35PM via RT <[EMAIL]> wrote: Greetings, This message has been automatically generated in response to the creation of a ticket regarding: Subject: 'Migrating Research Storage Volume to Ceph Cluster - pi_[USER]' Message: Hello [USER], As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 250GB quota on the old storage server. To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at '/umbc/rs/[USER]'. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume. Below we've listed two options for handling this data migration - please let us know which of these you'd prefer. Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day. Option 2: If you don't respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes. Note: After this process has completed, the new storage volume will have a new name. For example, group 'pi_doit' will find its data under '/umbc/rs/pi_doit', or in your group's case you will find your volume under '/umbc/rs/pi_[USER]'. Thank you, [STAFF] -- [STAFF] System Administrator for Research and Enterprise Computing UMBCC - DoIT"
"3283044","2025-11-04 21:27:21","Migrating Research Storage Volume to Ceph Cluster - pi_ece","Hello [USER], We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note: The path has changed, and is now available under /umbc/rs/pi_ece. The alias used to reach the volume is now pi_ece_common and pi_ece_user. Your new volume has a quota of 10TB. When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good? Thank you, [STAFF] On Tue Nov 04 12:05:33 2025, [EMAIL] wrote: Hello [USER], This is a reminder that we will be performing your group's migration to the Ceph storage cluster today. During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated. We will provide an update once completed. Best, [STAFF] On Fri Oct 17 13:02:36 2025, [EMAIL] wrote: Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 4th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3283046","2025-10-29 21:28:36","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Hello again [USER], We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note: The path has changed, and is now available under /umbc/rs/pi_[STAFF]. The alias used to reach the volume is now pi_[STAFF]_common and pi_[STAFF]_user. Your new volume has a quota of 25TB. When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good? Thank you, [STAFF]. On Wed Oct 29 09:48:02 2025, [EMAIL] wrote: Hello [USER], This is a reminder that we will be performing your group's migration to the Ceph storage cluster today. During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated. We will provide an update once completed. Best, [STAFF]. On Fri Oct 17 12:22:56 2025, [EMAIL] wrote: Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of October 29th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]."
"3283048","2025-10-21 14:09:34","Migrating Research Storage Volume to Ceph Cluster - [ID]","We have finished migrating your volume to the Ceph cluster. As far as we can tell everything seems to have gone smoothly. There are a few things to note: The path has changed and is now available under /umbc/rs/[STAFF]. The alias used to reach the volume is now [ALIAS] and [ALIAS]. Your new volume has a quota of 25TB. When you have a chance please try running some jobs on Chip using the new volume to verify everything looks good. On Tue Oct 21 09:40:27 2025 [STAFF] wrote: This is just a reminder that we are starting your migration today. During this time please ensure there are not any jobs being run in your research group otherwise these may be terminated. We will provide an update once completed. On Fri Oct 10 13:45:17 2025 [STAFF] wrote: Hello [USER] I've put your group on our schedule for Tuesday October 21st. We will send you an email alert via this RT ticket when we begin the migration process and again once it has completed. Please let us know if you have any questions or concerns. Best [STAFF]. On Fri Oct 10 10:38:11 2025 [USER] wrote: Option 2 works for me Sent from my iPhone > On Oct 10 2025 at 10:18 AM [EMAIL] via RT <[EMAIL]> wrote: Ticket URL https://rt.umbc.edu/Ticket/Display.html?id=3283048 Last Update From Ticket Hello [USER] This is a reminder email in case you missed the first. We need a response by October 15th or we'll be forced to go with Option 2 randomly scheduling a time. As per the communication via myUMBC earlier this summer https://my3.my.umbc.edu/groups/hpcf/posts/150838 DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server. To perform these migrations we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at '/umbc/rs/[STAFF]'. During the migration we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume. Below we've listed two options for handling this data migration - please let us know which of these you'd prefer. Option 1: Schedule a group-wide downtime date during standard business hours which can be done by responding to this email with your preferred date(s) to perform the migration. During this time DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes this process should take less than a business day. Option 2: If you don't respond to this email by October 15th DoIT staff will assign a day over the following month October 16th through November 15th to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration and will be notified when the migration begins and completes. Note: After this process has completed the new storage volume will have a new name. For example group 'pi_doit' will find its data under '/umbc/rs/pi_doit' or in your group's case you will find your volume under '/umbc/rs/[STAFF]'. Thank you Elliot"
"3283387","2025-09-30 15:55:36","HPC User Account: [USER] in [ID]","Hi [USER], Your account ([USER]) has been added to the [GROUP] group on chip.rs.umbc.edu . You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link: https://doit.umbc.edu/request-tracker-rt/doit-research-computing/. On Tue Sep 30 11:30:11 2025, [STAFF] wrote: Approve it. Thanks! This e-mail is a notification that a UMBC user: [USER] <[EMAIL]> has requested an account within UMBC's HPC environment in your group <[GROUP]>. As the PI, we request that you acknowledge and approve this account creation by replying to this message. Alternatively you can go to this link and review the ticket and indicate your decision here: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3283387>. Once we have your approval, we will create the account and you and the new user will receive another e-mail notifying you that the account has been created. If you have any other questions or concerns please contact us. - UMBC DoIT Research Computing Support Staff. Best, [STAFF]"
"3283454","2025-09-30 17:15:05","HPC User Account: [USER] in Student Group","Hello there, Here is how to request an account on chip: https://umbc.atlassian.net/wiki/spaces/faq/pages/1327431728/How+to+request+a+user+group+account+on+chip Let me know if that helps, [STAFF]"
"3283590","2025-10-01 13:55:24","HPC User Account: [ID] in [STAFF]","Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is [STAFF]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://[EMAIL]/wiki/spaces/faq/pages/[NUMBER]/Getting+Started+on+chip. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link: (https://[EMAIL]-computing/). On Tue Sep 30 14:29:01 2025, [STAFF] wrote: Yes, [USER] is a PhD student working in my group. Thanks. Best regards, [STAFF]. Ticket <URL: https://rt.[EMAIL]/Ticket/Display.html?id=[NUMBER]> Last Update From Ticket: First Name: [USER], Last Name: [USER], Email: [EMAIL], Campus ID: [USER]. Request Type: High Performance Cluster. Create/Modify account in existing PI group. Existing PI Email: [STAFF], Existing Group: [STAFF], Project Title: Password Leak Detection with Machine Learning, Project Abstract: Training Machine Learning Models to Detect Password Leaks. Best, [STAFF], DOIT Unix infra, Graduate Assistant."
"3283593","2025-09-30 19:09:58","HPC Other Issue: recent publication","Hello I have added this publication to our website: https://hpcf.umbc.edu/publications/ On Tue Sep 30 13:52:23 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster Hi [STAFF], Geophysical Trends Inferred From 20 Years of AIRS Infrared Global Observations [USER], L. [USER], R. J. Kramer First published: 11 August 2025 https://doi.org/10.1029/2025JD043501 See the link for bib info [USER] Best, [STAFF]"
"3283767","2025-10-07 16:00:19","HPC Slurm/Software Issue: Deleting .julia directories","Hi [USER], The solution is still to run `chmod -R u+wrx DataFrames` before attempting to remove the directory. The process is hanging because there are many files and directories to modify, which takes time. While the command is running, if your session is idle for too long, you may get logged out before it completes. To avoid this, I recommend using a `tmux` session to run the command so it can finish without interruption. You can learn more about `tmux` here (https://www.geeksforgeeks.org/linux-unix/tmux-in-linux/). Once the permissions have been updated, you can remove the directory. I have already removed the `/umbc/rs/pi_jkestner/users/[USER]/.julia/packages/DataFrames` directory for you after changing the permissions. Please reply to the ticket if you still have questions. On Tue Sep 30 16:50:49 2025, [STAFF] wrote: First Name:                [USER] Last Name:                 [LAST NAME] Email:                     [EMAIL] Campus ID:                 [CAMPUS ID] Request Type:              High Performance Cluster Ok, so this goes back to an issue I was at office hours last Friday with [STAFF]. We thought we resolved it, and it is kind of resolved, but I still have an issue deleting what I think are essentially corrupted directories. So, I have been dealing with issues downloading packages in Julia on the cluster, and I narrowed down the issue to the specific package 'DataFrames', which is actually quite a simple package, all it does is allows you to read CSV and other text files. The workaround that I have to do is that I just wrote my code in such a way that does not require the usage of DataFrames which is fine, but I still have a few directories in my research storage that house DataFrames package that I need to be deleted. Specifically, under /umbc/rs/pi_jkestner/users/[USER] I have 2 directories, '.julia' and '.julia_new' both of which need to be deleted as they house the package DataFrame which prevents me from really running anything. I also have the directory '.julia_test' which I am currently using and does work, so it shouldn’t be deleted. I also have another .julia directory under /umbc/rs/pi_jkestner/common that needs to be deleted as well. I have tried running 'rm -rf .julia' for hours on end and it hangs. Even going into the directory itself, following the path, '/umbc/rs/pi_jkestner/users/[USER]/.julia/packages' and then doing 'rm -rf DataFrames' will hang as well. What ended up working on Friday, which was doing 'chmod -R u+wrx DataFrames' hangs as well now too. I have narrowed the issue all the way down to a specific file in DataFrames, it would be in the path '/umbc/rs/pi_jkestner/users/[USER]/.julia/packages/DataFrames/C5AEe/src' which is the source code for this specific package. The file in 'subdataframe' in the source directory is always the issue, which is weird because looking on github, the subdataframe.jl file, with which this is located, looks perfectly fine to me. Regardless, I would still like there to be a way to delete these excess directories in my research storage. Thanks."
"3284012","2025-10-03 13:44:54","HPC Slurm/Software Issue: sbatch job is not running","Yes it is working. Thank you. Best [STAFF]. On Fri Oct 3 2025 9:43 AM [STAFF] via RT <[EMAIL]> wrote Ticket URL https rt umbc edu Ticket Display html id=3284012 Last Update From Ticket Hello I wanted to follow up to make sure everything is working as expected now If so Ill close out this ticket but you can always reach back out if you need more help Best [STAFF]"
"3284272","2025-10-03 16:05:31","HPC User Account: [USER] in Student Group","The user account has been created: Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is student. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) Hi [USER], Your user has been added to pi_nilanb as a secondary group. Your home directory has additional symbolic links to your group storage space. Please read through the documentation found at hpcf.umbc.edu. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) As you can see, I added [USER] to both the 'student' group (That was the group mentioned in the original request), and also added him to pi_nilanb. Let me know if that was an error, and I can remove him from any unnecessary groups. For the 'premium access' question, I'll forward you to [STAFF]. Best, [STAFF]"
"3284322","2025-10-02 13:28:43","HPC User Account: [USER] in pi_[ID]","Your account has been created: Hi [USER], Your account ([STAFF]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://[EMAIL]/wiki/spaces/faq/pages/[NUMBER]/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://[EMAIL]/request-tracker-rt/[EMAIL]-research-computing/) Let me know if you have any more questions, [STAFF]"
"3284639","2025-10-07 15:49:45","HPC Other Issue: slurm mem error","Hi [USER] OK I'll keep an eye out. The job asked for 240 processors, which had issues when I started them out but then magically worked fine after doing the unset SLURM_MEM_PER_CPU SLURM_MEM_PER_GPU SLURM_MEM_PER_NODE Last two days I started jobs which asked for 64 cpus, all ran fine! SO like you, I have no idea why this happened. Cheers [STAFF] On Tue, Oct 7, 2025 at 11:45 AM [STAFF] via RT <[EMAIL]> wrote If you agree your issue is resolved, please give us feedback on your experience by completing a brief satisfaction survey https//umbc.us2.qualtrics.com/SE/SID=SV_etfDUq3MTISF6Ly&customeremail=[EMAIL]&groupid=EIS&ticketid=3284639&ticketowner=[STAFF]&ticketsubject=HPC+Other+Issue%3A+slurm+mem+error If you believe your issue has not been resolved, please respond to this message, which will reopen your ticket. Note A full record of your request can be found at Ticket https//rt.umbc.edu/Ticket/Display.html?id=3284639 Thank You Hi [USER] I've tried this a few times and am not able to replicate the issue you're having. My first thought was that something was getting set in your files and then not being unset in future runs on the same terminal, but I've tried a few times and am unable to replicate. My runs I did today can be found in the output of CLUST_MAKE_ERA_RTP-442284.out and CLUST_MAKE_ERA_RTP-442245.out. Without being able to replicate the issue there's not much else I can do to debug, as I've looked through your file and didn't see anything that stood out to me. I would advise removing as many of the commented SBATCH directives as possible as it could be reading them in funny unexpected ways, but as a policy, we don't make changes to people's files without their presence so they can confirm the changes that are being made. If you see this pop up again, feel free to reopen this ticket, but for now I'll going to close it. On Thu Oct 02 09:27:50 2025, [USER] wrote Hi [STAFF], Try it with argument 10 or with argument 4 eg sbatch --array=241-276 sergio_matlab_chip.sbatch 10 The problem is intermittent ie does not happen each time I submit a job. Looking backwards at history,I believe it happened last night with job 435223 -[USER] On Thu, Oct 2, 2025 at 8:45 AM [STAFF] via RT <[EMAIL]> wrote Ticket https//rt.umbc.edu/Ticket/Display.html?id=3284639 Last Update From Ticket Hi [USER], I added a .err and a .out file location to your sbatch to help with debugging a little bit. First, I must recommend cleaning up all the stray SBATCH directives. There are quite a few that will conflict with each other if you're not careful (see line 45 and 55 in your sbatch file). Second, I ran the sbatch file myself and didn't get the same error you did. Instead, I got [USER@chip-login1 CLUSTMAKE_ERA5]$ cat CLUST_MAKE_ERA_RTP-435429.err Unrecognized function or variable 'clustbatch_make_eracloudrtp_sergio_sarta_filelist'. Which seems to be missing some sort of command. When you run this file are you doing it from the login node? Do you normally have any modules loaded? On Thu Oct 02 07:49:47 2025, [USER] wrote First Name [USER] Last Name [USER] Email [EMAIL] Campus ID [USER] Request Type High Performance Cluster For some reason the last couple days I have been getting the following error on a script that (I think) has worked for ages on eg taki, now modified for chip srun: fatal: SLURM_MEM_PER_CPU, SLURM_MEM_PER_GPU, and SLURM_MEM_PER_NODE are mutually exclusive. And googling, I finally found a solution on https//harvardmed.atlassian.net/wiki/spaces/O2/pages/1586793613/Troubleshooting+Slurm+Jobs which says at the command line to first do unset SLURM_MEM_PER_CPU SLURM_MEM_PER_GPU SLURM_MEM_PER_NODE Can you look at the following to see if there is a double call to srun or something? /home/[USER]/MATLABCODE/RTPMAKE/CLUST_RTPMAKE/CLUSTMAKE_ERA5/sergio_matlab_chip.sbatch Thanks [USER]"
"3284658","2025-10-16 12:23:42","Assistance using [ID] HPCF Cluster to run LS Dyna simulations","Hi [USER], I saw the groups were created for the class and the lab. I finally heard back from our licensing team, and it seems like we are unable to install the program that [STAFF] asked for on a system wide scale available for all users. That being said, it's probably possible to install the program for individual users. [STAFF], have you been added to the class yet? If not I can add you now. If you have, have you taken a shot at installing the program yourself? If you have any issues you can schedule an office hours with our team here: https://hpcf.umbc.edu/help/office-hours/. On Thu Oct 09 12:55:44 2025, [STAFF] wrote: Just did. One group for class, one for my research lab. On Thu, Oct 9, 2025 at 11:40 AM Max Breitmeyer via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3284658 > Last Update From Ticket: Hi all, Checking in on this. Were you able to submit a group request? On Tue Oct 07 13:04:16 2025, [STAFF] wrote: This is awesome. I will setup this up after lunch. Thank you immensely Max! On Tue, Oct 7, 2025 at 11:33 AM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3284658 > Last Update From Ticket: Sorry for the late response to this. We're looking at making this available on our shared resource, but am waiting to hear back about our licenses. In the meantime it might be possible to install these programs in your user directory. What I would like for your professor to do is set up a new group as a class, so if possible have them go here: https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research%20Computing and request a new group under a PI, and in the notes write that this is for the ENME 444 class. Once that's done we can get you on the cluster, and if we still don't know anything about the license, we'll work to try to set up the software for you as an individual. On Thu Oct 02 08:39:49 2025, [USER] wrote: First Name: [STAFF] Last Name: [STAFF] Email: [EMAIL] Campus ID: [CAMPUSID] Cc: [EMAIL], [EMAIL], [EMAIL] Hello, I'm a student currently taking ENME 444 Capstone and I am using Ansys and LS Dyna to run simulations of a car crash. The simulations are too large for the student version of the software and also too large to run locally on my laptop. My professor recommended reaching out to see how I can set up LS Dyna to run on the UMBC cluster. Attachment 1: Ansys remote.png Best, [STAFF] DOIT HPC System Administrator"
"3285275","2025-10-03 15:45:44","HPC User Account: [USER] in pi_[ID]","The accounts have been created: Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link: https://doit.umbc.edu/request-tracker-rt/doit-research-computing/. Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link: https://doit.umbc.edu/request-tracker-rt/doit-research-computing/. Let me know if you have any more questions! [STAFF]"
"3285334","2025-10-07 16:09:58","HPC User Account: [USER] in [STAFF]","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) On Thu Oct 02 17:05:48 2025, [STAFF] wrote: Please approve the account for [USER]. Sent from my iPhone > On Oct 2, 2025, at 3:42 PM, RT API via RT <[EMAIL]> wrote: This e-mail is a notification that a UMBC user: [USER] <[EMAIL]> has requested an account within UMBC's HPC environment in your group <[STAFF]>. As the PI, we request that you acknowledge and approve this account creation by replying to this message. Alternatively you can go to this link and review the ticket and indicate your decision here: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3285334> Once we have your approval, we will create the account and you and the new user will receive another e-mail notifying you that the account has been created. If you have any other questions or concerns please contact us. - UMBC DoIT Research Computing Support Staff"
"3285426","2025-10-03 17:25:32","HPC Other Issue: Resource Usage Concern on Shared Compute Node","Thank you for your reply. That makes sense. On Fri Oct 03 10:50:43 2025, [STAFF] wrote: Hello, I understand the concern about shared node performance. However, as long as jobs are submitted within the set limits, they are considered valid and within policy. If we see repeated issues that affect the whole system, we can look at changing the limits. For now, the user is working within their allowed usage. Best, [STAFF]. On Thu Oct 02 16:49:15 2025, [STAFF] wrote: I would like to bring to your attention that one of the users (ID:[USER], see attached) appears to be occupying a significant portion of the compute resources on our shared node. This high usage may impact the efficiency and workflow of other users. Could you please remind this user to be mindful of resource usage and try to avoid monopolizing the node, so that everyone can work more smoothly?"
"3285512","2025-10-03 19:05:03","HPC User Account: [USER] in pi_[STAFF]","No problem, here is the information on the new account: Hi [USER], Your account ([USERNAME]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link: (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/). Best, [STAFF]"
"3286322","2025-10-06 14:39:34","HPC Other Issue: Can [USER] request for a cpu and a gpu using sbatch?","Hi [USER], All nodes on chip have CPUs, as they are required to function. However, nodes on chip-gpu also have GPUs available. So yes, when using chip-gpu you can request both GPUs and CPU cores. You would request this the same way as chip-cpu. For more information about the hardware specification, check out this page: https://umbc.atlassian.net/wiki/spaces/faq/pages/1289486353/Cluster+Specifications#CPU-and-GPU-Specifications For more information on requesting CPU cores, check out this page: https://umbc.atlassian.net/wiki/spaces/faq/pages/1335951387/Basic+Slurm+Commands#Jobs,-Tasks,-CPU-cores,-and-Nodes Let me know if you have any additional questions or clarification. -- Kind regards, [STAFF]"
"3286362","2025-10-06 20:24:14","HPC Other Issue: c24-01 tmp area full","Thanks, interesting suggestion. I solved the problem by going to a different node. [STAFF] On Mon, Oct 6, 2025 at 10:21 AM [USER] via RT <[EMAIL]> wrote: If you agree your issue is resolved, please give us feedback on your experience by completing a brief satisfaction survey: https://umbc.us2.qualtrics.com/SE/?SID=SV_etfDUq3MTISF6Ly&customeremail=[EMAIL]&groupid=EIS&ticketid=3286362&ticketowner=[STAFF]%40umbc.edu&ticketsubject=HPC%20Other%20Issue:%20c24-01%20tmp%20area%20full If you believe your issue has not been resolved, please respond to this message, which will reopen your ticket. Note: A full record of your request can be found at: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3286362> Thank You Hi [USER], Thank you for letting us know. The issue has been resolved. Additionally, if this occurs in the future, you could attempt to change the location that the compiler uses to temporarily store files. I believe this would be achieved by setting the $TMPDIR environment variable. For example, you could try to run... TMPDIR=/scratch/$JOB_ID/ mpiicc -O3 trap.c -o trap -- Kind regards, [STAFF] On Sun Oct 05 20:20:51 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              High Performance Cluster Hi, This is surely a funky error, but if trying to compile on c24-01 in an interactive session, for Intel MPI, it says [gobbert@c24-01 ver1.0solution]$ mpiicc -O3 trap.c -o trap icx: error #10295: error generating temporary file name, check disk space and permissions A student also reported this to me. The /tmpfs area or something like seems to be full. [USER] Original Request: Requestors: [USER] First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              High Performance Cluster Hi, This is surely a funky error, but if trying to compile on c24-01 in an interactive session, for Intel MPI, it says [gobbert@c24-01 ver1.0solution]$ mpiicc -O3 trap.c -o trap icx: error #10295: error generating temporary file name, check disk space and permissions A student also reported this to me. The /tmpfs area or something like seems to be full."
"3286765","2025-10-06 19:11:20","HPC Other Issue: Can [USER] get the H100","Hi [USER], Chip is a shared resource. Unless your group has contributed the H100 nodes, access to the node is shared between all of the cluster's users. Normally, your job would run after their job has completed. I suggest waiting until the currently running jobs complete, then your job will run. Or, if your job does not actually require two H100s, you could try to run it on other GPU hardware, such as L40S's or RTX_8000s. There is a greater amount of these nodes available, which would reduce the time it takes for your job to run. Let me know if you have any additional questions. Have a nice day! -- Kind regards, [STAFF] On Mon Oct 06 12:49:34 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: High Performance Cluster I've submitted a SLURM job requesting 2 H100 GPUs, but it's currently pending. I'd like to request access to those resources. I'm working with the UMBC-CREM Center. CLUSTER: chip-gpu JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 101669 gpu gemma_fi [USER] PD 0:00 1 (Priority)"
"3286853","2025-10-10 16:40:56","HPC Other Issue: need authorization in the common directories","Hi [USER], The issue should now be resolved. If you continue to have issues with this, feel free to let us know. Have a nice day! -- Kind regards, [STAFF] On Fri Oct 10 10:04:01 2025, [USER] wrote: Thank you for your help I will talk with [USER] to see if I understand the issue, [USER]. Sent from my iPhone > On Oct 10, 2025, at 9:41 AM, [STAFF] via RT <[EMAIL]> wrote: Ticket https://rt.umbc.edu/Ticket/Display.html?id=3286853 Last Update From Ticket: Hi [USER], I have verified again that the permissions for the common directory are correct-- So I do not understand why your students are unable to access the common directory. Please see below where I tested the permissions for the common directory using a student account. Could either you or the student who submitted an additional ticket provide some more information with exactly what you are attempting to do? Thanks [ptembei1@chip-login2 ~]$ pi_mkann_common [ptembei1@chip-login2 common]$ pwd /umbc/rs/pi_mkann/common [ptembei1@chip-login2 common]$ touch test [ptembei1@chip-login2 common]$ ls dbraw downloaded_data Projects test [ptembei1@chip-login2 common]$ -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Thu Oct 09 19:23:10 2025, [USER] wrote: My student still gets access denied when she tried maybe you can see what the problem is, M. Sent from my iPhone > On Oct 6, 2025, at 1:57 PM, [STAFF] via RT <[EMAIL]> wrote: If you agree your issue is resolved, please give us feedback on your experience by completing a brief satisfaction survey https://umbc.us2.qualtrics.com/SE/?SID=SV_etfDUq3MTISF6Ly&customereemail=[EMAIL]&groupid=EIS&ticketid=3286853 If you believe your issue has not been resolved, please respond to this message, which will reopen your ticket. Note: A full record of your request can be found at Ticket https://rt.umbc.edu/Ticket/Display.html?id=3286853 Thank You -- R e s o l u t i o n Hi [USER], I resolved the permission issues with pi_mkann/common. Let me know if you continue to experience errors when accessing that directory. Have a nice day! -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Mon Oct 06 13:41:02 2025, [USER] wrote: First Name Maricel Last Name Kann Email [EMAIL] Campus ID RS61560 Request Type High Performance Cluster Hello, I have recently had taki files transfer to chip, could you helps setting up the new common directory so all users in my lab have access and can edit, etc. The way it is now we don't have permissions set up for that. Thanks Maricel."
"3286959","2025-10-07 13:27:01","HPC User Account: [USER] in Student Group","Hello there, If you're under [STAFF], if he has an HPC account, he could easily sponsor your account, making things much easier on the cluster. Feel free to join the student group if [STAFF] requested you to do that, however. Anyways, here is the official link (A guide on how to create an HPC account): https://umbc.atlassian.net/wiki/spaces/faq/pages/1327431728/How+to+request+a+user+group+account+on+chip Remember that if [STAFF] is sponsoring your account, we would need written permission from him. That's as simple as cc'ing him to the ticket creation, and having him say 'I approve' or something similar in a reply. Best, [STAFF]"
"3287359","2025-10-10 14:26:50","HPC Slurm/Software Issue: Module Not Available","All the documentation for these programs are publicly available online. We are able to assist with HPC related issues you encounter along the way, however we are unable to walk you through everything. We mainly provide support with HPC/Slurm related issues, and utilizing all of those modules is a little out of scope. If you have any specific questions to help get started, I can do my best to answer them. Otherwise, I recommend taking a look at the documentation for the modules you requested to utilize them. Thanks! Yes, I loaded CGNS successfully. Now I am not sure how to use all of the modules for my code compilation and run the cases eventually. Could you please help me out with this? Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3287359 > Last Update From Ticket: I attempted to install CGNS as a custom module. You should be able to load the module with 'module load CGNS/4.5.0'. Can you attempt to load/use the module to verify it works as expected? If you run into issues with the module, let me know. -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Thu Oct 09 12:05:07 2025, [USER] wrote: Hi [USER], You do not need to reinstall the software every time you log in/out. After you compile the software, it is installed to whichever directory of your choosing. From there, all you need to do is add the path to your install directory to your PATH environment variable or just run the binaries from inside the install directory. The software is not readily supported by the module system, easybuild. There are some ways to possibly working around this, but for most use cases, to ensure compatibility it is recommended to just compile from source. I can take a crack at installing it as a custom module though. Also, could you elaborate on what you mean by 'a bash file so that I don't have to install all of the modules'? All the modules have already been installed, and just need to be loaded using 'module load $MODULE_NAME'. -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Thu Oct 09 11:52:15 2025, [USER] wrote: Hi [STAFF], Thank you very much for the update. Couldn't you please load the CGNS also? Because I could be wrong as I am not that much familiar with this chip system yet. My assumption is, if I install it by myself, then I have to do it again and again whenever I enter into the system, and run some cases. Please correct me if I am wrong! And I also need your help to make a bash file so that I don't have to install all of the modules that I need to run my cases every time separately. I need lots of modules to run every single test case of mine. Thanks, [USER] On Thu, Oct 9, 2025 at 11:27 AM [STAFF] via RT <[EMAIL]> wrote: If you agree your issue is resolved, please give us feedback on your experience by completing a brief satisfaction survey: https://umbc.us2.qualtrics.com/SE/?SID=SV_etfDUq3MTISF6Ly&customeremail=[EMAIL]&groupid=EIS&ticketid=3287359&ticketowner=[STAFF]&ticketsubject=HPC Slurm/Software Issue: Module Not Available If you believe your issue has not been resolved, please respond to this message, which will reopen your ticket. Note: A full record of your request can be found at: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3287359 > Thank You _________________________________________ R e s o l u t i o n: Hi [USER], Yes, I was just about to update this ticket. I installed MPICH versions 4.2.2 and 4.2.1, along with PETSc 3.20.3. For CGNS, I recommend that you compile it from source. The latest version of these three modules is now available on the cluster. Thanks, [STAFF] On Tue, Oct 7, 2025 at 10:15 AM via RT <[EMAIL]> wrote: Greetings, This message has been automatically generated in response to the creation of a ticket regarding: Subject: 'HPC Slurm/Software Issue: Module Not Available' Message: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUS ID] Request Type: High Performance Cluster Hello, I need MPICH, CGNS, PETSC modules to run my cases on hpcf cluster. But these modules are not available there to load! Could you please load the latest version of these three modules to the cluster and let me know when it's ready? Thanks, [USER]"
"3287360","2025-10-07 18:04:47","HPC Slurm/Software Issue: Job Resource Allocation Failing","Seems to be working now. Thank you! Ticket Last Update From Ticket: Ha, I think you cleared out *too much*. The file '.bashrc' is what's generally in charge of creating your environment. If you just deleted everything in your home directory, it doesn't know how to create your environment. I added it back in with the default set up we create for users in their bashrc and was able to confirm that your environment now looks normal: [USER]@chip-login2 ~]$ srun --cluster=chip-gpu --time=01:00:00 --mem=4000 --gres=gpu:1 --cpus-per-task=1 --pty $SHELL srun: job 101868 queued and waiting for resources srun: job 101868 has been allocated resources [USER]@g24-01 ~]$ Let me know if you still have issues. On Tue Oct 07 13:08:07 2025, [USER] wrote: Thank you for the quick reply, I cleared out my home directory. [USER]@chip-login2 ~]$ df -h /home/[USER]/ Filesystem Size Used Avail Use% Mounted on nfs.iss:/ifs/data/chip/home/[USER] 500M 0 500M 0% /home/[USER] But when I try and run an interactive session I still have the same issue [USER]@chip-login2 ~]$ srun --cluster=chip-gpu --time=01:00:00 --mem=4000 --gres=gpu:1 --cpus-per-task=1 --pty /bin/bash srun: job 101858 queued and waiting for resources srun: job 101858 has been allocated resources bash-5.1$ On Tue, Oct 7, 2025 at 11:24 AM [STAFF] via RT <[EMAIL]> wrote: Ticket Last Update From Ticket: Hi [USER], Your home directory is completely filled which can sometimes cause unexpected things to happen when creating new sessions (like what happens slurm starts a new interactive shell). Please remove some of the stuff from your home directory. If you're still having an issue after that, please let me know. bash-5.1$ df -h /home/[USER]/ Filesystem Size Used Avail Use% Mounted on nfs.iss:/ifs/data/chip/home/[USER] 500M 500M 0 100% /home/[USER] On Tue Oct 07 10:16:15 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: High Performance Cluster I was trying to get a conda environment set up, but after running conda create to make one, the next time I started an interactive job (command was: srun --cluster=chip-gpu --time=04:00:00 --mem=40000 --gres=gpu:1 --constraint=rtx_6000 --cpus-per-task=18 --pty /bin/bash) I noticed that instead of putting me into a node like normal, my terminal now says 'bash-5.1$' instead of showing what node I'm on and when I try to run: module load Anaconda3/2024.02-1 I get a note saying: Note: Modules do not function on the login node If I then exit I am returned to the login node and I have to exit again to leave my ssh session. Best, [STAFF] DOIT HPC System Administrator"
"3287721","2025-10-07 16:32:20","HPC User Account: [ID] in oates","Hi [USER] Your account ([USER]) has been added to your primary group is [GROUP]. Please read through the documentation found at hpcf.[DOMAIN] > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://[DOMAIN]/request-tracker-rt/[DOMAIN]-research-computing/) On [DATE], [STAFF] wrote: I approve. On [DATE], RT API via RT wrote: This e-mail is a notification that a [DOMAIN] user: [USER] <[EMAIL]> has requested an account within [DOMAIN]'s HPC environment in your group <[GROUP]>. As the PI, we request that you acknowledge and approve this account creation by replying to this message. Alternatively you can go to this link and review the ticket and indicate your decision here: Ticket <URL: https://rt.[DOMAIN]/Ticket/Display.html?id=[NUMBER]> Once we have your approval, we will create the account and you and the new user will receive another e-mail notifying you that the account has been created. If you have any other questions or concerns please contact us. - [DOMAIN] DoIT Research Computing Support Staff Best, [STAFF]"
"3287907","2025-10-09 00:52:09","URGENT: [SOFTWARE] Access Policy Hindering Research - Request for Immediate Remote Desktop Access","Excellent! Glad to hear this is working. I'll mark this as resolved -- do reopen if there are related issues. On Wed Oct 08 15:59:49 2025, [STAFF] wrote: The student was able to download the software. Thank you for your help on this matter! I have no further requests. Best, [STAFF]. On Tue, Oct 7, 2025 at 3:00 PM [STAFF] wrote: Thank you for your prompt response! I will reach out to the student and update you accordingly. With appreciation, [STAFF]. On Tue, Oct 7, 2025 at 2:57 PM [STAFF] via RT wrote: Ticket https://rt.umbc.edu/Ticket/Display.html?id=3287907. Hi [STAFF], Could you have your student try this environment? https://elum.in/umbc-desktop-sosc. I believe it offers the appropriate access. -- Roy Prouty DoIT Research Computing Team"
"3287915","2025-10-28 19:36:23","Migrating Research Storage Volume to Ceph Cluster - [ID]","Hello [USER], We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note: The path has changed, and is now available under /umbc/rs/[STAFF]. The alias used to reach the volume is now [STAFF]_common and [STAFF]_user. Your new volume has a quota of 10TB. When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good? Thank you, [STAFF]. On Tue Oct 28 10:49:13 2025, [EMAIL] wrote: Hello [USER], This is a reminder that we will be performing your group's migration to the Ceph storage cluster today. During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated. We will provide an update once completed. Best, [STAFF]. On Fri Oct 17 12:13:50 2025, [EMAIL] wrote: Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of October 28th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]."
"3287918","2025-10-07 18:40:58","Migrating Research Storage Volume to Ceph Cluster","Dear [STAFF], As per the communication via myUMBC earlier this summer (June HPCF Newsletter), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 488.3 GB quota on the old storage server. To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at '/umbc/rs/[USER]'. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume. Below we've listed two options for handling this data migration - please let us know which of these you'd prefer. Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day. Option 2: If you don't respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes. Note: After this process has completed, the new storage volume will have a new name. For example, group 'pi_doit' will find its data under '/umbc/rs/pi_doit', or in your group's case you will find your volume under '/umbc/rs/[USER]'. Thank you, [STAFF]"
"3287921","2025-10-29 21:28:31","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Hello again [USER], We have finished migrating your volume to the Ceph cluster. As far as we can tell everything seems to have gone smoothly. There are a few things to note: The path has changed, and is now available under /umbc/rs/[STAFF]. The alias used to reach the volume is now [STAFF]_common and [STAFF]_user. Your new volume has a quota of 25TB. When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good? Thank you, [STAFF]. On Wed Oct 29 09:47:56 2025, [USER] wrote: Hello [USER], This is a reminder that we will be performing your group's migration to the Ceph storage cluster today. During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated. We will provide an update once completed. Best, [STAFF]. On Fri Oct 17 12:21:16 2025, [USER] wrote: Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of October 29th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]."
"3287925","2025-10-30 23:27:40","Migrating Research Storage Volume to Ceph Cluster - [ID]","Hello [USER], We have finished migrating your volume to the Ceph cluster. As far as we can tell everything seems to have gone smoothly. There are a few things to note: The path has changed, and is now available under /umbc/rs/[STAFF]. The alias used to reach the volume is now [ALIAS] and [ALIAS]. Your new volume has a quota of 10TB. When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good? Thank you, [STAFF]. On Thu Oct 30 10:33:54 2025, [USER] wrote: Hello [USER], This is a reminder that we will be performing your group's migration to the Ceph storage cluster today. During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated. We will provide an update once completed. Best, [STAFF]. On Fri Oct 17 12:28:50 2025, [USER] wrote: Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of October 30th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]."
"3287928","2025-10-10 17:14:46","Migrating Research Storage Volume to Ceph Cluster - [USER]","Hello [USER], We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note: The path has changed, and is now available under /umbc/rs/[STAFF]. The alias used to reach the volume is now [STAFF]_common and [STAFF]_user. Your new volume has a quota of 10TB. When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good? Thank you, [STAFF]."
"3287931","2025-10-31 18:20:24","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Hello [USER], We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note: The path has changed, and is now available under /umbc/rs/[STAFF]. The alias used to reach the volume is now [ALIAS] and [ALIAS]. Your new volume has a quota of 10TB. When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good? Thank you, [STAFF]. On Fri Oct 31 10:42:15 2025, [STAFF] wrote: Good morning [USER], This is a reminder that we will be performing your group's migration to the Ceph storage cluster today. During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated. We will provide an update once completed. Best, [STAFF]. On Fri Oct 17 12:49:18 2025, [STAFF] wrote: Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of October 31st for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]."
"3287935","2025-10-17 17:10:28","Migrating Research Storage Volume to Ceph Cluster","Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 5th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3287940","2025-10-21 15:52:25","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","On Tue Oct 21 11:50:01 2025, [STAFF] wrote: Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 6th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF] Dear [STAFF], Thank you for your email and for letting me know. It sounds good to me. Best regards, [USER]."
"3287942","2025-10-21 15:51:13","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 6th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3287945","2025-10-21 15:56:02","Migrating Research Storage Volume to Ceph Cluster - [ID]","Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 6th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3287949","2025-10-21 15:58:29","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Dear [STAFF], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 7th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3287950","2025-10-10 17:15:32","Migrating Research Storage Volume to Ceph Cluster - [ID]","Hello [USER], We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note: The path has changed, and is now available under /umbc/rs/pi_[USER]. The alias used to reach the volume is now pi_[USER]_common and pi_[USER]_user. Your new volume has a quota of 10TB. When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good? Thank you, [STAFF]."
"3287957","2025-10-21 16:04:53","Migrating Research Storage Volume to Ceph Cluster - [ID]","Yes, please proceed. [STAFF] proceeded with Option 2, randomly assigning a date between October 16 and November 15 for [USER]'s migration. [STAFF] assigned the date of November 10th for [USER]'s migration. Let [USER] know if there is a better day and within reason, [STAFF] can reschedule that date. [USER] will be notified when the migration begins and completes. Thank you, [STAFF]."
"3287959","2025-10-21 16:03:03","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 10th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3287961","2025-10-21 16:04:01","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 10th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3287965","2025-10-28 14:35:37","Migrating Research Storage Volume to Ceph Cluster - [ID]","Ticket https://rt.[INSTITUTION].edu/Ticket/Display.html?id=[TICKET_NUMBER] Comment just added. Hello, Yes, the data you currently have will be moved to '/umbc/rs/pi_[USERNAME]'. Looking at your storage personally, apologies, it seems our estimate was off, it seems your pi actually has around 69 GB used in '/umbc/rs/[USERNAME]', and will be moved to '/umbc/rs/pi_[USERNAME]'. No worries, the transfer process will still be very quick. Best, [STAFF]"
"3287967","2025-11-04 20:25:12","Migrating Research Storage Volume to Ceph Cluster","Thanks a lot for the update. I saw that some additional disk has been allocated to our disk partition. I really appreciate it. Hopefully the migration process can finish soon and successfully. -[USER] On Tue, Nov 4, 2025 at 10:18 AM [STAFF] via RT <[EMAIL]> wrote: Ticket Last Update From Ticket: Hello [USER], The migration is still moving along (thankfully), and I can confirm that your new volume was almost full. The Ceph storage server may have some slight differences in how it compresses and de-duplicates data, which would be the most likely reason why the storage usage number is different from what was on the old server. On your old storage volume your group was using 486TB out of a 505TB quota - we will make sure you have at least 20TB of free space on the new storage server when the migration completes. Please let me know if you have any more questions or concerns! Best, [STAFF] On Tue Nov 04 09:33:10 2025, [USER] wrote: Hi [STAFF] I need to determine the exact date with my team members though. Can I get back to you by Friday? Thanks-[USER] On Tue, Oct 7, 2025 at 3:20 PM via RT <[EMAIL]> wrote: Greetings, This message has been automatically generated in response to the creation of a ticket regarding: 'Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos' Dear [USER], As per the communication via myUMBC earlier this summer (June HPCF Newsletter), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 458.94 TB of a 505.0000 TB quota on the old storage server. To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at '/umbc/rs/zzbatmos'. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume. Below we've listed two options for handling this data migration - please let us know which of these you'd prefer. Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day. Option 2: If you don't respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes. Note: After this process has completed, the new storage volume will have a new name. For example, group 'pi_doit' will find its data under '/umbc/rs/pi_doit', or in your group's case you will find your volume under '/umbc/rs/pi_zzbatmos'. Thank you, [STAFF]"
"3287969","2025-10-10 15:31:13","Migrating Research Storage Volume to Ceph Cluster","Ticket [TICKET ID] Comment just added. I'll look into this and try and resend the email. Cc'ing myself to this ticket for updates."
"3287971","2025-10-15 14:16:57","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Hello [USER], We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note: The path has changed, and is now available under /umbc/rs/pi_[STAFF]. The alias used to reach the volume is now pi_[STAFF]_common and pi_[STAFF]_user. Your new volume has a quota of 10TB. When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good? Thank you, [STAFF] On Wed Oct 15 09:44:38 2025, [EMAIL] wrote: Hello [USER], I was out yesterday, so I will be performing your group's migration to the Ceph storage cluster today. During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated. We will provide an update once completed. Best, [STAFF] On Mon Oct 13 11:42:33 2025, [EMAIL] wrote: Dear [USER], I will be away Tues and Wed, so that would be the optimal time to execute the change. However, I do not think that the move would affect any of my ongoing projects at any time over the next month. So, I leave it to your discretion to schedule. Please keep me apprised. [USER] On 10/7/25 3:25 PM, via RT wrote: Greetings, This message has been automatically generated in response to the creation of a ticket regarding: Subject: 'Migrating Research Storage Volume to Ceph Cluster - pi_[STAFF]' Message: Dear [USER], As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 48.8 GB quota on the old storage server. To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at '/umbc/rs/kturpie'. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume. Below we've listed two options for handling this data migration - please let us know which of these you'd prefer. Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day. Option 2: If you don't respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes. Note: After this process has completed, the new storage volume will have a new name. For example, group 'pi_doit' will find its data under '/umbc/rs/pi_doit', or in your group's case you will find your volume under '/umbc/rs/pi_[STAFF]'. Thank you, [STAFF]"
"3287973","2025-10-23 14:20:25","Migrating Research Storage Volume to Ceph Cluster","Dear [STAFF], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3287979","2025-10-23 14:18:38","Migrating Research Storage Volume to Ceph Cluster - [ID]","Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288361","2025-10-16 01:37:55","HPC Other Issue: [USER] access to chip","Yes, she has! Thanks for checking!  [STAFF] (she/her/hers) Associate Professor Department of Biological Sciences University of Maryland, Baltimore County 1000 Hilltop Circle Baltimore, MD 21250 Office: [PHONE NUMBER REMOVED] [WEBSITE URL REMOVED]  On Wed, Oct 15, 2025, 1:08 PM [STAFF] via RT <[EMAIL ADDRESS REMOVED]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3288361 > Last Update From Ticket: Hi [USER], Just checking in, was your student able to access the cluster? On Fri Oct 10 11:46:59 2025, [USER] wrote: Please do! I'll also ask that you submit an RT ticket to formally add the student to your group ( https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research Computing ). I looked into the VPN issue, and it seems like the student should be fine as long as they install the vpn. Once they're added we'll test it out and see what happens from there. On Thu Oct 09 16:26:28 2025, [USER] wrote: OK! I've just received notice that the student's affiliation has been reinstated; her name is [USER] and her ID# is [CAMPUS ID REMOVED] ([USER]). She should have access through 6/30/2026.What would be the next step for allowing her to access my lab's volume on chip? Should I direct her to download the VPN too? On Thu, Oct 9, 2025 at 11:55 AM [STAFF] via RT <[EMAIL ADDRESS REMOVED]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3288361 > Last Update From Ticket: Hi [USER], Typically we don't allow for users into UMBC services if they are not in North America (even with VPN). That being said, we can make some exceptions to this rule. For now start by sponsoring an account for the student (information for that can be found here: https://umbc.atlassian.net/wiki/spaces/faq/pages/30739140/How+do+I+request+myUMBC+accounts+for+non-UMBC+users+Create+a+Sponsored+Account ), and we'll go from there. On Wed Oct 08 11:01:26 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL ADDRESS REMOVED] Campus ID: [CAMPUS ID REMOVED] Request Type: High Performance Cluster Hello, I have a Brazilian graduate student on fellowship, [USER], who generated a large genomic dataset while working in my lab, and who now is trying to analyze this dataset using the program Stacks. She has submitted jobs to servers at her home institution of U Sao Paolo and the UK school where her fellowship continued, but they haven't been running due to timeouts and/or job backlogs. She would like to access chip via my lab's login, but we have a couple of questions about this. 1) Is this allowed? My lab has tried to access chip while abroad and was unsuccessful, but this may have been because the hotel internet was slow. I know the GlobalProtect VPN would probably need to be downloaded too. 2) Can I create a log-in for her? She was assigned a UMBC account while working here in 2023, but I assume that has lapsed. Thank you for the help!"
"3288390","2025-10-10 15:28:16","Utilizing matched nodes on chip-cpu","Hello To run a job across 12 nodes, you can submit a job to the contrib partition and request 12 nodes. SLURM will allocate your 6 dedicated nodes first (--nodelist=c24-[14-19]) and then fill the remaining slots from available match nodes. Here's an example sbatch script you can use: #SBATCH --cluster=chip-cpu #SBATCH --mem=5000M #SBATCH --time=01:00:00 #SBATCH --account=[ACCOUNT] #SBATCH --partition=contrib #SBATCH --nodes=12 #SBATCH --qos=shared #SBATCH --nodelist=c24-[14-19] Let us know if that doesn't work. On Wed Oct 08 11:26:08 2025, [USER] wrote: From [STAFF]: 'We need to help to understand how the PI partitions are working in Chip. When I look at the pi_[ACCOUNT], I can see that we have nodes 14 to 19 (6 in total) assigned to it. But I should have access also to the +6 additional nodes from the matching funding, right? How can we use these additional nodes? We will have to run some very large simulations, which will require using all 12 nodes at once. We don't know how to queue such a job, since my partition only shows 6 nodes.' -- [STAFF]"
"3288602","2025-10-09 17:20:23","HPC Other Issue: [USER] cannot log into chip","Hi [STAFF], I just wanted to give you an update regarding the status of chip. Access to the cluster was restored yesterday afternoon, and access to ada volumes have also been restored. For more information, please check out the myUMBC post: https://my3.my.umbc.edu/groups/hpcf/posts/153425 -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Wed Oct 08 14:43:59 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster Completely Hangs after Last login : Wed Oct 8 ......"
"3288614","2025-10-09 17:19:20","HPC Other Issue: Unable to connect to chip","Hi [USER], I just wanted to give you an update regarding the status of chip. Access to the cluster was restored yesterday afternoon, and access to ada volumes have also been restored. For more information, please check out the myUMBC post: https://my3.my.umbc.edu/groups/hpcf/posts/153425 -- Kind regards, [STAFF]"
"3288634","2025-10-09 17:21:27","HPC Other Issue: Unable to access login shell after connecting to chip.rs.umbc.edu","Hi [USER], I just wanted to give you an update regarding the status of chip. Access to the cluster was restored yesterday afternoon, and access to ada volumes have also been restored. For more information, please check out the myUMBC post: https://my3.my.umbc.edu/groups/hpcf/posts/153425 -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Thu Oct 09 12:13:04 2025, [EMAIL] wrote: I appreciate your clarification~ On Wed Oct 08 16:03:51 2025, [EMAIL] wrote: Hello [USER], Our team is aware of the issue, and we are working on resolving this at the moment. We understand the inconvenience this may cause you, but we will send an update once this issue has been fixed. Best regards, [STAFF]"
"3288676","2025-10-09 17:19:49","HPC Slurm/Software Issue: SSH Connection Hanging","Hi [USER], I just wanted to give you an update regarding the status of chip. Access to the cluster was restored yesterday afternoon, and access to ada volumes have also been restored. For more information, please check out the myUMBC post: https://my3.my.umbc.edu/groups/hpcf/posts/153425 Have a nice day! -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Wed Oct 08 15:55:51 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              High Performance Cluster Good afternoon, When I ssh into chip.rs.umbc.edu, I am able to be successfully logged in. However, the connection just hangs there without presenting a shell prompt afterwards. Is there any way I could resolve this issue? This happens when I try to use putty and my windows powershell."
"3288709","2025-10-09 17:20:47","HPC Slurm/Software Issue: Can't log in","Hi [USER], I just wanted to give you an update regarding the status of chip. Access to the cluster was restored yesterday afternoon, and access to ada volumes have also been restored. For more information, please check out the myUMBC post: https://my3.my.umbc.edu/groups/hpcf/posts/153425 -- Kind regards, [STAFF] On Wed Oct 08 16:26:02 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              High Performance Cluster I cant log in to chip"
"3288849","2025-10-09 15:58:20","HPC Slurm/Software Issue: ADA storage","Hi [USER], Thanks for your patience and understanding. The Ada volume are back! https://my3.my.umbc.edu/groups/hpcf/posts/153425 On Wed Oct 08 23:32:33 2025, [STAFF] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster I know the ada storage is not working and its under maintenance. But do you have any timeframe on how much time it will take? My all experiment data is stored in ada, so i cant work on rs without access to that data. Thanks for your understanding. Best, [STAFF] DOIT HPC System Administrator"
"3288952","2025-10-21 16:10:21","Migrating Research Storage Volume to Ceph Cluster - [ID]","Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 11th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288957","2025-10-23 13:51:15","Migrating Research Storage Volume to Ceph Cluster - [ID]","Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288960","2025-10-23 14:12:36","Migrating Research Storage Volume to Ceph Cluster - [ID]","Dear [STAFF], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288964","2025-10-23 14:06:28","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288965","2025-10-23 13:50:09","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288967","2025-10-28 14:40:05","Migrating Research Storage Volume to Ceph Cluster - [SERVER]","Hi, No worries, you don't have to migrate anything; this is all on our end. We're just letting you know what day we're doing it, so you don't use the cluster that day. Yes, you have almost no data on the cluster. I expect the migration to take almost no time at all. The only thing you have to do is not use the cluster on November 14th, and you'll be all set. Best, [STAFF]"
"3288968","2025-10-23 14:11:53","Migrating Research Storage Volume to Ceph Cluster - [ID]","Dear [STAFF], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288973","2025-10-23 13:55:50","Migrating Research Storage Volume to Ceph Cluster","Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288975","2025-10-21 16:09:26","Migrating Research Storage Volume to Ceph Cluster - [ID]","Dear [STAFF], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 11th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288979","2025-10-21 16:12:43","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Dear [STAFF], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 12th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288980","2025-10-21 16:20:21","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Dear [STAFF], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288982","2025-10-21 16:21:27","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Dear [STAFF], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288983","2025-10-21 16:22:17","Migrating Research Storage Volume to Ceph Cluster - [ID]","Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288987","2025-10-23 13:54:18","Migrating Research Storage Volume to Ceph Cluster - [ID]","Dear [STAFF], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288988","2025-10-23 13:55:08","Migrating Research Storage Volume to Ceph Cluster","Dear [STAFF], as per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288991","2025-10-23 14:04:39","Migrating Research Storage Volume to Ceph Cluster - [ID]","Dear [STAFF], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288993","2025-10-23 19:01:22","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","That is fine. Thank you for letting me know. Best Regards, [USER]. On Thu, Oct 23, 2025 at 10:07 AM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.[DOMAIN]/Ticket/Display.html?id=3288993> Last Update From Ticket: Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]."
"3288998","2025-10-23 14:17:11","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Dear [STAFF], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3289003","2025-10-10 15:29:16","Migrating Research Storage Volume to Ceph Cluster - [ID]","Ticket  Comment just added.  I'll also mark [USER] as 'Special Case' in the spreadsheet"
"3289011","2025-10-28 14:42:59","Migrating Research Storage Volume to Ceph Cluster - [ID]","Ticket [TICKET_URL] Comment just added. Hi, No worries, no meeting of any kind. The only thing you have to do is not use the cluster on Nov. 14, and we'll do the migration for you. Best, [STAFF]"
"3289012","2025-10-23 13:56:44","Migrating Research Storage Volume to Ceph Cluster - [ID]","Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3289018","2025-10-16 19:09:44","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","That's okay, take your time with checking out your volume. I hope you get well soon! Best, [STAFF] On Thu Oct 16 15:05:22 2025, [USER] wrote: Hi [STAFF], Thanks for the message. I'll do that next week as I am unwell right now. [USER] On Wed, Oct 15, 2025 at 10:16 AM via RT [EMAIL] wrote: Ticket https://rt.umbc.edu/Ticket/Display.html?id=3289018 Last Update From Ticket: Hello [USER], We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note: * The path has changed, and is now available under /umbc/rs/pi_[USER]. * The alias used to reach the volume is now pi_[USER]_common and pi_[USER]_user. * Your new volume has a quota of 10TB. When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good? Thank you, [STAFF] On Wed Oct 15 09:53:44 2025, [STAFF] wrote: Good morning [USER], This is a reminder that we will be migrating your group's research storage volume to the Ceph storage cluster today. During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated. We will provide an update once completed. Best, [STAFF] On Thu Oct 09 14:14:50 2025, [STAFF] wrote: Hello [USER], Sounds good, I've put your group's migration on our schedule for Wednesday October 15th. We will send you an email alert via this RT ticket when we begin the migration, and again once it has completed. Please let me know if you have any questions or concerns. Best, [STAFF] On Thu Oct 09 13:45:18 2025, [USER] wrote: Hello, The needed migration can be done sometime next week (Oct 15,16,17); any or some of those would work fine with me. Thanks [USER]"
"3289025","2025-10-29 13:46:59","Migrating Research Storage Volume to Ceph Cluster - [ID]","I've updated my files with the new file path and run a couple of them. Things seem to be working fine as far as I can tell. Thanks and good luck with the rest of the migration process! [USER] On Tue, Oct 28, 2025 at 3:37 PM [STAFF] via RT <[EMAIL]> wrote: Ticket https://rt.umbc.edu/Ticket/Display.html?id=3289025 Last Update From Ticket: Hello [USER], We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note: * The path has changed, and is now available under /umbc/rs/pi_[STAFF]. * The alias used to reach the volume is now pi_[STAFF]_common and pi_[STAFF]_user. * Your new volume has a quota of 10TB. When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good? Thank you, [STAFF] On Tue Oct 28 10:49:40 2025, [STAFF] wrote: Hello [USER], This is a reminder that we will be performing your group's migration to the Ceph storage cluster today. During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated. We will provide an update once completed. Best, [STAFF] On Thu Oct 09 11:54:13 2025, [STAFF] wrote: Hello [USER], Sounds good, I've put your group's migration on our schedule for October 28th. We will send you an email alert via this RT ticket when we begin the migration, and again once it has completed. Please let me know if you have any questions or concerns. Best, [STAFF] On Thu Oct 09 11:31:27 2025, [USER] wrote: October 28 would be great. Or Oct 29 if that isn't possible. Thanks! Greetings, This message has been automatically generated in response to the creation of a ticket regarding: Subject: 'Migrating Research Storage Volume to Ceph Cluster - pi_[STAFF]' Message: Dear [USER], As per the communication via myUMBC earlier this summer (June HPCF Newsletter), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 502.9 GB of a 488.3 GB quota on the old storage server. To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at “/umbc/rs/[STAFF]”. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume. Below we’ve listed two options for handling this data migration - please let us know which of these you’d prefer. Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day. Option 2: If you don’t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes. Note: After this process has completed, the new storage volume will have a new name. For example, group “pi_doit” will find its data under “/umbc/rs/pi_doit”, or in your group’s case you will find your volume under “/umbc/rs/pi_[STAFF]”. Thank you, [STAFF]"
"3289155","2025-10-21 15:27:18","HPC New Group: [ID]","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[USER]. Your home directory has 500M of storage. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) Welcome to chip, UMBC's High Performance Computing Cluster! The group, pi_[USER], now exists on the chip cluster. Members of this group can access and contribute to the research storage space allocated to the group. This storage space is located at /umbc/rs/pi_[USER], and currently has a quota of 25T. For information on accessing the cluster, adding accounts to your group, and getting started using the cluster, check out the tutorial on our wiki: https://umbc.atlassian.net/wiki/x/R4BPQg Additional documentation is also available here: https://umbc.atlassian.net/wiki/x/FwCHQ If you have any questions or issues, please submit a new RT ticket at: https://doit.umbc.edu/request-tracker-rt/doit-research-computing/ Account creation for users: Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[USER]. Your home directory has 500M of storage. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) Let me know if you have any more questions! [STAFF]"
"3289186","2025-10-09 16:54:34","Start a new group under a PI","First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [ID] Request Type: Help with something else I’d like to start two new groups. 1) for a class “ENME444 - Mechanical Engineering Capstone Design” 2) my research group “eMACS Lab” Thank you"
"3289187","2025-10-09 18:11:12","Start a new group under [STAFF]","Hi [USER], I created two groups for you, along with an account for your user. First, Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[USER]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://[EMAIL]/wiki/spaces/faq/pages/[NUMBER]/Getting+Started+on+chip. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Secondly, The group pi_[USER] now exists on the chip cluster. Members of this group can therefore access and contribute to the research storage space allocated to the group. This storage is located at /umbc/rs/pi_[USER] and currently has a quota of 25T. And lastly, The group [CLASS] now exists on the chip cluster. Members of this group can therefore access and contribute to the research storage space allocated to the group. This storage is located at /umbc/class/[CLASS] and currently has a quota of 5T. Please review documentation on the hpcf.umbc.edu website. Submit any questions or issues as separate RT Tickets at: https://[EMAIL]/request-tracker-rt/doit-research-computing/. Feel free to let us know if you encounter any issues with either of your groups! Have a good day!-- Kind regards, [STAFF] DoIT Unix Infra Student Worker. On Thu Oct 09 12:54:38 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [ID] Request Type:              Help with something else I’d like to start two new groups.   1) for a class “ENME444 - Mechanical Engineering Capstone Design” 2) my research group “eMACS Lab”. Thank you"
"3289431","2025-10-10 17:33:55","HPC Other Issue: Users lack global access in the common directory in [ID] lab space on CHIP","Thank you all for troubleshooting On Oct 10, 2025, at 1:08 PM, [USER] wrote: Hello [USER], It works for me now. Thank you so much for your help. Best, [USER] On Fri, Oct 10, 2025 at 12:36 PM [STAFF] via RT wrote: Ticket https://rt.umbc.edu/Ticket/Display.html?id=3289431 Last Update From Ticket: Hi [USER], When you have a chance, try again. It should all be working now. Let me know if it works as expected! -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Fri Oct 10 11:33:35 2025, [USER] wrote: Hi [STAFF], I just tried again and attached a screenshot of my attempt. I’m able to create and work freely within folders that I created inside the common directory. However, I can’t perform any operations in subfolders within common that I didn’t create. For example, the folder downloaded_data was created by my PI, Dr. [USER]. To illustrate the issue, I tried creating a test folder within downloaded_data, but I was denied permission. I also tried moving some data into a subfolder within downloaded_data and received the same error. Everything works fine when I’m working within folders I created myself in common. This is what I mean by a global access issue in the common directory. Best, [USER] On Fri, Oct 10, 2025 at 9:37 AM [STAFF] via RT wrote: Ticket https://rt.umbc.edu/Ticket/Display.html?id=3289431 Last Update From Ticket: Hi [USER], Students in your group do have access to the common directory of the research volume. I have tested permissions with multiple student accounts, and there are no issues with the common directory. Please share exactly what you are attempting to do, and what directory you are in. Please see below: [ptembei1@chip-login2 ~]$ pi_mkann_common [ptembei1@chip-login2 common]$ pwd /umbc/rs/pi_mkann/common [ptembei1@chip-login2 common]$ touch test [ptembei1@chip-login2 common]$ ls dbraw downloaded_data Projects test [ptembei1@chip-login2 common]$ -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Thu Oct 09 18:22:47 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUS_ID] Request Type: High Performance Cluster I work in Dr. [USER]’s lab, where we collaborate using shared directories. Without global rwx access to the common directory, we’re unable to move files, create subdirectories, or delete files that are no longer needed in directories created by other users. We would really appreciate your help with this."
"3289503","2025-10-10 19:14:02","HPC User Account: [USER] in Center for Navigation, Timing & Frequency Research","Hi [USER], Your account ([USER]) has been created on [SERVER]. Your primary group is [GROUP]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://[DOMAIN]/wiki/spaces/faq/pages/[PAGEID]/Getting+Started+on+[SERVER]. Please read through the documentation found at [SERVER] > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://[DOMAIN]/request-tracker-rt/[DEPT]-research-computing/) Let me know if you have any more questions, [STAFF]"
"3289659","2025-10-12 20:10:29","RCD Consult: Deep Lab Cut","Thanks [STAFF]! I have submitted a group request. Please let me know if you need additional information.  Thanks, [USER] On Fri, Oct 10, 2025 at 1:08 PM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3289659 > Last Update From Ticket: Hi [USER], First we'll have you start by putting a group request. Documentation on how to do that can be found here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1219461147/Connecting#Requesting-a-User-Account-on-chip, with a link to the RT form to request a group being found here: https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research Computing Additionally, once you've been set up on the cluster, there is a getting started tutorial that can be found here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip If you have trouble with any of the above let me know and we can walk through it. On Fri Oct 10 10:18:16 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [ID] Request Type: Research Computing & Data Consultation Good Morning, I am a Prof in the Math/Stat department. My area of research is not high performance computing, but I am working on a project that may need access to a machine with GPUs. We need to use software called Deep Lab Cut https://deeplabcut.github.io/DeepLabCut/docs/installation.html to analyze experimental data. As I understand it, the first step is to use Deep Lab Cut on a laptop to recreate an *.eml file. That file would then be used within a batch job on a GPU computer to generate other files. I have two undergraduate students who will be working on this project. I am also collaborating with a postdoc at JHU. I am not sure where to begin, which is why I am submitting this request. Thanks, [USER]"
"3289689","2025-10-10 19:52:19","HPC New Group: [ID]","Hi [USER], Welcome to chip, UMBC's High Performance Computing Cluster! The group, [GROUPNAME], now exists on the chip cluster. Members of this group can access and contribute to the research storage space allocated to the group. This storage space is located at /umbc/rs/[GROUPNAME], and currently has a quota of 10TB. For information on accessing the cluster, adding accounts to your group, and getting started using the cluster, check out the tutorial on our wiki: [URL]. Additional documentation is also available here: [URL]. Submit any questions or issues as separate RT tickets at: [URL]. -- Kind regards, [STAFF]. On Fri Oct 10 10:47:09 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster Group Type:            Project Title:        Sampling strategy and design for Chesapeake Bay habitat assessment Project Abstract:     The Chesapeake Bay has had a large monitoring program for water quality for the last four decades. Given the advancement of continuous monitoring infrastructure, recent studies pointed to the high values of small numbers of in-situ monitoring stations with high temporal frequency to provide a sound basis for water quality assessment. In addition, a coupling of on-shore and off-shore monitoring can further inform the habitat assessment. This study will couple big data from hydrodynamic models to evaluate the monitoring strategies that incorporate novel continuous in-situ monitoring technologies. The computing needs involve storage of large NetCDF files generated from a high-resolution hydrodynamic model covering the Chesapeake Bay, as well as Monte Carlo simulations to evaluate a large collection of potential sampling scenarios. The newer monitoring, in turn, will enhance the Chesapeake Bay habitat assessment and address stakeholder planning needs. This study will consider agency-specific logistic constraints on site locations while optimizing for habitat assessment."
"3289693","2025-10-16 17:12:44","HPC New Group: [ID]","Hi [USER], Welcome to chip, UMBC's High Performance Computing Cluster! The group, pi_[STAFF], now exists on the chip cluster. Members of this group can access and contribute to the research storage space allocated to the group. This storage space is located at /umbc/rs/pi_[STAFF], and currently has a quota of 10T. For information on accessing the cluster, adding accounts to your group, and getting started using the cluster, check out the tutorial on our wiki: https://umbc.atlassian.net/wiki/x/R4BPQg Additional documentation is also available here: https://umbc.atlassian.net/wiki/x/FwCHQ If you have any questions or issues, please submit a new RT ticket at: https://doit.umbc.edu/request-tracker-rt/doit-research-computing/ Kind regards, [STAFF] DoIT Unix Infra Student Worker On Tue Oct 14 16:14:47 2025, [EMAIL] wrote: Leveraging HPC Resources to Accelerate LiDAR and Remote Sensing Workflows"
"3289694","2025-10-16 15:32:18","HPC New Group: [ID]","Hi [USER], Welcome to chip, UMBC's High Performance Computing Cluster! The group, pi_[USER], now exists on the chip cluster. Members of this group can access and contribute to the research storage space allocated to the group. This storage space is located at /umbc/rs/pi_[USER], and currently has a quota of 10T. For information on accessing the cluster, adding accounts to your group, and getting started using the cluster, check out the tutorial on our wiki: https://umbc.atlassian.net/wiki/x/R4BPQg Additional documentation is also available here: https://umbc.atlassian.net/wiki/x/FwCHQ If you have any questions or issues, please submit a new RT ticket at: https://doit.umbc.edu/request-tracker-rt/doit-research-computing/ Project title: [PROJECT TITLE] Abstract: [PROJECT ABSTRACT] The large-scale data processing, model training, and structural mapping tasks require access to GPU-enabled high-performance computing (HPC) resources with distributed processing capabilities. This project is conducted in collaboration with the Maize Genetics and Genomics Database (MaizeGDB), USDA-ARS, Corn Insects and Crop Genetics Research Unit, Iowa State University, Ames, IA, leveraging their extensive maize genomic and proteomic datasets. Expected outcomes include (1) novel computational methods for extracting structural features from biological sequences, (2) deeper understanding of gene-to-structure relationships in maize, and (3) new machine learning tools to support functional genomics and crop improvement research within the MaizeGDB community."
"3289695","2025-10-31 17:48:54","HPC New Group: [ID]","Hi [USER], An account was created for Dr. [STAFF] at the time the group was created. They should be able to follow the log in instructions located on our wiki to connect to the cluster. Please find documentation for that at this link: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip#Accessing-chip Additionally, the broken link has been fixed, here is the corrected link: https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research+Computing Have a nice day! -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Thu Oct 30 10:27:39 2025, [EMAIL] wrote: Good morning, Can you kindly create an account for Dr. [STAFF] for this group? From the information provided, I see a group was created, but I do not see any account for her. The link https://doit.umbc.edu/request-tracker-rt/doit-research-computing/ is broken. Thanks. Sincerely, [USER] Systems Administrator Department of Computer Science [ORGANIZATION] [ADDRESS] :(PHONE NUMBER) :(PHONE NUMBER) [EMAIL] On Thu Oct 16 12:16:29 2025, [EMAIL] wrote: Greetings, Please find attached the projects title and abstract. On Tue, Oct 14, 2025 at 1:41 PM [STAFF] via RT <UMBCHelp@rt.umbc.edu> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3289695 > Last Update From Ticket: Hi [USER], Once I receive your project title and abstract, I can create an account on chip for you! Have a nice day! -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Fri Oct 10 10:54:30 2025, ZZ99999 wrote: > First Name: Roy > Last Name: Prouty > Email: proutyr1@umbc.edu > Campus ID: WH39335 > > Request Type: High Performance Cluster > > Group Type: > Project Title: Get from PI > Project Abstract: Get from PI > > This group should be named pi_hodae1 . Don't proceed without getting title/abstract from PI."
"3289729","2025-10-10 15:34:14","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Ticket  [TICKET_URL] Comment just added. Cc'ing [STAFF] to this ticket, in case there's any more problems"
"3289747","2025-10-10 19:07:35","HPC User Account: [USER] in Student Group","Hi [USER], Your account ([USER]) has been created on [SERVER]. Your primary group is [GROUP]. Your home directory has 500M of storage. You can find a short tutorial on how to use [SERVER] here: https://[WEBSITE]/wiki/spaces/[SPACE]/pages/[PAGE]/Getting+Started+on+[SERVER]. Please read through the documentation found at [WEBSITE] > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://[REQUEST_TRACKER]). Let me know if you have any more questions! [STAFF]"
"3289785","2025-10-10 16:32:34","HPC User Account: [USER] in misc-lab","First Name: [USER] Last Name: [STAFF] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster Create/Modify account in existing PI group Existing PI Email: [EMAIL] Existing Group: misc-lab Project Title: Misc Lab Project Abstract: group for the Misc Lab requesting new PI group"
"3289788","2025-10-10 16:36:53","create new HPC PI group","First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: Help with something else group name: misc-lab list of members: [EMAIL]"
"3290086","2025-10-14 17:31:42","HPC Other Issue: Not able to connect to JupyterLab on Chip","Hi [USER], I identified two nodes (c21-15 and c21-16) that had a missing symbolic link in /usr/ebuild/installs. This is what likely prevented you from finding the jupyter-lab binaries. It has now been resolved, so you shouldn't experience issues now. I will close this ticket now, however if you do encounter another issue, feel free to submit a new ticket. Have a nice day! -- Kind regards, [STAFF] On Mon Oct 13 23:11:20 2025, [USER] wrote: Hi [STAFF], I have seen this error a few times, but I did not note the node on which it occurred. I will do that the next time I notice it. You are right about 'some of the more recent .err/.out files in the working directory ... running fine recently.' I actually restarted my machine, and SSHed into CHIP again, and got it to run. It could very well be a node issue. In the meantime, please keep the ticket open so that we can address the issue the next time it pops up. For the Office Hours tomorrow, if I still don't observe the same issue, I was wondering if we can do it anyway to discuss a new thing I'm trying to do - open two jupyter notebooks on chip (through two tunnels) using a single jupyter.slurm file from a single folder. Is that possible? Or do we need two jupyter.slurm files for that? Thank you. Regards, [USER] On Mon, Oct 13, 2025 at 12:36 PM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3290086 > Last Update From Ticket: Hi [USER], Based on the error you attached as a screenshot, it appears that the system was unable to find the binary for jupyter-lab. This binary is provided via the Anaconda3 package, which is loaded at the start of the Jupyter SBATCH file. The PATH for this module is also already present in your PATH env variable. This is as expected, however it is still unable to find the binary. Do you happen to know what node the job was running on? It is possible that this could be due to a specific node. Is this error reproducible? Based on some of the more recent .err/.out files in your working directory, it seems like the notebooks have been running fine recently. Let me know! -- Kind regards, [STAFF] On Sat Oct 11 12:40:16 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUS_ID] Request Type: High Performance Cluster Hello, I am referring to this guide - https://umbc.atlassian.net/wiki/spaces/faq/pages/1104805915/How+do+I+run+a+new+jupyter+notebook+on+chip - to run 'an existing jupyter notebook on chip' in the '/umbc/rs/pi_slaha/users/[USER]/astrophysics-anom_det' folder. I am stuck on Step 7 - i.e. in the .err file, I am not able to see the required section. I cannot find the line that starts with “http://127.0.0.1”. What I see instead is attached as a screenshot. It says the following: /usr/bin/which: no jupyter-lab in (/usr/ebuild/installs/software/Anaconda3/2024.02-1:/usr/ebuild/installs/software/Anaconda3/2024.02-1/sbin:/usr/ebuild/installs/software/Anaconda3/2024.02-1/bin:/usr/ebuild/installs/software/Anaconda3/2024.02-1/condabin:/cm/shared/apps/git/2.33.1/bin:/cm/shared/apps/slurm/current/sbin:/cm/shared/apps/slurm/current/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/[USER]) /cm/local/apps/slurm/var/spool/job456706/slurm_script: line 63: jupyter-lab: command not found Can you please help fix this? Thank you. Regards, [USER]"
"3290310","2025-10-13 16:06:20","HPC User Account: [USER] in [ID]_electric_fish_project","Hi [USER], Welcome to chip, UMBC's High Performance Computing Cluster! The group, pi_[STAFF], now exists on the chip cluster. Members of this group can access and contribute to the research storage space allocated to the group. Additionally, I created user accounts for you, and the two students requested. This storage space is located at /umbc/rs/pi_[STAFF], and currently has a quota of 10T. For information on accessing the cluster, adding accounts to your group, and getting started using the cluster, check out the tutorial on our wiki: https://[DOMAIN].atlassian.net/wiki/x/R4BPQg Additional documentation is also available here: https://[DOMAIN].atlassian.net/wiki/x/FwCHQ For the software, Deep Lab Cut, you should be able to install it via a Conda environment. Here is a wiki page for getting started with anaconda on chip: https://[DOMAIN].atlassian.net/wiki/x/LYCPPQ If you have any questions or issues, please submit a new RT ticket at: https://doit.[DOMAIN]/request-tracker-rt/doit-research-computing/ First Name: [USER] Last Name: [STAFF] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: High Performance Cluster Create/Modify account in existing PI group Existing PI Email: [EMAIL] Existing Group: [GROUP] Project Title: Computational Mechanisms for State-Driven Active Sensing Project Abstract: Dr. [STAFF] and an undergraduate student supervised by Dr. [STAFF] will perform the research for the project “Computational Mechanisms for State-Driven Active Sensing” at UMBC. The overall goal of the project is to understand sensing and information gathering behaviors through experiments with electric fish. We need to use software called Deep Lab Cut https://deeplabcut.github.io/DeepLabCut/docs/installation.html to analyze experimental data. As I understand it, the first step is to use Deep Lab Cut on a laptop to recreate an *.eml file. That file would then be used within a batch job on a GPU computer to generate other files. The undergraduate students working on this project are [USER] and [USER]."
"3291129","2025-10-13 21:58:15","HPC User Account: [USER] in Student Group","This has been fixed. Please try again and let us know if there is still a permission denied issue. -- [STAFF]"
"3291162","2025-11-04 16:40:26","HPC Other Issue: change with Intel C compiler?","You shouldn't need to recompile on each node, just each partition since each partition is using the same cpu architecture. On Tue Nov 04 11:11:19 2025, [STAFF] wrote: So, lets say i want to run the ver 1.4 on every node in the cluster, to test performance. Do you think I should recompile it on each node? or just run the same executable?"
"3291564","2025-10-16 19:35:40","HPC Other Issue: g24-12 issue","Hi [USER], We've confirmed the machine is functioning normally again. Thank you for the email! On Tue Oct 14 12:18:39 2025, [STAFF] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: High Performance Cluster One gpu on g24-12 is not working. attached screenshot. Attachment 1: Screenshot 2025-10-14 at 12.17.01.png -- Best, [STAFF] DOIT HPC System Administrator"
"3293453","2025-10-15 17:13:25","HPC Other Issue: [USER]'s jobs not running on Chip today","My jobs are now running. There must have been a very big job using up computing power and delaying mine more than usual. You can disregard this issue. Thanks! Hi, I'm using the same code that has been running fine the past few weeks but today my jobs are sitting in Chip and not running. Is there a migration going on? Or another reason that would prevent me from accessing pi_gobbert nodes like I usually do? I can't run on the general nodes either. Thank you! [USER]$ squeue -u [USER] CLUSTER: chip-cpu JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 526469 general weather [USER] PD 0:00 1 (Priority) 517473 match temp [USER] PD 0:00 1 (Priority) 526413 match cal [USER] PD 0:00 1 (Priority) 526432 match dhsregs [USER] PD 0:00 1 (Priority)"
"3293474","2025-10-15 17:09:11","CMake issue on Cluster (Office Hours Follow up)","Hello [USER], I hope this email finds you well. Thank you for working with us during our office hours' time at 12:15-12:45. We wanted to provide you a step-by-step guide on how to use CMake so you are able to use this moving forward. Please also refer to the documents below that talk about 'How to run an interactive job', 'SBATCH', and information on how to use modules. 1. https://[DOMAIN].atlassian.net/wiki/spaces/faq/pages/[PAGEID] 2. https://[DOMAIN].atlassian.net/wiki/spaces/faq/pages/[PAGEID]/Basic+Slurm+Commands#sbatch 3. https://[DOMAIN].atlassian.net/wiki/spaces/faq/pages/[PAGEID]/How+to+use+modules 4. Here is also attached the Cmake documentation for any issues you might have when building (https://cmake.org/cmake/help/latest/guide/tutorial/index.html) When using CMake: 1. Log into [SERVER] 2. run an interactive job using the srun command (This can be found in the 1st article we linked) 3. module load <modulename> (Cmake) 4. Optional (if you would like to view the current modules being ran you can run <module list>) 5. Start working with cmake :) If you have any issues, please reach back out! Best regards, [STAFF]"
"3293526","2025-10-21 16:00:08","HPC Slurm/Software Issue: Conda environment","The text appears to be an email or a ticketing system conversation about an issue with the High-Performance Computing (HPC) environment at a research institution. Here's a breakdown of the content:  **Initial Message**  A user, likely a researcher, reports an issue with the Conda environment not being applied consistently across all nodes in the HPC cluster after loading it. They provide detailed steps to reproduce the problem and include error messages.  **Response from Max Breitmeyer**  Max Breitmeyer, a DOIT HPC System Administrator, responds to the user's message. He asks for clarification on how the environment is initialized across nodes in the current cluster setup and suggests that the issue might be related to this process. He also mentions that there has been an increase in issues since the storage migration.  **Additional Context**  The conversation includes additional context about the HPC environment, such as:  * The user's job submission command using `srun` * The modules loaded (`GRASS/8.2.0-foss-2021b` and `Anaconda3/2024.02-1`) * The Conda environment activation command * Error messages indicating that only one node can use the `fiona` library, while others return a `ModuleNotFoundError`  **Ticketing System Information**  The conversation includes ticketing system information, such as the ticket ID (`#3293526`) and links to view open tickets.  Overall, the conversation is about troubleshooting an issue with the Conda environment in an HPC cluster, and Max Breitmeyer is trying to help the user resolve the problem by understanding how the environment is initialized across nodes."
"3294124","2025-10-16 18:03:42","HPC User Account: [USER] in FSI","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_dli. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: High Performance Cluster Create/Modify account in existing PI group Existing PI Email: [EMAIL] Existing Group: FSI Project Title: Sleep Disorder Monitoring Project Abstract: This project aims to create a sleep stage monitoring system that utilizes contact-free multi-modal data. The long term goal is to extend the system to be able to detect and monitor sleep disorders. By using cutting-edge signal processing and machine learning methods, the system will be capable of providing precise and non-invasive sleep pattern analysis. In order to make the system scalable and capable of real-time execution, the ultimate solution will be refined for deployment on edge devices. I am requesting access to a GPU cluster to help experiments on contact-free, multi-modal sleep stage monitoring and sleep disorder detection. This is a project to create and train machine learning models that will be used on edge devices."
"3295053","2025-10-20 18:36:23","HPC User Account: [ID] in FSI","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_dli. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Fri Oct 17 15:50:27 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [ID] Request Type:              High Performance Cluster Create/Modify account in existing PI group Existing PI Email:    [EMAIL] Existing Group:       FSI Project Title:        Sleep Disorder Monitoring Project Abstract:     This project aims to create a sleep stage monitoring system that utilizes contact-free multi-modal data. The long term goal is to extend the system to be able to detect and monitor sleep disorders. By using cutting-edge signal processing and machine learning methods, the system will be capable of providing precise and non-invasive sleep pattern analysis. In order to make the system scalable and capable of real-time execution, the ultimate solution will be refined for deployment on edge devices.  I am requesting access to a GPU cluster to help experiments on contact-free, multi-modal sleep stage monitoring and sleep disorder detection. This is a project to create and train machine learning models that will be used on edge devices."
"3295071","2025-10-20 18:33:56","HPC User Account: [USER] in Machine Learning for Signals Processing Lab","Hi [USER], your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: High Performance Cluster Create/Modify account in existing PI group Existing PI Email: [STAFF]@umbc.edu Existing Group: Machine Learning for Signals Processing Lab Project Title: Multivariate feature selection for fMRI analysis Project Abstract: fMRI has become a widely used imaging tool for exploring the normal neural functions as well as disordered brain functions like schizophrenia. Among all fMRI data analysis strategies, data-driven-based methods have a unique advantage of capturing the whole picture of available information since they effectively minimize assumptions imposed on the brain activity. With the increasing number of multimodal data and multisite data, the problem of balancing the computation cost and analysis performance is becoming more important than ever before. In this project, our interest is in identifying the most informative multivariate features when analyzing multiple fMRI datasets. Our goal is the development of flexible new decomposition methods as well as identifying the best feature extraction strategy for a given problem."
"3295672","2025-10-21 13:24:05","Request for Additional Shared Storage (2TB)","Hi [USER], As I write this, your shared research group volume is using 0% of its 25TiB of space. This volume is located at /umbc/rs/pi_dli. I'll mark this as resolved for now. See this wiki page for additional information: https://umbc.atlassian.net/wiki/spaces/faq/pages/1072267344/Storage On Mon Oct [DATE] [TIME], [STAFF] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              Help with something else Dear HPCF Team, I am a member of the FSI research group led by [STAFF]. For our project titled 'Sleep Disorder Monitoring', we are currently working with large datasets and require an additional 2TB of shared storage to support our ongoing research activities. We would appreciate your support in accommodating this request. Thank you for your understanding. Best regards, [USER]"
"3295788","2025-10-20 19:08:52","Requesting Additional Shared Storage (2TB)","Hi [USER], As I write this, the pi_dli group has used 0% of its 25TiB allocation on chip. The root for this storage volume is located at /umbc/rs/pi_dli . Let me know if you have any questions about this, but I'll resolve this request for now. On Mon Oct 20 14:51:37 2025, [STAFF] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              Help with something else Dear HPCF Team, I am a member of the FSI research group led by [STAFF]. For our project titled 'Sleep Disorder Monitoring', we are currently working with large datasets and require an additional 2TB of shared storage to support our ongoing research activities. We would appreciate your support in accommodating this request. Thank you for your understanding. Best regards, [USER]"
"3296672","2025-10-23 16:45:16","HPC User Account: [USER] in pi_[ID]","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Tue Oct 21 13:22:34 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: High Performance Cluster Create/Modify account in existing PI group Existing PI Email: [STAFF]@umbc.edu Existing Group: pi_[STAFF] Project Title: Coupled Groundwater–Surface Water Modeling of Baltimore Using ParFlow.CLM Project Abstract: This study develops high-resolution ParFlow.CLM models for urban watersheds in Baltimore, Maryland, to investigate the interactions between groundwater and surface water under recent climatic conditions."
"3296766","2025-10-23 18:21:16","HPC Other Issue: [USER] running unexpectedly slow","Hi [USER], There were some issues with flags that were automatically generating incorrect slurm variables, which may have been causing the slowness. Please try it again and let us know if it seems faster. On Wed Oct 22 13:44:19 2025, [STAFF] wrote: Hi [STAFF], I followed the instructions on the HPCF wiki page (https://umbc.atlassian.net/wiki/spaces/faq/pages/1408335873/Running+an+LLM+using+Ollama+on+chip) and simply ran the example on that page. It took more than 30minutes to generate only 2 words. I used an interactive job to run the OLLAMA. I even tried to use the GPU partition for [SERVER]. It was a little faster but still super slow. I share the ollama_server.log file in the attachment for your reference. Thank you in advance for your help. Best regards, [USER] On Tue Oct 21 15:20:39 2025, [STAFF] wrote: Hi [USER], I need some more information about your LLM job. Can you please provide your slurm script, and working directory? Are you working with a data, if so how big is it and where is it located? How much time do you expect the job to take ? On Tue Oct 21 14:18:03 2025, [STAFF] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster User [USER] reports following ollama setup according to: https://umbc.atlassian.net/wiki/spaces/faq/pages/1408335873/Running+an+LLM+using+Ollama+on+chip User reports that running LLMs via ollama is running unexpectedly slow. Please follow-up with user to replicate the issue and perhaps set a meeting to understand the issue and to resolve. Best, [STAFF] DOIT Unix infra, Graduate Assistant"
"3296910","2025-10-30 18:44:55","HPC Other Issue: Running Hspice software on HPCF","Yes, thanks for all help. Regards, [USER] On Thu, Oct 30, 2025 at 2:11 PM Max Breitmeyer via RT <[EMAIL]> wrote: Ticket Last Update From Ticket: Hi all, Is it fair to say this ticket can now be closed? On Thu Oct 30 12:57:01 2025, [USER] wrote: Thanks a lot [STAFF]. Both hspice and waveviewer work. Regards,[USER] On Wed, Oct 29, 2025 at 5:01 PM [STAFF] <[EMAIL]> wrote: Wave view should also be working now, give it a try when you have a chance. ---[STAFF] Specialist, Linux System Administrator & Lab Technical Support On Oct 29, 2025, at 16:12, [USER] <[EMAIL]> wrote: Hi [STAFF], Thanks. Hspice works but we also need WV (waveviewer) to see Hspice results as well. Could you please add that also and let us know to check? Thanks a lot,[USER] On Wed, Oct 29, 2025 at 2:30 PM [STAFF] via RT <[EMAIL]> wrote: Ticket Last Update From Ticket: On the HPC cluster, it should now be possible to run hspice using the command: /umbc/software/csee/scripts/launch_synopsys_hspice.sh Please test it and let me know if it works. Also, let me know what other tools you need to use, if any, so I can adapt those scripts as well."
"3298342","2025-10-23 15:52:23","HPC Other Issue: 5-days time limit not enough for jobs","Hi [USER], Unfortunately, under the current cluster model only contributing groups have access to the 'shared' QOS option. In your situation, I would recommend seeing if you can optimize your code to run faster, in parallel, or across multiple nodes. You can also attempt to break apart your one large job into two smaller sets of jobs, which would allow you to complete the run in less than 5 days. Let me know if you have any additional questions! Have a nice day! Kind regards, [STAFF] On Thu Oct 23 10:09:11 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [ID] Request Type:              High Performance Cluster Hello all! I'd like to ask your help with the jobs I'm submitting in chip cluster. A few of them are pretty heavy, and the 5-days limit to run are not being enough. I saw there's the possibility to run in the partition 'shared', with 14-days limit, but I don't have access to that. Is it possible to give me access to that partition, or maybe to increase the time limit in the partition 'general'? In general my samples run within the 5 days, I would require more time only for samples whose jobs abort after 5 days. Thank you! Best, [USER]"
"3298471","2025-10-23 16:21:05","HPC Other Issue: [USER] cannot login to their ada account","Many Thanks [STAFF], Regards, [USER] On Thu, Oct 23, 2025 at 12:18 PM [STAFF] via RT <[EMAIL]> wrote: If you agree your issue is resolved, please give us feedback on your experience by completing a brief satisfaction survey: https://umbc.us2.qualtrics.com/SE/?SID=SV_[generic id]&customeremail=[EMAIL]&groupid=EIS&ticketid=[ID]&ticketowner=[STAFF EMAIL]&ticketsubject=HPC Other Issue: I cannot login to my ada account If you believe your issue has not been resolved, please respond to this message, which will reopen your ticket. Note: A full record of your request can be found at: Ticket https://rt.umbc.edu/Ticket/Display.html?id=[ID] Thank You Hi [USER], The hpc cluster 'ada' has been completely absorbed by 'chip' as of April 10th. Please see this myumbc posting detailing the rollout of chip and the deletion of ada. https://my3.my.umbc.edu/groups/hpcf/posts/147513 On Thu Oct 23 12:01:48 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster Hello, I’m currently unable to access my ADA account. When I try to connect via SSH using the command below: ssh [USER]@ada.rs.umbc.edu I receive the following error message and do not even get the password prompt: ssh: connect to host ada.rs.umbc.edu port 22: Connection timed out Could you please help me resolve this issue? Thank you, [USER] Best, [STAFF] DOIT HPC System Administrator Original Request: Requestors: [USER] First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster Hello, I’m currently unable to access my ADA account. When I try to connect via SSH using the command below: ssh [USER]@ada.rs.umbc.edu I receive the following error message and do not even get the password prompt: ssh: connect to host ada.rs.umbc.edu port 22: Connection timed out Could you please help me resolve this issue? Thank you, [USER]"
"3298811","2025-10-24 15:50:45","HPC User Account: [USER] in H.A.R.M.O.N.I. Lab","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://[EMAIL]/wiki/spaces/faq/pages/[NUMBER]/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://[EMAIL]/request-tracker-rt/[NUMBER]/) -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Thu Oct 23 16:30:39 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster Create/Modify account in existing PI group Existing PI Email: [EMAIL] Existing Group: [GROUP] Project Title: Multimodal Stock Price Direction Prediction Project Abstract: This project, Multimodal Stock Price Direction Prediction Using Historical Prices, News, and Sentiment, aims to predict short-term stock movements up, down, or stable by integrating numerical and textual financial data. The approach combines historical stock price features with sentiment analysis of financial news to capture both market trends and investor sentiment. Historical price data are collected from Yahoo Finance, while relevant news articles and headlines are sourced from the News API or GDELT. Sentiment scores are derived using FinBERT, a transformer-based model optimized for financial text, and aggregated on a daily basis. These sentiment features are then merged with technical indicators such as moving averages, relative strength index (RSI), and multi-day returns. The resulting multimodal dataset is labeled according to future price direction and used to train classification models including Random Forest, XGBoost, and Multilayer Perceptrons. Model performance is evaluated using accuracy, F1 score, and confusion matrices to compare the predictive power of price-only, sentiment-only, and combined feature sets. The study explores both early and late fusion approaches to integrate modalities and applies feature importance analysis (e.g., SHAP) to interpret model behavior. The findings aim to demonstrate how combining quantitative and qualitative signals can improve stock trend forecasting and provide insights into the interplay between market sentiment and price dynamics. N/A Attachment 1: Project Guide_ Multimodal Stock Price Direction Prediction.pdf"
"3299581","2025-10-28 13:33:04","HPC User Account: [ID] in FSI","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_dli. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) On Sat Oct 25 12:59:22 2025, [STAFF] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster Create/Modify account in existing PI group Existing PI Email:    [EMAIL] Existing Group:       FSI Project Title:        Sleep Disorder Monitoring Project Abstract:     This project aims to create a sleep stage monitoring system that utilizes contact-free multi-modal data. The long term goal is to extend the system to be able to detect and monitor sleep disorders. By using cutting-edge signal processing and machine learning methods, the system will be capable of providing precise and non-invasive sleep pattern analysis. In order to make the system scalable and capable of real-time execution, the ultimate solution will be refined for deployment on edge devices.  I am requesting access to a GPU cluster to help experiments on contact-free, multi-modal sleep stage monitoring and sleep disorder detection. This is a project to create and train machine learning models that will be used on edge devices. Best, [STAFF]"
"3299984","2025-10-28 13:43:27","HPC User Account: [USER] in Quantum Thermodynamics Group","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUS_ID] Request Type: High Performance Cluster Create/Modify account in existing PI group Existing PI Email: [EMAIL] Existing Group: [GROUP_NAME] Project Title: [PROJECT_TITLE] Project Abstract: [PROJECT_ABSTRACT] Dear HPC team, I would like to request HPC access to execute quantum simulation algorithms. My advisor, Dr. [STAFF], already has group access on the system, and I would like to be added to the group. Please let me know if you need anything else from me to complete my application. Thank you, [USER]"
"3300174","2025-10-28 13:29:02","HPC User Account: [USER] in Deffner","Hi [USER], You weren't able to log in because you didn't have a chip account. Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is [GROUP]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://[DOMAIN]/wiki/spaces/faq/pages/[PAGEID]/Getting+Started+on+chip. Please read through the documentation found at hpcf.[DOMAIN] > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link: (https://[DOMAIN]/request-tracker-rt/[TRACKERID]). On [DATE], [STAFF] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID]  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    [EMAIL] Existing Group:       [GROUP] Project Title:        [PROJECTTITLE] Project Abstract:     N/A. I am unable to logon to Chip notes. Following the access instructions, I get to the point where I am prompted for my password, but it does not accept the password."
"3300690","2025-11-04 18:52:46","Data upload and server processing","Hi [USER], Welcome to chip, UMBC's High Performance Computing Cluster! The group, usda-eb, now exists on the chip cluster. Members of this group can access and contribute to the research storage space allocated to the group. This storage space is located at /umbc/rs/usda-eb, and currently has a quota of 50T. For information on accessing the cluster, adding accounts to your group, and getting started using the cluster, check out the tutorial on our wiki: https://umbc.atlassian.net/wiki/x/R4BPQg Additional documentation is also available here: https://umbc.atlassian.net/wiki/x/FwCHQ If you have any questions or issues, please submit a new RT ticket at: https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research Computing Currently, [STAFF] is the 'owner' of the group, and is the only member of the group. To request users to be added to your group, please submit an RT ticket from the following link: https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research Computing Additionally, I will need a little bit more information regarding your grant. Could you please provide an award number, title, and abstract? If you have any additional concerns, questions, or run into any problems, please feel free to submit a new ticket! Have a great day! On Tue Nov 04 09:32:16 2025, [USER] wrote: Maybe we can use the following name - 'USDA-EB' for the cluster. On Tue, Nov 4, 2025 at 9:26 AM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3300690> Last Update From Ticket: Hi, Just checking back in with you guys. Any specific name for the Center that we should use for the cluster? Best, [STAFF]"
"3301107","2025-10-29 17:44:30","HPC User Account: [ID] in [USER]","Hi [STAFF], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://[EMAIL]/wiki/spaces/faq/pages/[NUMBER]/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link (https://[EMAIL]/request-tracker-rt/[STAFF]-research-computing/). You can also view your project information here on our wiki (https://hpcf.umbc.edu/libraries/research-projects-hpcf/?preview_id=[NUMBER]&preview_nonce=[STRING]&preview=true) Best regards, [STAFF]"
"3301187","2025-10-31 17:12:23","HPC New Group: [ID]","Hi [USER], Welcome to chip, UMBC's High Performance Computing Cluster! The group, pi_[STAFF], now exists on the chip cluster. Members of this group can access and contribute to the research storage space allocated to the group. This storage space is located at /umbc/rs/pi_[STAFF], and currently has a quota of 25T. For information on accessing the cluster, adding accounts to your group, and getting started using the cluster, check out the tutorial on our wiki: https://[DOMAIN]/wiki/x/R4BPQg Additional documentation is also available here: https://[DOMAIN]/wiki/x/FwCHQ If you have any questions or issues, please submit a new RT ticket at: https://rtforms.[DOMAIN]/rt_authenticated/doit/DoIT-support.php?auto=Research%20Computing First Name: [USER] Last Name: [STAFF] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster Group Type: Project Title: Towards Designing for Resilience: Community-Centered Deployment of an AI Business Planning Tool in a Feminist Makerspace Project Abstract: Entrepreneurs in resource-constrained communities often lack the time and support to translate ideas into actionable business plans. While generative AI promises assistance, most systems assume high digital literacy and overlook community infrastructures that shape adoption. We report on the community-centered design and deployment of BizChat, an LLM-powered tool for business plan development, introduced across four workshops at a feminist busi- ness incubator and makerspace. BizChat was designed to center entrepreneurs' knowledge and workflows while providing just-in-time micro-learning and low-floor-high-ceiling accessibility. Through system log data (N=30) and semi-structured interviews (N=10) with entrepreneurs, we show how the design and deploy- ment of BizChat with existing community contexts lowered bar- riers to accessing capital, encouraged reflection, and empowered entrepreneurs to support AI-literacy within their own communities. We contribute insights into how AI tools can be deployed within local support networks, and implications for design that strengthen community resilience amid rapid technological change."
"3301564","2025-10-30 18:16:49","HPC Other Issue: very slow editing on /home/[USER] on c24-52","Hi [USER], OK all logged out of chip and our machine -[STAFF] On Thu, Oct 30, 2025 at 2:10 PM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3301564> Last Update From Ticket: Hi [USER], Please exit out of /umbc/xfs2/strow on all devices where you may be logged in. We can't unmount the device until it's no longer active. On Thu Oct 30 13:44:40 2025, [USER] wrote: Hi [STAFF] Thanks for the reply. Maybe you should also cc [STAFF] as you all work on this ([EMAIL]) [USER] On Thu, Oct 30, 2025 at 1:30 PM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3301564> Last Update From Ticket: [USER], There is currently some degradation on the file system that sits on xfs2. We're investigating into how to resolve, but out of an abundance of caution, we are going to unmount xfs2 so that the degradation doesn't spread. We'll let you know when we have more information for you, but expect to not have access to your mount for a while. On Thu Oct 30 13:23:27 2025, [USER] wrote: Hi [USER], We are aware of the issue and are currently investigating. I'm going to merge this with the other ticket since they are related. On Thu Oct 30 12:53:30 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [USER] Request Type: High Performance Cluster Have the disk issues been worked on yet (ticket #3301564)? I still can't edit files And now Matlab took about 3 minutes to start up on c24-52 There is something seriously wrong! Thanks Sergio"
"3302104","2025-10-31 15:07:37","HPC Other Issue: Need cuDNN 8.4.1.50 module on [SERVER]","I have installed the module cuDNN/8.4.1.50-CUDA-11.7.0, let us know if you still unable to find the module. On Wed Oct 29 14:22:27 2025, [STAFF] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: High Performance Cluster Hi, I need to work on TensorFlow 2.11 which using CUDA 11.7 version and cuDNN 8.4. I saw CUDA 11.7, however there is no cuDNN 8.4. The previous cluster (ADA) have module 'cuDNN/8.4.1.50-CUDA-11.7.0'. Can you install that module on CHIP also? Thank you so much, [USER] Best, [STAFF]"
"3304171","2025-10-30 17:23:27","HPC Other Issue: so slooooooooow, can't get anything done","Hi [USER], We are aware of the issue and are currently investigating. I'm going to merge this with the other ticket since they are related. On Thu Oct 30 12:53:30 2025, [STAFF] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              High Performance Cluster Have the disk issues been worked on yet (ticket #3301564)? I still can't edit files And now Matlab took about 3 minutes to start up on c24-52 There is something seriously wrong! Thanks [USER]"
"3304395","2025-10-31 19:53:50","HPC New Group: [ID]","Hi [USER], Welcome to chip, [ORGANIZATION]'s High Performance Computing Cluster!  The group, pi_[STAFF], now exists on the chip cluster. Members of this group can access and contribute to the research storage space allocated to the group. This storage space is located at /umbc/rs/pi_[STAFF], and currently has a quota of 10T. For information on accessing the cluster, adding accounts to your group, and getting started using the cluster, check out the tutorial on our wiki: https://[ORGANIZATION].atlassian.net/wiki/x/R4BPQg An account for the PI ([USER]) has been created. To add additional users to the group, please submit a new add user ticket. Additional documentation is also available here: https://[ORGANIZATION].atlassian.net/wiki/x/FwCHQ If you have any questions or issues, please submit a new RT ticket at: https://rtforms.[ORGANIZATION].edu/rt_authenticated/doit/[DEPARTMENT]-support.php?auto=Research%20Computing On [DATE], [USER] wrote: First Name:                [STAFF] Last Name:                 [STAFF] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID]  Request Type:              High Performance Cluster  Group Type:            Project Title:        Multiscale Behavior Mapping to Decode Neuronal Health and Diseases in Animal Models Project Abstract:     Automated, high-fidelity behavioral phenotyping is essential for uncover the underlying neural circuits and functional perturbations in vivo. Yet current computer-vision tools often miss the subtle, ethologically relevant kinematic motifs that report changes in internal state. In this project, we will build a neuroscience-focused computational pipeline that couples the broad foundation model with the efficient and specialized detector to identify the animal behavior responded to sensory signal inputs. Specifically, we will distill knowledge from Vision Transformers into customized single-state Convolutional Neural Networks (CNNs) and pretrain these networks on a large, unlabeled corpus of murine behavioral videos spanning diverse common behavior experimental assays. The resulting models will be fine-tuned for high-throughput quantification of classified behavior phenotypes/signatures that serve as proxies for cognitive and affective states.  This pipeline will enable experimenters to (i) stratify behavior with ethological precision, (ii) align these phenotypes with neural activities or perturbations by endogenous and environmental factors, and (iii) standardize assays across labs for reproducible neuroscience. Training and optimization demand large-scale data parallelism and extensive hyperparameter searches; thus, access to the [ORGANIZATION] HPC cluster is indispensable. This work will yield a biologically grounded, scalable toolset for objective behavior measurement in animal models, such as mice.     The PI of this project is Dr. [USER], a Professor at Department of Biological Sciences ([EMAIL]). We would like to setup a new group to use HPC and data storage space.  The [STAFF] lab recently published an analysis pipeline for animal behavior and benchmark comparison with other available tools (IntegraPose: A unified framework for simultaneous pose estimation and behavior classification.  https://doi.org/10.1016/j.neuroscience.[DATE]).  We need to scale up the model by training with a larger set of image data using the HPC.  Our project members include [ORGANIZATION] students who are familiar with script coding.  The project title and the abstract are included in this request.  Please let us know if you have any questions. Thank you."
"3304442","2025-11-04 14:26:41","HPC Other Issue: more hardware details","Everything is EDR. On Thu Oct 30 21:44:22 2025, [USER] wrote: Hi, [STAFF], Thanks for the Dell model. Yes, I read that Wiki page. My request is to know more precisely than 'high speed backend Infiniband network supporting 100 Gbps'. That phrase is well-written to cover all nodes from 2018 to 2024. But is the 2024 network not a newer one? Purchased in 2024? Either way, I am asking for a technical term like DDR = dual data rate, QDR = quad data rate, EDR = extended data rate, like I am recalling from the past. Does the 2024 portion's network not have a phrase like that with it? On Thu, Oct 30, 2025 at 7:03 PM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=33404442 > Last Update From Ticket: Hi [USER], The model for the nodes used in the 2024 are PowerEdge R660. Specifications on the network abilities and cpus themselves can be found here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1289486353/Cluster+Specifications On Thu Oct 30 16:07:02 2025, [USER] wrote: > First Name: [USER] > Last Name: [USER] > Email: [EMAIL] > Campus ID: [USER] > Request Type: High Performance Cluster > Hi, I would like some more fine points of hardware information. I am referring to the 2024 portion of chip. - CPU: what is the model number of the node from Dell for the 2024 compute nodes? - The 2024 CPU nodes are connected by which InfiniBand? EDR? Some specs available? Best, [STAFF] DOIT HPC System Administrator"
"3304534","2025-10-30 22:55:41","HPC Other Issue: Reset [USER]'s Password","Hi [USER], Your password is linked to your umbc account. So you would just need to use the myumbc reset password. https://umbc.atlassian.net/wiki/spaces/faq/pages/30736467/I+have+forgotten+my+myUMBC+password.+What+should+I+do Note that it may take a minute to update on chip. On Thu Oct 30 17:52:57 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster Dear Sir/Madam, I would like to access the chip server, but after logging in with my ID “[USER]”, my password does not work. I am not sure of the reason. Could you please provide me with a link or instructions to reset my password? Thank you very much for your assistance. Regards, [USER] ID: [CAMPUSID] Best, [STAFF] DOIT HPC System Administrator"
"3304703","2025-10-31 14:44:35","HPC Other Issue: Request for sample SLURM job script to run WRF on CHIP","Hi [USER], First off, I wanted to clarify a couple points. 1. 'I was advised that running ./wrf.exe interactively is not permitted': This is untrue. You are able to run interactive jobs, where you create a slurm allocation on a node, then connect to it and run your code directly. You are not permitted to run jobs on the login node, which I believe is what you are thinking of. If you would like some documentation on how to run an interactive job, I'll include a link to a page with the commands required to do that. How to run interactive job: https://umbc.atlassian.net/wiki/x/CYCbQw 2. There is also a WRF module available to be loaded on chip. You can load WRF with: 'module load WRF/4.4-foss-2022a-dmpar'. WPS is also available, and can be loaded with: 'module load WPS/4.4-foss-2022a-dmpar'. These installations should work, however it is a slightly older version (4.4), whereas you compiled a newer one (4.5). 3. For your locally installed and compiled copy of WRF, if you desire to use it you must use the compiled binaries located under /umbc/rs/pi_cichoku/users/[USER]/model2/sources/WRF/main. This is where the actual WRF binaries are stored, the directory you provided was a test directory for testing the software. You will want to add this location to your PATH environment variable to utilize the binaries. It is also possible, depending on how your install was compiled, that it would need to be recompiled for MPI support. For more info, check out the Building WRF section of this page: https://www2.mmm.ucar.edu/wrf/OnLineTutorial/compilation_tutorial.php We do not have any existing documentation on running WRF/WPS explicitly, however we have documentation on running MPI jobs, including some example SBATCH scripts: https://umbc.atlassian.net/wiki/x/AQCKV There is documentation on WRF's website for running WRF using mpi, take a look here: https://www2.mmm.ucar.edu/wrf/users/wrf_users_guide/build/html/running_wrf.html If you have any additional questions or need clarification, please feel free to let me know. For now I will mark this as resolved. Have a good day! -- Kind regards, [STAFF] DoIT Unix Infra Student Worker"
"3305879","2025-11-04 20:24:14","HPC Other Issue: Not able to login to the cluster (chip) through terminal","Hi [USER], Apologies for late reply Yes, still I face the issue and I tried with campus vpn and also my personal wifi. Both are giving the same problem. Could you help me in checking the issue. Thankyou for your help.  Regards, [STAFF]. Campus ID: [CAMPUS_ID] On Mon, Nov 3, 2025 at 11:01 AM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3305879 > Last Update From Ticket: Hi [USER], Is this still an issue? We haven't seen any other reports of people being unable to login. Are you on campus vpn? What sort of internet connection do you have (wifi or wired)? On Fri Oct 31 15:04:48 2025, [CAMPUS_ID] wrote: First Name: [USER] Last Name: [STAFF] Email: [EMAIL] Campus ID: [CAMPUS_ID] Request Type: High Performance Cluster Previously, I was able to login through the cluster (chip) but suddenly the login to cluster is not working. Attachment 1: Screenshot 2025-10-31 at 3.03.21PM.png Best, [STAFF] DOIT HPC System Administrator"
"3305960","2025-11-04 16:29:06","HPC User Account: [USER] in [STAFF]","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research%20Computing) Kind regards, [STAFF] DoIT Unix Infra Student Worker On Fri Oct 31 16:25:15 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: High Performance Cluster Create/Modify account in existing PI group Existing PI Email: [EMAIL] Existing Group: pi_[STAFF] Project Title: A Human-Centered Approach to Building Generative AI System for Small Business Owners Project Abstract: Entrepreneurs in resource-constrained communities often lack the time and support to translate ideas into actionable business plans. While generative AI promises assistance, most systems assume high digital literacy and overlook community infrastructures that shape adoption. We report on the community-centered design and deployment of BizChat, an LLM-powered tool for business plan development, introduced across four workshops at a feminist business incubator and makerspace in a city. BizChat was designed to center entrepreneurs’ knowledge and workflows while providing just-in-time micro-learning and low-floor-high-ceiling accessibility. Through system log data (N=30) and semi-structured interviews (N=10) with entrepreneurs, we show how the design and deployment of BizChat with existing community contexts lowered barriers to accessing capital, encouraged reflection, and empowered entrepreneurs to support AI-literacy within their own communities. We contribute insights into how AI tools can be deployed within local support networks, and implications for design that strengthen community resilience amid rapid technological change. I am a phd student and my advisor is [STAFF]. I am requesting an account in order to join this existing group."
"3306147","2025-11-01 17:20:44","HPC Slurm/Software Issue: [USER] Cannot Cancel Their Jobs","Understood and no problem. I've cancelled all of your jobs submitted to the match partition that were pending. When canceling your own jobs, be sure to specify the cluster with something like `scancel -M chip-cpu JOBID`. On Sat Nov 01 12:18:16 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              High Performance Cluster Hello, I am having an issue with chip where I cannot cancel a number of jobs that are in queue. I used a script to create and submit the jobs last night but realized that I had a small mistake. I have tried to cancel all jobs under my username, [USER], and cancel specific jobs by id but they remain in the queue. The jobs have remained in the queue for more than 12 hours since I first tried to cancel them. All pending jobs on the match partition under my username need to be cancelled."
"3306171","2025-11-04 16:38:35","HPC Other Issue: software request","Hi [USER], The modules, RSEM and BOWTIE2 have been installed on chip! I installed RSEM/1.3.3-foss-2022a, and Bowtie2/2.4.5-GCC-11.3.0. These can be loaded with 'module load RSEM/1.3.3-foss-2022a' (note, RSEM depends on BOWTIE2, so the correct version of BOWTIE2 is loaded when you load RSEM). If you encounter any issues using the modules, or need any additional modules, feel free to submit a new ticket! Have a great day! -- Kind regards, [STAFF] On Sat Nov 01 13:50:40 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster Is it possible to request modules/packages to be added to the cluster? I would like to be able to use RSEM and BOWTIE2. Thanks!"
"3306427","2025-11-03 16:57:43","HPC Slurm/Software Issue: SSH connection failing","Since the load balancer issue was fixed, we have not seen any other reports of this issue. I am currently testing myself, and have not encountered any disconnects. Would you mind sharing a bit more about your setup? How are you sshing (ie, a dedicated client like putty, or just through your terminal)? Do you have any local SSH configuration? Are you logged in using GlobalProtect VPN? Let me know and I can look into this further for you! Have a nice day! -- Kind regards, [STAFF]. On Sun Nov 02 19:04:14 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUS_ID] Request Type: High Performance Cluster. I have a few ssh sessions open connecting to chip and they keep disconnecting every few minutes. Previously, this had been a problem and it was resolved (something to do with a load balancer) but its back now."
"3306608","2025-11-03 15:16:42","HPC Slurm/Software Issue: Slurm Not Loading Anaconda Module - Job ID 113232-113234","Hi [USER], There appears to have been a minor configuration error with the node that job was running on (g24-11). This has now been resolved. If you continue to experience issues, please feel free to submit a new ticket! Have a nice day! -- Kind regards, [STAFF] On Mon Nov 03 09:47:01 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster I went to run a job this morning that I have run quite often. The first request for this job (ID 113231) started just fine. Subsequent requests error out in my Python script indicating issues loading modules from my Conda environment. The job log shows that 'conda' is not a known command, indicating an issue loading the Anaconda module."
"3308276","2025-11-03 18:12:12","HPC Other Issue: Data missing from '[ID]/rs/zzbatmos/users/[USER]'","Hi [USER], Currently, the research volume for pi_zzbatmos is undergoing migration to a new storage server (ceph). This date was scheduled with your PI in advance, and during the time of migration we request that all users in the group do not use chip to avoid disturbing your groups migration. Your PI should have more information regarding the specifics. However, since pi_zzbatmos has a very large research volume, the migration is taking a long time (it was started on Friday, and is still going). After the migration properly completes, you should have access to your data. Something that you already noticed, the new Ceph research volumes start with 'pi_'. For example, the old volume was mounted at '/umbc/rs/zzbatmos', where the new volume (the one you noticed) is located at '/umbc/rs/pi_zzbatmos'. Your PI should let the group know when the migration is finished. But if you have any additional questions in the meantime, feel free to let us know! Have a good day! First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: High Performance Cluster Hi, I hope this email finds you well. I noticed that my data from 'umbc/rs/zzbatmos/users/[USER]' is missing. I found that there is another directory 'umbc/rs/pi_zzbatmos/users/[USER]' in a similar path but this new directory contains partial data. Could you please let me know what happened to my original data or if it was moved somewhere else? thank you."
"3308743","2025-11-04 14:20:34","HPC Slurm/Software Issue: Pending Jobs Not Cancelling","Hi, when I checked your jobs just now, none of them came up, meaning I suppose they finished. If you could give me the exact path to the jobs you were trying to cancel, I could test some things myself. Without more information, I can't test much myself. However, looking at the history of your 'scancel' commands, I do notice a mistake you made, which could very well be the reason it didn't work. You ran: scancel -u [USER]. This is not correct on the Chip cluster; the correct command would've been: scancel --cluster=chip-cpu -u [USER] or scancel --cluster=chip-gpu -u [USER]. Yes, I know it's a bit confusing, but you have to specify the cluster CPU/GPU to cancel all jobs for your user. Here's some wiki documentation about that: https://umbc.atlassian.net/wiki/spaces/faq/pages/[PAGEID]/Basic+Slurm+Commands#Managing-and-Controlling-Jobs. Let me know if that was the issue. Best, [STAFF]."
"3308993","2025-11-04 16:34:11","HPC User Account: [ID] in pi_[USER]","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research%20Computing) Kind regards, [STAFF] DoIT Unix Infra Student Worker On Tue Nov 04 09:43:25 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster Create/Modify account in existing PI group Existing PI Email:    [EMAIL] Existing Group:       pi_[STAFF] Project Title:        Towards Designing for Resilience: Community-Centered Deployment of an AI Business Planning Tool in a Feminist Makerspace Project Abstract:     Entrepreneurs in resource-constrained communities often lack the time and support to translate ideas into actionable business plans. While generative AI promises assistance, most systems assume high digital literacy and overlook community infrastructures that shape adoption. We report on the community-centered design and deployment of BizChat, an LLM-powered tool for business plan development, introduced across four workshops at a feminist busi- ness incubator and makerspace. BizChat was designed to center entrepreneurs’ knowledge and workflows while providing just-in-time micro-learning and low-floor-high-ceiling accessibility. Through system log data (N=30) and semi-structured interviews (N=10) with entrepreneurs, we show how the design and deploy- ment of BizChat with existing community contexts lowered bar- riers to accessing capital, encouraged reflection, and empowered entrepreneurs to support AI-literacy within their own communities. We contribute insights into how AI tools can be deployed within local support networks, and implications for design that strengthen community resilience amid rapid technological change.  I am a master's student and requesting this account to join in existing group. I am working under [STAFF]'s Lab."
"3309956","2025-11-04 17:57:29","HPC Slurm/Software Issue: Request to reserve GPU space for a deadline in December ACL 2026","First Name: [USER] Last Name: [STAFF] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster My student, [USER], would like to reserve GPU nodes (preferably H100) for 2 weeks to complete her experiments for the ACL deadline. Can you please assist her with this?"
"3310226","2025-11-04 21:01:54","N-Mode cavity laser simulation - In need of computing power","Please disregard this ticket request."
