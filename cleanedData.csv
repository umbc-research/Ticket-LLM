"TicketID","TransactionID","CreatedDate","SubjectNoHTML","TransactionContent"
"3288390","72207384","2025-10-10 15:28:16","Utilizing matched nodes on chip-cpu","Hello To run a job across 12 nodes you can submit a job to the contrib partition and request 12 nodes. SLURM will allocate your 6 dedicated nodes first (--nodelist=c24-[14-19]) and then fill the remaining slots from available match nodes. Here's an example sbatch script you can use: #SBATCH --cluster=chip-cpu #SBATCH --mem=5000M #SBATCH --time=01:00:00 #SBATCH --account=pi_[STAFF] #SBATCH --partition=contrib #SBATCH --nodes=12 #SBATCH --qos=shared #SBATCH --nodelist=c24-[14-19] Let us know if that doesn't work. On Wed Oct 08 11:26:08 2025 [USER] wrote: From [STAFF]: 'We need to help to understand how the PI partitions are working in Chip. When I look at the pi_[STAFF] I can see that we have nodes 14 to 19 (6 in total) assigned to it. But I should have access also to the +6 additional nodes from the matching funding right? How can we use these additional nodes? We will have to run some very large simulations which will require using all 12 nodes at once. We don't know how to queue such a job since my partition only shows 6 nodes.' -- [STAFF] DOIT Research Computing Team Best [STAFF] DOIT Unix infra Graduate Assistant"
"3288602","72189897","2025-10-09 17:20:23","HPC Other Issue: [USER] cannot log into chip","Hi [USER], I just wanted to give you an update regarding the status of chip. Access to the cluster was restored yesterday afternoon, and access to ada volumes have also been restored. For more information, please check out the myUMBC post: https://my3.my.umbc.edu/groups/hpcf/posts/153425 -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Wed Oct 08 14:43:59 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster Completely Hangs after Last login : Wed Oct 8 ......"
"3288614","72189864","2025-10-09 17:19:20","HPC Other Issue: Unable to connect to chip","Hi [USER], I just wanted to give you an update regarding the status of chip. Access to the cluster was restored yesterday afternoon, and access to ada volumes have also been restored. For more information, please check out the myUMBC post: https://my3.my.umbc.edu/groups/hpcf/posts/153425. -- Kind regards, [STAFF] DoIT Unix Infra Student Worker"
"3288634","72189966","2025-10-09 17:21:27","HPC Other Issue: Unable to access login shell after connecting to chip.rs.umbc.edu","Hi [USER], I just wanted to give you an update regarding the status of chip. Access to the cluster was restored yesterday afternoon, and access to ada volumes have also been restored. For more information, please check out the myUMBC post: https://my3.my.umbc.edu/groups/hpcf/posts/153425. Kind regards, [STAFF] DoIT Unix Infra Student Worker. I appreciate your clarification~ On Wed Oct 08 16:03:51 2025, [USER] wrote: Hello [USER], Our team is aware of the issue, and we are working on resolving this at the moment. We understand the inconvenience this may cause you, but we will send an update once this issue has been fixed. Best regards, [STAFF]."
"3288676","72189880","2025-10-09 17:19:49","HPC Slurm/Software Issue: SSH Connection Hanging","Hi [USER], I just wanted to give you an update regarding the status of chip. Access to the cluster was restored yesterday afternoon, and access to ada volumes have also been restored. For more information, please check out the myUMBC post: https://my3.my.umbc.edu/groups/hpcf/posts/153425 Have a nice day! -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Wed Oct 08 15:55:51 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster Good afternoon, When I ssh into chip.rs.umbc.edu, I am able to be successfully logged in. However, the connection just hangs there without presenting a shell prompt afterwards. Is there any way I could resolve this issue? This happens when I try to use putty and my windows powershell."
"3288709","72189922","2025-10-09 17:20:47","HPC Slurm/Software Issue: Can't log in","Hi [USER], I just wanted to give you an update regarding the status of chip. Access to the cluster was restored yesterday afternoon, and access to ada volumes have also been restored. For more information, please check out the myUMBC post: https://my3.my.umbc.edu/groups/hpcf/posts/153425 -- Kind regards, [STAFF] On Wed Oct 08 16:26:02 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              High Performance Cluster I cant log in to chip"
"3288849","72187131","2025-10-09 15:58:20","HPC Slurm/Software Issue: ADA storage","Hi [USER], Thanks for your patience and understanding. The Ada volume are back! https://my3.my.umbc.edu/groups/hpcf/posts/153425 On Wed Oct 08 23:32:33 2025, [STAFF] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster I know the ada storage is not working and its under maintenance. But do you have any timeframe on how much time it will take? My all experiment data is stored in ada, so i cant work on rs without access to that data. Thanks for your understanding. Best, [STAFF] DOIT HPC System Administrator"
"3288952","72398956","2025-10-21 16:10:21","Migrating Research Storage Volume to Ceph Cluster","Dear [STAFF], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 11th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288957","72450993","2025-10-23 13:51:15","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288960","72452058","2025-10-23 14:12:36","Migrating Research Storage Volume to Ceph Cluster - [ID]","Dear [STAFF], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288964","72451669","2025-10-23 14:06:28","Migrating Research Storage Volume to Ceph Cluster - [ID]","Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288965","72450932","2025-10-23 13:50:09","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288967","72527248","2025-10-28 14:40:05","Migrating Research Storage Volume to Ceph Cluster - knacc1","Here is the cleaned text:  Hi, No worries, you don't have to migrate anything; this is all on our end. We're just letting you know what day we're doing it, so you don't use the cluster that day. Yes, you have almost no data on the cluster. I expect the migration to take almost no time at all. The only thing you have to do is not use the cluster on November 14th, and you'll be all set. Best, [STAFF]"
"3288968","72452016","2025-10-23 14:11:53","Migrating Research Storage Volume to Ceph Cluster - [ID]","Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288973","72451242","2025-10-23 13:55:50","Migrating Research Storage Volume to Ceph Cluster","Dear [STAFF], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288975","72398908","2025-10-21 16:09:26","Migrating Research Storage Volume to Ceph Cluster - [ID]","Dear [USER], as per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 11th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288979","72399083","2025-10-21 16:12:43","Migrating Research Storage Volume to Ceph Cluster - [ID]","Dear [STAFF], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 12th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288980","72399594","2025-10-21 16:20:21","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Dear [STAFF], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288982","72399670","2025-10-21 16:21:27","Migrating Research Storage Volume to Ceph Cluster","Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288983","72399720","2025-10-21 16:22:17","Migrating Research Storage Volume to Ceph Cluster - [ID]","Dear [STAFF], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288987","72451147","2025-10-23 13:54:18","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288988","72451198","2025-10-23 13:55:08","Migrating Research Storage Volume to Ceph Cluster - [ID]","Dear [STAFF], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288991","72451583","2025-10-23 14:04:39","Migrating Research Storage Volume to Ceph Cluster - [ID]","Dear [STAFF], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3288993","72464521","2025-10-23 19:01:22","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","That is fine. Thank you for letting me know. Best Regards, [USER]. On Thu, Oct 23, 2025 at 10:07 AM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.[DOMAIN]/Ticket/Display.html?id=[NUMBER]> Last Update From Ticket: Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]."
"3288998","72452211","2025-10-23 14:17:11","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Dear [USER], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]"
"3289003","72207416","2025-10-10 15:29:16","Migrating Research Storage Volume to Ceph Cluster - [ID]","Ticket [TICKET] Comment just added. I'll also mark him as 'Special Case' in the spreadsheet"
"3289011","72527475","2025-10-28 14:42:59","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Here is the cleaned text: Ticket [NUMBER] Comment just added. Hi, No worries, no meeting of any kind. The only thing you have to do is not use the cluster on Nov. 14, and we'll do the migration for you. Best, [STAFF]"
"3289012","72451285","2025-10-23 13:56:44","Migrating Research Storage Volume to Ceph Cluster - [ID]","Dear [STAFF], As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration. We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date. You will be notified when the migration begins and completes. Thank you, [STAFF]."
"3289018","72336371","2025-10-16 19:09:44","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","Here is the cleaned text:  That's okay, take your time with checking out your volume. I hope you get well soon! Best, [STAFF] Hi [STAFF], Thanks for the message. I'll do that next week as I am unwell right now. [USER] On Wed, Oct 15, 2025 at 10:16 AM Greg Ballantine via RT [EMAIL] wrote: Ticket https://rt.umbc.edu/Ticket/Display.html?id=3289018 Last Update From Ticket: Hello [USER], We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note: * The path has changed, and is now available under /umbc/rs/pi_[USER]. * The alias used to reach the volume is now pi_[USER]_common and pi_[USER]_user. * Your new volume has a quota of 10TB. When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good? Thank you, [STAFF] On Wed Oct 15 09:53:44 2025, [STAFF] wrote: Good morning [USER], This is a reminder that we will be migrating your group's research storage volume to the Ceph storage cluster today. During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated. We will provide an update once completed. Best, [STAFF] On Thu Oct 09 14:14:50 2025, [STAFF] wrote: Hello [USER], Sounds good, I've put your group's migration on our schedule for Wednesday October 15th. We will send you an email alert via this RT ticket when we begin the migration, and again once it has completed. Please let me know if you have any questions or concerns. Best, [STAFF] On Thu Oct 09 13:45:18 2025, [USER] wrote: Hello, The needed migration can be done sometime next week (Oct 15, 16,17); any or some of those would work fine with me. Thanks [USER] On Thu, Oct 9, 2025 at 10:12 AM via RT [EMAIL] wrote: Greetings, This message has been automatically generated in response to the creation of a ticket regarding: Subject: 'Migrating Research Storage Volume to Ceph Cluster - pi_[USER]' Message: Dear [USER], As per the communication via myUMBC earlier this summer (June HPCF Newsletter), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 488.3 GB quota on the old storage server. To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at '/umbc/rs/pi_[USER]'. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume. Below we've listed two options for handling this data migration - please let us know which of these you'd prefer. Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by replying to this email. Option 2: We will choose a date and time that works best for our team, and notify you in advance. Note: After this process has completed, the new storage volume will have a new name. For example, group 'pi_doit' will find its data under '/umbc/rs/pi_doit', or in your group's case you will find your volume under '/umbc/rs/pi_[USER]'. Thank you, [STAFF]"
"3289025","72549278","2025-10-29 13:46:59","Migrating Research Storage Volume to Ceph Cluster - [STAFF]","I've updated my files with the new file path and run a couple of them. Things seem to be working fine as far as I can tell.  Thanks and good luck with the rest of the migration process! [USER] On Tue, Oct 28, 2025 at 3:37 PM [STAFF] via RT <[EMAIL]> wrote:  Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3289025 > Last Update From Ticket: Hello [USER], We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note: * The path has changed, and is now available under /umbc/rs/pi_[USER]. * The alias used to reach the volume is now pi_[USER]_common and pi_[USER]_user. * Your new volume has a quota of 10TB. When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good? Thank you, [STAFF] On Tue Oct 28 10:49:40 2025, [STAFF] wrote: Hello [USER], This is a reminder that we will be performing your group's migration to the Ceph storage cluster today. During this time, please ensure there are no jobs being run in your research group, otherwise these may be terminated. We will provide an update once completed. Best, [STAFF] On Thu Oct 09 11:54:13 2025, [STAFF] wrote: Hello [USER], Sounds good, I've put your group's migration on our schedule for October 28th. We will send you an email alert via this RT ticket when we begin the migration, and again once it has completed. Please let me know if you have any questions or concerns. Best, [STAFF] On Thu Oct 09 11:31:27 2025, [USER] wrote: October 28 would be great. Or Oct 29 if that isn't possible. Thanks! On Thu, Oct 9, 2025 at 10:15 AM via RT <[EMAIL]> wrote: Greetings, This message has been automatically generated in response to the creation of a ticket regarding: Subject: 'Migrating Research Storage Volume to Ceph Cluster - pi_[USER]' Message: Dear [USER], As per the communication via myUMBC earlier this summer (June HPCF Newsletter), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 502.9 GB of a 488.3 GB quota on the old storage server. To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at '/umbc/rs/[USER]'. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume. Below we've listed two options for handling this data migration - please let us know which of these you'd prefer. Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day. Option 2: If you don't respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes. Note: After this process has completed, the new storage volume will have a new name. For example, group 'pi_doit' will find its data under '/umbc/rs/pi_doit', or in your group's case you will find your volume under '/umbc/rs/pi_[USER]'. Thank you, [STAFF]"
"3289155","72396333","2025-10-21 15:27:18","HPC New Group: [ID]","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[USER]. Your home directory has 500M of storage. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) Welcome to chip, UMBC's High Performance Computing Cluster! The group, pi_[USER], now exists on the chip cluster. Members of this group can access and contribute to the research storage space allocated to the group. This storage space is located at /umbc/rs/pi_[USER], and currently has a quota of 25T. For information on accessing the cluster, adding accounts to your group, and getting started using the cluster, check out the tutorial on our wiki: https://umbc.atlassian.net/wiki/x/R4BPQg Additional documentation is also available here: https://umbc.atlassian.net/wiki/x/FwCHQ If you have any questions or issues, please submit a new RT ticket at: https://doit.umbc.edu/request-tracker-rt/doit-research-computing/ Account creation for users: Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[USER]. Your home directory has 500M of storage. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) Let me know if you have any more questions! [STAFF]"
"3289186","72188998","2025-10-09 16:54:34","Start a new group under a [STAFF]","Here is the cleaned text:  First Name: [USER] Last Name: [STAFF] Email: [EMAIL] Campus ID: [ID] Request Type: Help with something else I’d like to start two new groups. 1) for a class “ENME444 - Mechanical Engineering Capstone Design” 2) my research group “eMACS Lab” Thank you"
"3289187","72191668","2025-10-09 18:11:12","Start a new group under a [STAFF]","Hi [USER], I created two groups for you, along with an account for your user. First, Your account ([USER]) has been created on chip.rs.[DOMAIN]. Your primary group is pi_[USER]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: [LINK] Please read through the documentation found at hpcf.[DOMAIN] > User Support. All available modules can be viewed using the command 'module avail'. Secondly, The group pi_[USER] now exists on the chip cluster. Members of this group can therefore access and contribute to the research storage space allocated to the group. This storage is located at /umbc/rs/pi_[USER] and currently has a quota of 25T. And lastly, The group enme444fa25 now exists on the chip cluster. Members of this group can therefore access and contribute to the research storage space allocated to the group. This storage is located at /umbc/class/enme444fa25 and currently has a quota of 5T. Please review documentation on the hpcf.[DOMAIN] website. Submit any questions or issues as separate RT Tickets at: [LINK]. Feel free to let us know if you encounter any issues with either of your groups! Have a good day! On Thu Oct 09 12:54:38 2025, [STAFF] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              Help with something else I’d like to start two new groups.   1) for a class “ENME444 - Mechanical Engineering Capstone Design” 2) my research group “eMACS Lab”."
"3289431","72211306","2025-10-10 17:33:55","HPC Other Issue: Users lack global access in the common directory in [ID]_lab space on CHIP","Here is the cleaned text:  Thank you all for troubleshooting. Hello [USER], It works for me now. Thank you so much for your help. Best, [STAFF] On Fri, Oct 10, 2025 at 12:36 PM [STAFF] via RT <[EMAIL]> wrote: Ticket https://rt.umbc.edu/Ticket/Display.html?id=3289431 Last Update From Ticket: Hi [USER], When you have a chance, try again. It should all be working now. Let me know if it works as expected! -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Fri Oct 10 11:33:35 2025, [USER] wrote: Hi [STAFF], I just tried again and attached a screenshot of my attempt. I’m able to create and work freely within folders that I created inside the common directory. However, I can’t perform any operations in subfolders within common that I didn’t create. For example, the folder downloaded_data was created by my PI, Dr. [STAFF]. To illustrate the issue, I tried creating a test folder within downloaded_data, but I was denied permission. I also tried moving some data into a subfolder within downloaded_data and received the same error. Everything works fine when I’m working within folders I created myself in common. This is what I mean by a global access issue in the common directory. Best, [USER] On Fri, Oct 10, 2025 at 9:37 AM [STAFF] via RT <[EMAIL]> wrote: Ticket https://rt.umbc.edu/Ticket/Display.html?id=3289431 Last Update From Ticket: Hi [USER], Students in your group do have access to the common directory of the research volume. I have tested permissions with multiple student accounts, and there are no issues with the common directory. Please share exactly what you are attempting to do, and what directory you are in. Please see below: [ptembei1@chip-login2 ~]$ pi_mkann_common [ptembei1@chip-login2 common]$ pwd /umbc/rs/pi_mkann/common [ptembei1@chip-login2 common]$ touch test [ptembei1@chip-login2 common]$ ls dbraw downloaded_data Projects test [ptembei1@chip-login2 common]$ -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Thu Oct 09 18:22:47 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUS_ID] Request Type: High Performance Cluster I work in Dr. [STAFF]'s lab, where we collaborate using shared directories. Without global rwx access to the common directory, we’re unable to move files, create subdirectories, or delete files that are no longer needed in directories created by other users. We would really appreciate your help with this."
"3289503","72214706","2025-10-10 19:14:02","HPC User Account: [USER] in Center for Navigation, Timing & Frequency Research","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_menyuk. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://[DOMAIN]/wiki/spaces/faq/pages/[PAGE_ID]/Getting+Started+on+chip. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://[DOMAIN]/request-tracker-rt/[DEPARTMENT]-research-computing/). Let me know if you have any more questions, [STAFF]."
"3289659","72223269","2025-10-12 20:10:29","RCD Consult: Deep Lab Cut","Here is the cleaned text:  I have submitted a group request. Please let me know if you need additional information.  Thanks, [STAFF] On Fri, Oct 10, 2025 at 1:08 PM [STAFF] via RT <[EMAIL]> wrote: Ticket Last Update From Ticket: Hi [USER], First we'll have you start by putting a group request. Documentation on how to do that can be found here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1219461147/Connecting#Requesting-a-User-Account-on-chip, with a link to the RT form to request a group being found here: https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research+Computing Additionally, once you've been set up on the cluster, there is a getting started tutorial that can be found here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip If you have trouble with any of the above let me know and we can walk through it. On Fri Oct 10 10:18:16 2025, [USER] wrote: First Name: [USER] Last Name: [STAFF] Email: [EMAIL] Campus ID: [ID] Request Type: Research Computing & Data Consultation Good Morning, I am a Prof in the Math/Stat department. My area of research is not high performance computing, but I am working on a project that may need access to a machine with GPUs. We need to use software called Deep Lab Cut https://deeplabcut.github.io/DeepLabCut/docs/installation.html to analyze experimental data. As I understand it, the first step is to use Deep Lab Cut on a laptop to recreate an *.eml file. That file would then be used within a batch job on a GPU computer to generate other files. I have two undergraduate students who will be working on this project. I am also collaborating with a postdoc at JHU. I am not sure where to begin, which is why I am submitting this request. Thanks, [STAFF]"
"3289689","72216003","2025-10-10 19:52:19","HPC New Group: pi_[ID]","Hi [USER], Welcome to chip, UMBC's High Performance Computing Cluster! The group, [GROUPNAME], now exists on the chip cluster. Members of this group can access and contribute to the research storage space allocated to the group. This storage space is located at /umbc/rs/[GROUPNAME], and currently has a quota of 10TB. For information on accessing the cluster, adding accounts to your group, and getting started using the cluster, check out the tutorial on our wiki: https://umbc.atlassian.net/wiki/x/R4BPQg Additional documentation is also available here: https://umbc.atlassian.net/wiki/x/FwCHQ Submit any questions or issues as separate RT tickets at: https://doit.umbc.edu/request-tracker-rt/doit-research-computing/ -- Kind regards, [STAFF] On Fri Oct 10 10:47:09 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster Group Type:            Project Title:        Sampling strategy and design for Chesapeake Bay habitat assessment Project Abstract:     The Chesapeake Bay has had a large monitoring program for water quality for the last four decades. Given the advancement of continuous monitoring infrastructure, recent studies pointed to the high values of small numbers of in-situ monitoring stations with high temporal frequency to provide a sound basis for water quality assessment. In addition, a coupling of on-shore and off-shore monitoring can further inform the habitat assessment. This study will couple big data from hydrodynamic models to evaluate the monitoring strategies that incorporate novel continuous in-situ monitoring technologies. The computing needs involve storage of large NetCDF files generated from a high-resolution hydrodynamic model covering the Chesapeake Bay, as well as Monte Carlo simulations to evaluate a large collection of potential sampling scenarios.  The newer monitoring, in turn, will enhance the Chesapeake Bay habitat assessment and address stakeholder planning needs. This study will consider agency-specific logistic constraints on site locations while optimizing for habitat assessment."
"3289693","72329932","2025-10-16 17:12:44","HPC New Group: [ID]","Hi [USER], Welcome to chip, UMBC's High Performance Computing Cluster! The group, pi_[STAFF], now exists on the chip cluster. Members of this group can access and contribute to the research storage space allocated to the group. This storage space is located at /umbc/rs/pi_[STAFF], and currently has a quota of 10T. For information on accessing the cluster, adding accounts to your group, and getting started using the cluster, check out the tutorial on our wiki: https://umbc.atlassian.net/wiki/x/R4BPQg Additional documentation is also available here: https://umbc.atlassian.net/wiki/x/FwCHQ If you have any questions or issues, please submit a new RT ticket at: https://doit.umbc.edu/request-tracker-rt/doit-research-computing/ Kind regards, [STAFF]"
"3289694","72327037","2025-10-16 15:32:18","HPC New Group: pi_[ID]","Hi [USER], Welcome to chip, UMBC's High Performance Computing Cluster! The group, pi_[STAFF], now exists on the chip cluster. Members of this group can access and contribute to the research storage space allocated to the group. This storage space is located at /umbc/rs/pi_[STAFF], and currently has a quota of 10T. For information on accessing the cluster, adding accounts to your group, and getting started using the cluster, check out the tutorial on our wiki: https://umbc.atlassian.net/wiki/x/R4BPQg Additional documentation is also available here: https://umbc.atlassian.net/wiki/x/FwCHQ If you have any questions or issues, please submit a new RT ticket at: https://doit.umbc.edu/request-tracker-rt/doit-research-computing/ Project title: Deep Structural Feature Extraction from Maize Proteins and Gene Sequences Using DNA Language Models Abstract: Understanding the relationship between the maize genome and its corresponding protein structures remains a major challenge in computational biology. This project applies advanced machine learning approaches to analyze large-scale maize gene and protein sequence data, aiming to uncover novel structural and functional insights encoded within DNA. We propose to develop and implement a DNA language model trained on maize genomic sequences to learn biologically meaningful representations that capture the intrinsic syntax and semantics of DNA. These learned embeddings will be integrated with our custom k-mer distance-based model and deep neural architectures to predict tertiary structural and physicochemical features directly from gene and protein sequences. The large-scale data processing, model training, and structural mapping tasks require access to GPU-enabled high-performance computing resources with distributed processing capabilities. This project is conducted in collaboration with the Maize Genetics and Genomics Database, USDA-ARS, Corn Insects and Crop Genetics Research Unit, Iowa State University, Ames, IA, leveraging their extensive maize genomic and proteomic datasets. Expected outcomes include novel computational methods for extracting structural features from biological sequences, deeper understanding of gene-to-structure relationships in maize, and new machine learning tools to support functional genomics and crop improvement research within the MaizeGDB community."
"3289695","72638112","2025-10-31 17:48:54","HPC New Group: [ID]","Hi [USER], An account was created for [STAFF] at the time the group was created. They should be able to follow the log in instructions located on our wiki to connect to the cluster. Please find documentation for that at this link: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip#Accessing-chip Additionally, the broken link has been fixed, here is the corrected link: https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research+Computing Have a nice day! Kind regards, [STAFF] On Thu Oct 30 10:27:39 2025, [EMAIL] wrote: Good morning, Can you kindly create an account for [STAFF] for this group? From the information provided, I see a group was created, but I do not see any account for her. The link https://doit.umbc.edu/request-tracker-rt/doit-research-computing/ is broken. Thanks. Sincerely, [USER] On Thu Oct 16 12:16:29 2025, [EMAIL] wrote: Greetings, Please find attached the projects title and abstract. On Tue, Oct 14, 2025 at 1:41 PM [STAFF] via RT [EMAIL] wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3289695 >; Last Update From Ticket: Hi [USER], Once I receive your project title and abstract, I can create an account on chip for you! Have a nice day! Kind regards, [STAFF] On Fri Oct 10 10:54:30 2025, [USER] wrote: First Name: Roy Last Name: Prouty Email: proutyr1@umbc.edu Campus ID: WH39335 Request Type: High Performance Cluster Group Type: Project Title: Get from PI Project Abstract: Get from PI This group should be named pi_hodae1 . Don't proceed without getting title/abstract from PI."
"3289729","72207588","2025-10-10 15:34:14","Migrating Research Storage Volume to Ceph Cluster","Ticket [TICKET_ID] Comment just added. Cc'ing [STAFF] to this ticket, in case there's any more problems"
"3289747","72214552","2025-10-10 19:07:35","HPC User Account: [USER] in Student Group","Hi [USER], Your account ([USER]) has been created on [SERVER]. Your primary group is [GROUP]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://[LINK]/wiki/spaces/[SPACE]/pages/[PAGE]/Getting+Started+on+[SERVER] Please read through the documentation found at [SERVER] > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://[LINK]/request-tracker-rt/[DEPT]-research-computing/) Let me know if you have any more questions! [STAFF]"
"3289785","72209705","2025-10-10 16:32:34","HPC User Account: [USER] in misc-lab","First Name: [USER] Last Name: [STAFF] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster Create/Modify account in existing PI group Existing PI Email: [EMAIL] Existing Group: misc-lab Project Title: Misc Lab Project Abstract: group for the Misc Lab requesting new PI group"
"3289788","72209865","2025-10-10 16:36:53","create new HPC PI group for [USER]","First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [ID] Request Type: Help with something else group name: misc-lab list of members: [EMAIL]"
"3290086","72268108","2025-10-14 17:31:42","HPC Other Issue: Not able to connect to JupyterLab on Chip","Hi [USER], I identified two nodes (c21-15 and c21-16) that had a missing symbolic link in /usr/ebuild/installs. This is what likely prevented you from finding the jupyter-lab binaries. It has now been resolved, so you shouldn't experience issues now. I will close this ticket now, however if you do encounter another issue, feel free to submit a new ticket. Have a nice day! -- Kind regards, [STAFF] On Mon Oct 13 23:11:20 2025, [USER] wrote: Hi [STAFF], I have seen this error a few times, but I did not note the node on which it occurred. I will do that the next time I notice it. You are right about ' some of the more recent .err/.out files in the working directory ... running fine recently.' I actually restarted my machine, and SSHed into CHIP again, and got it to run. It could very well be a node issue. In the meantime, please keep the ticket open so that we can address the issue the next time it pops up. For the Office Hours tomorrow, if I still don't observe the same issue, I was wondering if we can do it anyway to discuss a new thing I'm trying to do - open two jupyter notebooks on chip (through two tunnels) using a single jupyter.slurm file from a single folder. Is that possible? Or do we need two jupyter.slurm files for that? Thank you. Regards, [USER] On Mon, Oct 13, 2025 at 12:36 PM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3290086> Last Update From Ticket: Hi [USER], Based on the error you attached as a screenshot, it appears that the system was unable to find the binary for jupyter-lab. This binary is provided via the Anaconda3 package, which is loaded at the start of the Jupyter SBATCH file. The PATH for this module is also already present in your PATH env variable. This is as expected, however it is still unable to find the binary. Do you happen to know what node the job was running on? It is possible that this could be due to a specific node. Is this error reproducible? Based on some of the more recent .err/.out files in your working directory, it seems like the notebooks have been running fine recently. Let me know! -- Kind regards, [STAFF] On Sat Oct 11 12:40:16 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster Hello, I am referring to this guide - https://umbc.atlassian.net/wiki/spaces/faq/pages/1104805915/How+do+I+run+a+new+jupyter+notebook+on+chip - to run 'an existing jupyter notebook on chip' in the '/umbc/rs/pi_slaha/users/[USER]/astrophysics-anom_det' folder. I am stuck on Step 7 - i.e. in the .err file, I am not able to see the required section. I cannot find the line that starts with “http://127.0.0.1”. What I see instead is attached as a screenshot. It says the following: /usr/bin/which: no jupyter-lab in (/usr/ebuild/installs/software/Anaconda3/2024.02-1:/usr/ebuild/installs/software/Anaconda3/2024.02-1/sbin:/usr/ebuild/installs/software/Anaconda3/2024.02-1/bin:/usr/ebuild/installs/software/Anaconda3/2024.02-1/condabin:/cm/shared/apps/git/2.33.1/bin:/cm/shared/apps/slurm/current/sbin:/cm/shared/apps/slurm/current/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/[USER]) /cm/local/apps/slurm/var/spool/job456706/slurm_script: line 63: jupyter-lab: command not found Can you please help fix this? Thank you. Regards, [USER] Attachment 1: Screenshot 2025-10-11 123702.png"
"3290310","72240064","2025-10-13 16:06:20","HPC User Account: [USER] in [ID]'s electric fish project","Hi [USER], Welcome to chip, UMBC's High Performance Computing Cluster! The group, pi_[STAFF], now exists on the chip cluster. Members of this group can access and contribute to the research storage space allocated to the group. Additionally, I created user accounts for you, and the two students requested. This storage space is located at /umbc/rs/pi_[STAFF], and currently has a quota of 10T. For information on accessing the cluster, adding accounts to your group, and getting started using the cluster, check out the tutorial on our wiki: https://umbc.atlassian.net/wiki/x/R4BPQg. Additional documentation is also available here: https://umbc.atlassian.net/wiki/x/FwCHQ. For the software, Deep Lab Cut, you should be able to install it via a Conda environment. Here is a wiki page for getting started with anaconda on chip: https://umbc.atlassian.net/wiki/x/LYCPPQ. If you have any questions or issues, please submit a new RT ticket at: https://doit.umbc.edu/request-tracker-rt/doit-research-computing/. First Name: [USER] Last Name: [STAFF] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster Create/Modify account in existing PI group Existing PI Email: [EMAIL] Existing Group: [GROUP] Project Title: Computational Mechanisms for State-Driven Active Sensing Project Abstract: Dr. [STAFF] and an undergraduate student supervised by Dr. [STAFF] will perform the research for the project “Computational Mechanisms for State-Driven Active Sensing” at UMBC. The overall goal of the project is to understand sensing and information gathering behaviors through experiments with electric fish. We need to use software called Deep Lab Cut https://deeplabcut.github.io/DeepLabCut/docs/installation.html to analyze experimental data. As I understand it, the first step is to use Deep Lab Cut on a laptop to recreate an *.eml file. That file would then be used within a batch job on a GPU computer to generate other files. The undergraduate students working on this project are [USER] and [USER]."
"3291129","72250889","2025-10-13 21:58:15","HPC User Account: [ID] in Student Group","This has been fixed. Please try again and let us know if there is still a permission denied issue. -- [STAFF] DoIT Research Computing Team"
"3291162","72712867","2025-11-04 16:40:26","HPC Other Issue: issue with Intel C compiler?","Ticket Comment just added. You shouldn't need to recompile on each node, just each partition since each partition is using the same cpu architecture On Tue Nov 04 11:11:19 2025, [STAFF] wrote: So, lets say i want to run the ver 1.4 on every node in the cluster, to test performance. Do you think I should recompile it on each node? or just run the same executable? -- Best, [STAFF]"
"3291564","72337137","2025-10-16 19:35:40","HPC Other Issue: g24-12 issue","Hi [USER], We've confirmed the machine is functioning normally again. Thank you for the email! On Tue Oct 14 12:18:39 2025, [STAFF] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [USER] Request Type: High Performance Cluster One gpu on g24-12 is not working. attached screenshot. Attachment 1: Screenshot 2025-10-14 at 12.17.01.png -- Best, [STAFF] DOIT HPC System Administrator"
"3293453","72309050","2025-10-15 17:13:25","HPC Other Issue: [USER]'s jobs not running on Chip today","My jobs are now running. There must have been a very big job using up computing power and delaying mine more than usual. You can disregard this issue. Thanks! Greetings, This message has been automatically generated in response to the creation of a ticket regarding: Subject: 'HPC Other Issue: My jobs not running on Chip today' Message: First Name: [USER] Last Name: [STAFF] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster Hi, I'm using the same code that has been running fine the past few weeks but today my jobs are sitting in Chip and not running. Is there a migration going on? Or another reason that would prevent me from accessing pi_gobbert nodes like I usually do? I can't run on the general nodes either. Thank you! [USER@chip-login1 USER]$ squeue -u USER CLUSTER: chip-cpu JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 526469 general weather USER PD 0:00 1 (Priority) 517473 match temp USER PD 0:00 1 (Priority) 526413 match cal USER PD 0:00 1 (Priority) 526432 match dhsregs USER PD 0:00 1 (Priority)"
"3293474","72308965","2025-10-15 17:09:11","CMake issue on Cluster","Hello [USER], I hope this email finds you well. Thank you for working with us during our office hours' time at 12:15-12:45. We wanted to provide you a step-by-step guide on how to use CMake so you are able to use this moving forward. Please also refer to the documents below that talk about 'How to run an interactive job', 'SBATCH', and information on how to use modules. 1. https://umbc.atlassian.net/wiki/spaces/faq/pages/[PAGEID] 2. https://umbc.atlassian.net/wiki/spaces/faq/pages/[PAGEID]/Basic+Slurm+Commands#sbatch 3. https://umbc.atlassian.net/wiki/spaces/faq/pages/[PAGEID]/How+to+use+modules 4. Here is also attached the Cmake documentation for any issues you might have when building (https://cmake.org/cmake/help/latest/guide/tutorial/index.html) When using CMake: 1. Log into [SERVER] 2. run an interactive job using the srun command (This can be found in the 1st article we linked) 3. module load <modulename> (Cmake) 4. Optional (if you would like to view the current modules being ran you can run <module list>) 5. Start working with cmake :) If you have any issues, please reach back out! Best regards, [STAFF]"
"3293526","72398003","2025-10-21 16:00:08","HPC Slurm/Software Issue: Conda environment","This appears to be an email thread discussing an issue with a High-Performance Computing (HPC) environment at the University of Maryland, Baltimore County (UMBC). The issue is related to the Conda environment not being applied consistently across all nodes in the cluster.  Here's a summary of the conversation:  * Matthew Cang (the original sender) reports an issue where the Conda environment is only being applied to one node in the cluster, despite loading it correctly. * Max Breitmeyer (DOIT HPC System Administrator) responds by asking if the team has checked the storage migration that occurred recently, which might be causing the issue. * Matthew Cang provides more details about the steps taken to allocate resources and load the Conda environment, but the issue persists.  The conversation is ongoing, with Max Breitmeyer seeking more information to troubleshoot the problem. The main question being discussed is whether the issue is related to how the environment is initialized across nodes in the current cluster setup.  Some technical details mentioned in the conversation include:  * The use of `srun` to allocate resources and `module load` to load the Conda environment. * The specific commands used to test the environment, including `mpirun python -c 'import fiona; print(fiona.__version__)'`. * The observation that only one node uses the Conda environment's Python interpreter, while others default to the system Python.  Overall, this conversation is a technical discussion between HPC administrators and users trying to resolve an issue with their computing environment."
"3294124","72331579","2025-10-16 18:03:42","HPC User Account: [ID] in FSI","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_dli. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Thu Oct 16 13:49:55 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              High Performance Cluster Create/Modify account in existing PI group Existing PI Email:    [EMAIL] Existing Group:       FSI Project Title:        Sleep Disorder Monitoring Project Abstract:     This project aims to create a sleep stage monitoring system that utilizes contact-free multi-modal data. The long term goal is to extend the system to be able to detect and monitor sleep disorders. By using cutting-edge signal processing and machine learning methods, the system will be capable of providing precise and non-invasive sleep pattern analysis. In order to make the system scalable and capable of real-time execution, the ultimate solution will be refined for deployment on edge devices. I am requesting access to a GPU cluster to help experiments on contact-free, multi-modal sleep stage monitoring and sleep disorder detection. This is a project to create and train machine learning models that will be used on edge devices."
"3295053","72375618","2025-10-20 18:36:23","HPC User Account: [USER] in FSI","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_dli. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Fri Oct 17 15:50:27 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster Create/Modify account in existing PI group Existing PI Email:    [EMAIL] Existing Group:       FSI Project Title:        Sleep Disorder Monitoring Project Abstract:     This project aims to create a sleep stage monitoring system that utilizes contact-free multi-modal data. The long term goal is to extend the system to be able to detect and monitor sleep disorders. By using cutting-edge signal processing and machine learning methods, the system will be capable of providing precise and non-invasive sleep pattern analysis. In order to make the system scalable and capable of real-time execution, the ultimate solution will be refined for deployment on edge devices. I am requesting access to a GPU cluster to help experiments on contact-free, multi-modal sleep stage monitoring and sleep disorder detection. This is a project to create and train machine learning models that will be used on edge devices."
"3295071","72375456","2025-10-20 18:33:56","HPC User Account: [USER] in Machine Learning for Signals Processing Lab","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Fri Oct 17 16:17:16 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster Create/Modify account in existing PI group Existing PI Email:    [STAFF EMAIL] Existing Group:       Machine Learning for Signals Processing Lab Project Title:        Multivariate feature selection for fMRI analysis Project Abstract:     fMRI has become a widely used imaging tool for exploring the normal neural functions as well as disordered brain functions like schizophrenia. Among all fMRI  data analysis strategies, data-driven-based methods have a unique advantage of capturing the whole picture of available information since they effectively minimize assumptions imposed on the brain activity. With the increasing number of multimodal data and multisite data, the problem of balancing the computation cost and analysis performance is becoming more important than ever before. In this project, our interest is in identifying the most informative multivariate  features when analyzing multiple fMRI datasets. Our goal is the development of flexible new decomposition methods as well as identifying the best feature extraction strategy for a given problem."
"3295672","72389751","2025-10-21 13:24:05","Request for Additional Shared Storage (2TB)","Hi [USER], As I write this, your shared research group volume is using 0% of it's 25TiB of space. This volume is located at /umbc/rs/pi_dli. I'll mark this as resolved for now. See this wiki page for additional information: https://umbc.atlassian.net/wiki/spaces/faq/pages/1072267344/Storage On Mon Oct [DATE] [TIME], [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID]  Request Type:              Help with something else Dear HPCF Team, I am a member of the FSI research group led by [STAFF]. For our project titled 'Sleep Disorder Monitoring', we are currently working with large datasets and require an additional 2TB of shared storage to support our ongoing research activities. We would appreciate your support in accommodating this request. Thank you for your understanding. Best regards, [USER]"
"3295788","72378520","2025-10-20 19:08:52","Requesting Additional Shared Storage (2TB)","Hi [USER], As I write this, the pi_dli group has used 0% of its 25TiB allocation on chip. The root for this storage volume is located at /umbc/rs/pi_dli . Let me know if you have any questions about this, but I'll resolve this request for now. On Mon Oct 20 14:51:37 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              Help with something else Dear HPCF Team, I am a member of the FSI research group led by [STAFF]. For our project titled 'Sleep Disorder Monitoring', we are currently working with large datasets and require an additional 2TB of shared storage to support our ongoing research activities. We would appreciate your support in accommodating this request. Thank you for your understanding. Best regards, [USER]"
"3296672","72458889","2025-10-23 16:45:16","HPC User Account: [ID] in pi_[USER]","Here is the cleaned text:  Hi [USER], Your account ([EMAIL]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/). Kind regards, [STAFF] On Tue Oct 21 13:22:34 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [USER] Request Type: High Performance Cluster Create/Modify account in existing PI group Existing PI Email: [STAFF]@umbc.edu Existing Group: pi_[STAFF] Project Title: Coupled Groundwater–Surface Water Modeling of Baltimore Using ParFlow.CLM Project Abstract: This study develops high-resolution ParFlow.CLM models for urban watersheds in Baltimore, Maryland, to investigate the interactions between groundwater and surface water under recent climatic conditions. The model setups include acquisition and processing of digital elevation data; slope generation using GRASS GIS; an overland flow test to ensure that the domains drain completely; acquisition and reclassification of land cover; generation of hydrogeologic layers with variable permeability; and preparation of NLDAS-2 meteorological forcing data. The models are being used for three objectives: (1) to evaluate the potential to predict whether simulated groundwater levels can predict basement flooding during storms, (2) to apply backward particle tracking with ECOSLIM to assess the correlation between stream water chemistry and land use/land cover, and (3) to compare water quantity results (prediction of stream discharge, aquifer levels) to other hydrologic models (HEC-RAS, SWMM, CityCat) being applied to the watersheds. This implementation provides a basis to evaluate hydrologic model robustness and improve understanding of hydrology in urban settings."
"3296766","72462863","2025-10-23 18:21:16","HPC Other Issue: [USER] running unexpectedly slow","Hi [USER], There were some issues with flags that were automatically generating incorrect slurm variables, which may have been causing the slowness. Please try it again and let us know if it seems faster. On Wed Oct 22 13:44:19 2025, [STAFF] wrote: Hi [USER], I followed the instructions on the HPCF wiki page (https://umbc.atlassian.net/wiki/spaces/faq/pages/1408335873/Running+an+LLM+using+Ollama+on+chip) and simply ran the example on that page. It took more than 30minutes to generate only 2 words. I used an interactive job to run the OLLAMA. I even tried to use the GPU partition for [SERVER]. It was a little faster but still super slow. I share the ollama_server.log file in the attachment for your reference. Thank you in advance for your help. Best regards, [USER] On Tue Oct 21 15:20:39 2025, [STAFF] wrote: Hi [USER], I need some more information about your LLM job. Can you please provide your slurm script, and working directory? Are you working with a data, if so how big is it and where is it located? How much time do you expect the job to take ? On Tue Oct 21 14:18:03 2025, [STAFF] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster User [USER] reports following ollama setup according to: https://umbc.atlassian.net/wiki/spaces/faq/pages/1408335873/Running+an+LLM+using+Ollama+on+chip User reports that running LLMs via ollama is running unexpectedly slow. Please follow-up with user to replicate the issue and perhaps set a meeting to understand the issue and to resolve. -- Best, [STAFF] DOIT Unix infra, Graduate Assistant"
"3296910","72606850","2025-10-30 18:44:55","HPC Other Issue: Running Hspice software on HPCF","Here is the cleaned text:  Yes, thanks for all help. Regards, [USER] On Thu, Oct 30, 2025 at 2:11 PM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3296910 > Last Update From Ticket: Hi all, Is it fair to say this ticket can now be closed? On Thu Oct 30 12:57:01 2025, [USER] wrote: Thanks a lot [STAFF]. Both hspice and waveviewer work. Regards,[USER] On Wed, Oct 29, 2025 at 5:01 PM [STAFF] <[EMAIL]> wrote: Wave view should also be working now, give it a try when you have a chance. ---[STAFF] Specialist, Linux System Administrator & Lab Technical Support On Oct 29, 2025, at 16:12, [USER] <[EMAIL]> wrote: Hi [STAFF], Thanks. Hspice works but we also need WV (waveviewer) to see Hspice results as well. Could you please add that also and let us know to check? Thanks a lot,[USER] On Wed, Oct 29, 2025 at 2:30 PM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3296910 > Last Update From Ticket: On the HPC cluster, it should now be possible to run hspice using the command: /umbc/software/csee/scripts/launch_synopsys_hspice.sh Please test it and let me know if it works. Also, let me know what other tools you need to use, if any, so I can adapt those scripts as well."
"3298342","72456450","2025-10-23 15:52:23","HPC Other Issue: 5-days time limit not enough for jobs","Hi [USER], Unfortunately, under the current cluster model only contributing groups have access to the 'shared' QOS option. In your situation, I would recommend seeing if you can optimize your code to run faster, in parallel, or across multiple nodes. You can also attempt to break apart your one large job into two smaller sets of jobs, which would allow you to complete the run in less than 5 days. Let me know if you have any additional questions! Have a nice day! -- Kind regards, [STAFF] On Thu Oct 23 10:09:11 2025, [EMAIL] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              High Performance Cluster Hello all! I'd like to ask your help with the jobs I'm submitting in chip cluster. A few of them are pretty heavy, and the 5-days limit to run are not being enough. I saw there's the possibility to run in the partition 'shared', with 14-days limit, but I don't have access to that. Is it possible to give me access to that partition, or maybe to increase the time limit in the partition 'general'? In general my samples run within the 5 days, I would require more time only for samples whose jobs abort after 5 days. Thank you! Best, [USER]"
"3298471","72457824","2025-10-23 16:21:05","HPC Other Issue: [USER] cannot login to their ada account","Many Thanks [STAFF], Regards, [USER] On Thu, Oct 23, 2025 at 12:18 PM [STAFF] via RT <[EMAIL]> wrote: If you agree your issue is resolved, please give us feedback on your experience by completing a brief satisfaction survey: https://umbc.us2.qualtrics.com/SE/?SID=SV_[RANDOM]&customeremail=[EMAIL]&groupid=EIS&ticketid=3298471&ticketowner=[STAFF EMAIL]&ticketsubject=HPC Other Issue: I cannot login to my ada account If you believe your issue has not been resolved, please respond to this message, which will reopen your ticket. Note: A full record of your request can be found at: Ticket https://rt.umbc.edu/Ticket/Display.html?id=3298471 Thank You R e s o l u t i o n: Hi [USER], The hpc cluster 'ada' has been completely absorbed by 'chip' as of April 10th. Please see this myumbc posting detailing the rollout of chip and the deletion of ada. https://my3.my.umbc.edu/groups/hpcf/posts/147513 On Thu Oct 23 12:01:48 2025, [USER] wrote: First Name: [USER] Last Name: [USER LAST NAME] Email: [EMAIL] Campus ID: [CAMPUS ID] Request Type: High Performance Cluster Hello, I’m currently unable to access my ADA account. When I try to connect via SSH using the command below: ssh [USER EMAIL USERNAME]@ada.rs.umbc.edu I receive the following error message and do not even get the password prompt: ssh: connect to host ada.rs.umbc.edu port 22: Connection timed out Could you please help me resolve this issue? Thank you, [USER] Best, [STAFF] DOIT HPC System Administrator Original Request: Requestors: [USER] First Name: [USER] Last Name: [USER LAST NAME] Email: [EMAIL] Campus ID: [CAMPUS ID] Request Type: High Performance Cluster Hello, I’m currently unable to access my ADA account. When I try to connect via SSH using the command below: ssh [USER EMAIL USERNAME]@ada.rs.umbc.edu I receive the following error message and do not even get the password prompt: ssh: connect to host ada.rs.umbc.edu port 22: Connection timed out Could you please help me resolve this issue? Thank you, [USER]"
"3298811","72480009","2025-10-24 15:50:45","HPC User Account: [ID] in H.A.R.M.O.N.I. Lab","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) Kind regards, [STAFF] DoIT Unix Infra Student Worker On Thu Oct 23 16:30:39 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: High Performance Cluster Create/Modify account in existing PI group Existing PI Email: [EMAIL] Existing Group: H.A.R.M.O.N.I. Lab Project Title: Multimodal Stock Price Direction Prediction Project Abstract: This project, Multimodal Stock Price Direction Prediction Using Historical Prices, News, and Sentiment, aims to predict short-term stock movements up, down, or stable by integrating numerical and textual financial data. The approach combines historical stock price features with sentiment analysis of financial news to capture both market trends and investor sentiment."
"3299581","72523983","2025-10-28 13:33:04","HPC User Account: [USER] in FSI","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_dli. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) On Sat Oct 25 12:59:22 2025, [STAFF] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster Create/Modify account in existing PI group Existing PI Email:    [EMAIL] Existing Group:       FSI Project Title:        Sleep Disorder Monitoring Project Abstract:     This project aims to create a sleep stage monitoring system that utilizes contact-free multi-modal data. The long term goal is to extend the system to be able to detect and monitor sleep disorders. By using cutting-edge signal processing and machine learning methods, the system will be capable of providing precise and non-invasive sleep pattern analysis. In order to make the system scalable and capable of real-time execution, the ultimate solution will be refined for deployment on edge devices.  I am requesting access to a GPU cluster to help experiments on contact-free, multi-modal sleep stage monitoring and sleep disorder detection. This is a project to create and train machine learning models that will be used on edge devices."
"3299984","72524540","2025-10-28 13:43:27","HPC User Account: [USER] in Quantum Thermodynamics Group","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: High Performance Cluster Create/Modify account in existing PI group Existing PI Email: [EMAIL] Existing Group: Quantum Thermodynamics Group Project Title: Noise-Aware Quantum Dynamics Compilation Via Tensor Networks Project Abstract: Quantum system simulation is quantum native, however compilation of variational simulation circuits is a difficult task. Efficiency of the compiled algorithm is paramount to successful simulation, yet algorithmic methods such as Trotterization result in suboptimal circuits. Prior work has identified machine learning techniques which efficiently improve on Trotterization by several orders of magnitude. This work advances these results by factoring noise into the compilation process, allowing for the generation of circuits which will perform optimally when executed on real, noisy quantum hardware. Dear HPC team, I would like to request HPC access to execute quantum simulation algorithms. My advisor, Dr. [STAFF], already has group access on the system, and I would like to be added to the group. Please let me know if you need anything else from me to complete my application. Thank you, [USER]"
"3300174","72523771","2025-10-28 13:29:02","HPC User Account: [USER] in Deffner","Hi [USER], You weren't able to log in because you didn't have a chip account. Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://[EMAIL]/wiki/spaces/faq/pages/[NUMBER]/Getting+Started+on+chip Please read through the documentation found at hpcf.[EMAIL] > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link.(https://[EMAIL]/request-tracker-rt/[EMAIL]-research-computing/) On Mon Oct 27 13:02:13 2025, [STAFF] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [ID] Request Type:              High Performance Cluster Create/Modify account in existing PI group Existing PI Email:    [EMAIL] Existing Group:       [GROUP] Project Title:        Thermodynamics of Quantum Computation Project Abstract:     N/A I am unable to logon to Chip notes. Following the access instructions, I get to the point where I am prompted for my password, but it does not accept the password"
"3300690","72727340","2025-11-04 18:52:46","Data upload and server processing","Hi [USER], Welcome to chip, UMBC's High Performance Computing Cluster! The group, usda-eb, now exists on the chip cluster. Members of this group can access and contribute to the research storage space allocated to the group. This storage space is located at /umbc/rs/usda-eb, and currently has a quota of 50T. For information on accessing the cluster, adding accounts to your group, and getting started using the cluster, check out the tutorial on our wiki: https://umbc.atlassian.net/wiki/x/R4BPQg Additional documentation is also available here: https://umbc.atlassian.net/wiki/x/FwCHQ If you have any questions or issues, please submit a new RT ticket at: https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research%20Computing Currently, [STAFF] is the 'owner' of the group, and is the only member of the group. To request users to be added to your group, please submit an RT ticket from the following link: https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research%20Computing Additionally, I will need a little bit more information regarding your grant. Could you please provide an award number, title, and abstract? If you have any additional concerns, questions, or run into any problems, please feel free to submit a new ticket! Have a great day! Kind regards, [STAFF] On Tue Nov 04 09:32:16 2025, [USER] wrote: Maybe we can use the following name - 'USDA-EB' for the cluster. On Tue, Nov 4, 2025 at 9:26 AM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3300690 > Last Update From Ticket: Hi, Just checking back in with you guys. Any specific name for the Center that we should use for the cluster? Best, [STAFF]"
"3301107","72562517","2025-10-29 17:44:30","HPC User Account: [ID] in [USER]","Hi [USER], Your account ([CAMPUSID]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/). You can also view your project information here on our wiki (https://hpcf.umbc.edu/libraries/research-projects-hpcf/?preview_id=76&preview_nonce=ee7c2f7bd1&preview=true). Best regards, [STAFF]"
"3301187","72636518","2025-10-31 17:12:23","HPC New Group: [ID]","Hi [USER], Welcome to chip, UMBC's High Performance Computing Cluster! The group, pi_[USER], now exists on the chip cluster. Members of this group can access and contribute to the research storage space allocated to the group. This storage space is located at /umbc/rs/pi_[USER], and currently has a quota of 25T. For information on accessing the cluster, adding accounts to your group, and getting started using the cluster, check out the tutorial on our wiki: https://umbc.atlassian.net/wiki/x/R4BPQg Additional documentation is also available here: https://umbc.atlassian.net/wiki/x/FwCHQ If you have any questions or issues, please submit a new RT ticket at: https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research%20Computing -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Tue Oct 28 16:05:31 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              High Performance Cluster Group Type:            Project Title:        Towards Designing for Resilience: Community-Centered Deployment of an AI Business Planning Tool in a Feminist Makerspace Project Abstract:     Entrepreneurs in resource-constrained communities often lack the time and support to translate ideas into actionable business plans. While generative AI promises assistance, most systems assume high digital literacy and overlook community infrastructures that shape adoption. We report on the community-centered design and deployment of BizChat, an LLM-powered tool for business plan development, introduced across four workshops at a feminist busi- ness incubator and makerspace. BizChat was designed to center entrepreneurs' knowledge and workflows while providing just-in-time micro-learning and low-floor-high-ceiling accessibility. Through system log data (N=30) and semi-structured interviews (N=10) with entrepreneurs, we show how the design and deploy- ment of BizChat with existing community contexts lowered bar- riers to accessing capital, encouraged reflection, and empowered entrepreneurs to support AI-literacy within their own communities. We contribute insights into how AI tools can be deployed within local support networks, and implications for design that strengthen community resilience amid rapid technological change."
"3301564","72605706","2025-10-30 18:16:49","HPC Other Issue: very slow editing on /home/[USER] on c24-52","Hi [USER], OK all logged outta chip and our machine -[STAFF] On Thu, Oct 30, 2025 at 2:10 PM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3301564 > Last Update From Ticket: Hi [USER], Please exit out of /umbc/xfs2/strow on all devices where you may be logged in. We can't unmount the device until it's no longer active. On Thu Oct 30 13:44:40 2025, [STAFF] wrote: Hi [STAFF] Thanks for the reply. Maybe you should also cc [STAFF] as you all work on this ([STAFF]) [USER] On Thu, Oct 30, 2025 at 1:30 PM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3301564 > Last Update From Ticket: [USER], There is currently some degradation on the file system that sits on xfs2. We're investigating into how to resolve, but out of an abundance of caution, we are going to unmount xfs2 so that the degradation doesn't spread. We'll let you know when we have more information for you, but expect to not have access to your mount for a while. On Thu Oct 30 13:23:27 2025, [STAFF] wrote: Hi [USER], We are aware of the issue and are currently investigating. I'm going to merge this with the other ticket since they are related. On Thu Oct 30 12:53:30 2025, [USER] wrote: First Name: [USER] Last Name: [LAST NAME] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster Have the disk issues been worked on yet (ticket #3301564)? I still can't edit files And now Matlab took about 3 minutes to start up on c24-52 There is something seriously wrong! Thanks [USER]"
"3302104","72622232","2025-10-31 15:07:37","HPC Other Issue: Need cuDNN 8.4.1.50 module on CHIP","I have installed the module cuDNN/8.4.1.50-CUDA-11.7.0, let us know if you still unable to find the module. On Wed Oct 29 14:22:27 2025, [STAFF] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUS_ID] Request Type: High Performance Cluster Hi, I need to work on TensorFlow 2.11 which using CUDA 11.7 version and cuDNN 8.4. I saw CUDA 11.7, however there is no cuDNN 8.4. The previous cluster (ADA) have module 'cuDNN/8.4.1.50-CUDA-11.7.0'. Can you install that module on CHIP also? Thank you so much, [USER] Best, [STAFF]"
"3304171","72603896","2025-10-30 17:23:27","HPC Other Issue: performance issue causing significant slowdown","We are aware of the issue and are currently investigating. I'm going to merge this with the other ticket since they are related. On Thu Oct 30 12:53:30 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster Have the disk issues been worked on yet (ticket #3301564)? I still can't edit files And now Matlab took about 3 minutes to start up on c24-52 There is something seriously wrong! Thanks [USER]"
"3304395","72643338","2025-10-31 19:53:50","HPC New Group: [ID]","Hi [USER], Welcome to chip, UMBC's High Performance Computing Cluster! The group, pi_[USER], now exists on the chip cluster. Members of this group can access and contribute to the research storage space allocated to the group. This storage space is located at /umbc/rs/pi_[USER], and currently has a quota of 10T. For information on accessing the cluster, adding accounts to your group, and getting started using the cluster, check out the tutorial on our wiki: [URL]. An account for the PI ([USER]) has been created. To add additional users to the group, please submit a new add user ticket. Additional documentation is also available here: [URL]. If you have any questions or issues, please submit a new RT ticket at: [URL]. Kind regards, [STAFF] (they/them/theirs) DoIT Unix Infra Student Worker On Thu Oct 30 15:40:39 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              High Performance Cluster Group Type:            Project Title:        Multiscale Behavior Mapping to Decode Neuronal Health and Diseases in Animal Models Project Abstract:     Automated, high-fidelity behavioral phenotyping is essential for uncover the underlying neural circuits and functional perturbations in vivo. Yet current computer-vision tools often miss the subtle, ethologically relevant kinematic motifs that report changes in internal state. In this project, we will build a neuroscience-focused computational pipeline that couples the broad foundation model with the efficient and specialized detector to identify the animal behavior responded to sensory signal inputs. Specifically, we will distill knowledge from Vision Transformers into customized single-state Convolutional Neural Networks (CNNs) and pretrain these networks on a large, unlabeled corpus of murine behavioral videos spanning diverse common behavior experimental assays. The resulting models will be fine-tuned for high-throughput quantification of classified behavior phenotypes/signatures that serve as proxies for cognitive and affective states. This pipeline will enable experimenters to (i) stratify behavior with ethological precision, (ii) align these phenotypes with neural activities or perturbations by endogenous and environmental factors, and (iii) standardize assays across labs for reproducible neuroscience. Training and optimization demand large-scale data parallelism and extensive hyperparameter searches; thus, access to the UMBC HPC cluster is indispensable. This work will yield a biologically grounded, scalable toolset for objective behavior measurement in animal models, such as mice. The PI of this project is [USER], a Professor at Department of Biological Sciences ([EMAIL]). We would like to setup a new group to use HPC and data storage space. Our project members include UMBC students who are familiar with script coding. The project title and the abstract are included in this request. Please let us know if you have any questions. Thank you."
"3304442","72705445","2025-11-04 14:26:41","HPC Other Issue: more hardware details","Here is the cleaned text: Everything is EDR. On Thu Oct 30 21:44:22 2025, [USER] wrote: Hi, [STAFF], Thanks for the Dell model. Yes, I read that Wiki page. My request is to know more precisely than 'high speed backend Infiniband network supporting 100 Gbps'. That phrase is well-written to cover all nodes from 2018 to 2024. But is the 2024 network not a newer one? Purchased in 2024? Either way, I am asking for a technical term like DDR = dual data rate, QDR = quad data rate, EDR = extended data rate, like I am recalling from the past. Does the 2024 portion's network not have a phrase like that with it? On Thu, Oct 30, 2025 at 7:03 PM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.[DOMAIN]/Ticket/Display.html?id=33404442> Last Update From Ticket: Hi [USER], The model for the nodes used in the 2024 are PowerEdge R660. Specifications on the network abilities and cpus themselves can be found here: https://[DOMAIN].atlassian.net/wiki/spaces/faq/pages/1289486353/Cluster+Specifications On Thu Oct 30 16:07:02 2025, [USER] wrote: > First Name: [USER] > Last Name: [USER] > Email: [EMAIL] > Campus ID: [ID] > Request Type: High Performance Cluster > Hi, I would like some more fine points of hardware information. I am referring to the 2024 portion of chip. - CPU: what is the model number of the node from Dell for the 2024 compute nodes? - The 2024 CPU nodes are connected by which InfiniBand? EDR? Some specs available? [USER]"
"3304534","72612575","2025-10-30 22:55:41","HPC Other Issue: Reset [USER] Password","Hi [USER], Your password is linked to your [UNIVERSITY] account. So you would just need to use the my[UNIVERSITY] reset password. https://[UNIVERSITY].atlassian.net/wiki/spaces/faq/pages/[PAGE_ID]/I+have+forgotten+my+my[UNIVERSITY]+password.+What+should+I+do Note that it may take a minute to update on [SERVER]. On Thu Oct 30 17:52:57 2025, [STAFF] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              High Performance Cluster Dear Sir/Madam, I would like to access the [SERVER] server, but after logging in with my ID “[USERNAME]”, my password does not work. I am not sure of the reason. Could you please provide me with a link or instructions to reset my password? Thank you very much for your assistance. Regards, [USER] ID: [CAMPUS_ID] Best, [STAFF] [DEPARTMENT] System Administrator"
"3304703","72621395","2025-10-31 14:44:35","HPC Other Issue: Request for sample SLURM job script to run WRF on CHIP","Hi [USER], First off, I wanted to clarify a couple points. 1. 'I was advised that running ./wrf.exe interactively is not permitted': This is untrue. You are able to run interactive jobs, where you create a slurm allocation on a node, then connect to it and run your code directly. You are not permitted to run jobs on the login node, which I believe is what you are thinking of. If you would like some documentation on how to run an interactive job, I'll include a link to a page with the commands required to do that. How to run interactive job: https://umbc.atlassian.net/wiki/x/CYCbQw 2. There is also a WRF module available to be loaded on chip. You can load WRF with: 'module load WRF/4.4-foss-2022a-dmpar'. WPS is also available, and can be loaded with: 'module load WPS/4.4-foss-2022a-dmpar'. These installations should work, however it is a slightly older version (4.4), whereas you compiled a newer one (4.5). 3. For your locally installed and compiled copy of WRF, if you desire to use it you must use the compiled binaries located under /umbc/rs/pi_cichoku/users/[USER]/model2/sources/WRF/main. This is where the actual WRF binaries are stored, the directory you provided was a test directory for testing the software. You will want to add this location to your PATH environment variable to utilize the binaries. It is also possible, depending on how your install was compiled, that it would need to be recompiled for MPI support. For more info, check out the Building WRF section of this page: https://www2.mmm.ucar.edu/wrf/OnLineTutorial/compilation_tutorial.php We do not have any existing documentation on running WRF/WPS explicitly, however we have documentation on running MPI jobs, including some example SBATCH scripts: https://umbc.atlassian.net/wiki/x/AQCKV There is documentation on WRF's website for running WRF using mpi, take a look here: https://www2.mmm.ucar.edu/wrf/users/wrf_users_guide/build/html/running_wrf.html If you have any additional questions or need clarification, please feel free to let me know. For now I will mark this as resolved. Have a good day! -- Kind regards, [STAFF] DoIT Unix Infra Student Worker"
"3305879","72731618","2025-11-04 20:24:14","HPC Other Issue: Not able to login to the cluster (chip) through terminal","Hi [USER], Apologies for late reply Yes, still I face the issue and I tried with campus vpn and also my personal wifi. Both are giving the same problem. Could you help me in checking the issue. Thankyou for your help.  Regards, [STAFF]. Campus ID: [ID]  On Mon, Nov 3, 2025 at 11:01 AM [STAFF] via RT <[EMAIL]> wrote: Ticket Last Update From Ticket: Hi [USER], Is this still an issue? We haven't seen any other reports of people being unable to login. Are you on campus vpn? What sort of internet connection do you have (wifi or wired)? On Fri Oct 31 15:04:48 2025, [ID] wrote: First Name: [USER] Last Name: [STAFF] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster Previously, I was able to login through the cluster (chip) but suddenly the login to cluster is not working. Attachment 1: Screenshot 2025-10-31 at 3.03.21PM.png -- Best, [STAFF]."
"3305960","72712232","2025-11-04 16:29:06","HPC User Account: [USER] in [ID]","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is [STAFF]. Your home directory has 500M of storage. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research%20Computing) -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Fri Oct 31 16:25:15 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [ID] Request Type:              High Performance Cluster Create/Modify account in existing PI group Existing PI Email:    [EMAIL] Existing Group:       [STAFF] Project Title:        A Human-Centered Approach to Building Generative AI System for Small Business Owners Project Abstract:     Entrepreneurs in resource-constrained communities often lack the time and support to translate ideas into actionable business plans. While generative AI promises assistance, most systems assume high digital literacy and overlook community infrastructures that shape adoption. We report on the community-centered design and deployment of BizChat, an LLM-powered tool for business plan development, introduced across four workshops at a feminist business incubator and makerspace in a city. BizChat was designed to center entrepreneurs' knowledge and workflows while providing just-in-time micro-learning and low-floor-high-ceiling accessibility. Through system log data (N=30) and semi-structured interviews (N=10) with entrepreneurs, we show how the design and deployment of BizChat with existing community contexts lowered barriers to accessing capital, encouraged reflection, and empowered entrepreneurs to support AI-literacy within their own communities. We contribute insights into how AI tools can be deployed within local support networks, and implications for design that strengthen community resilience amid rapid technological change. I am a phd student and my advisor is [STAFF]. I am requesting an account in order to join this existing group."
"3306147","72647496","2025-11-01 17:20:44","HPC Slurm/Software Issue: [USER] Cannot Cancel Jobs","Understood and no problem. I've cancelled all of your jobs submitted to the match partition that were pending. When canceling your own jobs, be sure to specify the cluster with something like `scancel -M chip-cpu JOBID`. On Sat Nov 01 12:18:16 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: High Performance Cluster Hello, I am having an issue with chip where I cannot cancel a number of jobs that are in queue. I used a script to create and submit the jobs last night but realized that I had a small mistake. I have tried to cancel all jobs under my username, [USER], and cancel specific jobs by id but they remain in the queue. The jobs have remained in the queue for more than 12 hours since I first tried to cancel them. All pending jobs on the match partition under my username need to be cancelled. -- [STAFF] DoIT Research Computing Team"
"3306171","72712777","2025-11-04 16:38:35","HPC Other Issue: software request","Hi [USER], The modules, RSEM and BOWTIE2 have been installed on chip! I installed RSEM/1.3.3-foss-2022a, and Bowtie2/2.4.5-GCC-11.3.0. These can be loaded with 'module load RSEM/1.3.3-foss-2022a' (note, RSEM depends on BOWTIE2, so the correct version of BOWTIE2 is loaded when you load RSEM). If you encounter any issues using the modules, or need any additional modules, feel free to submit a new ticket! Have a great day! -- Kind regards, [STAFF] On Sat Nov 01 13:50:40 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              High Performance Cluster Is it possible to request modules/packages to be added to the cluster? I would like to be able to use RSEM and BOWTIE2. Thanks!"
"3306427","72684558","2025-11-03 16:57:43","HPC Slurm/Software Issue: SSH connection failing","Hi [USER], Since the load balancer issue was fixed, we have not seen any other reports of this issue. I am currently testing myself, and have not encountered any disconnects. Would you mind sharing a bit more about your setup? How are you sshing (ie, a dedicated client like putty, or just through your terminal)? Do you have any local SSH configuration? Are you logged in using GlobalProtect VPN? Let me know and I can look into this further for you! Have a nice day! -- Kind regards, [STAFF] On Sun Nov 02 19:04:14 2025, [EMAIL] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [ID] Request Type:              High Performance Cluster I have a few ssh sessions open connecting to chip and they keep disconnecting every few minutes. Previously, this had been a problem and it was resolved (something to do with a load balancer) but its back now."
"3306608","72660450","2025-11-03 15:16:42","HPC Slurm/Software Issue: Slurm Not Loading Anaconda Module - Job ID 113232-113234","Hi [USER], There appears to have been a minor configuration error with the node that job was running on (g24-11). This has now been resolved. If you continue to experience issues, please feel free to submit a new ticket! Have a nice day! -- Kind regards, [STAFF] On Mon Nov 03 09:47:01 2025, [EMAIL] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: High Performance Cluster I went to run a job this morning that I have run quite often. The first request for this job (ID 113231) started just fine. Subsequent requests error out in my Python script indicating issues loading modules from my Conda environment. The job log shows that 'conda' is not a known command, indicating an issue loading the Anaconda module."
"3308276","72687865","2025-11-03 18:12:12","HPC Other Issue: Data missing from '[ID]/rs/[ID]/users/[ID]'","Hi [USER], Currently, the research volume for pi_zzbatmos is undergoing migration to a new storage server (ceph). This date was scheduled with your PI in advance, and during the time of migration we request that all users in the group do not use chip to avoid disturbing your groups migration. Your PI should have more information regarding the specifics. However, since pi_zzbatmos has a very large research volume, the migration is taking a long time (it was started on Friday, and is still going). After the migration properly completes, you should have access to your data. Something that you already noticed, the new Ceph research volumes start with 'pi_'. For example, the old volume was mounted at '/umbc/rs/zzbatmos', where the new volume (the one you noticed) is located at '/umbc/rs/pi_zzbatmos'. Your PI should let the group know when the migration is finished. But if you have any additional questions in the meantime, feel free to let us know! Have a good day! First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster Hi, I hope this email finds you well. I noticed that my data from 'umbc/rs/zzbatmos/users/[USER]' is missing. I found that there is another directory 'umbc/rs/pi_zzbatmos/users/[USER]' in a similar path but this new directory contains partial data. Could you please let me know what happened to my original data or if it was moved somewhere else? thank you."
"3308743","72705040","2025-11-04 14:20:34","HPC Slurm/Software Issue: Pending Jobs Not Cancelling","Hi, Well, when I squeued for your jobs just now, none of them came up, meaning I suppose they finished. If you could give me the exact path to the jobs you were trying to cancel, I could test some things myself. Without more information, I cant test much myself. However, looking at the history of your 'scancel' commands, I do notice a mistake you made, which could very well be the reason it didnt work. You ran: scancel -u [USER] This is not correct on the Chip cluster; the correct command would've been: scancel --cluster=chip-cpu -u [USER] or scancel --cluster=chip-gpu -u [USER] Yes, I know its a bit confusing, but you have to specify the cluster CPU/GPU to cancel all jobs for your user. Heres some wiki documentation about that: https://[DOMAIN].atlassian.net/wiki/spaces/faq/pages/[PAGEID]/Basic+Slurm+Commands#Managing-and-Controlling-Jobs Let me know if that was the issue. Best, [STAFF]"
"3308993","72712610","2025-11-04 16:34:11","HPC User Account: [ID] in pi_[ID]","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research Computing) -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Tue Nov 04 09:43:25 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [ID] Request Type:              High Performance Cluster Create/Modify account in existing PI group Existing PI Email:    [EMAIL] Existing Group:       pi_[STAFF] Project Title:        Towards Designing for Resilience: Community-Centered Deployment of an AI Business Planning Tool in a Feminist Makerspace Project Abstract:     Entrepreneurs in resource-constrained communities often lack the time and support to translate ideas into actionable business plans. While generative AI promises assistance, most systems assume high digital literacy and overlook community infrastructures that shape adoption. We report on the community-centered design and deployment of BizChat, an LLM-powered tool for business plan development, introduced across four workshops at a feminist busi- ness incubator and makerspace. BizChat was designed to center entrepreneurs’ knowledge and workflows while providing just-in-time micro-learning and low-floor-high-ceiling accessibility. Through system log data (N=30) and semi-structured interviews (N=10) with entrepreneurs, we show how the design and deploy- ment of BizChat with existing community contexts lowered bar- riers to accessing capital, encouraged reflection, and empowered entrepreneurs to support AI-literacy within their own communities. We contribute insights into how AI tools can be deployed within local support networks, and implications for design that strengthen community resilience amid rapid technological change. I am a master’s student and requesting this account to join in existing group. I am working under [STAFF]'s Lab."
"3309956","72724591","2025-11-04 17:57:29","HPC Slurm/Software Issue: Request to reserve GPU space for a deadline in December ACL 2026","Here is the cleaned text: First Name: [USER] Last Name: [STAFF] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster My student, [USER], would like to reserve GPU nodes (preferably H100) for 2 weeks to complete her experiments. Can you please assist her with this?"
"3310226","72733891","2025-11-04 21:01:54","N-Mode cavity laser simulation - In need of computing power","Please disregard this ticket request."
