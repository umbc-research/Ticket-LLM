"TicketID","CreatedDate","SubjectNoHTML","TransactionContent"
"3280927","2025-09-25 16:56:10","HPC Slurm/Software Issue: GPU Job QoS not Permitted","Hi [USER], I see that your job is currently running without issue. Did you get that output from squeue? If so, you can see that the job status is 'PD' which means pending. I am not 100% sure why it is showing this exact error, but it seems the nodes were allocated shortly after the start of the job. Are there any actual errors with the job itself? We are looking into the cause of this message, however it does not seem to actually be causing any errors. #!/bin/bash #SBATCH --job-name=[JOB_NAME] #SBATCH --output=klog/collect_%A_%a.out #SBATCH --error=klog/collect_%A_%a.err #SBATCH --mem=64G #SBATCH --time=72:00:00 #SBATCH --constraint=rtx_6000 #SBATCH --gres=gpu:4 #SBATCH --array=6,7,12,13,20 #SBATCH --mail-user=[EMAIL] #SBATCH --mail-type=END,FAIL #SBATCH --partition=gpu-general"
"3281243","2025-09-26 00:00:06","HPC Other Issue: Nodes on partition [ID] being used in partition 2024","I was unaware of the 10 minute grace period. Many apologies. My calculations did run. I appreciate your help and information on this. Thank you! [USER] On Thu, Sep 25, 2025 at 4:41 PM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=[ID]> Last Update From Ticket: Hello [USER], That is correct - nodes in the pi_bennettj partition will preempt jobs from users that are not in the pi_bennettj group after a 10 minute grace period. It looks like your jobs were submitted at 3:48pm EST and started ~10 minutes later at 3:58pm EST. Can you confirm that your job is running now? Thank you, [STAFF] On Thu Sep 25 15:55:41 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster Hello, I'm trying to run jobs on the pi_bennettj partition, however, 4 of our nodes are being used on partition 2024. I was under the impression that the pi researchers would preempt anyone using our nodes. I've attached a list of the nodes being used. Thank you, [USER] Attachment 1: nodes.txt"
"3281293","2025-09-29 16:34:21","HPC Other Issue: jobs getting killed on 2024 queue","Yes had much better success over the weekend Thanks [USER] On Mon Sep 29 2025 at 12:03PM [STAFF] via RT <[EMAIL]> wrote Ticket  Last Update From Ticket I see Is everything working as intended now Kind regards [STAFF] DoIT Unix Infra Student Worker On Fri Sep 26 16:41:53 2025 [USER] wrote Whoops sorry was submitting the batch file designed for taki -[USER] On Fri Sep 26 2025 at 4:38 PM [USER] <[EMAIL]> wrote Hi [STAFF] Thanks for the info I tried this just now #SBATCH --job-name=3DFIT_TILE_TRENDS name #SBATCH -N1 number of job step is to be allocated per instance of matlab #SBATCH --cpus-per-task 1 tasks per node/number of cores per matlab session will be #SBATCH --partition=match desired partition #SBATCH --cluster=chip-cpu desired cluster #SBATCH --account=pi_strow #SBATCH --qos=shared qos to get as many cpu2024 as possible else put pi_strow and get bin/rm: cannot remove '*~': No such file or directory /bin/rm: cannot remove 'slurm*.err': No such file or directory sbatch: error: Missing: '--gres' sbatch: error: You must specify a Generic RESource to use in your job. sbatch: error: See this webpage for more details https://hpcf.umbc.edu/compute/overview/. sbatch: error: Batch job submission failed Unspecified error which according to the webpage you pointed me to should be used for gpu processors Thanks [USER] On Fri Sep 26 2025 at 9:52 AM [STAFF] via RT <[EMAIL]> wrote If you agree your issue is resolved please give us feedback on your experience by completing a brief satisfaction survey https://umbc.us2.qualtrics.com/SE/?SID=SV_etfDUq3MTISF6Ly&customeremail=[EMAIL]&groupid=EIS&ticketid=3281293&ticketowner=desposi1%40umbc.edu&ticketsubject=HPC+Other+Issue%3A+jobs+getting+killed+on+2024+queue If you believe your issue has not been resolved please respond to this message which will reopen your ticket. Note A full record of your request can be found at Ticket  Thank You _________________________________________ R e s o l u t i o n Hi [USER] First off let me elaborate on the slurm job ID's you are seeing. When submitting an array job there is the main job ID which is 402067 in this case along with the array tasks for each which are like 402067_90. However each individual array task also gets its own actual job ID. For example the actual job ID for array task 402067_90 is 402157. Next your job was likely canceled due to someone in pi_bennettj attempting to run a job on their nodes. Nodes c24-[01-10] are all nodes contributed from pi_bennettj therefore they have priority access and the ability to preempt other users not in their group. If you want to run your jobs on 2024 nodes but do not want to risk preemption use the 'match' partition. If you would like more information on which partitions allow/disallow preemption check out this wiki page https://umbc.atlassian.net/wiki/spaces/faq/pages/1249509377/chip+Partitions+and+Usage#Partitions Let me know if that helps Have a nice day Kind regards [STAFF] DoIT Unix Infra Student Worker On Thu Sep 25 16:56:36 2025 [USER] wrote First Name [USER] Last Name [USER] Email [EMAIL] Campus ID [USER] Request Type High Performance Cluster For some reason many of my jobs are getting killed/preempted slurmsteps. I thought I had an issue with the code but seems fine to me. Haven't had that before Is there a way to limit the jobs only to Larrabee's computer Wierd this is suppose the slurm job ID is 402067 I keep getting told eg 402166 has been killed makes no sense [USER]@chip-login1 AI_RTA$ grep -in slurmst slurm* slurm-402067_90.out:17.........+slurmstepd error *** JOB 402157 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***"
"3281491","2025-10-03 15:01:47","Srun not working for a parallel task","I apologize for the late reply. I wanted to suggest trying to add --mpi=pmix to srun commands (srun --mpi=pmix -n 8 /path/to/your/test --verbose). I think srun might be defaulting to a single rank, which could explain why you're seeing the 'Insufficient processes' error even when requesting multiple tasks and srun working when task is set to 1. pmix (Process Management Interface for Exascale) is the plugin that enables Slurm to correctly launch and manage MPI tasks across multiple ranks. Without specifying it, srun may not properly initialize the MPI environment. Let me know if that doesn't resolve the issue. On Fri Sep 26 12:00:58 2025, [STAFF] wrote: Hi, I have listed the 6 files (SLURM scripts, job outputs, executable outputs) in the next section. In short, the 'mpirun' launches my program successfully, while 'srun' leads to several invalid memory access issues. Both SLURM scripts launch 31 executables at the same time (on 1 CPU node with 8 CPU cores). The only difference is the MPI launcher 'srun' v.s. 'mpirun'. 'srun' version: - Work directory for the 'srun' version: /umbc/rs/pi_dreynold/users/[USER]/20250926-genex-test-srun/ - 'srun' job script: /umbc/rs/pi_dreynold/users/[USER]/20250926-genex-test-srun/job.sh - 'srun' job output: /umbc/rs/pi_dreynold/users/[USER]/20250926-genex-test-srun/409289.genex-test-srun.out - Actual 'srun' commands and output: /umbc/rs/pi_dreynold/users/[USER]/MPCDF/phoenix/genex/build-20250926-111830/src/Testing/Temporary/LastTest.log 'mpirun' version: - Work directory for the 'mpirun' version: /umbc/rs/pi_dreynold/users/[USER]/20250926-genex-test-mpirun/ - 'mpirun' job script: /umbc/rs/pi_dreynold/users/[USER]/20250926-genex-test-mpirun/job.sh - 'mpirun' job output: /umbc/rs/pi_dreynold/users/[USER]/20250926-genex-test-mpirun/409290.genex-test-mpirun.out - Actual 'mpirun' commands and output: /umbc/rs/pi_dreynold/users/[USER]/MPCDF/phoenix/genex/build-20250926-111842/src/Testing/Temporary/LastTest.log The 'mpirun' command I use for this test is from a Spack build of OpenMPI with the following spec: -- linux-rhel9-cascadelake / %c,cxx,fortran=gcc@13.3.0 ---------- openmpi@5.0.8+atomics~cuda~debug+fortran~gpfs~internal-hwloc~internal-libevent~internal-pmix~ipv6~java~lustre~memchecker~openshmem~rocm~romio+rsh~static~two_level_namespace+vt+wrapper-rpath build_system=autotools fabrics:=none romio-filesystem:=none schedulers:=none Best, [USER] On Sep 26, 2025, at 10:38 AM, via RT <[EMAIL]> wrote: Greetings, This message has been automatically generated in response to the creation of a ticket regarding: Subject: 'Srun not working for a parallel task' Message: [STAFF] had an office hour with Beamlak and Tartela. We weren't able to provide an immediate solution. [USER], please add the errors you get, your slurm script, and the working directory. Your ticket has been assigned an ID of [Research Computing #3281491]. You can login to view your open tickets at any time by visiting https://my.umbc.edu and clicking on 'Help' and 'Request Help'. Alternately you can click on https://my.umbc.edu/help. Thank you -- Best, Beamlak Bekele DOIT Unix infra, Graduate Assistant"
"3281714","2025-09-26 18:20:20","HPC Slurm/Software Issue: Cannot create slurm job","Thank you On Fri Sep 26 2025 at 1 55 PM [STAFF] via RT <[EMAIL]> wrote Ticket Last Update From Ticket Hi [USER] Chip is not down That being said I did notice that the configuration was acting strange and it turns out that it was trying to submit jobs to the gpu-contrib partition which is non-submittable partition Ive changed that the gpu partition is back to being the default partition and should work In the future you can always check to see if chip is really down by using the sinfo command Additionally you can also specify the partition using the --partition=${gpu_partition} Let me know if you have more questions about this Ill leave the ticket open for a few days On Fri Sep 26 13 36 48 2025 [USER] wrote First Name [USER] Last Name [USER] Email [EMAIL] Campus ID [CAMPUSID] Request Type High Performance Cluster Hello I attempted to run a slurm job srun --cluster=chip-gpu --account=pi_laramar --mem=20000 --time=12 00 00 --gres=gpu 1 --pty $SHELL Recieved srun Requested partition configuration not available now I am pretty sure chip is down would it be possible to recieve an update when chip is available again Thank you Best [STAFF] DOIT HPC System Administrator"
"3282078","2025-10-31 13:11:06","Cloud: Google Vision","Waiting for info from [STAFF] to get GCP billing in place -- [STAFF], [TITLE], [DEPARTMENT], UMBC   Note: I removed the email footer and replaced Tim Champ with [STAFF] as per anonymization rules. However, since 'Associate Director of Research and Enterprise Computing' is a title, not an identifier, I left it intact but could replace it if you'd like (e.g., [TITLE])."
"3282188","2025-11-04 16:49:54","HPC Slurm/Software Issue: Please install newer Matlab","The text appears to be a email conversation between Matthias Gobbert, a professor at the University of Maryland, Baltimore County (UMBC), and Max Breitmeyer, the High-Performance Computing (HPC) System Administrator at UMBC's Department of Information Technology (DOIT).  The conversation is about installing MATLAB R2025b on the HPC system. Matthias initially requests that Max install the latest version of MATLAB, R2025b, which had recently been released. Max responds by saying that he has added MATLAB 2025b to some partitions and is working on getting it installed on others.  Matthias then reports an issue with running jobs on the HPC system using MATLAB 2025b, providing error messages from the slurm output files. Max investigates the issue and determines that the problem is due to a license issue. He explains that there are two types of licenses for MATLAB: one for individual use and another for shared use on the HPC system.  Max informs Matthias that he has updated the license for the HPC system, and MATLAB 2025b should now be available on all partitions. The conversation ends with Max asking Matthias to try running his jobs again and report any further issues.  There is no specific 'question' being asked in this conversation, but rather a request for assistance with installing software on an HPC system and troubleshooting an issue that arose during the process."
"3282435","2025-09-30 15:09:21","Need to create account in the [CHIP] cluster under [STAFF]'s group","Correct request was submitted: https://rt.[DOMAIN]/Ticket/Display.html?id=[TICKET_ID] -- [STAFF], [TEAM]"
"3282455","2025-10-03 13:48:18","HPC Other Issue: client_loop: send disconnect: Broken pipe happens every few minutes","Hi [USER], We've made a change to the ssh config that we are hoping will solve the issue. Could you keep an eye on it let us know if you notice a difference? On Mon Sep 29 13:24:40 2025, [STAFF] wrote: Hi [USER], we've received reports from other researchers who are experiencing something similar and we are currently investigating the issue. A little more information might help us: - First, can you give us exact times and dates for some of the disconnects you are seeing? This will help us look at the connection logs to find where it happened. - Second, what kind of connection do you have? Wired or wireless? Are you on the vpn when connecting to chip? - Third, are you experiencing it on one of the login nodes more than another? Let me know the above and we can go from there. If you experience anymore of these disconnects please write down the exact date and time you experienced it so we can look into it further. On Mon Sep 29 11:46:36 2025, [STAFF] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: High Performance Cluster Hello, since last week I have been disconnected repeatedly after logging into chip after only a few minutes of waiting for jobs to progress. I am using the standard 'ssh [USER]@chip.rs.umbc.edu' command that has been working for months, but it is disconnecting me too frequently. Why is the Connection to chip.rs.umbc.edu closed by remote host closed so much while I'm working? This is interrupting our research. Is there a different way to login now of which I am unaware? Thank you!"
"3282508","2025-09-30 20:11:03","HPC Other Issue: Need to install a module for WRF Simulation.","Hello [USER], good afternoon. Thank you very much for providing this information. Actually, I tried to find the module using 'module spider gfortran' to check if it is available or not. I will try to load the module using the way you mentioned and complete my work. If not I will ask for further help. Regards, [STAFF] On Mon Sep 29 12:57:26 2025, [EMAIL] wrote: Hi [USER], What happens when you attempt to use gfortran? Without loading any modules, there is a library for gfortran located in /usr/bin/gfortran, which should be on your PATH. Additionally, you can load more specific versions of gfortran through other modules. For example, if you load GCCcore with 'module load GCCcore', you can see that the gfortran library is also present, now located at '/usr/ebuild/installs/software/GCCcore/13.3.0/bin/gfortran' (you can verify yourself with 'which gfortran' to show you the location of the gfortran library). Let me know if this helps! -- Kind regards, [STAFF] On Mon Sep 29 12:33:11 2025, [EMAIL] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [ID] Request Type:              High Performance Cluster Hello, I need to run the WRF climate simulation model to generate datasets for my project. To run this simulation model, I need to use 'gfortran' library, but this is not available in the CHIP cluster. It would be very helpful if you could install the library. Please check the given link if you need further information about running the WRF model."
"3283593","2025-09-30 19:09:58","HPC Other Issue: recent publication","Hello [USER], I have added this publication to our website: https://hpcf.umbc.edu/publications/ On Tue Sep 30 13:52:23 2025, [STAFF] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              High Performance Cluster Hi [STAFF], Geophysical Trends Inferred From 20 Years of AIRS Infrared Global Observations [USER], L. [USER], R. J. [USER] First published: 11 August 2025 https://doi.org/10.1029/2025JD043501 See the link for bib info [USER] Best, [STAFF] DOIT Unix infra, Graduate Assistant"
"3283767","2025-10-07 16:00:19","HPC Slurm/Software Issue: Deleting .julia directories","Hi [USER], The solution is still to run chmod -R u+wrx DataFrames before attempting to remove the directory. The process is hanging because there are many files and directories to modify, which takes time. While the command is running, if your session is idle for too long, you may get logged out before it completes. To avoid this, I recommend using a tmux session to run the command so it can finish without interruption. You can learn more about tmux here (https://www.geeksforgeeks.org/linux-unix/tmux-in-linux/). Once the permissions have been updated, you can remove the directory. I have already removed the /umbc/rs/pi_[STAFF]/users/[USER]/.julia/packages/DataFrames directory for you after changing the permissions. Please reply to the ticket if you still have questions. On Tue Sep 30 16:50:49 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster Ok, so this goes back to an issue I was at office hours last Friday with [STAFF]. We thought we resolved it, and it is kind of resolved, but I still have an issue deleting what I think are essentially corrupted directories. So, I have been dealing with issues downloading packages in Julia on the cluster, and I narrowed down the issue to the specific package 'DataFrames', which is actually quite a simple package, all it does is allows you to read CSV and other text files. The workaround that I have to do is that I just wrote my code in such a way that does not require the usage of DataFrames which is fine, but I still have a few directories in my research storage that house DataFrames package that I need to be deleted. Specifically, under /umbc/rs/pi_[STAFF]/users/[USER] I have 2 directories, '.julia' and '.julia_new' both of which need to be deleted as they house the package DataFrame which prevents me from really running anything. I also have the directory '.julia_test' which I am currently using and does work, so it shouldn't be deleted. I also have another .julia directory under /umbc/rs/pi_[STAFF]/common that needs to be deleted as well. I have tried running 'rm -rf .julia' for hours on end and it hangs. Even going into the directory itself, following the path, '/umbc/rs/pi_[STAFF]/users/[USER]/.julia/packages' and then doing 'rm -rf DataFrames' will hang as well. What ended up working on Friday, which was doing 'chmod -R u+wrx DataFrames' hangs as well now too. I have narrowed the issue all the way down to a specific file in DataFrames, it would be in the path '/umbc/rs/pi_[STAFF]/users/[USER]/.julia/packages/DataFrames/C5AEe/src' which is the source code for this specific package. The file in 'subdataframe' in the source directory is always the issue, which is weird because looking on github, the subdataframe.jl file, with which this is located, looks perfectly fine to me. Regardless, I would still like there to be a way to delete these excess directories in my research storage. Thanks."
"3284012","2025-10-03 13:44:54","HPC Slurm/Software Issue: sbatch job is not running","Yes it is working Thank you Best [STAFF] On Fri Oct 3 2025 9 43 AM [STAFF] via RT [EMAIL] wrote Ticket URL https rt umbc edu Ticket Display html id =3284012 Last Update From Ticket Hello I wanted to follow up to make sure everything is working as expected now If so Ill close out this ticket but you can always reach back out if you need more help Best"
"3284639","2025-10-07 15:49:45","HPC Other Issue: slurm mem error","Hi [USER] OK I'll keep an eye out The job asked for 240 processors which had issues when I started them out but then magically worked fine after doing the unset SLURM_MEM_PER_CPU SLURM_MEM_PER_GPU SLURM_MEM_PER_NODE Last two days I started jobs which asked for 64 cpus all ran fine SO like you I have no idea why this happened Cheers [STAFF] On Tue Oct 7 2025 at 11:45 AM [STAFF] via RT <[EMAIL]> wrote If you agree your issue is resolved please give us feedback on your experience by completing a brief satisfaction survey https://umbc.us2.qualtrics.com/SE/?SID=SV_etfDUq3MTISF6Ly&customeremail=[EMAIL]&groupid=EIS&ticketid=3284639&ticketowner=[STAFF]&ticketsubject=HPC Other Issue: slurm mem error If you believe your issue has not been resolved please respond to this message which will reopen your ticket Note: A full record of your request can be found at Ticket https://rt.umbc.edu/Ticket/Display.html?id=3284639 Hi [USER] I've tried this a few times and am not able to replicate the issue you're having My first thought was that something was getting set in your files and then not being unset in future runs on the same terminal but I've tried a few times and am unable to replicate My runs I did today can be found in the output of CLUST_MAKE_ERA_RTP-442284.out and CLUST_MAKE_ERA_RTP-442245.out Without being able to replicate the issue there's not much else I can do to debug as I've looked through your file and didn't see anything that stood out to me I would advise removing as many of the commented SBATCH directives as possible as it could be reading them in funny unexpected ways but as a policy we don't make changes to people's files without their presence so they can confirm the changes that are being made If you see this pop up again feel free to reopen this ticket but for now I'll going to close it On Thu Oct 02 09:27:50 2025 [USER] wrote Hi [STAFF] Try it with argument 10 or with argument 4 eg sbatch --array=241-276 sergio_matlab_chip.sbatch 10 The problem is intermittent ie does not happen each time I submit a job Looking backwards at history I believe it happened last night with job 435223 On Thu Oct 2 2025 at 8:45 AM [STAFF] via RT <[EMAIL]> wrote Ticket https://rt.umbc.edu/Ticket/Display.html?id=3284639 Last Update From Ticket Hi [USER] I added a .err and a .out file location to your sbatch to help with debugging a little bit First I must recommend cleaning up all the stray SBATCH directives There are quite a few that will conflict with each other if you're not careful see line 45 and 55 in your sbatch file Second I ran the sbatch file myself and didn't get the same error you did Instead I got [USER@chip-login1 CLUSTMAKE_ERA5]$ cat CLUST_MAKE_ERA_RTP-435429.err Unrecognized function or variable 'clustbatch_make_eracloudrtp_sergio_sarta_filelist' Which seems to be missing some sort of command When you run this file are you doing it from the login node Do you normally have any modules loaded On Thu Oct 02 07:49:47 2025 [USER] wrote First Name [USER] Last Name [USER] Email [EMAIL] Campus ID [USER] Request Type High Performance Cluster For some reason the last couple days I have been getting the following error on a script that I think has worked for ages on eg taki now modified for chip srun fatal SLURM_MEM_PER_CPU SLURM_MEM_PER_GPU and SLURM_MEM_PER_NODE are mutually exclusive And googling I finally found a solution on https://harvardmed.atlassian.net/wiki/spaces/O2/pages/1586793613/Troubleshooting+Slurm+Jobs which says at the command line to first do unset SLURM_MEM_PER_CPU SLURM_MEM_PER_GPU SLURM_MEM_PER_NODE Can you look at the following to see if there is a double call to srun or something /home/[USER]/MATLABCODE/RTPMAKE/CLUST_RTPMAKE/CLUSTMAKE_ERA5/sergio_matlab_chip.sbatch Thanks [USER]"
"3284658","2025-10-16 12:23:42","Assistance using UMBC HPCF Cluster to run LS Dyna simulations","Hi [USER], I saw the groups were created for the class and the lab. I finally heard back from our licensing team, and it seems like we are unable to install the program that [STAFF] asked for on a system wide scale available for all users. That being said, it's probably possible to install the program for individual users. [USER], have you been added to the class yet? If not I can add you now. If you have, have you taken a shot at installing the program yourself? If you have any issues you can schedule an office hours with our team here: https://hpcf.umbc.edu/help/office-hours/. I'll leave this open for a few days in case there is any questions about it. On Thu Oct 09 12:55:44 2025, [USER] wrote: Just did. One group for class, one for my research lab. On Thu, Oct 9, 2025 at 11:40 AM Max Breitmeyer via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3284658>. Last Update From Ticket: Hi all, Checking in on this. Were you able to submit a group request? On Tue Oct 07 13:04:16 2025, [USER] wrote: This is awesome. I will setup this up after lunch. Thank you immensely Max! On Tue, Oct 7, 2025 at 11:33 AM Max Breitmeyer via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3284658>. Last Update From Ticket: Sorry for the late response to this. We're looking at making this available on our shared resource, but am waiting to hear back about our licenses. In the meantime it might be possible to install these programs in your user directory. What I would like for your professor to do is set up a new group as a class, so if possible have them go here: https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=3DResearch%20Computing and request a new group under a PI, and in the notes write that this is for the ENME 444 class. Once that's done we can get you on the cluster, and if we still don't know anything about the license, we'll work to try to set up the software for you as an individual. On Thu Oct 02 08:39:49 2025, [USER] wrote: First Name: [USER]. Last Name: [USER]. Email: [EMAIL]. Campus ID: [USER]. Cc: [EMAIL], [EMAIL], [EMAIL]. Hello, I'm a student currently taking ENME 444 Capstone and I am using Ansys and LS Dyna to run simulations of a car crash. The simulations are too large for the student version of the software and also too large to run locally on my laptop. My professor recommended reaching out to see how I can set up LS Dyna to run on the UMBC cluster. Attachment 1: Ansys remote.png. Best, [STAFF]."
"3285426","2025-10-03 17:25:32","HPC Other Issue: Resource Usage Concern on Shared Compute Node","Thank you for your reply. That makes sense. On Fri Oct 03 10:50:43 2025, [STAFF] wrote: Hello, I understand the concern about shared node performance. However, as long as jobs are submitted within the set limits, they are considered valid and within policy. If we see repeated issues that affect the whole system, we can look at changing the limits. For now, the user is working within their allowed usage. Best, [STAFF] On Thu Oct 02 16:49:15 2025, [STAFF] wrote: I would like to bring to your attention that one of the users (ID:[USER], see attached) appears to be occupying a significant portion of the compute resources on our shared node. This high usage may impact the efficiency and workflow of other users. Could you please remind this user to be mindful of resource usage and try to avoid monopolizing the node, so that everyone can work more smoothly?"
"3286322","2025-10-06 14:39:34","HPC Other Issue: Can I request for a cpu and a gpu using sbatch?","Hi [USER], All nodes on chip have CPUs, as they are required to function. However, nodes on chip-gpu also have GPUs available. So yes, when using chip-gpu you can request both GPUs and CPU cores. You would request this the same way as chip-cpu. For more information about the hardware specification, check out this page: https://umbc.atlassian.net/wiki/spaces/faq/pages/1289486353/Cluster+Specifications#CPU-and-GPU-Specifications For more information on requesting CPU cores, check out this page: https://umbc.atlassian.net/wiki/spaces/faq/pages/1335951387/Basic+Slurm+Commands#Jobs%2C-Tasks%2C-CPU-cores%2C-and-Nodes Let me know if you have any additional questions or clarification. -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Sun Oct 05 15:49:38 2025, [EMAIL] wrote: [USER]"
"3286362","2025-10-06 20:24:14","HPC Other Issue: c24-01 tmp area full","Thanks, interesting suggestion. I solved the problem by going to a different node. [STAFF] REU Site: Online Interdisciplinary Big Data Analytics ([EMAIL]) University of Maryland, Baltimore County On Mon, Oct 6, 2025 at 10:21 AM [STAFF] via RT <[EMAIL]> wrote: If you agree your issue is resolved, please give us feedback on your experience by completing a brief satisfaction survey: https://umbc.us2.qualtrics.com/SE/?SID=SV_etfDUq3MTISF6Ly&customeremail=[EMAIL]&groupid=EIS&ticketid=3286362&ticketowner=[STAFF]&ticketsubject=HPC Other Issue: c24-01 tmp area full If you believe your issue has not been resolved, please respond to this message, which will reopen your ticket. Note: A full record of your request can be found at: Ticket https://rt.umbc.edu/Ticket/Display.html?id=3286362 Thank You Hi [USER], Thank you for letting us know. The issue has been resolved. Additionally, if this occurs in the future, you could attempt to change the location that the compiler uses to temporarily store files. I believe this would be achieved by setting the $TMPDIR environment variable. For example, you could try to run... TMPDIR=/scratch/$JOB_ID/ mpiicc -O3 trap.c -o trap Kind regards, [STAFF] On Sun Oct 05 20:20:51 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [USER] Request Type: High Performance Cluster Hi, This is surely a funky error, but if trying to compile on c24-01 in an interactive session, for Intel MPI, it says [gobbert@c24-01 ver1.0solution]$ mpiicc -O3 trap.c -o trap icx: error #10295: error generating temporary file name, check disk space and permissions A student also reported this to me. The /tmpfs area or something like seems to be full. [USER] Original Request: Requestors: [USER] First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [USER] Request Type: High Performance Cluster Hi, This is surely a funky error, but if trying to compile on c24-01 in an interactive session, for Intel MPI, it says [gobbert@c24-01 ver1.0solution]$ mpiicc -O3 trap.c -o trap icx: error #10295: error generating temporary file name, check disk space and permissions A student also reported this to me. The /tmpfs area or something like seems to be full."
"3286765","2025-10-06 19:11:20","HPC Other Issue: Can I get the H100","Hi [USER], Chip is a shared resource. Unless your group has contributed the H100 nodes, access to the node is shared between all of the cluster's users. Normally, your job would run after their job has completed. I suggest waiting until the currently running jobs complete, then your job will run. Or, if your job does not actually require two H100s, you could try to run it on other GPU hardware, such as L40S's or RTX_8000s. There is a greater amount of these nodes available, which would reduce the time it takes for your job to run. Let me know if you have any additional questions. Have a nice day! Kind regards, [STAFF] On Mon Oct 06 12:49:34 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster I've submitted a SLURM job requesting 2 H100 GPUs, but it's currently pending. I'd like to request access to those resources. I'm working with the UMBC-CREM Center. CLUSTER: chip-gpu JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON) 101669       gpu gemma_fi [USER] PD       0:00      1 (Priority)"
"3286853","2025-10-10 16:40:56","HPC Other Issue: need authorization in the common directories","The issue should now be resolved. If you continue to have issues with this, feel free to let us know. Have a nice day! Thank you for your help I will talk with [USER] to see if I understand the issue, [USER]. Sent from my iPhone  On Oct 10, 2025, at 9:41 AM, [STAFF] via RT <[EMAIL]> wrote: Ticket  Last Update From Ticket: Hi [USER], I have verified again that the permissions for the common directory are correct-- So I do not understand why your students are unable to access the common directory. Please see below where I tested the permissions for the common directory using a student account. Could either you or the student who submitted an additional ticket provide some more information with exactly what you are attempting to do? Thanks [ptembei1@chip-login2 ~]$ pi_mkann_common [ptembei1@chip-login2 common]$ pwd /umbc/rs/pi_mkann/common [ptembei1@chip-login2 common]$ touch test [ptembei1@chip-login2 common]$ ls dbraw downloaded_data Projects test [ptembei1@chip-login2 common]$ -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Thu Oct 09 19:23:10 2025, [USER] wrote: My student still gets access denied when she tried maybe you can see what the problem is, M. Sent from my iPhone On Oct 6, 2025, at 1:57 PM, [STAFF] via RT <[EMAIL]> wrote: If you agree your issue is resolved, please give us feedback on your experience by completing a brief satisfaction survey: https://www.google.com/url?q=https://umbc.us2.qualtrics.com/SE/?SID%3DSV_etfDUq3MTISF6Ly%2526customere mail%253D[EMAIL]%2526groupid%253DEIS%2526ticketid%253D3286853%2526ticketowner%253D[STAFF]%252640umbc.edu%2526ticketsubject%253DHPC%252520Other%252520Issue%25253A%252520need%252520authorization%252520in%252520the%252520common%252520directories%252520%26source%3Dgmail-imap%26ust%3D1760708474000000%26usg%3DAOvVaw1QgVEjotkXyFKr3goPtkKa If you believe your issue has not been resolved, please respond to this message, which will reopen your ticket. Note: A full record of your request can be found at: Ticket  Thank You ______________________________________ R e s o l u t i o n: Hi [USER], I resolved the permission issues with pi_mkann/common. Let me know if you continue to experience errors when accessing that directory. Have a nice day! -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Mon Oct 06 13:41:02 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster Hello, I have recently had taki files transfer to chip, could you helps setting up the new common directory so all users in my lab have access and can edit, etc. The way it is now we don't have permissions set up for that. Thanks, [USER]. Original Request: Requestors: [USER] First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster Hello, I have recently had taki files transfer to chip, could you helps setting up the new common directory so all users in my lab have access and can edit, etc. The way it is now we don't have permissions set up for that. Thanks, [USER]."
"3287359","2025-10-10 14:26:50","HPC Slurm/Software Issue: Module Not Available","All the documentation for these programs are publicly available online. We are able to assist with HPC related issues you encounter along the way, however we are unable to walk you through everything. We mainly provide support with HPC/Slurm related issues, and utilizing all of those modules is a little out of scope. If you have any specific questions to help get started, I can do my best to answer them. Otherwise, I recommend taking a look at the documentation for the modules you requested to utilize them. Thanks! Yes, I loaded CGNS successfully. Now I am not sure how to use all of the modules for my code compilation and run the cases eventually. Could you please help me out with this? Ticket <URL: https://rt.[DOMAIN]/Ticket/Display.html?id=[NUMBER] > Last Update From Ticket: I attempted to install CGNS as a custom module. You should be able to load the module with 'module load CGNS/4.5.0'. Can you attempt to load/use the module to verify it works as expected? If you run into issues with the module, let me know. -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Thu Oct 09 12:05:07 2025, [USER] wrote: > Hi [USER], > You do not need to reinstall the software every time you log in/out. After you compile the software, it is installed to whichever directory of your choosing. From there, all you need to do is add the path to your install directory to your PATH environment variable (or just run the binaries from inside the install directory). The software is not readily supported by the module system, easybuild. There are some ways to possibly working around this, but for most use cases, to ensure compatibility it is recommended to just compile from source. I can take a crack at installing it as a custom module though. Also, could you elaborate on what you mean by 'a bash file so that I don't have to install all of the modules'? All the modules have already been installed, and just need to be loaded using 'module load $MODULE_NAME'. -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Thu Oct 09 11:52:15 2025, [USER] wrote: >> Hi [STAFF], Thank you very much for the update. Couldn't you please load the CGNS also? Because I could be wrong as I am not that much familiar with this chip system yet. My assumption is, if I install it by myself, then I have to do it again and again whenever I enter into the system, and run some cases. Please correct me if I am wrong! And I also need your help to make a bash file so that I don't have to install all of the modules that I need to run my cases every time separately. I need lots of modules to run every single test case of mine. Thanks, [USER] On Thu, Oct 9, 2025 at 11:27 AM [STAFF] via RT <[EMAIL]> wrote: >>> If you agree your issue is resolved, please give us feedback on your experience by completing a brief satisfaction survey: https://[DOMAIN].us2.qualtrics.com/SE/?SID=SV_[ID]&customeremail=[EMAIL]&groupid=EIS&ticketid=[NUMBER]&ticketowner=[STAFF]&ticketsubject=HPC%20Slurm/Software%20Issue:%20Module%20Not%20Available >>> If you believe your issue has not been resolved, please respond to this message, which will reopen your ticket. Note: A full record of your request can be found at: Ticket <URL: https://rt.[DOMAIN]/Ticket/Display.html?id=[NUMBER] > >>> Thank You >>> _________________________________________ >>> R e s o l u t i o n: >>> = = = = = = = = = = = = = = = = = = = = = = = = = = = = = >>> Hi [USER], >>> Yes, I was just about to update this ticket. I installed MPICH versions 4.2.2 and 4.2.1, PETSC 3.17.0. But these modules are not available there to load! Could you please load the latest version of these three modules to the cluster and let me know when it's ready? >>> Thanks, >>> [USER] On Tue, Oct 7, 2025 at 10:15 AM via RT <[EMAIL]> wrote: >>>> Greetings, >>>> This message has been automatically generated in response to the creation of a ticket regarding: >>>> -------------------------------------------------------------- >>>> Subject: 'HPC Slurm/Software Issue: Module Not Available' >>>> Message: >>>> First Name: [USER] >>>> Last Name: [USER] >>>> Email: [EMAIL] >>>> Campus ID: [ID] >>>> Request Type: High Performance Cluster >>>> Hello, >>>> I need MPICH, CGNS, PETSC modules to run my cases on hpcf cluster. But these modules are not available there to load! Could you please load the latest version of these three modules to the cluster and let me know when it's ready? >>>> Thanks, >>>> [USER] >>>> -------------------------------------------------------------- >>>> There is no need to reply to this message right now. >>>> Your ticket has been assigned an ID of [Research Computing #NUMBER] or you can go there directly by clicking the link below. >>>> Ticket <URL: https://rt.[DOMAIN]/Ticket/Display.html?id=[NUMBER] > >>>> You can login to view your open tickets at any time by visiting http://my.[DOMAIN] and clicking on 'Help' and 'Request Help'. >>>> Alternately you can click on http://my.[DOMAIN]/help >>>> Thank you >>> ______________________________________ >>> Original Request: >>> Requestors: [USER] >>> First Name: [USER] >>> Last Name: [USER] >>> Email: [EMAIL] >>> Campus ID: [ID] >>> Request Type: High Performance Cluster >>> Hello, >>> I need MPICH, CGNS, PETSC modules to run my cases on hpcf cluster. But these modules are not available there to load! Could you please load the latest version of these three modules to the cluster and let me know when it's ready? >>> Thanks, >>> [USER]"
"3287360","2025-10-07 18:04:47","HPC Slurm/Software Issue: Job Resource Allocation Failing","Seems to be working now. Thank you! On Tue Oct 7 2025 at 1:59 PM [STAFF] via RT <[EMAIL]> wrote: Ticket Last Update From Ticket: Ha, I think you cleared out *too much*. The file '.bashrc' is what's generally in charge of creating your environment. If you just deleted everything in your home directory it doesn't know how to create your environment. I added it back in with the default set up we create for users in their bashrc and was able to confirm that your environment now looks normal: [jrubins1@chip-login2 ~]$ srun --cluster=chip-gpu --time=01:00:00 --mem=4000 --gres=gpu:1 --cpus-per-task=1 --pty $SHELL srun: job 101868 queued and waiting for resources srun: job 101868 has been allocated resources [jrubins1@g24-01 ~]$ Let me know if you still have issues. On Tue Oct 07 13:08:07 2025 [USER] wrote: Thank you for the quick reply I cleared out my home directory. [jrubins1@chip-login2 ~]$ df -h /home/jrubins1/ Filesystem Size Used Avail Use% Mounted on nfs.iss:/ifs/data/chip/home/jrubins1 500M 0 500M 0% /home/jrubins1 But when I try and run an interactive session I still have the same issue [jrubins1@chip-login2 ~]$ srun --cluster=chip-gpu --time=01:00:00 --mem=4000 --gres=gpu:1 --cpus-per-task=1 --pty /bin/bash srun: job 101858 queued and waiting for resources srun: job 101858 has been allocated resources bash-5.1$ On Tue Oct 7 2025 at 11:24 AM [STAFF] via RT <[EMAIL]> wrote: Ticket Last Update From Ticket: Hi [USER] Your home directory is completely filled which can sometimes cause unexpected things to happen when creating new sessions (like what happens slurm starts a new interactive shell). Please remove some of the stuff from your home directory. If you're still having an issue after that please let me know. bash-5.1$ df -h /home/jrubins1/ Filesystem Size Used Avail Use% Mounted on nfs.iss:/ifs/data/chip/home/jrubins1 500M 500M 0 100% /home/jrubins1 On Tue Oct 07 10:16:15 2025 [USER] wrote: Request Type: High Performance Cluster I was trying to get a conda environment set up but after running conda create to make one the next time I started an interactive job (command was: srun --cluster=chip-gpu --time=04:00:00 --mem=40000 --gres=gpu:1 --constraint=rtx_6000 --cpus-per-task=18 --pty /bin/bash) I noticed that instead of putting me into a node like normal my terminal now says 'bash-5.1$' instead of showing what node I'm on and when I try to run: module load Anaconda3/2024.02-1 I get a note saying: Note: Modules do not function on the login node If I then exit I am returned to the login node and I have to exit again to leave my ssh session. Best [STAFF] DOIT HPC System Administrator"
"3287907","2025-10-09 00:52:09","URGENT: Software Access Policy Hindering Research - Request for Immediate Remote Desktop Access","Excellent! Glad to hear this is working. I'll mark this as resolved -- do reopen if there are related issues. On Wed Oct 08 15:59:49 2025, [STAFF] wrote: The student was able to download the software. Thank you for your help on this matter! I have no further requests. Best, [STAFF] On Tue, Oct 7, 2025 at 3:00 PM [STAFF] <[EMAIL]> wrote: Thank you for your prompt response! I will reach out to the student and update you accordingly. With appreciation, [STAFF] On Tue, Oct 7, 2025 at 2:57 PM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3287907> Last Update From Ticket: Hi [STAFF], Could you have your student try this environment? https://elum.in/umbc-desktop-sosc I believe it offers the appropriate access. -- [STAFF] DoIT Research Computing Team"
"3288361","2025-10-16 01:37:55","HPC Other Issue: [USER] access to chip?","Yes she has Thanks for checking Dr [STAFF] Associate Professor Department of Biological Sciences University of Maryland Baltimore County 1000 Hilltop Circle Baltimore MD 21250 Office +1 [PHONE NUMBER REMOVED] On Wed Oct 15 2025 108PM [STAFF] via RT wrote Ticket URL https//rt umbc edu/Ticket/Display html?id=3288361 Last Update From Ticket Hi [USER] Just checking in was your student able to access the cluster On Fri Oct 10 11:46:59 2025 [USER] wrote Please do Ill also ask that you submit an RT ticket to formally add the student to your group https//rtforms umbc edu/rt_authenticated/doit/DoIT-support php?auto=Research Computing I looked into the VPN issue and it seems like the student should be fine as long as they install the vpn Once theyre added well test it out and see what happens from there On Thu Oct 09 16:26:28 2025 [USER] wrote OK Ive just received notice that the students affiliation has been reinstated her name is [USER] and her ID# is [CAMPUS ID] She should have access through 6/30/2026 What would be the next step for allowing her to access my labs volume on chip Should I direct her to download the VPN too On Thu Oct 9 2025 at 11:55 AM [STAFF] via RT wrote Ticket URL https//rt umbc edu/Ticket/Display html?id=3288361 Last Update From Ticket Hi [USER] Typically we dont allow for users into UMBC services if they are not in North America even with VPN That being said we can make some exceptions to this rule For now start by sponsoring an account for the student information for that can be found here https//umbc atlassian net/wiki/spaces/faq/pages/30739140/How+do+I+request+myUMBC+accounts+for+non-UMBC+users+Create+a+Sponsored+Account and well go from there On Wed Oct 08 11:01:26 2025 [USER] wrote First Name [STAFF] Last Name [STAFF] Email [EMAIL] Campus ID [CAMPUS ID] Request Type High Performance Cluster Hello I have a Brazilian graduate student on fellowship [USER] who generated a large genomic dataset while working in my lab and who now is trying to analyze this dataset using the program Stacks She has submitted jobs to servers at her home institution of U Sao Paolo and the UK school where her fellowship continued but they havent been running due to timeouts and/or job backlogs She would like to access chip via my labs login but we have a couple of questions about this 1 Is this allowed My lab has tried to access chip while abroad and was unsuccessful but this may have been because the hotel internet was slow I know the GlobalProtect VPN would probably need to be downloaded too 2 Can I create a log-in for her She was assigned a UMBC account while working here in 2023 but I assume that has lapsed Thank you for the help"
"3288390","2025-10-10 15:28:16","Utilizing matched nodes on chip-cpu","Hello To run a job across 12 nodes, you can submit a job to the contrib partition and request 12 nodes. SLURM will allocate your 6 dedicated nodes first (--nodelist=c24-[14-19]) and then fill the remaining slots from available match nodes. Here's an example sbatch script you can use: #SBATCH --cluster=chip-cpu #SBATCH --mem=5000M #SBATCH --time=01:00:00 #SBATCH --account=pi_[STAFF] #SBATCH --partition=contrib #SBATCH --nodes=12 #SBATCH --qos=shared #SBATCH --nodelist=c24-[14-19] Let us know if that doesn't work. On Wed Oct 08 11:26:08 2025, [USER] wrote: From [STAFF]: 'We need to help to understand how the PI partitions are working in Chip. When I look at the pi_[STAFF], I can see that we have nodes 14 to 19 (6 in total) assigned to it. But I should have access also to the +6 additional nodes from the matching funding, right? How can we use these additional nodes? We will have to run some very large simulations, which will require using all 12 nodes at once. We don't know how to queue such a job, since my partition only shows 6 nodes.' -- [STAFF]"
"3288602","2025-10-09 17:20:23","HPC Other Issue: [USER] cannot log into chip","Hi [USER], I just wanted to give you an update regarding the status of chip. Access to the cluster was restored yesterday afternoon, and access to ada volumes have also been restored. For more information, please check out the myUMBC post: https://my3.my.umbc.edu/groups/hpcf/posts/153425 -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Wed Oct 08 14:43:59 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster Completely Hangs after Last login : Wed Oct 8 ......"
"3288614","2025-10-09 17:19:20","HPC Other Issue: Unable to connect to chip","Hi [USER], I just wanted to give you an update regarding the status of chip. Access to the cluster was restored yesterday afternoon, and access to ada volumes have also been restored. For more information, please check out the myUMBC post: https://my3.my.umbc.edu/groups/hpcf/posts/153425 -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Wed Oct 08 16:00:52 2025, [USER] wrote:"
"3288634","2025-10-09 17:21:27","HPC Other Issue: Unable to access login shell after connecting to chip.rs.umbc.edu","Hi [USER], I just wanted to give you an update regarding the status of chip. Access to the cluster was restored yesterday afternoon, and access to ada volumes have also been restored. For more information, please check out the myUMBC post: https://my3.my.umbc.edu/groups/hpcf/posts/153425 -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Thu Oct 09 12:13:04 2025, [USER] wrote: I appreciate your clarification~ On Wed Oct 08 16:03:51 2025, [USER] wrote: Hello [USER], Our team is aware of the issue, and we are working on resolving this at the moment. We understand the inconvenience this may cause you, but we will send an update once this issue has been fixed. Best regards, [STAFF]"
"3288676","2025-10-09 17:19:49","HPC Slurm/Software Issue: SSH Connection Hanging","Hi [USER], I just wanted to give you an update regarding the status of chip. Access to the cluster was restored yesterday afternoon, and access to ada volumes have also been restored. For more information, please check out the myUMBC post: https://my3.my.umbc.edu/groups/hpcf/posts/153425 Have a nice day! -- Kind regards, [STAFF] On Wed Oct 08 15:55:51 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: High Performance Cluster Good afternoon, When I ssh into chip.rs.umbc.edu, I am able to be successfully logged in. However, the connection just hangs there without presenting a shell prompt afterwards. Is there any way I could resolve this issue? This happens when I try to use putty and my windows powershell."
"3288709","2025-10-09 17:20:47","HPC Slurm/Software Issue: Can't log in","Hi [USER], I just wanted to give you an update regarding the status of chip. Access to the cluster was restored yesterday afternoon, and access to ada volumes have also been restored. For more information, please check out the myUMBC post: https://my3.my.umbc.edu/groups/hpcf/posts/153425. -- Kind regards, [STAFF]. On Wed Oct 08 16:26:02 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: High Performance Cluster I cant log in to chip."
"3288849","2025-10-09 15:58:20","HPC Slurm/Software Issue: ADA storage","Hi [USER], Thanks for your patience and understanding. The Ada volume are back! https://my3.my.umbc.edu/groups/hpcf/posts/153425 On Wed Oct 08 23:32:33 2025, [STAFF] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: High Performance Cluster I know the ada storage is not working and its under maintenance. But do you have any timeframe on how much time it will take? My all experiment data is stored in ada, so i cant work on rs without access to that data. Thanks for your understanding. Best, [STAFF]"
"3289186","2025-10-09 16:54:34","Start a new group under a [STAFF]","First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [ID] Request Type: Help with something else Id like to start two new groups. 1) for a class ENME444 - Mechanical Engineering Capstone Design 2) my research group eMACS Lab Thank you"
"3289187","2025-10-09 18:11:12","Start a new group under a [STAFF]","Hi [USER], I created two groups for you, along with an account for your user. First, Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[USER]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Secondly, The group pi_[USER] now exists on the chip cluster. Members of this group can therefore access and contribute to the research storage space allocated to the group. This storage is located at /umbc/rs/pi_[USER] and currently has a quota of 25T. And lastly, The group enme444fa25 now exists on the chip cluster. Members of this group can therefore access and contribute to the research storage space allocated to the group. This storage is located at /umbc/class/enme444fa25 and currently has a quota of 5T. Please review documentation on the hpcf.umbc.edu website. Submit any questions or issues as separate RT Tickets at: https://doit.umbc.edu/request-tracker-rt/doit-research-computing/. Feel free to let us know if you encounter any issues with either of your groups! Have a good day! -- Kind regards, [STAFF]. On Thu Oct 09 12:54:38 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              Help with something else. Id like to start two new groups.   1)  for a class ENME444 - Mechanical Engineering Capstone Design  2) my research group [USER] Lab. Thank you."
"3289431","2025-10-10 17:33:55","HPC Other Issue: [USER] lacks global access in the common directory in pi_[ID] lab space on CHIP","It works for me now. Thank you so much for your help. Best, [USER]. On Fri, Oct 10, 2025 at 12:36 PM [STAFF] via RT <[EMAIL]> wrote: Ticket https://rt.umbc.edu/Ticket/Display.html?id=3289431 Hi Petra, When you have a chance, try again. It should all be working now. Let me know if it works as expected! Kind regards, [STAFF]. On Fri Oct 10 11:33:35 2025, [USER] wrote: Hi Danielle, I just tried again and attached a screenshot of my attempt. Im able to create and work freely within folders that I created inside the common directory. However, I cant perform any operations in subfolders within common that I didnt create. For example, the folder downloaded_data was created by my PI, Dr. [USER]. To illustrate the issue, I tried creating a test folder within downloaded_data, but I was denied permission. I also tried moving some data into a subfolder within downloaded_data and received the same error. Everything works fine when Im working within folders I created myself in common. This is what I mean by a global access issue in the common directory. Best, [USER]. On Fri, Oct 10, 2025 at 9:37 AM [STAFF] via RT <[EMAIL]> wrote: Ticket https://rt.umbc.edu/Ticket/Display.html?id=3289431 Hi Petra, Students in your group do have access to the common directory of the research volume. I have tested permissions with multiple student accounts, and there are no issues with the common directory. Please share exactly what you are attempting to do, and what directory you are in. Please see below: [ptembei1@chip-login2 ~]$ pi_mkann_common [ptembei1@chip-login2 common]$ pwd /umbc/rs/pi_mkann/common [ptembei1@chip-login2 common]$ touch test [ptembei1@chip-login2 common]$ ls dbraw downloaded_data Projects test. Kind regards, [STAFF]. On Thu Oct 09 18:22:47 2025, [USER] wrote: First Name: [USER]. Last Name: [USER]. Email: [EMAIL]. Campus ID: [CAMPUSID]. Request Type: High Performance Cluster I work in Dr. [USER]'s lab, where we collaborate using shared directories. Without global rwx access to the common directory, were unable to move files, create subdirectories, or delete files that are no longer needed in directories created by other users. We would really appreciate your help with this."
"3289659","2025-10-12 20:10:29","RCD Consult: Deep Lab Cut","Thanks [STAFF]! I have submitted a group request. Please let me know if you need additional information. Thanks, [USER]. Hi [USER], First we'll have you start by putting a group request. Documentation on how to do that can be found here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1219461147/Connecting#Requesting-a-User-Account-on-chip, with a link to the RT form to request a group being found here: https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research%20Computing. Additionally, once you've been set up on the cluster, there is a getting started tutorial that can be found here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip. If you have trouble with any of the above let me know and we can walk through it. Request Type: Research Computing & Data Consultation. I am a Prof in the Math/Stat department. My area of research is not high performance computing, but I am working on a project that may need access to a machine with GPUs. We need to use software called Deep Lab Cut https://deeplabcut.github.io/DeepLabCut/docs/installation.html to analyze experimental data. As I understand it, the first step is to use Deep Lab Cut on a laptop to recreate an *.eml file. That file would then be used within a batch job on a GPU computer to generate other files. I have two undergraduate students who will be working on this project. I am also collaborating with a postdoc at JHU. I am not sure where to begin, which is why I am submitting this request."
"3289788","2025-10-10 16:36:53","create new HPC PI group","First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [ID] Request Type: Help with something else group name: misc-lab list of members: [EMAIL]"
"3290086","2025-10-14 17:31:42","HPC Other Issue: Not able to connect to JupyterLab on Chip","Hi [USER], I identified two nodes (c21-15 and c21-16) that had a missing symbolic link in /usr/ebuild/installs. This is what likely prevented you from finding the jupyter-lab binaries. It has now been resolved, so you shouldn't experience issues now. I will close this ticket now, however if you do encounter another issue, feel free to submit a new ticket. Have a nice day! On Mon Oct 13 23:11:20 2025, [USER] wrote: Hi [STAFF], I have seen this error a few times, but I did not note the node on which it occurred. I will do that the next time I notice it. You are right about 'some of the more recent .err/.out files in the working directory ... running fine recently.' I actually restarted my machine, and SSHed into CHIP again, and got it to run. It could very well be a node issue. In the meantime, please keep the ticket open so that we can address the issue the next time it pops up. For the Office Hours tomorrow, if I still don't observe the same issue, I was wondering if we can do it anyway to discuss a new thing I'm trying to do - open two jupyter notebooks on chip (through two tunnels) using a single jupyter.slurm file from a single folder. Is that possible? Or do we need two jupyter.slurm files for that? Thank you. Regards, [USER]. On Mon, Oct 13, 2025 at 12:36 PM [STAFF] via RT wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3290086 > Last Update From Ticket: Hi [USER], Based on the error you attached as a screenshot, it appears that the system was unable to find the binary for jupyter-lab. This binary is provided via the Anaconda3 package, which is loaded at the start of the Jupyter SBATCH file. The PATH for this module is also already present in your PATH env variable. This is as expected, however it is still unable to find the binary. Do you happen to know what node the job was running on? It is possible that this could be due to a specific node. Is this error reproducible? Based on some of the more recent .err/.out files in your working directory, it seems like the notebooks have been running fine recently. Let me know! -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Sat Oct 11 12:40:16 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [USER]. Request Type: High Performance Cluster. Hello, I am referring to this guide - https://umbc.atlassian.net/wiki/spaces/faq/pages/1104805915/How+do+I+run+a+new+jupyter+notebook+on+chip - to run 'an existing jupyter notebook on chip' in the '/umbc/rs/pi_slaha/users/[USER]/astrophysics-anom_det' folder. I am stuck on Step 7 - i.e. in the .err file, I am not able to see the required section. I cannot find the line that starts with http://127.0.0.1. What I see instead is attached as a screenshot. It says the following: /usr/bin/which: no jupyter-lab in (/usr/ebuild/installs/software/Anaconda3/2024.02-1:/usr/ebuild/installs/software/Anaconda3/2024.02-1/sbin:/usr/ebuild/installs/software/Anaconda3/2024.02-1/bin:/usr/ebuild/installs/software/Anaconda3/2024.02-1/condabin:/cm/shared/apps/git/2.33.1/bin:/cm/shared/apps/slurm/current/sbin:/cm/shared/apps/slurm/current/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/[USER]). /cm/local/apps/slurm/var/spool/job456706/slurm_script: line 63: jupyter-lab: command not found. Can you please help fix this? Thank you. Regards, [USER]."
"3291162","2025-11-04 16:40:26","HPC Other Issue: change with Intel C compiler?","Ticket [TICKET_NUMBER] Comment just added. You shouldn't need to recompile on each node, just each partition since each partition is using the same cpu architecture On Tue Nov 04 11:11:19 2025, [STAFF] wrote: So, lets say i want to run the ver 1.4 on every node in the cluster, to test performance. Do you think I should recompile it on each node? or just run the same executable? -- Best, [STAFF]"
"3291564","2025-10-16 19:35:40","HPC Other Issue: g24-12 issue","Hi [USER], We've confirmed the machine is functioning normally again. Thank you for the email! On Tue Oct 14 12:18:39 2025, [STAFF] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUS_ID] Request Type: High Performance Cluster One gpu on g24-12 is not working. attached screenshot. Attachment 1: Screenshot 2025-10-14 at 12.17.01.png -- Best, [STAFF]"
"3293453","2025-10-15 17:13:25","HPC Other Issue: [USER]'s jobs not running on Chip today","My jobs are now running. There must have been a very big job using up computing power and delaying mine more than usual. You can disregard this issue. Thanks! Greetings, This message has been automatically generated in response to the creation of a ticket regarding: Subject: 'HPC Other Issue: My jobs not running on Chip today' Message: First Name: [USER] Last Name: [STAFF] Email: [EMAIL] Campus ID: [CAMPUS_ID] Request Type: High Performance Cluster Hi, I'm using the same code that has been running fine the past few weeks but today my jobs are sitting in Chip and not running. Is there a migration going on? Or another reason that would prevent me from accessing pi_gobbert nodes like I usually do? I can't run on the general nodes either. Thank you! [USER@chip-login1 USER]$ squeue -u USER CLUSTER: chip-cpu JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 526469 general weather [USER] PD 0:00 1 (Priority) 517473 match temp [USER] PD 0:00 1 (Priority) 526413 match cal [USER] PD 0:00 1 (Priority) 526432 match dhsregs [USER] PD 0:00 1 (Priority)"
"3293474","2025-10-15 17:09:11","CMake issue on Cluster","Hello [USER], I hope this email finds you well. Thank you for working with us during our office hours' time at 12:15-12:45. We wanted to provide you a step-by-step guide on how to use CMake so you are able to use this moving forward. Please also refer to the documents below that talk about 'How to run an interactive job', 'SBATCH', and information on how to use modules. 1. https://umbc.atlassian.net/wiki/spaces/faq/pages/1134264329 2. https://umbc.atlassian.net/wiki/spaces/faq/pages/1335951387/Basic+Slurm+Commands#sbatch 3. https://umbc.atlassian.net/wiki/spaces/faq/pages/1330053125/How+to+use+modules 4. Here is also attached the Cmake documentation for any issues you might have when building (https://cmake.org/cmake/help/latest/guide/tutorial/index.html) When using CMake: 1. Log into chip 2. run an interactive job using the srun command (This can be found in the 1st article we linked) 3. module load <modulename> (Cmake) 4. Optional (if you would like to view the current modules being ran you can run <module list> 5. Start working with cmake :) If you have any issues, please reach back out! -- Best regards, [STAFF]"
"3293526","2025-10-21 16:00:08","HPC Slurm/Software Issue: Conda environment","This appears to be an email thread between a researcher and the High-Performance Computing (HPC) team at the University of Maryland, Baltimore County (UMBC). The researcher is experiencing issues with their Conda environment not being applied consistently across all nodes in the HPC cluster.  Here's a summary of the issue:  * The researcher has allocated resources on the HPC cluster using `srun` and loaded the computation environment using `module load` commands. * They then activate the Conda environment using `conda activate`. * However, when they test the environment with `mpirun python -c 'import fiona; print(fiona.__version__)'`, only one node can use the Fiona library, while the others return a `ModuleNotFoundError`. * Further testing reveals that only one node uses the Conda environment's Python interpreter, while the others default to the system Python.  The HPC team responds with questions and suggestions:  * Max Breitmeyer asks if the issue is related to how the environment is initialized across nodes in the current cluster setup. * He also mentions that there may be a problem with the storage migration and suggests checking the `module load` commands.  To resolve this issue, the researcher could try the following:  1. Check the `module load` commands to ensure they are correct and consistent across all nodes. 2. Verify that the Conda environment is properly activated on each node by running `conda info --envs` or `conda list`. 3. Test the environment using a different method, such as running a Python script that imports the Fiona library. 4. If the issue persists, contact the HPC team for further assistance.  If you are experiencing similar issues with your Conda environment on an HPC cluster, I recommend trying these troubleshooting steps and reaching out to your cluster's support team if needed."
"3295672","2025-10-21 13:24:05","Request for Additional Shared Storage (2TB)","Hi [USER], As I write this, your shared research group volume is using 0% of it's 25TiB of space. This volume is located at /umbc/rs/pi_dli. I'll mark this as resolved for now. See this wiki page for additional information: https://umbc.atlassian.net/wiki/spaces/faq/pages/1072267344/Storage On Mon Oct [DATE] [TIME], [STAFF] wrote: First Name:                [USER] Last Name:                 [LAST NAME] Email:                     [EMAIL] Campus ID:                 [CAMPUS ID] Request Type:              Help with something else Dear HPCF Team, I am a member of the FSI research group led by [STAFF]. For our project titled 'Sleep Disorder Monitoring', we are currently working with large datasets and require an additional 2TB of shared storage to support our ongoing research activities. We would appreciate your support in accommodating this request. Thank you for your understanding. Best regards, [USER]"
"3295788","2025-10-20 19:08:52","Requesting Additional Shared Storage (2TB)","Hi [USER], As I write this, the pi_dli group has used 0% of its 25TiB allocation on chip. The root for this storage volume is located at /umbc/rs/pi_dli . Let me know if you have any questions about this, but I'll resolve this request for now. On Mon Oct 20 14:51:37 2025, [STAFF] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              Help with something else Dear HPCF Team, I am a member of the FSI research group led by [STAFF]. For our project titled 'Sleep Disorder Monitoring', we are currently working with large datasets and require an additional 2TB of shared storage to support our ongoing research activities. We would appreciate your support in accommodating this request. Thank you for your understanding. Best regards, [USER]"
"3296766","2025-10-23 18:21:16","HPC Other Issue: [USER] running unexpectedly slow","Hi [USER], There were some issues with flags that were automatically generating incorrect slurm variables, which may have been causing the slowness. Please try it again and let us know if it seems faster. On Wed Oct 22 13:44:19 2025, [STAFF] wrote: Hi [STAFF], I followed the instructions on the HPCF wiki page (https://umbc.atlassian.net/wiki/spaces/faq/pages/1408335873/Running+an+LLM+using+Ollama+on+chip) and simply ran the example on that page. It took more than 30minutes to generate only 2 words. I used an interactive job to run the OLLAMA. I even tried to use the GPU partition for [SERVER]. It was a little faster but still super slow. I share the ollama_server.log file in the attachment for your reference. Thank you in advance for your help. Best regards, [USER] On Tue Oct 21 15:20:39 2025, [STAFF] wrote: Hi [USER], I need some more information about your LLM job. Can you please provide your slurm script, and working directory? Are you working with a data, if so how big is it and where is it located? How much time do you expect the job to take ? On Tue Oct 21 14:18:03 2025, [STAFF] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              High Performance Cluster User [USER] reports following ollama setup according to: https://umbc.atlassian.net/wiki/spaces/faq/pages/1408335873/Running+an+LLM+using+Ollama+on+chip User reports that running LLMs via ollama is running unexpectedly slow. Please follow-up with user to replicate the issue and perhaps set a meeting to understand the issue and to resolve."
"3296910","2025-10-30 18:44:55","HPC Other Issue: Running Hspice software on HPCF","Yes, thanks for all help. Regards, [USER] On Thu, Oct 30, 2025 at 2:11 PM [STAFF] via RT <[EMAIL]> wrote: Ticket Last Update From Ticket: Hi all, Is it fair to say this ticket can now be closed? On Thu Oct 30 12:57:01 2025, [USER] wrote: Thanks a lot [STAFF]. Both hspice and waveviewer work. Regards,[USER] On Wed, Oct 29, 2025 at 5:01 PM [STAFF] <[EMAIL]> wrote: Wave view should also be working now, give it a try when you have a chance. ---[STAFF] Specialist, Linux System Administrator & Lab Technical Support On Oct 29, 2025, at 16:12, [USER] <[EMAIL]> wrote: Hi [STAFF], Thanks. Hspice works but we also need WV (waveviewer) to see Hspice results as well. Could you please add that also and let us know to check? Thanks a lot,[USER] On Wed, Oct 29, 2025 at 2:30 PM [STAFF] via RT <[EMAIL]> wrote: Ticket Last Update From Ticket: On the HPC cluster, it should now be possible to run hspice using the command: /umbc/software/csee/scripts/launch_synopsys_hspice.sh Please test it and let me know if it works. Also, let me know what other tools you need to use, if any, so I can adapt those scripts as well. -- [USER] Associate Professor Department of Computer Science and Electrical Engineering University of Maryland, Baltimore County Baltimore, MD 21250 Tel: ***** E-mail: [EMAIL] Web: http://www.csee.umbc.edu/~[USER]/ -- Best, [STAFF] DOIT HPC System Administrator"
"3298342","2025-10-23 15:52:23","HPC Other Issue: 5-days time limit not enough for jobs","Unfortunately, under the current cluster model only contributing groups have access to the 'shared' QOS option. In your situation, I would recommend seeing if you can optimize your code to run faster, in parallel, or across multiple nodes. You can also attempt to break apart your one large job into two smaller sets of jobs, which would allow you to complete the run in less than 5 days. Let me know if you have any additional questions! Have a nice day! -- Kind regards, [STAFF]. On Thu Oct 23 10:09:11 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUS_ID] Request Type: High Performance Cluster Hello all! I'd like to ask your help with the jobs I'm submitting in chip cluster. A few of them are pretty heavy, and the 5-days limit to run are not being enough. I saw there's the possibility to run in the partition 'shared', with 14-days limit, but I don't have access to that. Is it possible to give me access to that partition, or maybe to increase the time limit in the partition 'general'? In general my samples run within the 5 days, I would require more time only for samples whose jobs abort after 5 days. Thank you! Best, [USER]."
"3298471","2025-10-23 16:21:05","HPC Other Issue: [USER] cannot login to their ada account","Many Thanks [STAFF], Regards, [USER] On Thu, Oct 23, 2025 at 12:18 PM [STAFF] via RT <[EMAIL]> wrote: If you agree your issue is resolved, please give us feedback on your experience by completing a brief satisfaction survey https://umbc.us2.qualtrics.com/SE/?SID=SV_etfDUq3MTISF6Ly&customeremail=[EMAIL]&groupid=EIS&ticketid=3298471&ticketowner=[STAFF]&ticketsubject=HPC Other Issue: I cannot login to my ada account If you believe your issue has not been resolved, please respond to this message, which will reopen your ticket Note: A full record of your request can be found at Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3298471> Hi [USER], The hpc cluster 'ada' has been completely absorbed by 'chip' as of April 10th Please see this myumbc posting detailing the rollout of chip and the deletion of ada https://my3.my.umbc.edu/groups/hpcf/posts/147513 On Thu Oct 23 12:01:48 2025, [USER] wrote First Name [USER] Last Name [USER] Email [EMAIL] Campus ID [USER] Request Type High Performance Cluster Hello Im currently unable to access my ADA account When I try to connect via SSH using the command below ssh [USER]@ada.rs.umbc.edu I receive the following error message and do not even get the password prompt ssh: connect to host ada.rs.umbc.edu port 22 Connection timed out Could you please help me resolve this issue Thank you [USER] Best [STAFF] DOIT HPC System Administrator Original Request Requestors [USER] First Name [USER] Last Name [USER] Email [EMAIL] Campus ID [USER] Request Type High Performance Cluster Hello Im currently unable to access my ADA account When I try to connect via SSH using the command below ssh [USER]@ada.rs.umbc.edu I receive the following error message and do not even get the password prompt ssh: connect to host ada.rs.umbc.edu port 22 Connection timed out Could you please help me resolve this issue Thank you [USER]"
"3300690","2025-11-04 18:52:46","Data upload and server processing","Hi [USER], Welcome to chip, UMBC's High Performance Computing Cluster! The group, usda-eb, now exists on the chip cluster. Members of this group can access and contribute to the research storage space allocated to the group. This storage space is located at /umbc/rs/usda-eb, and currently has a quota of 50T. For information on accessing the cluster, adding accounts to your group, and getting started using the cluster, check out the tutorial on our wiki: https://umbc.atlassian.net/wiki/x/R4BPQg. Additional documentation is also available here: https://umbc.atlassian.net/wiki/x/FwCHQ. If you have any questions or issues, please submit a new RT ticket at: https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research Computing. Currently, [STAFF] is the 'owner' of the group, and is the only member of the group. To request users to be added to your group, please submit an RT ticket from the following link: https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research Computing. Additionally, I will need a little bit more information regarding your grant. Could you please provide an award number, title, and abstract? If you have any additional concerns, questions, or run into any problems, please feel free to submit a new ticket! Have a great day! Kind regards, [STAFF]"
"3301564","2025-10-30 18:16:49","HPC Other Issue: very slow editing on /home/[USER] on c24-52","Hi [USER], OK all logged outta chip and our machine -[STAFF] On Thu, Oct 30, 2025 at 2:10 PM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3301564> Last Update From Ticket: Hi [USER], Please exit out of /umbc/xfs2/strow on all devices where you may be logged in. We can't unmount the device until it's no longer active. On Thu Oct 30 13:44:40 2025, [USER] wrote: Hi [STAFF] Thanks for the reply. Maybe you should also cc [STAFF] as you all work on this ([STAFF]) [USER] On Thu, Oct 30, 2025 at 1:30 PM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3301564> Last Update From Ticket: [USER], There is currently some degradation on the file system that sits on xfs2. We're investigating into how to resolve, but out of an abundance of caution, we are going to unmount xfs2 so that the degradation doesn't spread. We'll let you know when we have more information for you, but expect to not have access to your mount for a while. On Thu Oct 30 13:23:27 2025, [USER] wrote: Hi [USER], We are aware of the issue and are currently investigating. I'm going to merge this with the other ticket since they are related. On Thu Oct 30 12:53:30 2025, [USER] wrote: First Name: [USER] Last Name: [STAFF] Email: [EMAIL] Campus ID: [USER] Request Type: High Performance Cluster Have the disk issues been worked on yet (ticket #3301564)? I still can't edit files And now Matlab took about 3 minutes to start up on c24-52 There is something seriously wrong! Thanks [USER]"
"3302104","2025-10-31 15:07:37","HPC Other Issue: Need cuDNN 8.4.1.50 module on CHIP","Hello I have installed the module cuDNN/8.4.1.50-CUDA-11.7.0, let us know if you still unable to find the module. On Wed Oct 29 14:22:27 2025, [STAFF] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: High Performance Cluster Hi, I need to work on TensorFlow 2.11 which using CUDA 11.7 version and cuDNN 8.4. I saw CUDA 11.7, however there is no cuDNN 8.4. The previous cluster (ADA) have module 'cuDNN/8.4.1.50-CUDA-11.7.0'. Can you install that module on CHIP also? Thank you so much, [USER] Attachment 1: Screenshot 2025-10-29 141531.png Best, [STAFF] DOIT Unix infra, Graduate Assistant"
"3304171","2025-10-30 17:23:27","HPC Other Issue: system running slow, can't get anything done","Hi [USER], We are aware of the issue and are currently investigating. I'm going to merge this with the other ticket since they are related. On Thu Oct 30 12:53:30 2025, [STAFF] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              High Performance Cluster Have the disk issues been worked on yet (ticket #3301564)? I still can't edit files And now Matlab took about 3 minutes to start up on c24-52 There is something seriously wrong! Thanks [USER] Best, [STAFF]"
"3304442","2025-11-04 14:26:41","HPC Other Issue: more hardware details","Everything is EDR. On Thu Oct 30 21:44:22 2025, [STAFF] wrote: Hi, [USER], Thanks for the Dell model. Yes, I read that Wiki page. My request is to know more precisely than 'high speed backend Infiniband network supporting 100 Gbps'. That phrase is well-written to cover all nodes from 2018 to 2024. But is the 2024 network not a newer one? Purchased in 2024? Either way, I am asking for a technical term like DDR = dual data rate, QDR = quad data rate, EDR = extended data rate, like I am recalling from the past. Does the 2024 portion's network not have a phrase like that with it? On Thu, Oct 30, 2025 at 7:03 PM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.[DOMAIN]/Ticket/Display.html?id=3304442> Last Update From Ticket: Hi [USER], The model for the nodes used in the 2024 are PowerEdge R660. Specifications on the network abilities and cpus themselves can be found here: https://[DOMAIN].atlassian.net/wiki/spaces/faq/pages/1289486353/Cluster+Specifications On Thu Oct 30 16:07:02 2025, [STAFF] wrote: > First Name: [USER] > Last Name: [USER] > Email: [EMAIL] > Campus ID: [ID] > Request Type: High Performance Cluster > Hi, I would like some more fine points of hardware information. I am referring to the 2024 portion of chip. - CPU: what is the model number of the node from Dell for the 2024 compute nodes? - The 2024 CPU nodes are connected by which InfiniBand? EDR? Some specs available?"
"3304534","2025-10-30 22:55:41","HPC Other Issue: Reset [USER]'s Password","Hi [USER], Your password is linked to your umbc account. So you would just need to use the myumbc reset password. https://umbc.atlassian.net/wiki/spaces/faq/pages/30736467/I+have+forgotten+my+myUMBC+password.+What+should+I+do Note that it may take a minute to update on chip. On Thu Oct 30 17:52:57 2025, [STAFF] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUS_ID] Request Type: High Performance Cluster Dear Sir/Madam, I would like to access the chip server, but after logging in with my ID [USER], my password does not work. I am not sure of the reason. Could you please provide me with a link or instructions to reset my password? Thank you very much for your assistance. Regards, [USER] [CAMPUS_ID]. Best, [STAFF] DOIT HPC System Administrator"
"3304703","2025-10-31 14:44:35","HPC Other Issue: Request for sample SLURM job script to run WRF on [SYSTEM]","Hi [USER], First off, I wanted to clarify a couple points. 1. 'I was advised that running ./wrf.exe interactively is not permitted': This is untrue. You are able to run interactive jobs, where you create a slurm allocation on a node, then connect to it and run your code directly. You are not permitted to run jobs on the login node, which I believe is what you are thinking of. If you would like some documentation on how to run an interactive job, I'll include a link to a page with the commands required to do that. How to run interactive job: https://umbc.atlassian.net/wiki/x/CYCbQw 2. There is also a WRF module available to be loaded on chip. You can load WRF with: 'module load WRF/4.4-foss-2022a-dmpar'. WPS is also available, and can be loaded with: 'module load WPS/4.4-foss-2022a-dmpar'. These installations should work, however it is a slightly older version (4.4), whereas you compiled a newer one (4.5). 3. For your locally installed and compiled copy of WRF, if you desire to use it you must use the compiled binaries located under /umbc/rs/pi_cichoku/users/[USER]/model2/sources/WRF/main. This is where the actual WRF binaries are stored, the directory you provided was a test directory for testing the software. You will want to add this location to your PATH environment variable to utilize the binaries. It is also possible, depending on how your install was compiled, that it would need to be recompiled for MPI support. For more info, check out the Building WRF section of this page: https://www2.mmm.ucar.edu/wrf/OnLineTutorial/compilation_tutorial.php We do not have any existing documentation on running WRF/WPS explicitly, however we have documentation on running MPI jobs, including some example SBATCH scripts: https://umbc.atlassian.net/wiki/x/AQCKV There is documentation on WRF's website for running WRF using mpi, take a look here: https://www2.mmm.ucar.edu/wrf/users/wrf_users_guide/build/html/running_wrf.html If you have any additional questions or need clarification, please feel free to let me know. For now I will mark this as resolved. Have a good day! -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Fri Oct 31 05:37:55 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: High Performance Cluster Hi HPC Team, I'm a graduate student working on the WRF model on the UMBC CHIP cluster. I was advised that running ./wrf.exe interactively is not permitted and that I should submit the run via SLURM because of the job's size and runtime. Could you please provide (or point me to) a sample SLURM job script for a parallel WRF run on CHIP? A template with the recommended directives and module loads would be very helpful. Specifically, I'm looking for guidance on: For context (in case it helps tailor the template): Executable(s): real.exe and wrf.exe (WRF-ARW) Input: single domain (d01) for a short 48-hour case; netCDF met_em and boundary files are ready Code location: /umbc/rs/pi_cichoku/users/[USER]/model2/sources/WRF/test/em_real/ I can rebuild with the cluster's preferred MPI stack if needed. If there's existing documentation or example scripts for CHIP users running WRF (or other large MPI jobs), a link would be great. Thank you very much for your help! Best regards, [USER] Graduate Student, GES UMBC"
"3305879","2025-11-04 20:24:14","HPC Other Issue: Not able to login to the cluster (chip) through terminal","Hi [STAFF], Apologies for late reply Yes, still I face the issue and I tried with campus vpn and also my personal wifi. Both are giving the same problem. Could you help me in checking the issue. Thankyou for your help. Regards, [USER]. Campus ID: [CAMPUS_ID] On Mon, Nov 3, 2025 at 11:01 AM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3305879> Last Update From Ticket: Hi [USER], Is this still an issue? We haven't seen any other reports of people being unable to login. Are you on campus vpn? What sort of internet connection do you have (wifi or wired)? On Fri Oct 31 15:04:48 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUS_ID] Request Type: High Performance Cluster Previously, I was able to login through the cluster (chip) but suddenly the login to cluster is not working."
"3306147","2025-11-01 17:20:44","HPC Slurm/Software Issue: [USER] Cannot Cancel Their Jobs","Understood and no problem. I've cancelled all of your jobs submitted to the match partition that were pending. When canceling your own jobs, be sure to specify the cluster with something like `scancel -M chip-cpu JOBID`. On Sat Nov 01 12:18:16 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CID] Request Type:              High Performance Cluster Hello, I am having an issue with chip where I cannot cancel a number of jobs that are in queue. I used a script to create and submit the jobs last night but realized that I had a small mistake. I have tried to cancel all jobs under my username, [USER], and cancel specific jobs by id but they remain in the queue. The jobs have remained in the queue for more than 12 hours since I first tried to cancel them. All pending jobs on the match partition under my username need to be cancelled."
"3306171","2025-11-04 16:38:35","HPC Other Issue: software request","Hi [USER], The modules, RSEM and BOWTIE2 have been installed on chip! I installed RSEM/1.3.3-foss-2022a, and Bowtie2/2.4.5-GCC-11.3.0. These can be loaded with 'module load RSEM/1.3.3-foss-2022a' (note, RSEM depends on BOWTIE2, so the correct version of BOWTIE2 is loaded when you load RSEM). If you encounter any issues using the modules, or need any additional modules, feel free to submit a new ticket! Have a great day! -- Kind regards, [STAFF] On Sat Nov 01 13:50:40 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster Is it possible to request modules/packages to be added to the cluster? I would like to be able to use RSEM and BOWTIE2. Thanks!"
"3306427","2025-11-03 16:57:43","HPC Slurm/Software Issue: SSH connection failing","Hi [USER], Since the load balancer issue was fixed, we have not seen any other reports of this issue. I am currently testing myself, and have not encountered any disconnects. Would you mind sharing a bit more about your setup? How are you sshing (ie, a dedicated client like putty, or just through your terminal)? Do you have any local SSH configuration? Are you logged in using GlobalProtect VPN? Let me know and I can look into this further for you! Have a nice day! -- Kind regards, [STAFF] On Sun Nov 02 19:04:14 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster I have a few ssh sessions open connecting to chip and they keep disconnecting every few minutes. Previously, this had been a problem and it was resolved (something to do with a load balancer) but its back now."
"3306608","2025-11-03 15:16:42","HPC Slurm/Software Issue: Slurm Not Loading Anaconda Module - Job ID 113232-113234","There appears to have been a minor configuration error with the node that job was running on (g24-11). This has now been resolved. If you continue to experience issues, please feel free to submit a new ticket! Have a nice day! Kind regards, [STAFF] On Mon Nov 03 09:47:01 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster I went to run a job this morning that I have run quite often. The first request for this job (ID 113231) started just fine. Subsequent requests error out in my Python script indicating issues loading modules from my Conda environment. The job log shows that 'conda' is not a known command, indicating an issue loading the Anaconda module."
"3308276","2025-11-03 18:12:12","HPC Other Issue: Data missing from '[ID]/rs/[ID]/users/[ID]'","Hi [USER], Currently, the research volume for pi_zzbatmos is undergoing migration to a new storage server (ceph). This date was scheduled with your PI in advance, and during the time of migration we request that all users in the group do not use chip to avoid disturbing your groups migration. Your PI should have more information regarding the specifics. However, since pi_zzbatmos has a very large research volume, the migration is taking a long time (it was started on Friday, and is still going). After the migration properly completes, you should have access to your data. Something that you already noticed, the new Ceph research volumes start with 'pi_'. For example, the old volume was mounted at '/umbc/rs/zzbatmos', where the new volume (the one you noticed) is located at '/umbc/rs/pi_zzbatmos'. Your PI should let the group know when the migration is finished. But if you have any additional questions in the meantime, feel free to let us know! Have a good day! First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster Hi, I hope this email finds you well. I noticed that my data from 'umbc/rs/zzbatmos/users/[USER]' is missing. I found that there is another directory 'umbc/rs/pi_zzbatmos/users/[USER]' in a similar path but this new directory contains partial data. Could you please let me know what happened to my original data or if it was moved somewhere else? thank you."
"3308743","2025-11-04 14:20:34","HPC Slurm/Software Issue: Pending Jobs Not Cancelling","Hi, Well, when I squeued for your jobs just now, none of them came up, meaning I suppose they finished. If you could give me the exact path to the jobs you were trying to cancel, I could test some things myself. Without more information, I can't test much myself. However, looking at the history of your 'scancel' commands, I do notice a mistake you made, which could very well be the reason it didn't work. You ran: scancel -u [USER] This is not correct on the Chip cluster; the correct command would've been: scancel --cluster=chip-cpu -u [USER] or scancel --cluster=chip-gpu -u [USER] Yes, I know it's a bit confusing, but you have to specify the cluster CPU/GPU to cancel all jobs for your user. Here's some wiki documentation about that: https://[DOMAIN]/wiki/spaces/faq/pages/[PAGEID]/Basic+Slurm+Commands#Managing-and-Controlling-Jobs Let me know if that was the issue. Best, [STAFF]"
"3309956","2025-11-04 17:57:29","HPC Slurm/Software Issue: Request to reserve GPU space for a deadline in December ACL 2026","First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster My student, [USER], would like to reserve GPU nodes (preferably H100) for 2 weeks to complete her experiments for the ACL deadline. Can you please assist her with this?"
"3310226","2025-11-04 21:01:54","N-Mode cavity laser simulation - In need of computing power","Please disregard this ticket request."
