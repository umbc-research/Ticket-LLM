"TicketID","CreatedDate","SubjectNoHTML","TransactionContent"
"3280927","2025-09-25 16:56:10","HPC Slurm/Software Issue: GPU Job QoS not Permitted","Hi [USER], I see that your job is currently running without issue. Did you get that output from squeue? If so, you can see that the job status is 'PD' which means pending. I am not 100% sure why it is showing this exact error, but it seems the nodes were allocated shortly after the start of the job. Are there any actual errors with the job itself? We are looking into the cause of this message, however it does not seem to actually be causing any errors. #!/bin/bash #SBATCH --job-name=KcollectLVL #SBATCH --output=klog/collect_%A_%a.out #SBATCH --error=klog/collect_%A_%a.err #SBATCH --mem=64G #SBATCH --time=72:00:00 #SBATCH --constraint=rtx_6000 #SBATCH --gres=gpu:4 #SBATCH --array=6,7,12,13,20 #SBATCH --mail-user=[EMAIL] #SBATCH --mail-type=END,FAIL #SBATCH --partition=gpu-general"
"3281158","2025-09-30 17:10:20","HPC User Account: [ID] in Student Group","approved Ticket [STAFF] via RT [EMAIL] wrote: I confirm [USER] is also an iHARP student besides his current group setup. Thanks! On Tue, Sep 30, 2025 at 9:46 AM [STAFF] via RT [EMAIL] wrote: Hello, We just need the written permission from [STAFF] and [STAFF]. They are cc'd. Then I can add you to the iHarp group as usual. No rush, Elliot [STAFF]"
"3281243","2025-09-26 00:00:06","HPC Other Issue: Nodes on partition [ID] being used in partition 2024","I was unaware of the 10 minute grace period.  Many apologies.  My calculations did run.  I appreciate your help and information on this.  Thank you! [USER] On Thu, Sep 25, 2025 at 4:41 PM [STAFF] via RT <[EMAIL]> wrote: Ticket Last Update From Ticket: Hello [USER], That is correct - nodes in the pi_bennettj partition will preempt jobs from users that are not in the pi_bennettj group after a 10 minute grace period. It looks like your jobs were submitted at 3:48pm EST and started ~10 minutes later at 3:58pm EST. Can you confirm that your job is running now? Thank you, [STAFF] On Thu Sep 25 15:55:41 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster Hello, I'm trying to run jobs on the pi_bennettj partition, however, 4 of our nodes are being used on partition 2024. I was under the impression that the pi researchers would preempt anyone using our nodes. I've attached a list of the nodes being used. Thank you, [USER] Attachment 1: nodes.txt"
"3281293","2025-09-29 16:34:21","HPC Other Issue: jobs getting killed on 2024 queue","Yes had much better success over the weekend Thanks [USER] On Mon Sep 29 2025 at 12=3D03=E2=80=AFPM [STAFF] via RT <[EMAIL]> wrote Ticket <URL https//rt umbc edu/Ticket/Display html?id=3D3281293> Last Update From Ticket I see Is everything working as intended now Kind regards [STAFF] DoIT Unix Infra Student Worker On Fri Sep 26 16=3A41=3A53 2025 [USER] wrote Whoops sorry was submitting the batch file designed for taki ... -[USER] On Fri Sep 26 2025 at 4=3A38 PM [USER] <[EMAIL]> wrote Hi [STAFF] Thanks for the info I tried this just now #SBATCH --job-name=3DFIT_TILE_TRENDS ## name #SBATCH -N1 ## number of job step is to be allocated per instance of matlab #SBATCH --cpus-per-task 1 ## tasks per node/number of cores per matlab session will be ##SBATCH --partition=3D2024 ## desired partition #SBATCH --partition=3Dmatch ## desired partition #SBATCH --cluster=3Dchip-cpu ## desired cluster #SBATCH --account=3Dpi_strow #SBATCH --qos=3Dshared ## qos to get as many cpu2024 as possible else put pi_strow and get /bin/rm cannot remove '*~' No such file or directory /bin/rm cannot remove 'slurm*.err' No such file or directory sbatch error Missing '--gres' sbatch error You must specify a Generic RESource to use in your job sbatch error See this webpage for more details https//hpcf umbc edu/compute/overview/. sbatch error Batch job submission failed Unspecified error which according to the webpage you pointed me to should be used for gpu processors Thanks [USER] On Fri Sep 26 2025 at 9=3A52 AM [STAFF] via RT <[EMAIL]> wrote If you agree your issue is resolved please give us feedback on your experience by completing a brief satisfaction survey https//umbc us2 qualtrics com/SE/?SID=3DSV_etfDUq3MTISF6Ly&customeremail= =3D[EMAIL]&groupid=3DEIS&ticketid=3D3281293&ticketowner=3D[STAFF]%40umbc edu&ticketsubject=3DHPC+Other+Issue%3A+jobs+getting+killed+on+2024+queue If you believe your issue has not been resolved please respond to this message which will reopen your ticket Note A full record of your request can be found at Ticket <URL https//rt umbc edu/Ticket/Display html?id=3D3281293> Thank You _________________________________________ R e s o l u t i o n Hi [USER] First off let me elaborate on the slurm job ID's you are seeing When submitting an array job there is the main job ID which is 402067 in this case along with the array tasks for each which are like 402067_90 However each individual array task also gets its own actual job ID For example the actual job ID for array task 402067_90 is 402157 Next your job was likely canceled due to someone in pi_bennettj attempting to run a job on their nodes Nodes c24-[01-10] are all nodes contributed from pi_bennettj therefore they have priority access and the ability to preempt other users not in their group If you want to run your jobs on 2024 nodes but do not want to risk preemption use the 'match' partition If you would like more information on which partitions allow/disallow preemption check out this wiki page https//umbc atlassian net/wiki/spaces/faq/pages/1249509377/chip+Partitions+and+Usage#Partitions Let me know if that helps Have a nice day Kind regards [STAFF] DoIT Unix Infra Student Worker On Thu Sep 25 16=3A56=3A36 2025 [USER] wrote First Name [USER] Last Name [USER] Email [EMAIL] Campus ID [CAMPUSID] Request Type High Performance Cluster For some reason many of my jobs are getting killed/preempted slurmsteps I thought I had an issue with the code but seems fine to me Haven't had that before Is there a way to limit the jobs only to Larrabee's computer Wierd this is suppose the slurm job ID is 402067 I keep getting told eg 402166 has been killed?? makes no sense [USER]@chip-login1 AI_RTA$ grep -in slurmst slurm* slurm-402067_90 out=3A17=3A.........+slurmstepd error *** JOB 402157 ON c24-02 CANCELLED AT 2025-09-25T15=3A58=3A40 DUE TO PREEMPTION *** slurm-402067_91 out=3A17=3A.........+slurmstepd error *** JOB 402158 ON c24-02 CANCELLED AT 2025-09-25T15=3A58=3A40 DUE TO PREEMPTION *** slurm-402067_92 out=3A17=3A.........slurmstepd error *** JOB 402159 ON c24-02 CANCELLED AT 2025-09-25T15=3A58=3A40 DUE TO PREEMPTION *** slurm-402067_93 out=3A17=3A.........+.slurmstepd error *** JOB 402160 ON c24-02 CANCELLED AT 2025-09-25T15=3A58=3A40 DUE TO PREEMPTION *** slurm-402067_94 out=3A17=3A.........+..slurmstepd error *** JOB 402161 ON c24-02 CANCELLED AT 2025-09-25T15=3A58=3A40 DUE TO PREEMPTION *** slurm-402067_95 out=3A17=3A.........+.slurmstepd error *** JOB 402162 ON c24-02 CANCELLED AT 2025-09-25T15=3A58=3A40 DUE TO PREEMPTION *** slurm-402067_96 out=3A17=3A.........slurmstepd error *** JOB 402163 ON c24-02 CANCELLED AT 2025-09-25T15=3A58=3A40 DUE TO PREEMPTION *** slurm-402067_97 out=3A17=3A.........+slurmstepd error *** JOB 402164 ON c24-02 CANCELLED AT 2025-09-25T15=3A58=3A40 DUE TO PREEMPTION *** slurm-402067_98 out=3A17=3A.........+.........+...slurmstepd error *** JOB 402165 ON c24-03 CANCELLED AT 2025-09-25T16=3A00=3A57 DUE TO PREEMPTION *** slurm-402067_99 out=3A17=3A.........+.........slurmstepd error *** JOB 402166 ON c24-03 CANCELLED AT 2025-09-25T16=3A00=3A57 DUE TO PREEMPTION *** Original Request Requestors [USER] First Name [USER] Last Name [USER] Email [EMAIL] Campus ID [CAMPUSID] Request Type High Performance Cluster For some reason many of my jobs are getting killed/preempted slurmsteps I thought I had an issue with the code but seems fine to me Haven't had that before Is there a way to limit the jobs only to Larrabee's computer Wierd this is suppose the slurm job ID is 402067 I keep getting told eg 402166 has been killed?? makes no sense [USER]@chip-login1 AI_RTA$ grep -in slurmst slurm* slurm-402067_90 out=3A17=3A.........+slurmstepd error *** JOB 402157 ON c24-02 CANCELLED AT 2025-09-25T15=3A58=3A40 DUE TO PREEMPTION *** slurm-402067_91 out=3A17=3A.........+slurmstepd error *** JOB 402158 ON c24-02 CANCELLED AT 2025-09-25T15=3A58=3A40 DUE TO PREEMPTION *** slurm-402067_92 out=3A17=3A.........slurmstepd error *** JOB 402159 ON c24-02 CANCELLED AT 2025-09-25T15=3A58=3A40 DUE TO PREEMPTION *** slurm-402067_93 out=3A17=3A.........+.slurmstepd error *** JOB 402160 ON c24-02 CANCELLED AT 2025-09-25T15=3A58=3A40 DUE TO PREEMPTION ***"
"3281491","2025-10-03 15:01:47","Srun not working for a parallel task","I apologize for the late reply. I wanted to suggest trying to add --mpi=pmix to srun commands (srun --mpi=pmix -n 8 /path/to/your/test --verbose). I think srun might be defaulting to a single rank, which could explain why you're seeing the 'Insufficient processes' error even when requesting multiple tasks and srun working when task is set to 1. pmix (Process Management Interface for Exascale) is the plugin that enables Slurm to correctly launch and manage MPI tasks across multiple ranks. Without specifying it, srun may not properly initialize the MPI environment, Let me know if that doesn’t resolve the issue. On Fri Sep 26 12:00:58 2025, [STAFF] wrote: Hi, I have listed the 6 files (SLURM scripts, job outputs, executable outputs) in the next section. In short, the 'mpirun' launches my program successfully, while 'srun' leads to several invalid memory access issues. Both SLURM scripts launch 31 executables at the same time (on 1 CPU node with 8 CPU cores). The only difference is the MPI launcher 'srun' v.s. 'mpirun'. 'srun' version: - Work directory for the 'srun' version: [DIRECTORY] - 'srun' job script: [FILE] - 'srun' job output: [FILE] - Actual 'srun' commands and output: [FILE] 'mpirun' version: - Work directory for the 'mpirun' version: [DIRECTORY] - 'mpirun' job script: [FILE] - 'mpirun' job output: [FILE] - Actual 'mpirun' commands and output: [FILE] The 'mpirun' command I use for this test is from a Spack build of OpenMPI with the following spec: -- linux-rhel9-cascadelake / %c,cxx,fortran=gcc@13.3.0 ---------- openmpi@5.0.8+atomics~cuda~debug+fortran~gpfs~internal-hwloc~internal-libevent~internal-pmix~ipv6~java~lustre~memchecker~openshmem~rocm~romio+rsh~static~two_level_namespace+vt+wrapper-rpath build_system=autotools fabrics:=none romio-filesystem:=none schedulers:=none Best, [USER] On Sep 26, 2025, at 10:38 AM, via RT [EMAIL] wrote: Greetings, This message has been automatically generated in response to the creation of a ticket regarding: ---------------------------------------------------------------------- Subject: 'Srun not working for a parallel task ' Message: [STAFF] had an office hour with [STAFF] and [STAFF]. We weren’t able to provide an immediate solution. [USER], please add the errors you get, your slurm script, and the working directory. ---------------------------------------------------------------------- There is no need to reply to this message right now. Your ticket has been assigned an ID of [TICKET] or you can go there directly by clicking the link below. Ticket <URL: https://www.google.com/url?q=https://rt.umbc.edu/Ticket/Display.html?id%3D[TICKET]&source=gmail-imap&ust=1759502303000000&usg=DAOvVaw1BhV3ToX1x16wyeHk2LQlO> You can login to view your open tickets at any time by visiting https://www.google.com/url?q=http://my.umbc.edu&source=gmail-imap&ust=1759502303000000&usg=DAOvVaw0xuQYMPyzRNQkSErohTnsE and clicking on 'Help' and 'Request Help'. Alternately you can click on https://www.google.com/url?q=http://my.umbc.edu/help&source=gmail-imap&ust=1759502303000000&usg=DAOvVaw0m_jvA-yDKTyCKcX8QkLj3 Thank you -- Best, [STAFF] DOIT Unix infra, Graduate Assistant"
"3281714","2025-09-26 18:20:20","HPC Slurm/Software Issue: Cannot create slurm job","Thank you On Fri Sep 26 2025 at 1=55 PM [STAFF] via RT [EMAIL] wrote Ticket Last Update From Ticket Hi [USER] Chip is not down That being said I did notice that the configuration was acting strange and it turns out that it was trying to submit jobs to the gpu-contrib partition which is non-submittable partition Ive changed that the gpu partition is back to being the default partition and should work In the future you can always check to see if chip is really down by using the sinfo command Additionally you can also specify the partition using the --partition=3D${gpu_partition} Let me know if you have more questions about this Ill leave the ticket open for a few days On Fri Sep 26 13=36=48 2025 [USER] wrote First Name [USER] Last Name [USER] Email [EMAIL] Campus ID [CAMPUS_ID] Request Type High Performance Cluster Hello I attempted to run a slurm job srun --cluster=3Dchip-gpu --account=3Dpi_[USER] --mem=3D20000 --time=3D12=00=00 --gres=3Dgpu=1 --pty $SHELL Recieved srun Requested partition configuration not available now I am pretty sure chip is down would it be possible to recieve an update when chip is available again Thank you Best [STAFF] DOIT HPC System Administrator"
"3282078","2025-10-31 13:11:06","Cloud: Google Vision","Waiting for info from [LOCATION] to get GCP billing in place. -- [STAFF], Associate Director of Research and Enterprise Computing, Enterprise Infrastructure Solutions, [ORGANIZATION]."
"3282188","2025-11-04 16:49:54","HPC Slurm/Software Issue: Please install newer Matlab","The provided text appears to be an email conversation between Matthias Gobbert and Max Breitmeyer, discussing the installation of MATLAB R2025b on a high-performance cluster at UMBC. Here's a summary of the conversation:  * Matthias Gobbert requests the installation of MATLAB R2025b, mentioning that the current version (R2023b) is two years old. * Max Breitmeyer responds, stating that he has added MATLAB 2025b to the 2024 partitions and is working on installing it on the 2018 and 2021 partitions. * Matthias Gobbert reports an error when trying to use MATLAB 2025b and provides logs from a job that failed. * Max Breitmeyer investigates the issue and determines that there are concerns about licensing and compatibility with older partitions. * The conversation continues, with Max Breitmeyer providing updates on his progress and Matthias Gobbert offering suggestions and feedback.  The text also includes some extraneous information, such as email signatures and requestor information."
"3282261","2025-09-29 16:59:49","HPC User Account: [USER] in Pi_ksolaima","Hi [USER], Your account ([USER]) has been added to the pi_[STAFF] group on chip.rs.umbc.edu . Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/). Kind regards, [STAFF] DoIT Unix Infra Student Worker On Mon Sep 29 05:51:37 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster Create/Modify account in existing PI group Existing PI Email:    [STAFF]@umbc.edu Existing Group:       Pi_[STAFF] Project Title:        Multimodal information retriever Project Abstract:     This project focuses on the development of a robust system for anomaly detection in multivariate, irregularly-sampled time-series data, with an application to identifying electricity theft from the Smart Grid Generated Data (SGCC) dataset. The primary methodology involves the implementation and training of Neural Controlled Differential Equations, a deep learning architecture designed to capture complex temporal dependencies. I am developing and training a series of deep learning models for my time-series analysis research. On my current resources, the training process is prohibitively slow, with each epoch taking approximately 40-45 minutes to complete. This makes iterative development, debugging, and essential hyperparameter tuning impractical. Access to the HPC cluster would significantly accelerate this research by enabling me to run multiple experiments in parallel and iterate on model architectures and feature engineering more efficiently. Thank you for your time and consideration."
"3282435","2025-09-30 15:09:21","Need to create account in the chip cluster under [STAFF]'s group","Correct request was submitted: https://rt.umbc.edu/Ticket/Display.html?id=[TICKET_ID] -- Best, [STAFF], [POSITION]"
"3282455","2025-10-03 13:48:18","HPC Other Issue: client_loop: send disconnect: Broken pipe happens every few minutes","Hi [USER], We've made a change to the ssh config that we are hoping will solve the issue. Could you keep an eye on it let us know if you notice a difference? On Mon Sep 29 13:24:40 2025, [STAFF] wrote: Hi [USER], we've received reports from other researchers who are experiencing something similar and we are currently investigating the issue. A little more information might help us: - First, can you give us exact times and dates for some of the disconnects you are seeing? This will help us look at the connection logs to find where it happened. - Second, what kind of connection do you have? Wired or wireless? Are you on the vpn when connecting to chip? - Third, are you experiencing it on one of the login nodes more than another? Let me know the above and we can go from there. If you experience anymore of these disconnects please write down the exact date and time you experienced it so we can look into it further. On Mon Sep 29 11:46:36 2025, [STAFF] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              High Performance Cluster Hello, since last week I have been disconnected repeatedly after logging into chip after only a few minutes of waiting for jobs to progress. I am using the standard 'ssh [USER]@chip.rs.[DOMAIN]' command that has been working for months, but it is disconnecting me too frequently. Why is the Connection to chip.rs.[DOMAIN] closed by remote host closed so much while I'm working? This is interrupting our research. Is there a different way to login now of which I am unaware? Thank you!"
"3282508","2025-09-30 20:11:03","HPC Other Issue: Need to install a module for WRF Simulation.","Hello [USER], good afternoon. Thank you very much for providing this information. Actually, I tried to find the module using 'module spider gfortran' to check if it is available or not. I will try to load the module using the way you mentioned and complete my work. If not I will ask for further help. Regards, [STAFF]. On Mon Sep 29 12:57:26 2025, [EMAIL] wrote: Hi [USER], What happens when you attempt to use gfortran? Without loading any modules, there is a library for gfortran located in /usr/bin/gfortran, which should be on your PATH. Additionally, you can load more specific versions of gfortran through other modules. For example, if you load GCCcore with 'module load GCCcore', you can see that the gfortran library is also present, now located at '/usr/ebuild/installs/software/GCCcore/13.3.0/bin/gfortran' (you can verify yourself with 'which gfortran' to show you the location of the gfortran library). Let me know if this helps! -- Kind regards, [STAFF] DoIT Unix Infra Student Worker. On Mon Sep 29 12:33:11 2025, [EMAIL] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster Hello, I need to run the WRF climate simulation model to generate datasets for my project. To run this simulation model, I need to use 'gfortran' library, but this is not available in the CHIP cluster. It would be very helpful if you could install the library. Please check the given link if you need further information about running the WRF model. WRF Tutorial Link: https://www2.mmm.ucar.edu/wrf/OnLineTutorial/compilation_tutorial.php#STEP1"
"3283387","2025-09-30 15:55:36","HPC User Account: [ID] in pi_[USER]","Hi [USER], Your account ([USER]) has been added to the [GROUP] group on chip.rs.umbc.edu. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) On Tue Sep 30 11:30:11 2025, [STAFF] wrote: Approve it. Thanks! On Tue, Sep 30, 2025 at 11:07 AM RT API via RT <[EMAIL]> wrote: This e-mail is a notification that a UMBC user: [USER] <[EMAIL]> has requested an account within UMBC's HPC environment in your group <[GROUP]>. As the PI, we request that you acknowledge and approve this account creation by replying to this message. Alternatively you can go to this link and review the ticket and indicate your decision here: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3283387> Once we have your approval, we will create the account and you and the new user will receive another e-mail notifying you that the account has been created. If you have any other questions or concerns please contact us. - UMBC DoIT Research Computing Support Staff Best wishes Sincerely yours [STAFF] <[EMAIL]> Website: https://bdal.umbc.edu/people/[STAFF]/ WebEx: https://umbc.webex.com/meet/[STAFF] Professor of Data Science, Department of Information Systems Director, Big Data Analytics Lab Director, Center for Scalable Data and Computational Science (ScaleS) Co-Director, NSF REU Site on Online Big Data Analytics Co-Lead, NSF HDR institute for Data and Model Revolution in the Polar Regions (iHARP) University of Maryland, Baltimore County [PHONE NUMBER] ITE 423 -- Best, [STAFF] DOIT Unix infra, Graduate Assistant"
"3283454","2025-09-30 17:15:05","HPC User Account: [USER] in Student Group","Hello there, Here is how to request an account on chip: https://[SERVER_URL]/wiki/spaces/faq/pages/[PAGE_ID]/How+to+request+a+[ACCOUNT_TYPE]+account+on+[SERVER_NAME] Let me know if that helps, [STAFF]"
"3283590","2025-10-01 13:55:24","HPC User Account: [ID] in pi_[STAFF]","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://[EMAIL]/wiki/spaces/faq/pages/[NUMBER]/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link (https://[EMAIL]/request-tracker-rt/[STAFF]-research-computing/). On [DATE] [STAFF] wrote: Yes, [USER] is a PhD student working in my group. Thanks. Best regards, [STAFF]. On [DATE], at [TIME]PM, RT API via RT <[EMAIL]> wrote: Ticket https://rt.[EMAIL]/Ticket/Display.html?id=[NUMBER] First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [USER] Request Type: High Performance Cluster Create/Modify account in existing PI group Existing PI Email: [EMAIL] Existing Group: pi_[STAFF] Project Title: Password Leak Detection with Machine Learning Project Abstract: Training Machine Learning Models to Detect Password Leaks. Best, [STAFF]."
"3283593","2025-09-30 19:09:58","HPC Other Issue: recent publication","I have added this publication to our website: https://hpcf.umbc.edu/publications/ On Tue Sep 30 13:52:23 2025, [USER] wrote: First Name:                [FIRST NAME] Last Name:                 [LAST NAME] Email:                     [EMAIL] Campus ID:                 [CAMPUS ID] Request Type:              High Performance Cluster Hi [STAFF], Geophysical Trends Inferred From 20 Years of AIRS Infrared Global Observations [FIRST NAME] [LAST NAME], L. Larrabee Strow, R. J. Kramer First published: 11 August 2025 https://doi.org/10.1029/2025JD043501 See the link for bib info [FIRST NAME] Best, [STAFF]"
"3283767","2025-10-07 16:00:19","HPC Slurm/Software Issue: Deleting .julia directories","Hi [USER], The solution is still to run chmod -R u+wrx DataFrames before attempting to remove the directory. The process is hanging because there are many files and directories to modify, which takes time. While the command is running, if your session is idle for too long, you may get logged out before it completes. To avoid this, I recommend using a tmux session to run the command so it can finish without interruption. You can learn more about tmux here (https://www.geeksforgeeks.org/linux-unix/tmux-in-linux/). Once the permissions have been updated, you can remove the directory. I have already removed the /umbc/rs/pi_[STAFF]/users/[USER]/.julia/packages/DataFrames directory for you after changing the permissions. Please reply to the ticket if you still have questions. On Tue Sep 30 16:50:49 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              High Performance Cluster Ok, so this goes back to an issue I was at office hours last Friday with [STAFF]. We thought we resolved it, and it is kind of resolved, but I still have an issue deleting what I think are essentially corrupted directories. So, I have been dealing with issues downloading packages in Julia on the cluster, and I narrowed down the issue to the specific package 'DataFrames', which is actually quite a simple package, all it does is allows you to read CSV and other text files. The workaround that I have to do is that I just wrote my code in such a way that does not require the usage of DataFrames which is fine, but I still have a few directories in my research storage that house DataFrames package that I need to be deleted. Specifically, under /umbc/rs/pi_[STAFF]/users/[USER] I have 2 directories, '.julia' and '.julia_new' both of which need to be deleted as they house the package DataFrame which prevents me from really running anything. I also have the directory '.julia_test' which I am currently using and does work, so it shouldn’t be deleted. I also have another .julia directory under /umbc/rs/pi_[STAFF]/common that needs to be deleted as well. I have tried running 'rm -rf .julia' for hours on end and it hangs. Even going into the directory itself, following the path, '/umbc/rs/pi_[STAFF]/users/[USER]/.julia/packages' and then doing 'rm -rf DataFrames' will hang as well. What ended up working on Friday, which was doing 'chmod -R u+wrx DataFrames' hangs as well now too. I have narrowed the issue all the way down to a specific file in DataFrames, it would be in the path '/umbc/rs/pi_[STAFF]/users/[USER]/.julia/packages/DataFrames/C5AEe/src' which is the source code for this specific package. The file in 'subdataframe' in the source directory is always the issue, which is weird because looking on github, the subdataframe.jl file, with which this is located, looks perfectly fine to me. Regardless, I would still like there to be a way to delete these excess directories in my research storage. Thanks."
"3284012","2025-10-03 13:44:54","HPC Slurm/Software Issue: sbatch job is not running","Yes it is working. Thank you. Best [STAFF] On Fri Oct 3 2025 943AM [STAFF] via RT [EMAIL] wrote Ticket URL https//rt.[DOMAIN]/Ticket/Display.html?id=3284012 Last Update From Ticket Hello I wanted to follow up to make sure everything is working as expected now. If so Ill close out this ticket but you can always reach back out if you need more help. Best [STAFF]  Note: I replaced 'UMBCHelp@rt.u= mbc.edu' with '[EMAIL]' and assumed the domain is not meant to be anonymized, but I removed the subdomain 'rt.' as it's likely internal and not public-facing. If you'd like me to replace the entire email address or keep the original subdomain, please let me know!"
"3284272","2025-10-03 16:05:31","HPC User Account: [USER] in Student Group","The user account has been created: Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is student. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) Hi [USER], Your user has been added to pi_nilanb as a secondary group. Your home directory has additional symbolic links to your group storage space. Please read through the documentation found at hpcf.umbc.edu. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) As you can see, I added [USER] to both the 'student' group (That was the group mentioned in the original request), and also added him to pi_nilanb. Let me know if that was an error, and I can remove him from any unnecessary groups. For the 'premium access' question, I'll forward you to [STAFF]."
"3284322","2025-10-02 13:28:43","HPC User Account: [ID] in pi_[STAFF]","Your account has been created: Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://[EMAIL]/wiki/spaces/faq/pages/[NUMBER]/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://[EMAIL]/request-tracker-rt/[STAFF]-research-computing/) Let me know if you have any more questions, [STAFF]"
"3284639","2025-10-07 15:49:45","HPC Other Issue: slurm mem error","Hi [USER] OK I'll keep an eye out. The job asked for 240 processors, which had issues when I started them out but then magically worked fine after doing the unset SLURM_MEM_PER_CPU SLURM_MEM_PER_GPU SLURM_MEM_PER_NODE Last two days I started jobs which asked for 64 cpus, all ran fine! SO like you, I have no idea why this happened. Cheers [STAFF] If you agree your issue is resolved, please give us feedback on your experience by completing a brief satisfaction survey: https://umbc.us2.qualtrics.com/SE/?SID=SV_etfDUq3MTISF6Ly&customeremail=[EMAIL]&groupid=EIS&ticketid=3284639&ticketowner=[STAFF]&ticketsubject=HPC Other Issue: slurm mem error If you believe your issue has not been resolved, please respond to this message, which will reopen your ticket. Note: A full record of your request can be found at: Ticket https://rt.umbc.edu/Ticket/Display.html?id=3284639 Hi [USER] I've tried this a few times and am not able to replicate the issue you're having. My first thought was that something was getting set in your files and then not being unset in future runs on the same terminal, but I've tried a few times and am unable to replicate. My runs I did today can be found in the output of CLUST_MAKE_ERA_RTP-442284.out and CLUST_MAKE_ERA_RTP-442245.out. Without being able to replicate the issue there's not much else I can do to debug, as I've looked through your file and didn't see anything that stood out to me. I would advise removing as many of the commented SBATCH directives as possible as it could be reading them in funny unexpected ways, but as a policy, we don't make changes to people's files without their presence so they can confirm the changes that are being made. If you see this pop up again, feel free to reopen this ticket, but for now I'll going to close it. On Thu Oct 02 09:27:50 2025, [USER] wrote: Hi [STAFF], Try it with argument 10 or with argument 4 eg sbatch --array=241-276 sergio_matlab_chip.sbatch 10 The problem is intermittent ie does not happen each time I submit a job. Looking backwards at history,I believe it happened last night with job 435223 -[USER] On Thu, Oct 2, 2025 at 8:45 AM [STAFF] via RT <UMBCHelp@rt.umbc.edu> wrote: Ticket https://rt.umbc.edu/Ticket/Display.html?id=3284639 Last Update From Ticket: Hi [USER], I added a .err and a .out file location to your sbatch to help with debugging a little bit. First, I must recommend cleaning up all the stray SBATCH directives. There are quite a few that will conflict with each other if you're not careful (see line 45 and 55 in your sbatch file). Second, I ran the sbatch file myself and didn't get the same error you did. Instead, I got [USER@chip-login1 CLUSTMAKE_ERA5]$ cat CLUST_MAKE_ERA_RTP-435429.err Unrecognized function or variable 'clustbatch_make_eracloudrtp_sergio_sarta_filelist'. Which seems to be missing some sort of command. When you run this file are you doing it from the login node? Do you normally have any modules loaded? On Thu Oct 02 07:49:47 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUS_ID] Request Type: High Performance Cluster For some reason the last couple days I have been getting the following error on a script that (I think) has worked for ages on eg taki, now modified for chip srun: fatal: SLURM_MEM_PER_CPU, SLURM_MEM_PER_GPU, and SLURM_MEM_PER_NODE are mutually exclusive. And googling, I finally found a solution on https://harvardmed.atlassian.net/wiki/spaces/O2/pages/1586793613/Troubleshooting+Slurm+Jobs which says at the command line to first do unset SLURM_MEM_PER_CPU SLURM_MEM_PER_GPU SLURM_MEM_PER_NODE Can you look at the following to see if there is a double call to srun or something? /home/[USER]/MATLABCODE/RTPMAKE/CLUST_RTPMAKE/CLUSTMAKE_ERA5/[USER]_matlab_chip.sbatch Thanks [USER]"
"3284658","2025-10-16 12:23:42","Assistance using [ID] HPCF Cluster to run LS Dyna simulations","Hi [USER], I saw the groups were created for the class and the lab. I finally heard back from our licensing team, and it seems like we are unable to install the program that [STAFF] asked for on a system wide scale available for all users. That being said, it's probably possible to install the program for individual users. [STAFF], have you been added to the class yet? If not I can add you now. If you have, have you taken a shot at installing the program yourself? If you have any issues you can schedule an office hours with our team here: https://hpcf.umbc.edu/help/office-hours/. I'll leave this open for a few days in case there is any questions about it. On Thu Oct 09 12:55:44 2025, [USER] wrote: Just did. One group for class, one for my research lab. On Thu, Oct 9, 2025 at 11:40 AM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3284658 > Last Update From Ticket: Hi all, Checking in on this. Were you able to submit a group request? On Tue Oct 07 13:04:16 2025, [USER] wrote: This is awesome. I will setup this up after lunch. Thank you immensely Max! On Tue, Oct 7, 2025 at 11:33 AM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3284658 > Last Update From Ticket: Sorry for the late response to this. We're looking at making this available on our shared resource, but am waiting to hear back about our licenses. In the meantime it might be possible to install these programs in your user directory. What I would like for your professor to do is set up a new group as a class, so if possible have them go here: https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research%20Computing and request a new group under a PI, and in the notes write that this is for the ENME 444 class. Once that's done we can get you on the cluster, and if we still don't know anything about the license, we'll work to try to set up the software for you as an individual. On Thu Oct 02 08:39:49 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [USER] Cc: [EMAIL], [EMAIL], [EMAIL] Hello, I'm a student currently taking ENME 444 Capstone and I am using Ansys and LS Dyna to run simulations of a car crash. The simulations are too large for the student version of the software and also too large to run locally on my laptop. My professor recommended reaching out to see how I can set up LS Dyna to run on the UMBC cluster. Attachment 1: Ansys remote.png"
"3285275","2025-10-03 15:45:44","HPC User Account: [USER] in pi_[ID]","The accounts have been created: Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) Let me know if you have any more questions! [STAFF]"
"3285334","2025-10-07 16:09:58","HPC User Account: [USER] in [STAFF]","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) On Thu Oct 02 17:05:48 2025, [STAFF] wrote:  Please approve the account for [USER]. Sent from my iPhone > On Oct 2, 2025, at 3:42 PM, RT API via RT <[EMAIL]> wrote: > This e-mail is a notification that a UMBC user: [USER] <[EMAIL]> has requested an account within UMBC's HPC environment in your group <[STAFF]>. As the PI, we request that you acknowledge and approve this account creation by replying to this message. Alternatively you can go to this link and review the ticket and indicate your decision here: > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3285334> > Once we have your approval, we will create the account and you and the new user will receive another e-mail notifying you that the account has been created. If you have any other questions or concerns please contact us. > - UMBC DoIT Research Computing Support Staff"
"3285426","2025-10-03 17:25:32","HPC Other Issue: Resource Usage Concern on Shared Compute Node","Thank you for your reply. That makes sense. On Fri Oct 03 10:50:43 2025, [STAFF] wrote: Hello, I understand the concern about shared node performance. However, as long as jobs are submitted within the set limits, they are considered valid and within policy. If we see repeated issues that affect the whole system, we can look at changing the limits. For now, the user is working within their allowed usage. Best, [STAFF] On Thu Oct 02 16:49:15 2025, [STAFF] wrote: I would like to bring to your attention that one of the users (ID:[USER], see attached) appears to be occupying a significant portion of the compute resources on our shared node. This high usage may impact the efficiency and workflow of other users. Could you please remind this user to be mindful of resource usage and try to avoid monopolizing the node, so that everyone can work more smoothly?"
"3285512","2025-10-03 19:05:03","HPC User Account: [USER] in pi_[STAFF]","No problem, here is the information on the new account: Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://[EMAIL]/wiki/spaces/faq/pages/[ID]/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://[EMAIL]-request-tracker-rt/[EMAIL]-research-computing/) Best, [STAFF]"
"3286322","2025-10-06 14:39:34","HPC Other Issue: Can [USER] request for a cpu and a gpu using sbatch?","Hi [USER], All nodes on chip have CPUs, as they are required to function. However, nodes on chip-gpu also have GPUs available. So yes, when using chip-gpu you can request both GPUs and CPU cores. You would request this the same way as chip-cpu. For more information about the hardware specification, check out this page: https://[DOMAIN]/wiki/spaces/faq/pages/[PAGEID]/Cluster+Specifications#CPU-and-GPU-Specifications. For more information on requesting CPU cores, check out this page: https://[DOMAIN]/wiki/spaces/faq/pages/[PAGEID]/Basic+Slurm+Commands#Jobs,-Tasks,-CPU-cores,-and-Nodes. Let me know if you have any additional questions or clarification. -- Kind regards, [STAFF]"
"3286362","2025-10-06 20:24:14","HPC Other Issue: c24-01 tmp area full","Thanks, interesting suggestion. I solved the problem by going to a different node. [STAFF], Ph.D., Professor of Mathematics Department of Mathematics and Statistics Center for Interdisciplinary Research and Consulting (circ.umbc.edu) UMBC High Performance Computing Facility (hpcf.umbc.edu) REU Site: Online Interdisciplinary Big Data Analytics (BigDataREU.umbc.edu) University of Maryland, Baltimore County 1000 Hilltop Circle, Baltimore, MD 21250 http://www.umbc.edu/~[STAFF] On Mon, Oct 6, 2025 at 10:21 AM [STAFF] via RT <[EMAIL]> wrote: If you agree your issue is resolved, please give us feedback on your experience by completing a brief satisfaction survey: https://umbc.us2.qualtrics.com/SE/?SID=SV_[ID]&customeremail=[EMAIL]&groupid=EIS&ticketid=3286362&ticketowner=[EMAIL]&ticketsubject=HPC Other Issue: c24-01 tmp area full If you believe your issue has not been resolved, please respond to this message, which will reopen your ticket. Note: A full record of your request can be found at: Ticket https://rt.umbc.edu/Ticket/Display.html?id=3286362 Thank You R e s o l u t i o n: Hi [USER], Thank you for letting us know. The issue has been resolved. Additionally, if this occurs in the future, you could attempt to change the location that the compiler uses to temporarily store files. I believe this would be achieved by setting the $TMPDIR environment variable. For example, you could try to run... TMPDIR=/scratch/$JOB_ID/ mpiicc -O3 trap.c -o trap -- Kind regards, [STAFF] (she/her/hers) DoIT Unix Infra Student Worker On Sun Oct 05 20:20:51 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster Hi, This is surely a funky error, but if trying to compile on c24-01 in an interactive session, for Intel MPI, it says [gobbert@c24-01 ver1.0solution]$ mpiicc -O3 trap.c -o trap icx: error #10295: error generating temporary file name, check disk space and permissions A student also reported this to me. The /tmpfs area or something like seems to be full. [USER] Original Request: Requestors: [USER] First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster Hi, This is surely a funky error, but if trying to compile on c24-01 in an interactive session, for Intel MPI, it says [gobbert@c24-01 ver1.0solution]$ mpiicc -O3 trap.c -o trap icx: error #10295: error generating temporary file name, check disk space and permissions A student also reported this to me. The /tmpfs area or something like seems to be full. [USER]"
"3286765","2025-10-06 19:11:20","HPC Other Issue: Can I get the H100","Hi [USER], Chip is a shared resource. Unless your group has contributed the H100 nodes, access to the node is shared between all of the cluster's users. Normally, your job would run after their job has completed. I suggest waiting until the currently running jobs complete, then your job will run. Or, if your job does not actually require two H100s, you could try to run it on other GPU hardware, such as L40S's or RTX_8000s. There is a greater amount of these nodes available, which would reduce the time it takes for your job to run. Let me know if you have any additional questions. Have a nice day! -- Kind regards, [STAFF]. On Mon Oct 06 12:49:34 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [ID] Request Type:              High Performance Cluster. I've submitted a SLURM job requesting 2 H100 GPUs, but it's currently pending. I'd like to request access to those resources. I'm working with the UMBC-CREM Center. CLUSTER: chip-gpu              JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)             101669       gpu gemma_fi [USER] PD       0:00      1 (Priority)."
"3286853","2025-10-10 16:40:56","HPC Other Issue: need authorization in the common directories","The issue should now be resolved. If you continue to have issues with this, feel free to let us know. Have a nice day! Thank you for your help I will talk with [USER] to see if I understand the issue, [USER]. Sent from my iPhone. On Oct 10, 2025, at 9:41 AM, [STAFF] via RT wrote: Ticket. Last Update From Ticket: Hi [USER], I have verified again that the permissions for the common directory are correct-- So I do not understand why your students are unable to access the common directory. Please see below where I tested the permissions for the common directory using a student account. Could either you or the student who submitted an additional ticket provide some more information with exactly what you are attempting to do? Thanks [ptembei1@chip-login2 ~]$ pi_mkann_common [ptembei1@chip-login2 common]$ pwd /umbc/rs/pi_mkann/common [ptembei1@chip-login2 common]$ touch test [ptembei1@chip-login2 common]$ ls dbraw downloaded_data Projects test [ptembei1@chip-login2 common]$. My student still gets access denied when she tried maybe you can see what the problem is, M. Sent from my iPhone. On Oct 6, 2025, at 1:57 PM, [STAFF] via RT wrote: If you agree your issue is resolved, please give us feedback on your experience by completing a brief satisfaction survey: https://www.google.com/url?q=https://umbc.us2.qualtrics.com/SE/?SID%3DSV_etfDUq3MTISF6Ly. If you believe your issue has not been resolved, please respond to this message, which will reopen your ticket. Note: A full record of your request can be found at: Ticket https://www.google.com/url?q=https://rt.umbc.edu/Ticket/Display.html?id%253D3286853. Thank You. Resolution: Hi [USER], I resolved the permission issues with pi_mkann/common. Let me know if you continue to experience errors when accessing that directory. Have a nice day! -- Kind regards, [STAFF] DoIT Unix Infra Student Worker. On Mon Oct 06 13:41:02 2025, [USER] wrote: First Name: [USER]. Last Name: [LAST NAME]. Email: [EMAIL]. Campus ID: [CAMPUS ID]. Request Type: High Performance Cluster. Hello, I have recently had taki files transfer to chip, could you helps setting up the new common directory so all users in my lab have access and can edit, etc. The way it is now we don't have permissions set up for that. Thanks, [USER]."
"3286959","2025-10-07 13:27:01","HPC User Account: [USER] in Student Group","Hello there. If you're under [STAFF], if he has an HPC account, he could easily sponsor your account, making things much easier on the cluster. Feel free to join the student group if [STAFF] requested you to do that, however. Anyways, here is the official link (A guide on how to create an HPC account): https://umbc.atlassian.net/wiki/spaces/faq/pages/1327431728/How+to+request+a+user+group+account+on+chip. Remember that if [STAFF] is sponsoring your account, we would need written permission from him. That's as simple as cc'ing him to the ticket creation, and having him say 'I approve' or something similar in a reply. Best, [STAFF]."
"3287359","2025-10-10 14:26:50","HPC Slurm/Software Issue: Module Not Available","All the documentation for these programs are publicly available online. We are able to assist with HPC related issues you encounter along the way, however we are unable to walk you through everything. We mainly provide support with HPC/Slurm related issues, and utilizing all of those modules is a little out of scope. If you have any specific questions to help get started, I can do my best to answer them. Otherwise, I recommend taking a look at the documentation for the modules you requested to utilize them. Thanks! Yes, I loaded CGNS successfully. Now I am not sure how to use all of the modules for my code compilation and run the cases eventually. Could you please help me out with this? Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3287359> Last Update From Ticket: I attempted to install CGNS as a custom module. You should be able to load the module with 'module load CGNS/4.5.0'. Can you attempt to load/use the module to verify it works as expected? If you run into issues with the module, let me know. -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Thu Oct 09 12:05:07 2025, [USER] wrote: > Hi Imran, > You do not need to reinstall the software every time you log in/out. After you compile the software, it is installed to whichever directory of your choosing. From there, all you need to do is add the path to your install directory to your PATH environment variable (or just run the binaries from inside the install directory). > The software is not readily supported by the module system, easybuild. There are some ways to possibly working around this, but for most use cases, to ensure compatibility it is recommended to just compile from source. I can take a crack at installing it as a custom module though. > Also, could you elaborate on what you mean by 'a bash file so that I don't have to install all of the modules'? All the modules have already been installed, and just need to be loaded using 'module load $MODULE_NAME'. -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Thu Oct 09 11:52:15 2025, [USER] wrote: >> Hi Danielle, Thank you very much for the update. Couldn't you please load the CGNS also? Because I could be wrong as I am not that much familiar with this chip system yet. My assumption is, if I install it by myself, then I have to do it again and again whenever I enter into the system, and run some cases. Please correct me if I am wrong! And I also need your help to make a bash file so that I don't have to install all of the modules that I need to run my cases every time separately. I need lots of modules to run every single test case of mine. Thanks, [USER] On Thu, Oct 9, 2025 at 11:27 AM [STAFF] via RT <[EMAIL]> wrote: >>> If you agree your issue is resolved, please give us feedback on your experience by completing a brief satisfaction survey: https://umbc.us2.qualtrics.com/SE/?SID=SV_etfDUq3MTISF6Ly&customeremail=wv96094%40umbc.edu&groupid=EIS&ticketid=3287359&ticketowner=desposi1%40umbc.edu&ticketsubject=HPC Slurm/Software Issue: Module Not Available >>> If you believe your issue has not been resolved, please respond to this message, which will reopen your ticket. Note: A full record of your request can be found at: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3287359> Thank You _________________________________________ R e s o l u t i o n: ========= Hi Imran, Yes, I was just about to update this ticket. I installed MPICH versions 4.2.2 and 4.2.1, along with PETSc 3.20. For CGNS, we are currently on version 4.5.0. However, the user must load the HDF5 module first before loading the CGNS module. If you have any questions or issues, please don't hesitate to ask. -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Thu Oct 09 11:20:57 2025, [USER] wrote: >>> Hello, Is there any update on the ticket? Thanks,[USER] On Tue, Oct 7, 2025 at 10:15 AM via RT <[EMAIL]> wrote: >>>> Greetings, This message has been automatically generated in response to the creation of a ticket regarding: -------------------------------------------------------------- Subject: 'HPC Slurm/Software Issue: Module Not Available' Message: First Name: Abdullah Al Last Name: Imran Email: [EMAIL] Campus ID: WV96094 Request Type: High Performance Cluster Hello, I need MPICH, CGNS, PETSC modules to run my cases on hpcf cluster. But these modules are not available there to load! Could you please load the latest version of these three modules to the cluster and let me know when it's ready? Thanks, [USER] -------------------------------------------------------------- There is no need to reply to this message right now. Your ticket has been assigned an ID of [Research Computing #3287359] or you can go there directly by clicking the link below. Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3287359> You can login to view your open tickets at any time by visiting http://my.umbc.edu and clicking on 'Help' and 'Request Help'. Alternately you can click on http://my.umbc.edu/help Thank you _______________________________________ Original Request: Requestors: Abdullah Al Imran First Name: Abdullah Al Last Name: Imran Email: [EMAIL] Campus ID: WV96094 Request Type: High Performance Cluster Hello, I need MPICH, CGNS, PETSC modules to run my cases on hpcf cluster. But these modules are not available there to load! Could you please load the latest version of these three modules to the cluster and let me know when it's ready? Thanks, [USER]"
"3287360","2025-10-07 18:04:47","HPC Slurm/Software Issue: Job Resource Allocation Failing","Seems to be working now. Thank you! On Tue Oct 7 2025 at 1:59 PM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.[DOMAIN]/Ticket/Display.html?id=[NUMBER]> Last Update From Ticket: Ha, I think you cleared out *too much*. The file '.bashrc' is what's generally in charge of creating your environment. If you just deleted everything in your home directory it doesn't know how to create your environment. I added it back in with the default set up we create for users in their bashrc and was able to confirm that your environment now looks normal: [USER@chip-login2 ~]$ srun --cluster=chip-gpu --time=01:00:00 --mem=4000 --gres=gpu:1 --cpus-per-task=1 --pty $SHELL srun: job [NUMBER] queued and waiting for resources srun: job [NUMBER] has been allocated resources [USER@g24-01 ~]$ Let me know if you still have issues. On Tue Oct 07 13:08:07 2025 [USER] wrote: Thank you for the quick reply I cleared out my home directory [USER@chip-login2 ~]$ df -h /home/[USER]/ Filesystem Size Used Avail Use% Mounted on nfs.[DOMAIN]:/ifs/data/chip/home/[USER] 500M 0 500M 0% /home/[USER] But when I try and run an interactive session I still have the same issue [USER@chip-login2 ~]$ srun --cluster=chip-gpu --time=01:00:00 --mem=4000 --gres=gpu:1 --cpus-per-task=1 --pty /bin/bash srun: job [NUMBER] queued and waiting for resources srun: job [NUMBER] has been allocated resources bash-5.1$ On Tue Oct 7 2025 at 11:24 AM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.[DOMAIN]/Ticket/Display.html?id=[NUMBER]> Last Update From Ticket: Hi [USER] Your home directory is completely filled which can sometimes cause unexpected things to happen when creating new sessions like what happens slurm starts a new interactive shell Please remove some of the stuff from your home directory If you're still having an issue after that please let me know bash-5.1$ df -h /home/[USER]/ Filesystem Size Used Avail Use% Mounted on nfs.[DOMAIN]:/ifs/data/chip/home/[USER] 500M 500M 0 100% /home/[USER] On Tue Oct 07 10:16:15 2025 [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: High Performance Cluster I was trying to get a conda environment set up but after running conda create to make one the next time I started an interactive job command was: srun --cluster=chip-gpu --time=04:00:00 --mem=40000 --gres=gpu:1 --constraint=rtx_6000 --cpus-per-task=18 --pty /bin/bash I noticed that instead of putting me into a node like normal my terminal now says 'bash-5.1$' instead of showing what node I'm on and when I try to run: module load Anaconda3/2024.02-1 I get a note saying: Note: Modules do not function on the login node If I then exit I am returned to the login node and I have to exit again to leave my ssh session Best [STAFF] DOIT HPC System Administrator"
"3287721","2025-10-07 16:32:20","HPC User Account: [USER] in oates","Hi [USER]. Your account ([USER]) has been added to your primary group is pi_[STAFF]. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) On Tue Oct 07 11:56:14 2025, [STAFF]@cs.umbc.edu wrote: I approve. On Tue, 7 Oct 2025, RT API via RT wrote: This e-mail is a notification that a UMBC user: [USER] <[EMAIL]> has requested an account within UMBC's HPC environment in your group <[STAFF]>. As the PI, we request that you acknowledge and approve this account creation by replying to this message. Alternatively you can go to this link and review the ticket and indicate your decision here: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=[NUMBER]> Once we have your approval, we will create the account and you and the new user will receive another e-mail notifying you that the account has been created. If you have any other questions or concerns please contact us. - UMBC DoIT Research Computing Support Staff. Best, [STAFF]."
"3287907","2025-10-09 00:52:09","URGENT: Software Access Policy Hindering [USER]'s Research - Request for Immediate Remote Desktop Access","Excellent! Glad to hear this is working. I'll mark this as resolved -- do reopen if there are related issues. On Wed Oct 08 15:59:49 2025, [STAFF] wrote: The student was able to download the software. Thank you for your help on this matter! I have no further requests. Best, [STAFF]. On Tue, Oct 7, 2025 at 3:00 PM [STAFF] <[EMAIL]> wrote: Thank you for your prompt response! I will reach out to the student and update you accordingly. With appreciation, [STAFF]. On Tue, Oct 7, 2025 at 2:57 PM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3287907>. Last Update From Ticket: Hi [STAFF], Could you have your student try this environment? https://elum.in/umbc-desktop-sosc. I believe it offers the appropriate access. -- [STAFF]."
"3288361","2025-10-16 01:37:55","HPC Other Issue: [USER] access to chip?","Yes she has Thanks for checking Dr [STAFF] Associate Professor Department of Biological Sciences University of Maryland Baltimore County 1000 Hilltop Circle Baltimore MD 21250 Office +1 410 455-2147 On Wed Oct 15 2025 108PM [USER] via RT wrote Ticket URL https//rtumbcedu/Ticket/Displayhtmlid=3288361 Last Update From Ticket Hi [STAFF] Just checking in was your student able to access the cluster On Fri Oct 10 11:46:59 2025 [USER] wrote Please do Ill also ask that you submit an RT ticket to formally add the student to your group https//rtformsumbcedu/rt_authenticated/doit/DoIT-supportphp?auto=Research Computing I looked into the VPN issue and it seems like the student should be fine as long as they install the vpn Once theyre added well test it out and see what happens from there On Thu Oct 09 16:26:28 2025 [USER] wrote OK Ive just received notice that the students affiliation has been reinstated her name is [USER] and her ID# is [CAMPUS ID] She should have access through 6/30/2026 What would be the next step for allowing her to access my labs volume on chip Should I direct her to download the VPN too On Thu Oct 9 2025 at 11:55 AM [USER] via RT wrote Ticket URL https//rtumbcedu/Ticket/Displayhtmlid=3288361 Last Update From Ticket Hi [STAFF] Typically we dont allow for users into UMBC services if they are not in North America even with VPN That being said we can make some exceptions to this rule For now start by sponsoring an account for the student information for that can be found here https//umbcatlassian net/wiki/spaces/faq/pages/30739140/How+do+I+request+myUMBC+accounts+for+non-UMBC+users+Create+a+Sponsored+Account and well go from there On Wed Oct 08 11:01:26 2025 [USER] wrote First Name [STAFF] Last Name [STAFF] Email [EMAIL] Campus ID [CAMPUS ID] Request Type High Performance Cluster Hello I have a Brazilian graduate student on fellowship [USER] who generated a large genomic dataset while working in my lab and who now is trying to analyze this dataset using the program Stacks She has submitted jobs to servers at her home institution of U Sao Paolo and the UK school where her fellowship continued but they havent been running due to timeouts and/or job backlogs She would like to access chip via my labs login but we have a couple of questions about this 1 Is this allowed My lab has tried to access chip while abroad and was unsuccessful but this may have been because the hotel internet was slow I know the GlobalProtect VPN would probably need to be downloaded too 2 Can I create a log-in for her She was assigned a UMBC account while working here in 2023 but I assume that has lapsed Thank you for the help"
"3288390","2025-10-10 15:28:16","Utilizing matched nodes on chip-cpu","Hello To run a job across 12 nodes, you can submit a job to the contrib partition and request 12 nodes. SLURM will allocate your 6 dedicated nodes first (--nodelist=c24-[14-19]) and then fill the remaining slots from available match nodes. Here's an example sbatch script you can use: #SBATCH --cluster=chip-cpu #SBATCH --mem=5000M #SBATCH --time=01:00:00 #SBATCH --account=pi_[STAFF] #SBATCH --partition=contrib #SBATCH --nodes=12 #SBATCH --qos=shared #SBATCH --nodelist=c24-[14-19] Let us know if that doesn't work. On Wed Oct 08 11:26:08 2025, [USER] wrote: From [STAFF]: 'We need to help to understand how the PI partitions are working in Chip. When I look at the pi_[STAFF], I can see that we have nodes 14 to 19 (6 in total) assigned to it. But I should have access also to the +6 additional nodes from the matching funding, right? How can we use these additional nodes? We will have to run some very large simulations, which will require using all 12 nodes at once. We don't know how to queue such a job, since my partition only shows 6 nodes.' -- [STAFF] DoIT Research Computing Team Best, [STAFF] DOIT Unix infra, Graduate Assistant"
"3288602","2025-10-09 17:20:23","HPC Other Issue: cannot log into chip","Hi [USER], I just wanted to give you an update regarding the status of chip. Access to the cluster was restored yesterday afternoon, and access to ada volumes have also been restored. For more information, please check out the myUMBC post: https://my3.my.umbc.edu/groups/hpcf/posts/153425 -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Wed Oct 08 14:43:59 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [ID] Request Type:              High Performance Cluster Completely Hangs after Last login : Wed Oct 8 ......"
"3288614","2025-10-09 17:19:20","HPC Other Issue: Unable to connect to chip","Hi [USER], I just wanted to give you an update regarding the status of chip. Access to the cluster was restored yesterday afternoon, and access to ada volumes have also been restored. For more information, please check out the myUMBC post: https://my3.my.[DOMAIN]/groups/hpcf/posts/153425. Kind regards, [STAFF]"
"3288634","2025-10-09 17:21:27","HPC Other Issue: Unable to access login shell after connecting to [SERVER]","Hi [USER], I just wanted to give you an update regarding the status of chip. Access to the cluster was restored yesterday afternoon, and access to ada volumes have also been restored. For more information, please check out the myUMBC post: https://my3.my.umbc.edu/groups/hpcf/posts/153425. Kind regards, [STAFF] DoIT Unix Infra Student Worker. On Thu Oct 09 12:13:04 2025, [USER] wrote: I appreciate your clarification~ On Wed Oct 08 16:03:51 2025, [USER] wrote: Hello [USER], Our team is aware of the issue, and we are working on resolving this at the moment. We understand the inconvenience this may cause you, but we will send an update once this issue has been fixed. Best regards, [STAFF]."
"3288676","2025-10-09 17:19:49","HPC Slurm/Software Issue: SSH Connection Hanging","Hi [USER], I just wanted to give you an update regarding the status of chip. Access to the cluster was restored yesterday afternoon, and access to ada volumes have also been restored. For more information, please check out the myUMBC post: https://my3.my.umbc.edu/groups/hpcf/posts/153425 Have a nice day! -- Kind regards, [STAFF] On Wed Oct 08 15:55:51 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              High Performance Cluster Good afternoon, When I ssh into chip.rs.umbc.edu, I am able to be successfully logged in. However, the connection just hangs there without presenting a shell prompt afterwards. Is there any way I could resolve this issue? This happens when I try to use putty and my windows powershell."
"3288709","2025-10-09 17:20:47","HPC Slurm/Software Issue: Can't log in","Hi [USER], I just wanted to give you an update regarding the status of chip. Access to the cluster was restored yesterday afternoon, and access to ada volumes have also been restored. For more information, please check out the myUMBC post: https://my3.my.umbc.edu/groups/hpcf/posts/153425 -- Kind regards, [STAFF] DoIT Unix Infra Student Worker  On Wed Oct 08 16:26:02 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster I cant log in to chip"
"3288849","2025-10-09 15:58:20","HPC Slurm/Software Issue: ADA storage","Hi [USER], Thanks for your patience and understanding. The Ada volume are back! https://my3.my.umbc.edu/groups/hpcf/posts/153425 On Wed Oct 08 23:32:33 2025, [STAFF] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              High Performance Cluster I know the ada storage is not working and its under maintenance. But do you have any timeframe on how much time it will take? My all experiment data is stored in ada, so i cant work on rs without access to that data. Thanks for your understanding. Best, [STAFF]"
"3289186","2025-10-09 16:54:34","Start a new group under a [STAFF]","First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [ID] Request Type: Help with something else I’d like to start two new groups. 1) for a class “ENME444 - Mechanical Engineering Capstone Design” 2) my research group “eMACS Lab” Thank you"
"3289187","2025-10-09 18:11:12","Start a new group under [STAFF]","Hi [USER], I created two groups for you, along with an account for your user. First, Your account ([USERNAME]) has been created on chip.rs.umbc.edu. Your primary group is pi_[USERNAME]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Secondly, The group pi_[USERNAME] now exists on the chip cluster. Members of this group can therefore access and contribute to the research storage space allocated to the group. This storage is located at /umbc/rs/pi_[USERNAME] and currently has a quota of 25T. And lastly, The group [GROUPNAME] now exists on the chip cluster. Members of this group can therefore access and contribute to the research storage space allocated to the group. This storage is located at /umbc/class/[GROUPNAME] and currently has a quota of 5T. Please review documentation on the hpcf.umbc.edu website. Submit any questions or issues as separate RT Tickets at: https://doit.umbc.edu/request-tracker-rt/doit-research-computing/. Feel free to let us know if you encounter any issues with either of your groups! Have a good day! -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Thu Oct 09 12:54:38 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              Help with something else I’d like to start two new groups.   1)  for a class “ENME444 - Mechanical  Engineering Capstone Design”  2) my research group “[RESEARCHGROUP]” Thank you"
"3289431","2025-10-10 17:33:55","HPC Other Issue: [USER]s lack global access in the common directory in [ID]'s lab space on CHIP","Thank you all for troubleshooting.= [USER] wrote: Hello [USER], It works for me now. Thank you so much for your help. Best, [USER]. On Fri, Oct 10, 2025 at 12:36 PM [STAFF] via RT <[EMAIL]> wrote: Ticket https://rt.umbc.edu/Ticket/Display.html?id=3289431. Last Update From Ticket: Hi [USER], When you have a chance, try again. It should all be working now. Let me know if it works as expected! -- Kind regards, [STAFF] DoIT Unix Infra Student Worker. On Fri Oct 10 11:33:35 2025, [USER] wrote: Hi [STAFF], I just tried again and attached a screenshot of my attempt. I’m able to create and work freely within folders that I created inside the common directory. However, I can’t perform any operations in subfolders within common that I didn’t create. For example, the folder downloaded_data was created by my PI, Dr. [STAFF]. To illustrate the issue, I tried creating a test folder within downloaded_data, but I was denied permission. I also tried moving some data into a subfolder within downloaded_data and received the same error. Everything works fine when I’m working within folders I created myself in common. This is what I mean by a global access issue in the common directory. Best, [USER]. On Fri, Oct 10, 2025 at 9:37 AM [STAFF] via RT <[EMAIL]> wrote: Ticket https://rt.umbc.edu/Ticket/Display.html?id=3289431. Last Update From Ticket: Hi [USER], Students in your group do have access to the common directory of the research volume. I have tested permissions with multiple student accounts, and there are no issues with the common directory. Please share exactly what you are attempting to do, and what directory you are in. Please see below: [ptembei1@chip-login2 ~]$ pi_mkann_common [ptembei1@chip-login2 common]$ pwd /umbc/rs/pi_mkann/common [ptembei1@chip-login2 common]$ touch test [ptembei1@chip-login2 common]$ ls dbraw downloaded_data Projects test [ptembei1@chip-login2 common]$. On Thu Oct 09 18:22:47 2025, [USER] wrote: First Name: [USER]. Last Name: [USER]. Email: [EMAIL]. Campus ID: [CAMPUSID]. Request Type: High Performance Cluster. I work in Dr. [STAFF]'s lab, where we collaborate using shared directories. Without global rwx access to the common directory, we’re unable to move files, create subdirectories, or delete files that are no longer needed in directories created by other users. We would really appreciate your help with this."
"3289503","2025-10-10 19:14:02","HPC User Account: [USER] in Center for Navigation, Timing & Frequency Research","Hi [USER], Your account ([USER]) has been created on [SERVER]. Your primary group is [GROUP]. Your home directory has 500M of storage. You can find a short tutorial on how to use [SERVER] here: https://[DOMAIN]/wiki/spaces/[SPACE]/pages/[PAGE]/Getting+Started+on+[SERVER]. Please read through the documentation found at [WEBSITE] > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://[DOMAIN]/request-tracker-rt/[DEPARTMENT]/) Let me know if you have any more questions, [STAFF]."
"3289659","2025-10-12 20:10:29","RCD Consult: Deep Lab Cut","Thanks [STAFF]! I have submitted a group request. Please let me know if you need additional information. Thanks, [USER]. On Fri, Oct 10, 2025 at 1:08 PM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3289659> Last Update From Ticket: Hi [USER], First we'll have you start by putting a group request. Documentation on how to do that can be found here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1219461147/Connecting#Requesting-a-User-Account-on-chip, with a link to the RT form to request a group being found here: https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Re-search%20Computing. Additionally, once you've been set up on the cluster, there is a getting started tutorial that can be found here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip. If you have trouble with any of the above let me know and we can walk through it. On Fri Oct 10 10:18:16 2025, [USER] wrote: First Name: [USER]. Last Name: [USER]. Email: [EMAIL]. Campus ID: [USER]. Request Type: Research Computing & Data Consultation. Good Morning, I am a Prof in the Math/Stat department. My area of research is not high performance computing, but I am working on a project that may need access to a machine with GPUs. We need to use software called Deep Lab Cut https://deeplabcut.github.io/DeepLabCut/docs/installation.html to analyze experimental data. As I understand it, the first step is to use Deep Lab Cut on a laptop to recreate an *.eml file. That file would then be used within a batch job on a GPU computer to generate other files. I have two undergraduate students who will be working on this project. I am also collaborating with a postdoc at [ORGANIZATION]."
"3289747","2025-10-10 19:07:35","HPC User Account: [USER] in Student Group","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_burnsm. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) Let me know if you have any more questions! [STAFF]"
"3289785","2025-10-10 16:32:34","HPC User Account: [USER] in misc-lab","First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster Create/Modify account in existing PI group Existing PI Email: [EMAIL] Existing Group: misc-lab Project Title: Misc Lab Project Abstract: group for the Misc Lab requesting new PI group"
"3289788","2025-10-10 16:36:53","create new HPC PI group","First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [ID] Request Type: Help with something else group name: misc-lab list of members: [EMAIL]"
"3290086","2025-10-14 17:31:42","HPC Other Issue: Not able to connect to JupyterLab on Chip","I identified two nodes (c21-15 and c21-16) that had a missing symbolic link in /usr/ebuild/installs. This is what likely prevented you from finding the jupyter-lab binaries. It has now been resolved, so you shouldn't experience issues now. I will close this ticket now, however if you do encounter another issue, feel free to submit a new ticket. Have a nice day! -- Kind regards, [STAFF] On Mon Oct 13 23:11:20 2025, [USER] wrote: Hi [STAFF], I have seen this error a few times, but I did not note the node on which it occurred. I will do that the next time I notice it. You are right about ' some of the more recent .err/.out files in the working directory ... running fine recently.' I actually restarted my machine, and SSHed into CHIP again, and got it to run. It could very well be a node issue. In the meantime, please keep the ticket open so that we can address the issue the next time it pops up. For the Office Hours tomorrow, if I still don't observe the same issue, I was wondering if we can do it anyway to discuss a new thing I'm trying to do - open two jupyter notebooks on chip (through two tunnels) using a single jupyter.slurm file from a single folder. Is that possible? Or do we need two jupyter.slurm files for that? Thank you. Regards, [USER] On Mon, Oct 13, 2025 at 12:36 PM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3290086 > Last Update From Ticket: Hi [USER], Based on the error you attached as a screenshot, it appears that the system was unable to find the binary for jupyter-lab. This binary is provided via the Anaconda3 package, which is loaded at the start of the Jupyter SBATCH file. The PATH for this module is also already present in your PATH env variable. This is as expected, however it is still unable to find the binary. Do you happen to know what node the job was running on? It is possible that this could be due to a specific node. Is this error reproducible? Based on some of the more recent .err/.out files in your working directory, it seems like the notebooks have been running fine recently. Let me know! -- Kind regards, [STAFF] On Sat Oct 11 12:40:16 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: High Performance Cluster Hello, I am referring to this guide - https://umbc.atlassian.net/wiki/spaces/faq/pages/1104805915/How+do+I+run+a+new+jupyter+notebook+on+chip - to run 'an existing jupyter notebook on chip' in the '/umbc/rs/pi_slaha/users/[USER]/astrophysics-anom_det' folder. I am stuck on Step 7 - i.e. in the .err file, I am not able to see the required section. I cannot find the line that starts with “http://127.0.0.1”. What I see instead is attached as a screenshot. It says the following: /usr/bin/which: no jupyter-lab in (/usr/ebuild/installs/software/Anaconda3/2024.02-1:/usr/ebuild/installs/software/Anaconda3/2024.02-1/sbin:/usr/ebuild/installs/software/Anaconda3/2024.02-1/bin:/usr/ebuild/installs/software/Anaconda3/2024.02-1/condabin:/cm/shared/apps/git/2.33.1/bin:/cm/shared/apps/slurm/current/sbin:/cm/shared/apps/slurm/current/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/[USER]) /cm/local/apps/slurm/var/spool/job456706/slurm_script: line 63: jupyter-lab: command not found Can you please help fix this? Thank you. Regards, [USER]"
"3290310","2025-10-13 16:06:20","HPC User Account: [USER] in electric_fish_project","Hi [USER], Welcome to chip, UMBC's High Performance Computing Cluster! The group, pi_[STAFF], now exists on the chip cluster. Members of this group can access and contribute to the research storage space allocated to the group. Additionally, I created user accounts for you, and the two students requested. This storage space is located at /umbc/rs/pi_[STAFF], and currently has a quota of 10T. For information on accessing the cluster, adding accounts to your group, and getting started using the cluster, check out the tutorial on our wiki: https://umbc.atlassian.net/wiki/x/R4BPQg Additional documentation is also available here: https://umbc.atlassian.net/wiki/x/FwCHQ For the software, Deep Lab Cut, you should be able to install it via a Conda environment. Here is a wiki page for getting started with anaconda on chip: https://umbc.atlassian.net/wiki/x/LYCPPQ If you have any questions or issues, please submit a new RT ticket at: https://doit.umbc.edu/request-tracker-rt/doit-research-computing/ First Name: [USER] Last Name: [STAFF] Email: [EMAIL] Campus ID: [CAMPUS_ID] Request Type: High Performance Cluster Create/Modify account in existing PI group Existing PI Email: [EMAIL] Existing Group: [GROUP] Project Title: Computational Mechanisms for State-Driven Active Sensing Project Abstract: Dr. [STAFF] and an undergraduate student supervised by Dr. [STAFF] will perform the research for the project “Computational Mechanisms for State-Driven Active Sensing” at UMBC. The overall goal of the project is to understand sensing and information gathering behaviors through experiments with electric fish. The experiments will be conducted at partner institutions (Johns Hopkins University and New Jersey Institute of Technology), and Dr. [STAFF]'s team at UMBC will focus on advanced time-series analysis of experimental data for all three specific aims. The deliverables will be analysis of time-series data for identifying causal relationships among neural and behavioral signals, analysis of time-series data for detecting changes in neural and behavioral data, and informing control-theoretic models of the observed behaviors (developed at UMN). We need to use software called Deep Lab Cut https://deeplabcut.github.io/DeepLabCut/docs/installation.html to analyze experimental data. As I understand it, the first step is to use Deep Lab Cut on a laptop to recreate an *.eml file. That file would then be used within a batch job on a GPU computer to generate other files. I have two undergraduate students who will be working on this project. A guess is that each batch run will generate 5G of files. The undergraduate students working on this project are [USER] [USER]."
"3291129","2025-10-13 21:58:15","HPC User Account: [USER] in Student Group","This has been fixed. Please try again and let us know if there is still a permission denied issue. [STAFF]"
"3291162","2025-11-04 16:40:26","HPC Other Issue: change with Intel C compiler?","You shouldn't need to recompile on each node, just each partition since each partition is using the same cpu architecture. On Tue Nov 04 11:11:19 2025, [STAFF] wrote: So, lets say i want to run the ver 1.4 on every node in the cluster, to test performance. Do you think I should recompile it on each node? or just run the same executable?"
"3291564","2025-10-16 19:35:40","HPC Other Issue: g24-12 issue","Hi [USER], We've confirmed the machine is functioning normally again. Thank you for the email! On Tue Oct 14 12:18:39 2025, [STAFF] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster One gpu on g24-12 is not working. attached screenshot. Attachment 1: Screenshot 2025-10-14 at 12.17.01.png Best, [STAFF]"
"3293453","2025-10-15 17:13:25","HPC Other Issue: [USER]'s jobs not running on Chip today","My jobs are now running. There must have been a very big job using up computing power and delaying mine more than usual. You can disregard this issue. Thanks!  Greetings, This message has been automatically generated in response to the creation of a ticket regarding: Subject 'HPC Other Issue My jobs not running on Chip today' Message First Name [USER] Last Name [STAFF] Email [EMAIL] Campus ID [CAMPUS_ID] Request Type High Performance Cluster Hi I'm using the same code that has been running fine the past few weeks but today my jobs are sitting in Chip and not running. Is there a migration going on? Or another reason that would prevent me from accessing pi_gobbert nodes like I usually do? I can't run on the general nodes either. Thank you! [USER@chip-login1 USER]$ squeue -u [USER] CLUSTER chip-cpu JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 526469 general weather [USER] PD 0:00 1 (Priority) 517473 match temp [USER] PD 0:00 1 (Priority) 526413 match cal [USER] PD 0:00 1 (Priority) 526432 match dhsregs [USER] PD 0:00 1 (Priority)"
"3293474","2025-10-15 17:09:11","CMake issue on Cluster","Hello [USER], I hope this email finds you well. Thank you for working with us during our office hours' time at 12:15-12:45. We wanted to provide you a step-by-step guide on how to use CMake so you are able to use this moving forward. Please also refer to the documents below that talk about 'How to run an interactive job', 'SBATCH', and information on how to use modules. 1. https://umbc.atlassian.net/wiki/spaces/faq/pages/[PAGEID] 2. https://umbc.atlassian.net/wiki/spaces/faq/pages/[PAGEID]/Basic+Slurm+Commands#sbatch 3. https://umbc.atlassian.net/wiki/spaces/faq/pages/[PAGEID]/How+to+use+modules 4. Here is also attached the Cmake documentation for any issues you might have when building (https://cmake.org/cmake/help/latest/guide/tutorial/index.html) When using CMake: 1. Log into chip 2. run an interactive job using the srun command (This can be found in the 1st article we linked) 3. module load <modulename> (Cmake) 4. Optional (if you would like to view the current modules being ran you can run <module list>) 5. Start working with cmake :) If you have any issues, please reach back out! Best regards, [STAFF]"
"3293526","2025-10-21 16:00:08","HPC Slurm/Software Issue: Conda environment","This is an email thread between a researcher (Matthew Baker) and the High-Performance Computing (HPC) team at UMBC (University of Maryland, Baltimore County). The researcher is experiencing issues with their Conda environment not being applied consistently across all nodes in the HPC cluster.  Here's a summary of the issue:  * The researcher allocated resources on the HPC cluster using `srun` and loaded the Conda environment using `module load` and `conda activate`. * However, when they tested the environment using `mpirun python -c 'import fiona; print(fiona.__version__)'`, only one node was able to use the Fiona library, while the others returned a `ModuleNotFoundError`. * Further testing revealed that only one node was using the Conda environment's Python interpreter, while the others were defaulting to the system Python.  The HPC team responded with some questions and suggestions:  * Max Breitmeyer asked if the researcher had tried loading the Conda environment in their Slurm script instead of on the command line. * He also mentioned that there might be an issue with how the environment is initialized across nodes in the current cluster setup.  Overall, the email thread suggests that there may be a configuration or setup issue with the HPC cluster or the researcher's job submission script that is preventing the Conda environment from being applied consistently across all nodes."
"3294124","2025-10-16 18:03:42","HPC User Account: [ID] in FSI","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_dli. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Thu Oct 16 13:49:55 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster Create/Modify account in existing PI group Existing PI Email:    [EMAIL] Existing Group:       FSI Project Title:        Sleep Disorder Monitoring Project Abstract:     This project aims to create a sleep stage monitoring system that utilizes contact-free multi-modal data. The long term goal is to extend the system to be able to detect and monitor sleep disorders. By using cutting-edge signal processing and machine learning methods, the system will be capable of providing precise and non-invasive sleep pattern analysis. In order to make the system scalable and capable of real-time execution, the ultimate solution will be refined for deployment on edge devices.   I am requesting access to a GPU cluster to help experiments on contact-free, multi-modal sleep stage monitoring and sleep disorder detection. This is a project to create and train machine learning models that will be used on edge devices."
"3295053","2025-10-20 18:36:23","HPC User Account: [USER] in FSI","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_dli. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Fri Oct 17 15:50:27 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster Create/Modify account in existing PI group Existing PI Email:    [EMAIL] Existing Group:       FSI Project Title:        Sleep Disorder Monitoring Project Abstract:     This project aims to create a sleep stage monitoring system that utilizes contact-free multi-modal data. The long term goal is to extend the system to be able to detect and monitor sleep disorders. By using cutting-edge signal processing and machine learning methods, the system will be capable of providing precise and non-invasive sleep pattern analysis. In order to make the system scalable and capable of real-time execution, the ultimate solution will be refined for deployment on edge devices.  I am requesting access to a GPU cluster to help experiments on contact-free, multi-modal sleep stage monitoring and sleep disorder detection. This is a project to create and train machine learning models that will be used on edge devices."
"3295071","2025-10-20 18:33:56","HPC User Account: [USER] in Machine Learning for Signals Processing Lab","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) Kind regards, [STAFF] DoIT Unix Infra Student Worker On Fri Oct 17 16:17:16 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster Create/Modify account in existing PI group Existing PI Email: [EMAIL] Existing Group: Machine Learning for Signals Processing Lab Project Title: Multivariate feature selection for fMRI analysis Project Abstract: fMRI has become a widely used imaging tool for exploring the normal neural functions as well as disordered brain functions like schizophrenia. Among all fMRI data analysis strategies, data-driven-based methods have a unique advantage of capturing the whole picture of available information since they effectively minimize assumptions imposed on the brain activity. With the increasing number of multimodal data and multisite data, the problem of balancing the computation cost and analysis performance is becoming more important than ever before. In this project, our interest is in identifying the most informative multivariate features when analyzing multiple fMRI datasets. Our goal is the development of flexible new decomposition methods as well as identifying the best feature extraction strategy for a given problem."
"3295672","2025-10-21 13:24:05","Request for Additional Shared Storage (2TB)","Hi [USER], As I write this, your shared research group volume is using 0% of its 25TiB of space. This volume is located at /umbc/rs/pi_dli. I'll mark this as resolved for now. See this wiki page for additional information: https://umbc.atlassian.net/wiki/spaces/faq/pages/1072267344/Storage On Mon Oct 20 14:28:29 2025, [STAFF] wrote: First Name:                [USER] Last Name:                 [LAST NAME] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              Help with something else Dear HPCF Team, I am a member of the FSI research group led by [STAFF]. For our project titled 'Sleep Disorder Monitoring', we are currently working with large datasets and require an additional 2TB of shared storage to support our ongoing research activities. We would appreciate your support in accommodating this request. Thank you for your understanding. Best regards, [USER]"
"3295788","2025-10-20 19:08:52","Requesting Additional Shared Storage (2TB)","Hi [USER], As I write this, the pi_dli group has used 0% of its 25TiB allocation on chip. The root for this storage volume is located at /umbc/rs/pi_dli . Let me know if you have any questions about this, but I'll resolve this request for now. On Mon Oct 20 14:51:37 2025, [STAFF] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              Help with something else Dear HPCF Team, I am a member of the FSI research group led by [STAFF]. For our project titled 'Sleep Disorder Monitoring', we are currently working with large datasets and require an additional 2TB of shared storage to support our ongoing research activities. We would appreciate your support in accommodating this request. Thank you for your understanding. Best regards, [USER]"
"3296672","2025-10-23 16:45:16","HPC User Account: [USER] in pi_[ID]","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Tue Oct 21 13:22:34 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [USER] Request Type: High Performance Cluster Create/Modify account in existing PI group Existing PI Email: [EMAIL] Existing Group: pi_[STAFF] Project Title: Coupled Groundwater–Surface Water Modeling of Baltimore Using ParFlow.CLM Project Abstract: This study develops high-resolution ParFlow.CLM models for urban watersheds in Baltimore, Maryland, to investigate the interactions between groundwater and surface water under recent climatic conditions."
"3296766","2025-10-23 18:21:16","HPC Other Issue: [USER] running unexpectedly slow","Hi [USER], There were some issues with flags that were automatically generating incorrect slurm variables, which may have been causing the slowness. Please try it again and let us know if it seems faster. On Wed Oct 22 13:44:19 2025, [STAFF] wrote: Hi [STAFF], I followed the instructions on the HPCF wiki page (https://umbc.atlassian.net/wiki/spaces/faq/pages/1408335873/Running+an+LLM+using+Ollama+on+chip) and simply ran the example on that page. It took more than 30minutes to generate only 2 words. I used an interactive job to run the OLLAMA. I even tried to use the GPU partition for [SERVER]. It was a little faster but still super slow. I share the ollama_server.log file in the attachment for your reference. Thank you in advance for your help. Best regards, [USER] On Tue Oct 21 15:20:39 2025, [STAFF] wrote: Hi [USER] I need some more information about your LLM job. Can you please provide your slurm script, and working directory? Are you working with a data, if so how big is it and where is it located? How much time do you expect the job to take ? On Tue Oct 21 14:18:03 2025, [STAFF] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster User [USER] reports following ollama setup according to: https://umbc.atlassian.net/wiki/spaces/faq/pages/1408335873/Running+an+LLM+using+Ollama+on+chip User reports that running LLMs via ollama is running unexpectedly slow. Please follow-up with user to replicate the issue and perhaps set a meeting to understand the issue and to resolve. -- Best, [STAFF] DOIT Unix infra, Graduate Assistant"
"3296910","2025-10-30 18:44:55","HPC Other Issue: Running Hspice software on HPCF","Yes, thanks for all help. Regards, [STAFF] On Thu, Oct 30, 2025 at 2:11 PM Max Breitmeyer via RT <[EMAIL]> wrote: Ticket Last Update From Ticket: Hi all, Is it fair to say this ticket can now be closed? On Thu Oct 30 12:57:01 2025, [USER] wrote: Thanks a lot [STAFF]. Both hspice and waveviewer work. Regards,[USER] On Wed, Oct 29, 2025 at 5:01 PM [STAFF] <[EMAIL]> wrote: Wave view should also be working now, give it a try when you have a chance. ---[STAFF] Specialist, Linux System Administrator & Lab Technical Support On Oct 29, 2025, at 16:12, [USER] <[EMAIL]> wrote: Hi [STAFF], Thanks. Hspice works but we also need WV (waveviewer) to see Hspice results as well. Could you please add that also and let us know to check? Thanks a lot,[USER] On Wed, Oct 29, 2025 at 2:30 PM [STAFF] via RT <[EMAIL]> wrote: Ticket Last Update From Ticket: On the HPC cluster, it should now be possible to run hspice using the command: /umbc/software/csee/scripts/launch_synopsys_hspice.sh Please test it and let me know if it works. Also, let me know what other tools you need to use, if any, so I can adapt those scripts as well."
"3298342","2025-10-23 15:52:23","HPC Other Issue: 5-days time limit not enough for jobs","Hi [USER], Unfortunately, under the current cluster model only contributing groups have access to the 'shared' QOS option. In your situation, I would recommend seeing if you can optimize your code to run faster, in parallel, or across multiple nodes. You can also attempt to break apart your one large job into two smaller sets of jobs, which would allow you to complete the run in less than 5 days. Let me know if you have any additional questions! Have a nice day! -- Kind regards, [STAFF] On Thu Oct 23 10:09:11 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster Hello all! I'd like to ask your help with the jobs I'm submitting in chip cluster. A few of them are pretty heavy, and the 5-days limit to run are not being enough. I saw there's the possibility to run in the partition 'shared', with 14-days limit, but I don't have access to that. Is it possible to give me access to that partition, or maybe to increase the time limit in the partition 'general'? In general my samples run within the 5 days, I would require more time only for samples whose jobs abort after 5 days. Thank you! Best, [USER]"
"3298471","2025-10-23 16:21:05","HPC Other Issue: [USER] cannot login to their ADA account","Many Thanks [STAFF], Regards, [USER] On Thu, Oct 23, 2025 at 12:18 PM [STAFF] via RT <[EMAIL]> wrote: If you agree your issue is resolved, please give us feedback on your experience by completing a brief satisfaction survey: https://umbc.us2.qualtrics.com/SE/?SID=SV_[REMOVED]&customeremail=[EMAIL]&groupid=EIS&ticketid=3298471&ticketowner=[STAFF]&ticketsubject=HPC Other Issue: I cannot login to my ada account If you believe your issue has not been resolved, please respond to this message, which will reopen your ticket. Note: A full record of your request can be found at: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3298471> Thank You R e s o l u t i o n: Hi [USER], The hpc cluster 'ada' has been completely absorbed by 'chip' as of April 10th. Please see this myumbc posting detailing the rollout of chip and the deletion of ada. https://my3.my.umbc.edu/groups/hpcf/posts/147513 On Thu Oct 23 12:01:48 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: High Performance Cluster Hello, I=E2=80=99m currently unable to access my ADA account. When I try to connect via SSH using the command below: ssh [USER]@ada.rs.umbc.edu I receive the following error message and do not even get the password prompt: ssh: connect to host ada.rs.umbc.edu port 22: Connection timed out Could you please help me resolve this issue? Thank you, [USER] Best, [STAFF] DOIT HPC System Administrator Original Request: Requestors: [USER] First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: High Performance Cluster Hello, I=E2=80=99m currently unable to access my ADA account. When I try to connect via SSH using the command below: ssh [USER]@ada.rs.umbc.edu I receive the following error message and do not even get the password prompt: ssh: connect to host ada.rs.umbc.edu port 22: Connection timed out Could you please help me resolve this issue? Thank you, [USER]"
"3298811","2025-10-24 15:50:45","HPC User Account: [USER] in H.A.R.M.O.N.I. Lab","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) Kind regards, [STAFF] On Thu Oct 23 16:30:39 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster Create/Modify account in existing PI group Existing PI Email: [EMAIL] Existing Group: H.A.R.M.O.N.I. Lab Project Title: Multimodal Stock Price Direction Prediction Project Abstract: This project, Multimodal Stock Price Direction Prediction Using Historical Prices, News, and Sentiment, aims to predict short-term stock movements up, down, or stable by integrating numerical and textual financial data. The approach combines historical stock price features with sentiment analysis of financial news to capture both market trends and investor sentiment. Historical price data are collected from Yahoo Finance, while relevant news articles and headlines are sourced from the News API or GDELT. Sentiment scores are derived using FinBERT, a transformer-based model optimized for financial text, and aggregated on a daily basis. These sentiment features are then merged with technical indicators such as moving averages, relative strength index (RSI), and multi-day returns. The resulting multimodal dataset is labeled according to future price direction and used to train classification models including Random Forest, XGBoost, and Multilayer Perceptrons. Model performance is evaluated using accuracy, F1 score, and confusion matrices to compare the predictive power of price-only, sentiment-only, and combined feature sets. The study explores both early and late fusion approaches to integrate modalities and applies feature importance analysis (e.g., SHAP) to interpret model behavior. The findings aim to demonstrate how combining quantitative and qualitative signals can improve stock trend forecasting and provide insights into the interplay between market sentiment and price dynamics. N/A Attachment 1: Project Guide_ Multimodal Stock Price Direction Prediction.pdf"
"3299581","2025-10-28 13:33:04","HPC User Account: [USER] in FSI","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_dli. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) On Sat Oct 25 12:59:22 2025, [STAFF] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: High Performance Cluster Create/Modify account in existing PI group Existing PI Email: [EMAIL] Existing Group: FSI Project Title: Sleep Disorder Monitoring Project Abstract: This project aims to create a sleep stage monitoring system that utilizes contact-free multi-modal data. The long term goal is to extend the system to be able to detect and monitor sleep disorders. By using cutting-edge signal processing and machine learning methods, the system will be capable of providing precise and non-invasive sleep pattern analysis. In order to make the system scalable and capable of real-time execution, the ultimate solution will be refined for deployment on edge devices. I am requesting access to a GPU cluster to help experiments on contact-free, multi-modal sleep stage monitoring and sleep disorder detection. This is a project to create and train machine learning models that will be used on edge devices."
"3299984","2025-10-28 13:43:27","HPC User Account: [ID] in Quantum Thermodynamics Group","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_deffner. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/). On Mon Oct 27 10:31:08 2025, [STAFF] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster Create/Modify account in existing PI group Existing PI Email:    [EMAIL] Existing Group:       [GROUPNAME] Project Title:        Noise-Aware Quantum Dynamics Compilation Via Tensor Networks Project Abstract:     Quantum system simulation is quantum native, however compilation of variational simulation circuits is a difficult task. Efficiency of the compiled algorithm is paramount to successful simulation, yet algorithmic methods such as Trotterization result in suboptimal circuits. Prior work has identified machine learning techniques which efficiently improve on Trotterization by several orders of magnitude. This work advances these results by factoring noise into the compilation process, allowing for the generation of circuits which will perform optimally when executed on real, noisy quantum hardware. Dear HPC team, I would like to request HPC access to execute quantum simulation algorithms. My advisor, [STAFF], already has group access on the system, and I would like to be added to the group. Please let me know if you need anything else from me to complete my application. Thank you, [USER]. Best, [STAFF]"
"3300174","2025-10-28 13:29:02","HPC User Account: [USER] in Deffner","Hi [USER], You weren't able to log in because you didn't have a chip account. Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/) On Mon Oct 27 13:02:13 2025, [STAFF] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              High Performance Cluster Create/Modify account in existing PI group Existing PI Email:    [EMAIL] Existing Group:       [GROUP] Project Title:        Thermodynamics of Quantum Computation Project Abstract:     N/A I am unable to logon to Chip notes. Following the access instructions, I get to the point where I am prompted for my password, but it does not accept the password"
"3300690","2025-11-04 18:52:46","Data upload and server processing","Hi [USER], Welcome to chip, UMBC's High Performance Computing Cluster! The group, usda-eb, now exists on the chip cluster. Members of this group can access and contribute to the research storage space allocated to the group. This storage space is located at /umbc/rs/usda-eb, and currently has a quota of 50T. For information on accessing the cluster, adding accounts to your group, and getting started using the cluster, check out the tutorial on our wiki: https://umbc.atlassian.net/wiki/x/R4BPQg. Additional documentation is also available here: https://umbc.atlassian.net/wiki/x/FwCHQ. If you have any questions or issues, please submit a new RT ticket at: https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research Computing. Currently, [STAFF] is the 'owner' of the group, and is the only member of the group. To request users to be added to your group, please submit an RT ticket from the following link: https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research Computing. Additionally, I will need a little bit more information regarding your grant. Could you please provide an award number, title, and abstract? If you have any additional concerns, questions, or run into any problems, please feel free to submit a new ticket! Have a great day!"
"3301107","2025-10-29 17:44:30","HPC User Account: [ID] in [USER]","Hi [USER], Your account ([CAMPUS ID]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/). You can also view your project information here on our wiki (https://hpcf.umbc.edu/libraries/research-projects-hpcf/?preview_id=76&preview_nonce=ee7c2f7bd1&preview=true). Best regards, [STAFF]"
"3301564","2025-10-30 18:16:49","HPC Other Issue: very slow editing on /home/[USER] on c24-52","Hi [USER], OK all logged outta chip and our machine -[STAFF] On Thu, Oct 30, 2025 at 2:10 PM [STAFF] via RT [EMAIL] wrote: Ticket Last Update From Ticket: Hi [USER], Please exit out of /umbc/xfs2/strow on all devices where you may be logged in. We can't unmount the device until it's no longer active. On Thu Oct 30 13:44:40 2025, [STAFF] wrote: Hi [STAFF] Thanks for the reply. Maybe you should also cc [STAFF] as you all work on this ([EMAIL]) [USER] On Thu, Oct 30, 2025 at 1:30 PM [STAFF] via RT [EMAIL] wrote: Ticket Last Update From Ticket: [USER], There is currently some degradation on the file system that sits on xfs2. We're investigating into how to resolve, but out of an abundance of caution, we are going to unmount xfs2 so that the degradation doesn't spread. We'll let you know when we have more information for you, but expect to not have access to your mount for a while. On Thu Oct 30 13:23:27 2025, [STAFF] wrote: Hi [USER], We are aware of the issue and are currently investigating. I'm going to merge this with the other ticket since they are related. On Thu Oct 30 12:53:30 2025, [USER] wrote: Request Type: High Performance Cluster Have the disk issues been worked on yet (ticket #3301564)? I still can't edit files And now Matlab took about 3 minutes to start up on c24-52 There is something seriously wrong! Thanks [USER]."
"3302104","2025-10-31 15:07:37","HPC Other Issue: Need cuDNN 8.4.1.50 module on CHIP","I have installed the module cuDNN/8.4.1.50-CUDA-11.7.0, let us know if you still unable to find the module. On Wed Oct 29 14:22:27 2025, [STAFF] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster Hi, I need to work on TensorFlow 2.11 which using CUDA 11.7 version and cuDNN 8.4. I saw CUDA 11.7, however there is no cuDNN 8.4. The previous cluster (ADA) have module 'cuDNN/8.4.1.50-CUDA-11.7.0'. Can you install that module on CHIP also? Thank you so much, [USER]"
"3304171","2025-10-30 17:23:27","HPC Other Issue: system performance issue causing significant slowdown","We are aware of the issue and are currently investigating. I'm going to merge this with the other ticket since they are related. On Thu Oct 30 12:53:30 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              High Performance Cluster Have the disk issues been worked on yet (ticket #3301564)? I still can't edit files And now Matlab took about 3 minutes to start up on c24-52 There is something seriously wrong! Thanks [USER] -- Best, [STAFF]"
"3304442","2025-11-04 14:26:41","HPC Other Issue: more hardware details","Everything is EDR. On Thu Oct 30 21:44:22 2025, [USER] wrote: Hi, [STAFF], Thanks for the Dell model. Yes, I read that Wiki page. My request is to know more precisely than 'high speed backend Infiniband network supporting 100 Gbps'. That phrase is well-written to cover all nodes from 2018 to 2024. But is the 2024 network not a newer one? Purchased in 2024? Either way, I am asking for a technical term like DDR = dual data rate, QDR = quad data rate, EDR = extended data rate, like I am recalling from the past. Does the 2024 portion's network not have a phrase like that with it? On Thu, Oct 30, 2025 at 7:03 PM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=33404442> Last Update From Ticket: Hi [USER], The model for the nodes used in the 2024 are PowerEdge R660. Specifications on the network abilities and cpus themselves can be found here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1289486353/Cluster+Specifications On Thu Oct 30 16:07:02 2025, [USER] wrote: > Request Type: High Performance Cluster > Hi, I would like some more fine points of hardware information. I am referring to the 2024 portion of chip. - CPU: what is the model number of the node from Dell for the 2024 compute nodes? - The 2024 CPU nodes are connected by which InfiniBand? EDR? Some specs available?"
"3304534","2025-10-30 22:55:41","HPC Other Issue: Reset [USER] Password","Hi [USER], Your password is linked to your [INSTITUTION] account. So you would just need to use the my[INSTITUTION] reset password. https://[INSTITUTION].atlassian.net/wiki/spaces/faq/pages/[PAGEID]/I+have+forgotten+my+my[INSTITUTION]+password.+What+should+I+do Note that it may take a minute to update on [SERVER]. On [DATE], [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster Dear Sir/Madam, I would like to access the [SERVER] server, but after logging in with my ID “[USERNAME]”, my password does not work. I am not sure of the reason. Could you please provide me with a link or instructions to reset my password? Thank you very much for your assistance. Regards, [USER] ID: [CAMPUSID]"
"3304703","2025-10-31 14:44:35","HPC Other Issue: Request for sample SLURM job script to run WRF on [SERVER]","Hi [USER], First off, I wanted to clarify a couple points. 1. 'I was advised that running ./wrf.exe interactively is not permitted': This is untrue. You are able to run interactive jobs, where you create a slurm allocation on a node, then connect to it and run your code directly. You are not permitted to run jobs on the login node, which I believe is what you are thinking of. If you would like some documentation on how to run an interactive job, I'll include a link to a page with the commands required to do that. How to run interactive job: https://umbc.atlassian.net/wiki/x/CYCbQw 2. There is also a WRF module available to be loaded on chip. You can load WRF with: 'module load WRF/4.4-foss-2022a-dmpar'. WPS is also available, and can be loaded with: 'module load WPS/4.4-foss-2022a-dmpar'. These installations should work, however it is a slightly older version (4.4), whereas you compiled a newer one (4.5). 3. For your locally installed and compiled copy of WRF, if you desire to use it you must use the compiled binaries located under /umbc/rs/pi_cichoku/users/[USER]/model2/sources/WRF/main. This is where the actual WRF binaries are stored, the directory you provided was a test directory for testing the software. You will want to add this location to your PATH environment variable to utilize the binaries. It is also possible, depending on how your install was compiled, that it would need to be recompiled for MPI support. For more info, check out the Building WRF section of this page: https://www2.mmm.ucar.edu/wrf/OnLineTutorial/compilation_tutorial.php We do not have any existing documentation on running WRF/WPS explicitly, however we have documentation on running MPI jobs, including some example SBATCH scripts: https://umbc.atlassian.net/wiki/x/AQCKV There is documentation on WRF's website for running WRF using mpi, take a look here: https://www2.mmm.ucar.edu/wrf/users/wrf_users_guide/build/html/running_wrf.html If you have any additional questions or need clarification, please feel free to let me know. For now I will mark this as resolved. Have a good day! Kind regards, [STAFF] On Fri Oct 31 05:37:55 2025, [EMAIL] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [USER] Request Type:              High Performance Cluster Hi HPC Team, I’m a graduate student working on the WRF model on the UMBC CHIP cluster. I was advised that running ./wrf.exe interactively is not permitted and that I should submit the run via SLURM because of the job’s size and runtime. Could you please provide (or point me to) a sample SLURM job script for a parallel WRF run on CHIP? A template with the recommended directives and module loads would be very helpful. Specifically, I’m looking for guidance on: For context (in case it helps tailor the template): Executable(s): real.exe and wrf.exe (WRF-ARW) Input: single domain (d01) for a short 48-hour case; netCDF met_em and boundary files are ready Code location: /umbc/rs/pi_cichoku/users/[USER]/model2/sources/WRF/test/em_real/ I can rebuild with the cluster’s preferred MPI stack if needed. If there’s existing documentation or example scripts for CHIP users running WRF (or other large MPI jobs), a link would be great. Thank you very much for your help! Best regards, [USER] Graduate Student, GES UMBC"
"3305879","2025-11-04 20:24:14","HPC Other Issue: Not able to login to the cluster (chip) through terminal","Hi [STAFF], Apologies for late reply Yes, still I face the issue and I tried with campus vpn and also my personal wifi. Both are giving the same problem. Could you help me in checking the issue. Thankyou for your help. Regards, [USER]. Campus ID: [CAMPUSID] On Mon, Nov 3, 2025 at 11:01 AM [STAFF] via RT <[EMAIL]> wrote: Ticket <URL: https://rt.[DOMAIN]/Ticket/Display.html?id=[TICKETID]> Last Update From Ticket: Hi [USER], Is this still an issue? We haven't seen any other reports of people being unable to login. Are you on campus vpn? What sort of internet connection do you have (wifi or wired)? On Fri Oct 31 15:04:48 2025, [USER] wrote: First Name: [USER] Last Name: [USER] Email: [EMAIL] Campus ID: [CAMPUSID] Request Type: High Performance Cluster Previously, I was able to login through the cluster (chip) but suddenly the login to cluster is not working. Attachment 1: Screenshot 2025-10-31 at 3.03.21PM.png Best, [STAFF] [DEPARTMENT] System Administrator"
"3305960","2025-11-04 16:29:06","HPC User Account: [USER] in pi_[STAFF]","Hi [USER], Your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is [STAFF]. Your home directory has 500M of storage. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research Computing) -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Fri Oct 31 16:25:15 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              High Performance Cluster Create/Modify account in existing PI group Existing PI Email:    [STAFF] Existing Group:       [STAFF] Project Title:        A Human-Centered Approach to Building Generative AI System for Small Business Owners Project Abstract:     Entrepreneurs in resource-constrained communities often lack the time and support to translate ideas into actionable business plans. While generative AI promises assistance, most systems assume high digital literacy and overlook community infrastructures that shape adoption. We report on the community-centered design and deployment of BizChat, an LLM-powered tool for business plan development, introduced across four workshops at a feminist business incubator and makerspace in a city. BizChat was designed to center entrepreneurs’ knowledge and workflows while providing just-in-time micro-learning and low-floor-high-ceiling accessibility. Through system log data (N=30) and semi-structured interviews (N=10) with entrepreneurs, we show how the design and deployment of BizChat with existing community contexts lowered barriers to accessing capital, encouraged reflection, and empowered entrepreneurs to support AI-literacy within their own communities. We contribute insights into how AI tools can be deployed within local support networks, and implications for design that strengthen community resilience amid rapid technological change. I am a phd student and my advisor is [STAFF]. I am requesting an account in order to join this existing group."
"3306147","2025-11-01 17:20:44","HPC Slurm/Software Issue: [USER] Cannot Cancel Jobs","Understood and no problem. I've cancelled all of your jobs submitted to the match partition that were pending. When canceling your own jobs, be sure to specify the cluster with something like `scancel -M chip-cpu JOBID`. On Sat Nov 01 12:18:16 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              High Performance Cluster Hello, I am having an issue with chip where I cannot cancel a number of jobs that are in queue. I used a script to create and submit the jobs last night but realized that I had a small mistake. I have tried to cancel all jobs under my username, [USER], and cancel specific jobs by id but they remain in the queue. The jobs have remained in the queue for more than 12 hours since I first tried to cancel them. All pending jobs on the match partition under my username need to be cancelled."
"3306171","2025-11-04 16:38:35","HPC Other Issue: software request","Hi [USER], The modules, RSEM and BOWTIE2 have been installed on chip! I installed RSEM/1.3.3-foss-2022a, and Bowtie2/2.4.5-GCC-11.3.0. These can be loaded with 'module load RSEM/1.3.3-foss-2022a' (note, RSEM depends on BOWTIE2, so the correct version of BOWTIE2 is loaded when you load RSEM). If you encounter any issues using the modules, or need any additional modules, feel free to submit a new ticket! Have a great day! -- Kind regards, [STAFF] On Sat Nov 01 13:50:40 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster Is it possible to request modules/packages to be added to the cluster? I would like to be able to use RSEM and BOWTIE2. Thanks!"
"3306427","2025-11-03 16:57:43","HPC Slurm/Software Issue: SSH connection failing","Hi [USER], Since the load balancer issue was fixed, we have not seen any other reports of this issue. I am currently testing myself, and have not encountered any disconnects. Would you mind sharing a bit more about your setup? How are you sshing (ie, a dedicated client like putty, or just through your terminal)? Do you have any local SSH configuration? Are you logged in using GlobalProtect VPN? Let me know and I can look into this further for you! Have a nice day! -- Kind regards, [STAFF] On Sun Nov 02 19:04:14 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [ID] Request Type:              High Performance Cluster I have a few ssh sessions open connecting to chip and they keep disconnecting every few minutes. Previously, this had been a problem and it was resolved (something to do with a load balancer) but its back now."
"3306608","2025-11-03 15:16:42","HPC Slurm/Software Issue: Slurm Not Loading Anaconda Module - Job ID 113232-113234","There appears to have been a minor configuration error with the node that job was running on (g24-11). This has now been resolved. If you continue to experience issues, please feel free to submit a new ticket! Have a nice day! Kind regards, [STAFF] DoIT Unix Infra Student Worker On Mon Nov 03 09:47:01 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUSID] Request Type:              High Performance Cluster I went to run a job this morning that I have run quite often. The first request for this job (ID 113231) started just fine. Subsequent requests error out in my Python script indicating issues loading modules from my Conda environment. The job log shows that 'conda' is not a known command, indicating an issue loading the Anaconda module."
"3308276","2025-11-03 18:12:12","HPC Other Issue: Data missing from '[ID]/rs/zzbatmos/users/[ID]'","Hi [USER], Currently, the research volume for pi_zzbatmos is undergoing migration to a new storage server (ceph). This date was scheduled with your PI in advance, and during the time of migration we request that all users in the group do not use chip to avoid disturbing your groups migration. Your PI should have more information regarding the specifics. However, since pi_zzbatmos has a very large research volume, the migration is taking a long time (it was started on Friday, and is still going). After the migration properly completes, you should have access to your data. Something that you already noticed, the new Ceph research volumes start with 'pi_'. For example, the old volume was mounted at '/umbc/rs/zzbatmos', where the new volume (the one you noticed) is located at '/umbc/rs/pi_zzbatmos'. Your PI should let the group know when the migration is finished. But if you have any additional questions in the meantime, feel free to let us know! Have a good day! -- Kind regards, [STAFF] On Mon Nov 03 12:23:11 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              High Performance Cluster Hi, I hope this email finds you well. I noticed that my data from 'umbc/rs/zzbatmos/users/[USER]' is missing. I found that there is another directory 'umbc/rs/pi_zzbatmos/users/[USER]' in a similar path but this new directory contains partial data. Could you please let me know what happened to my original data or if it was moved somewhere else? thank you."
"3308743","2025-11-04 14:20:34","HPC Slurm/Software Issue: Pending Jobs Not Cancelling","Hi, Well, when I squeue'd for your jobs just now, none of them came up, meaning I suppose they finished. If you could give me the exact path to the jobs you were trying to cancel, I could test some things myself. Without more information, I can't test much myself. However, looking at the history of your 'scancel' commands, I do notice a mistake you made, which could very well be the reason it didn't work. You ran: scancel -u [USER] This is not correct on the Chip cluster; the correct command would've been: scancel --cluster=chip-cpu -u [USER] or scancel --cluster=chip-gpu -u [USER] Yes, I know it's a bit confusing, but you have to specify the cluster CPU/GPU to cancel all jobs for your user. Here's some wiki documentation about that: https://umbc.atlassian.net/wiki/spaces/faq/pages/1335951387/Basic+Slurm+Commands#Managing-and-Controlling-Jobs Let me know if that was the issue. Best, [STAFF]"
"3308993","2025-11-04 16:34:11","HPC User Account: [USER] in pi_[ID]","Hi [USER], your account ([USER]) has been created on chip.rs.umbc.edu. Your primary group is pi_[STAFF]. Your home directory has 500M of storage. Please read through the documentation found at hpcf.umbc.edu > User Support. All available modules can be viewed using the command 'module avail'. Please submit additional questions or issues as separate tickets via the following link. (https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research Computing) -- Kind regards, [STAFF] DoIT Unix Infra Student Worker On Tue Nov 04 09:43:25 2025, [USER] wrote: First Name:                [USER] Last Name:                 [USER] Email:                     [EMAIL] Campus ID:                 [CAMPUS_ID] Request Type:              High Performance Cluster Create/Modify account in existing PI group Existing PI Email:    [STAFF]@umbc.edu Existing Group:       pi_[STAFF] Project Title:        Towards Designing for Resilience: Community-Centered Deployment of an AI Business Planning Tool in a Feminist Makerspace Project Abstract:     Entrepreneurs in resource-constrained communities often lack the time and support to translate ideas into actionable business plans. While generative AI promises assistance, most systems assume high digital literacy and overlook community infrastructures that shape adoption. We report on the community-centered design and deployment of BizChat, an LLM-powered tool for business plan development, introduced across four workshops at a feminist busi- ness incubator and makerspace. BizChat was designed to center entrepreneurs' knowledge and workflows while providing just-in-time micro-learning and low-floor-high-ceiling accessibility. Through system log data (N=30) and semi-structured interviews (N=10) with entrepreneurs, we show how the design and deploy- ment of BizChat with existing community contexts lowered bar- riers to accessing capital, encouraged reflection, and empowered entrepreneurs to support AI-literacy within their own communities. We contribute insights into how AI tools can be deployed within local support networks, and implications for design that strengthen community resilience amid rapid technological change. I am a master's student and requesting this account to join in existing group. I am working under [STAFF]'s Lab."
"3309956","2025-11-04 17:57:29","HPC Slurm/Software Issue: Request to reserve GPU space for a deadline in December ACL 2026","First Name: [STAFF] Last Name: [STAFF] Email: [EMAIL] Campus ID: [ID] Request Type: High Performance Cluster My student, [USER], would like to reserve GPU nodes (preferably H100) for 2 weeks to complete her experiments for the ACL deadline. Can you please assist her with this?"
"3310226","2025-11-04 21:01:54","N-Mode cavity laser simulation - In need of computing power","Please disregard this ticket request."
