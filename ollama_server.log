time=2026-01-14T12:26:58.445-05:00 level=INFO source=routes.go:1554 msg="server config" env="map[CUDA_VISIBLE_DEVICES:0 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/elliotg2/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2026-01-14T12:26:58.882-05:00 level=INFO source=images.go:493 msg="total blobs: 21"
time=2026-01-14T12:26:58.882-05:00 level=INFO source=images.go:500 msg="total unused blobs removed: 0"
time=2026-01-14T12:26:58.883-05:00 level=INFO source=routes.go:1607 msg="Listening on 127.0.0.1:11434 (version 0.13.5)"
time=2026-01-14T12:26:58.895-05:00 level=INFO source=runner.go:67 msg="discovering available GPUs..."
time=2026-01-14T12:26:58.895-05:00 level=WARN source=runner.go:485 msg="user overrode visible devices" CUDA_VISIBLE_DEVICES=0
time=2026-01-14T12:26:58.895-05:00 level=WARN source=runner.go:489 msg="if GPUs are not correctly discovered, unset and try again"
time=2026-01-14T12:26:58.899-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 39935"
time=2026-01-14T12:27:01.692-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 35893"
time=2026-01-14T12:27:04.305-05:00 level=INFO source=runner.go:106 msg="experimental Vulkan support disabled.  To enable, set OLLAMA_VULKAN=1"
time=2026-01-14T12:27:04.305-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 34925"
time=2026-01-14T12:27:04.309-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 33173"
time=2026-01-14T12:27:04.514-05:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b filter_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA L40S" libdirs=ollama,cuda_v13 driver=13.1 pci_id=0000:e1:00.0 type=discrete total="45.0 GiB" available="44.4 GiB"
time=2026-01-14T12:28:49.988-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 42713"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-01-14T12:28:50.493-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --model /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --port 44259"
time=2026-01-14T12:28:50.493-05:00 level=INFO source=sched.go:443 msg="system memory" total="251.4 GiB" free="234.9 GiB" free_swap="12.0 GiB"
time=2026-01-14T12:28:50.493-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b library=CUDA available="43.9 GiB" free="44.4 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-01-14T12:28:50.493-05:00 level=INFO source=server.go:496 msg="loading model" "model layers"=33 requested=-1
time=2026-01-14T12:28:50.494-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="4.3 GiB"
time=2026-01-14T12:28:50.494-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="11.0 GiB"
time=2026-01-14T12:28:50.494-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="5.7 GiB"
time=2026-01-14T12:28:50.494-05:00 level=INFO source=device.go:272 msg="total memory" size="21.0 GiB"
time=2026-01-14T12:28:50.503-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /cm/shared/apps/ollama/0.13.5/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA L40S, compute capability 8.9, VMM: yes, ID: GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b
load_backend: loaded CUDA backend from /cm/shared/apps/ollama/0.13.5/lib/ollama/cuda_v13/libggml-cuda.so
time=2026-01-14T12:28:50.618-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2026-01-14T12:28:50.618-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:44259"
time=2026-01-14T12:28:50.620-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:90000 KvCacheType: NumThreads:32 GPULayers:33[ID:GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
time=2026-01-14T12:28:50.620-05:00 level=INFO source=server.go:1338 msg="waiting for llama runner to start responding"
time=2026-01-14T12:28:50.620-05:00 level=INFO source=server.go:1372 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory device GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b utilizing NVML memory reporting free: 47664726016 total: 48305799168
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA L40S) (0000:e1:00.0) - 45456 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_embd_inp       = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
load_tensors:        CUDA0 model buffer size =  4403.49 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 90112
llama_context: n_ctx_seq     = 90112
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (90112) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.50 MiB
llama_kv_cache:      CUDA0 KV buffer size = 11264.00 MiB
llama_kv_cache: size = 11264.00 MiB ( 90112 cells,  32 layers,  1/1 seqs), K (f16): 5632.00 MiB, V (f16): 5632.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   296.01 MiB
llama_context:  CUDA_Host compute buffer size =   184.01 MiB
llama_context: graph nodes  = 999
llama_context: graph splits = 2
time=2026-01-14T12:29:53.249-05:00 level=INFO source=server.go:1376 msg="llama runner started in 62.76 seconds"
time=2026-01-14T12:29:53.249-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=1
time=2026-01-14T12:29:53.249-05:00 level=INFO source=server.go:1338 msg="waiting for llama runner to start responding"
time=2026-01-14T12:29:53.249-05:00 level=INFO source=server.go:1376 msg="llama runner started in 62.76 seconds"
[GIN] 2026/01/14 - 12:30:00 | 200 |         1m10s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:30:31 | 200 |  31.01799237s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:30:35 | 200 |   4.02173848s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:30:36 | 200 |  1.720836399s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:30:42 | 200 |  5.596932712s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:30:44 | 200 |  1.848745854s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:30:45 | 200 |  1.524211564s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:30:46 | 200 |  1.057268856s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:30:48 | 200 |  1.161332336s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:30:49 | 200 |  1.852775386s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:30:51 | 200 |  1.903026806s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:30:56 | 200 |  4.370150655s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:30:58 | 200 |  1.775618197s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:31:00 | 200 |  2.104034354s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:31:04 | 200 |  3.906229183s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:31:09 | 200 |  5.346392899s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:31:17 | 200 |  8.271271202s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:31:29 | 200 |  11.36947699s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:31:32 | 200 |  2.968267755s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:31:34 | 200 |  2.715680821s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:31:37 | 200 |  2.893440198s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:31:40 | 200 |  2.676842447s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:31:42 | 200 |  1.659890465s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:31:43 | 200 |  1.926757894s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:31:51 | 200 |  7.526025082s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:31:53 | 200 |  2.332333984s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:32:31 | 200 | 37.341075706s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:32:33 | 200 |  2.466274466s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:33:05 | 200 | 31.618285509s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:33:07 | 200 |  2.418757375s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:33:10 | 200 |  2.459669409s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:33:12 | 200 |   2.42635838s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:33:15 | 200 |  3.302991302s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:33:18 | 200 |  2.619464672s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:33:21 | 200 |  2.693051808s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:33:23 | 200 |  1.826307007s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:33:34 | 200 | 11.112431885s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:33:36 | 200 |   2.29697753s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:33:39 | 200 |  3.137831255s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:33:41 | 200 |  2.112457685s |       127.0.0.1 | POST     "/api/generate"
time=2026-01-14T12:33:41.905-05:00 level=WARN source=runner.go:153 msg="truncating input prompt" limit=90000 prompt=106907 keep=5 new=90000
[GIN] 2026/01/14 - 12:34:21 | 200 |  39.38294504s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:34:23 | 200 |  2.655913446s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:34:25 | 200 |  2.066176996s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:34:28 | 200 |  2.664495229s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:34:30 | 200 |  2.236876205s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:34:32 | 200 |  1.878247779s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:34:35 | 200 |  2.469723524s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:34:37 | 200 |  2.547576946s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:34:41 | 200 |  4.001606089s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:34:48 | 200 |  6.697009011s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:34:49 | 200 |  1.377473003s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:34:50 | 200 |  762.637422ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:34:52 | 200 |  2.428870512s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:34:55 | 200 |  2.267279131s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:34:57 | 200 |  2.445051566s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:34:58 | 200 |  1.356640786s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:35:02 | 200 |  3.756562992s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:35:42 | 200 | 39.470008894s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:35:45 | 200 |  2.881701374s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:35:48 | 200 |  3.532002417s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:35:49 | 200 |  1.286825207s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:36:05 | 200 | 15.536745467s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:36:07 | 200 |  1.851048584s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:36:09 | 200 |   2.57453564s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:36:12 | 200 |  2.368469721s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:36:14 | 200 |  2.045643519s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:36:17 | 200 |  3.456511755s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:36:21 | 200 |  3.480287441s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:36:25 | 200 |  4.577132573s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:36:27 | 200 |  1.278145786s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:36:28 | 200 |    1.5438027s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:36:30 | 200 |  1.903884284s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:36:32 | 200 |   2.23326286s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:36:37 | 200 |  4.553994703s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:36:41 | 200 |   3.97887077s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:36:42 | 200 |  1.164330528s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:36:48 | 200 |   5.56939049s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:36:50 | 200 |  2.538924775s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:36:51 | 200 |  1.228708767s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:37:10 | 200 | 18.721559451s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:37:11 | 200 |   1.34580278s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:37:13 | 200 |  2.061750325s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:37:21 | 200 |  7.694751039s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:37:23 | 200 |  1.936940916s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:37:27 | 200 |  3.932086892s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:37:29 | 200 |  2.175009875s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:37:33 | 200 |  3.534724069s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:37:35 | 200 |  1.751024524s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:37:37 | 200 |  2.714886449s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:37:40 | 200 |  2.465294437s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:37:41 | 200 |  1.583734707s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:37:48 | 200 |  6.966901191s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:37:50 | 200 |  1.815299368s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:37:53 | 200 |  2.495161536s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:37:54 | 200 |  1.910604436s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:37:58 | 200 |  3.211571761s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:38:01 | 200 |  3.572054071s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:38:08 | 200 |  6.519180025s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:38:14 | 200 |  6.114608476s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:38:25 | 200 | 11.374844859s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:38:27 | 200 |   1.46230557s |       127.0.0.1 | POST     "/api/generate"
time=2026-01-14T12:38:27.571-05:00 level=WARN source=runner.go:153 msg="truncating input prompt" limit=90000 prompt=276149 keep=5 new=90000
[GIN] 2026/01/14 - 12:39:10 | 200 | 43.509230174s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:39:13 | 200 |  3.164312299s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:39:17 | 200 |  3.409288607s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:39:27 | 200 | 10.188423165s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:39:33 | 200 |  6.449823202s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:39:40 | 200 |  6.648935486s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:39:45 | 200 |   4.67422123s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:39:49 | 200 |  4.659714172s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:40:05 | 200 | 15.254831848s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:40:09 | 200 |  3.775376582s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:40:12 | 200 |  3.149788757s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:40:17 | 200 |  5.439512981s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:40:21 | 200 |  4.177452059s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:40:25 | 200 |  3.530104721s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:40:29 | 200 |  3.910150223s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:40:34 | 200 |  5.320168082s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:40:37 | 200 |  2.509865946s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:40:42 | 200 |  5.452510559s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:40:48 | 200 |    6.1492642s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:40:51 | 200 |  3.253967548s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:40:56 | 200 |  4.466928916s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:41:04 | 200 |  7.975899692s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:41:08 | 200 |  4.402821748s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:41:16 | 200 |  7.235287848s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:41:20 | 200 |  4.125518961s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:41:24 | 200 |   4.39406028s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:42:38 | 200 |         1m14s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:42:43 | 200 |  5.131623764s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:42:53 | 200 |  9.599015434s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:43:00 | 200 |   6.60447132s |       127.0.0.1 | POST     "/api/generate"
time=2026-01-14T12:43:00.359-05:00 level=WARN source=runner.go:153 msg="truncating input prompt" limit=90000 prompt=114789 keep=5 new=90000
[GIN] 2026/01/14 - 12:43:44 | 200 |  44.20938011s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:43:47 | 200 |  2.963957892s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:43:49 | 200 |  2.124825801s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:43:55 | 200 |  5.973950639s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:43:57 | 200 |  1.929650397s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:43:59 | 200 |  2.414079069s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:44:01 | 200 |  2.120052866s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:44:05 | 200 |  3.543544642s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:44:07 | 200 |  1.799511202s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:44:08 | 200 |  1.575638858s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:44:13 | 200 |  4.567991778s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:44:15 | 200 |  2.098217626s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:44:17 | 200 |  1.771396062s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:44:19 | 200 |  1.934386876s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:44:20 | 200 |  1.316262374s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:44:22 | 200 |  2.103392423s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:44:24 | 200 |  2.363543405s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:44:28 | 200 |  3.522364836s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:44:30 | 200 |   2.10395281s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:44:31 | 200 |  1.230022095s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:44:33 | 200 |  1.236327147s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:44:40 | 200 |  7.635712418s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:44:43 | 200 |  3.147027938s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:44:51 | 200 |   7.57280839s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:44:55 | 200 |  4.105193214s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:45:00 | 200 |  4.901095494s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:45:01 | 200 |  1.439130949s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:45:04 | 200 |  3.036987131s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:45:09 | 200 |  4.323286943s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:45:11 | 200 |  2.394353813s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:45:13 | 200 |  1.707422116s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:45:15 | 200 |  1.774010936s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:45:17 | 200 |  1.949400939s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:45:19 | 200 |  2.839250637s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:45:25 | 200 |  5.266610953s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:45:27 | 200 |  2.260278576s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:45:29 | 200 |  1.631151136s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:45:30 | 200 |  1.357999153s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:45:32 | 200 |  2.487017635s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:45:34 | 200 |  1.138649053s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:45:38 | 200 |  4.306393045s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:45:42 | 200 |  4.382629629s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:45:45 | 200 |  2.653719918s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:45:49 | 200 |  4.367207999s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:45:52 | 200 |  3.179316754s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:45:58 | 200 |  5.616480418s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:46:02 | 200 |  3.912188583s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:46:09 | 200 |  7.417084679s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:46:11 | 200 |  1.784262488s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:46:14 | 200 |  2.425067927s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:46:17 | 200 |  3.385142093s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:46:20 | 200 |  3.380584184s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:46:23 | 200 |  2.077149791s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:46:23 | 200 |  933.687493ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:46:25 | 200 |  1.351938888s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:46:33 | 200 |  8.115994446s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:46:35 | 200 |  2.292930141s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:46:38 | 200 |  2.844748893s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:46:44 | 200 |  5.907194017s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:46:45 | 200 |  1.281729975s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:46:48 | 200 |  3.166061335s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:46:50 | 200 |  1.124893085s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:46:52 | 200 |  2.165096314s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:46:58 | 200 |  6.153376252s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:47:00 | 200 |  2.163118537s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:47:01 | 200 |  918.449652ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:47:08 | 200 |  6.559404138s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:47:10 | 200 |   2.80192632s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:47:13 | 200 |   2.59508315s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:47:15 | 200 |  1.740382113s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:47:18 | 200 |  3.326593224s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:47:20 | 200 |  2.059826826s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:47:21 | 200 |  1.186747707s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:47:23 | 200 |  1.699628436s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:47:27 | 200 |  3.847471987s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:47:29 | 200 |  2.478250643s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:47:31 | 200 |  2.098510755s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:47:33 | 200 |  1.391356724s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:47:37 | 200 |  4.322975557s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:47:44 | 200 |    6.5187083s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:47:45 | 200 |  1.796467042s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:47:47 | 200 |  1.893709167s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:47:52 | 200 |  4.280552561s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:47:53 | 200 |  1.102670309s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:47:57 | 200 |  4.523874259s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:48:00 | 200 |  2.765337733s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:48:04 | 200 |  4.493424729s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:48:10 | 200 |  5.523333864s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:48:11 | 200 |   1.21908551s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:48:14 | 200 |  2.330713042s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:48:16 | 200 |  2.512661287s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:48:19 | 200 |  2.430326989s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:48:20 | 200 |  1.083951363s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:48:22 | 200 |  2.169337212s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:48:24 | 200 |  1.896578471s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:48:35 | 200 | 11.611886987s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:48:38 | 200 |   2.85217405s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:48:40 | 200 |  1.512435384s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:48:42 | 200 |  2.316612992s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:48:45 | 200 |  3.049966634s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:48:47 | 200 |  2.312418696s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:48:50 | 200 |  2.947181669s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:48:53 | 200 |  3.081845266s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:48:57 | 200 |  3.876383359s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:49:01 | 200 |  4.131714734s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:49:04 | 200 |  2.635412893s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:49:06 | 200 |  2.201635811s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:49:11 | 200 |  5.062150681s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:49:14 | 200 |  2.879381885s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:49:17 | 200 |  2.961446418s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:49:19 | 200 |  1.722193508s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:49:22 | 200 |  3.090509345s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:49:23 | 200 |  1.482670127s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:49:25 | 200 |  1.662580646s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:49:28 | 200 |  2.802193201s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:49:30 | 200 |  1.658954507s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:49:31 | 200 |    1.7553113s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:49:34 | 200 |  3.006065941s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:49:35 | 200 |  1.100044481s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:49:48 | 200 | 12.741570252s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:49:49 | 200 |   841.98005ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:49:51 | 200 |  2.393905814s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:49:53 | 200 |  1.675049844s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:49:56 | 200 |  3.000551324s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:49:58 | 200 |  1.589264615s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:50:00 | 200 |  1.937168431s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:50:01 | 200 |  1.831490092s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:50:04 | 200 |  2.267418979s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:50:06 | 200 |  1.808053924s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:50:08 | 200 |  2.340102979s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:50:10 | 200 |  2.123325553s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:50:12 | 200 |  2.113517443s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:50:17 | 200 |  5.064662645s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:50:31 | 200 | 13.799143126s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:50:41 | 200 | 10.243086417s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:50:43 | 200 |  1.521947706s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:50:44 | 200 |  1.686639639s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:50:52 | 200 |  7.261580099s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:50:53 | 200 |  1.370569272s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:51:01 | 200 |  7.767731931s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:51:02 | 200 |   1.18676978s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:51:03 | 200 |  1.312774812s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:51:05 | 200 |  1.396610837s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:51:06 | 200 |  1.353159827s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:51:09 | 200 |  2.604709287s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:51:27 | 200 | 17.998839881s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:51:28 | 200 |  1.447935239s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:51:29 | 200 |  1.073198183s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:51:31 | 200 |  2.141479152s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:51:38 | 200 |  6.520836402s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:51:40 | 200 |  2.187165039s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:51:42 | 200 |  1.472935865s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:51:44 | 200 |  2.307871277s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:51:47 | 200 |  2.697496246s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:51:49 | 200 |  2.916801418s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:51:53 | 200 |  3.476246276s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:51:55 | 200 |  1.688582729s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:51:56 | 200 |  1.413622387s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:51:58 | 200 |  1.530698098s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:52:00 | 200 |  1.930871565s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:52:06 | 200 |  6.897630856s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:52:15 | 200 |  8.692773525s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:52:17 | 200 |   2.31065421s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:52:19 | 200 |  2.000958305s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:52:22 | 200 |  2.249092191s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:52:23 | 200 |  1.537446062s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:52:26 | 200 |  2.917575752s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:52:29 | 200 |  2.554495576s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:52:31 | 200 |   2.25657348s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:52:33 | 200 |  1.996286538s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:52:35 | 200 |  1.786432866s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:52:43 | 200 |  8.565606307s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:52:49 | 200 |  5.171660116s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:52:57 | 200 |  8.331372295s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:54:19 | 200 |         1m21s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:54:44 | 200 | 25.651410379s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:54:46 | 200 |  1.226399988s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:55:00 | 200 | 14.576814952s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:55:04 | 200 |  3.498940406s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:55:08 | 200 |  4.166911717s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:55:09 | 200 |  1.353598574s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:55:11 | 200 |  1.998455475s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:55:13 | 200 |  1.796183262s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:55:15 | 200 |  1.847268783s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:55:17 | 200 |  2.316865467s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:55:19 | 200 |  2.078833021s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:55:38 | 200 | 18.409691867s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:55:40 | 200 |  2.209571837s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:55:43 | 200 |   2.73563171s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:55:47 | 200 |  4.122169524s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:55:50 | 200 |  2.727168023s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:55:52 | 200 |   2.24936455s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:55:55 | 200 |  3.557270695s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:55:57 | 200 |  1.348985057s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:55:59 | 200 |  1.884184013s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:56:00 | 200 |  1.121725776s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:56:03 | 200 |  3.130934832s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:56:05 | 200 |  2.334324787s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:56:09 | 200 |  3.409371066s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:56:12 | 200 |  2.912424654s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:56:12 | 200 |  874.656633ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:56:14 | 200 |  1.623184673s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:56:15 | 200 |  1.358168572s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:56:17 | 200 |  1.369173596s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:56:21 | 200 |  4.579152266s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:56:22 | 200 |  1.041286897s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:56:24 | 200 |  1.426664225s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:56:27 | 200 |  2.744459658s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:56:28 | 200 |  1.753443796s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:56:31 | 200 |  3.046042196s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:56:35 | 200 |  3.253262847s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:56:37 | 200 |   2.44442366s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:56:41 | 200 |  4.006865041s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:56:45 | 200 |  3.948435434s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:56:48 | 200 |  2.821430991s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:56:50 | 200 |  2.266187246s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:56:52 | 200 |  2.081947494s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:56:56 | 200 |  3.794700519s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:56:57 | 200 |  1.043140316s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:56:59 | 200 |  1.764696417s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:57:00 | 200 |  1.581274757s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:57:01 | 200 |  648.188667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:57:04 | 200 |  2.734897406s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:57:05 | 200 |  1.111322404s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:57:18 | 200 | 12.795852315s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:57:20 | 200 |  1.891441433s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:57:21 | 200 |  1.553198678s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:57:23 | 200 |   1.96750138s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:57:25 | 200 |   1.75466465s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:57:26 | 200 |  1.117049456s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:57:30 | 200 |  3.846004457s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:57:31 | 200 |  1.177215307s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:57:32 | 200 |   1.14087647s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:57:33 | 200 |  1.090965588s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:57:34 | 200 |  1.159323725s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:57:37 | 200 |   2.44187745s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:57:42 | 200 |   5.04892426s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:57:46 | 200 |  3.826778262s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:57:47 | 200 |  1.595985518s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:58:09 | 200 | 21.273716457s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:58:12 | 200 |  3.091263801s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:58:14 | 200 |  2.306820721s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:58:16 | 200 |  1.839287415s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:58:18 | 200 |  2.208568289s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:58:24 | 200 |  6.038540786s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:58:27 | 200 |  2.653063945s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:58:31 | 200 |  4.591554418s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:58:34 | 200 |   2.78487906s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:58:39 | 200 |  4.877051669s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:58:41 | 200 |  1.829449886s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 12:58:46 | 200 |  5.352062744s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:02:36 | 200 |         3m49s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:02:39 | 200 |  2.920461742s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:02:40 | 200 |  1.378951864s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:02:42 | 200 |  1.999196449s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:02:44 | 200 |  1.932035676s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:02:57 | 200 | 12.581416331s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:03:00 | 200 |  3.976274714s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:03:03 | 200 |  2.418943866s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:03:06 | 200 |  2.708444072s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:03:10 | 200 |  4.499883344s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:03:18 | 200 |  8.085890042s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:03:22 | 200 |  3.924086503s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:03:24 | 200 |  1.422522259s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:03:26 | 200 |   2.13426544s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:03:40 | 200 | 14.159970927s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:03:43 | 200 |  3.269918831s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:03:45 | 200 |  1.726415586s |       127.0.0.1 | POST     "/api/generate"
time=2026-01-14T13:03:45.563-05:00 level=WARN source=runner.go:153 msg="truncating input prompt" limit=90000 prompt=94551 keep=5 new=90000
[GIN] 2026/01/14 - 13:04:26 | 200 | 41.464134152s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:04:28 | 200 |  1.559813149s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:04:29 | 200 |  858.269139ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:04:32 | 200 |  3.447544306s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:04:39 | 200 |  6.351973655s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:04:40 | 200 |  1.000521771s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:05:07 | 200 | 27.784535207s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:05:10 | 200 |  2.232389274s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:05:11 | 200 |  1.677838032s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:05:13 | 200 |  1.987794884s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:05:16 | 200 |  3.058814011s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:05:19 | 200 |  2.869895463s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:05:22 | 200 |  2.399993054s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:05:32 | 200 | 10.259418215s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:05:35 | 200 |   2.96031498s |       127.0.0.1 | POST     "/api/generate"
time=2026-01-14T13:05:35.548-05:00 level=WARN source=runner.go:153 msg="truncating input prompt" limit=90000 prompt=108258 keep=5 new=90000
[GIN] 2026/01/14 - 13:06:19 | 200 |  44.53784972s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:06:21 | 200 |  2.106295471s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:06:25 | 200 |  3.700950978s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:06:28 | 200 |   2.40356495s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:06:29 | 200 |  1.178891111s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:06:30 | 200 |  848.390992ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:06:32 | 200 |  2.604640057s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:06:35 | 200 |  3.041352706s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:06:40 | 200 |  4.965265865s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:06:44 | 200 |  4.153108598s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:06:46 | 200 |  1.856124496s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:06:49 | 200 |  2.976627913s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:06:52 | 200 |  2.284835064s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:06:53 | 200 |  1.671532923s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:06:55 | 200 |  1.823829345s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:07:17 | 200 | 21.548986351s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:07:18 | 200 |  1.485933803s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:07:23 | 200 |  5.079838069s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:07:33 | 200 | 10.041218252s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:07:35 | 200 |  1.619260333s |       127.0.0.1 | POST     "/api/generate"
time=2026-01-14T13:07:35.541-05:00 level=WARN source=runner.go:153 msg="truncating input prompt" limit=90000 prompt=101460 keep=5 new=90000
[GIN] 2026/01/14 - 13:08:14 | 200 |   39.0832571s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:08:15 | 200 |  1.140867134s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:08:18 | 200 |  3.144439216s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:08:19 | 200 |  1.303037523s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:08:21 | 200 |  1.903349004s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:08:22 | 200 |  1.072156149s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:08:25 | 200 |  2.113220789s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:08:29 | 200 |  4.252650209s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:08:35 | 200 |  5.690737737s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:08:37 | 200 |  2.538029687s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:08:42 | 200 |  5.195761506s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:08:45 | 200 |  2.633426621s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:08:50 | 200 |  4.751938912s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:08:52 | 200 |  2.505478639s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:08:54 | 200 |  1.466573272s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:08:56 | 200 |  1.859671655s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:08:59 | 200 |  3.320129342s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:10:50 | 200 |         1m51s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:10:54 | 200 |  3.545049798s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:10:56 | 200 |  2.451064835s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:10:57 | 200 |  1.137743454s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:10:59 | 200 |  1.151207061s |       127.0.0.1 | POST     "/api/generate"
time=2026-01-14T13:10:59.324-05:00 level=WARN source=runner.go:153 msg="truncating input prompt" limit=90000 prompt=93207 keep=5 new=90000
[GIN] 2026/01/14 - 13:11:39 | 200 |  39.93885166s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:11:41 | 200 |  2.221567604s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:11:44 | 200 |  3.561902234s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:11:47 | 200 |  2.688030109s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:11:55 | 200 |  7.791009744s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:11:57 | 200 |  2.578523152s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:11:59 | 200 |  1.261784404s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:12:02 | 200 |  2.947613905s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:12:05 | 200 |  3.379316824s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:12:07 | 200 |  2.254203481s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:12:09 | 200 |  1.353627394s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:12:12 | 200 |  3.427526207s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:12:13 | 200 |  1.313673772s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:12:15 | 200 |  1.422640091s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:12:17 | 200 |  1.826343772s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:12:17 | 200 |  798.741654ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:12:19 | 200 |  1.750909684s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:12:28 | 200 |  8.502204066s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:12:31 | 200 |  3.292613842s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:12:34 | 200 |  2.956340671s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:12:36 | 200 |  1.608245472s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:12:37 | 200 |  1.764873424s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:12:41 | 200 |  3.774546521s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:12:45 | 200 |   3.60816852s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:12:48 | 200 |  2.988740681s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:12:51 | 200 |  2.897076258s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:14:34 | 200 |         1m43s |       127.0.0.1 | POST     "/api/generate"
time=2026-01-14T13:14:35.332-05:00 level=WARN source=runner.go:153 msg="truncating input prompt" limit=90000 prompt=1290674 keep=5 new=90000
[GIN] 2026/01/14 - 13:15:28 | 200 | 53.855723841s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:15:30 | 200 |  2.531751188s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:15:33 | 200 |  2.695577325s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:15:37 | 200 |  3.679610764s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:15:39 | 200 |  2.373944574s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:15:41 | 200 |  2.355799208s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:15:50 | 200 |   8.36635615s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:15:51 | 200 |  1.055638472s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:16:32 | 200 | 41.032125959s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:16:35 | 200 |  2.854303703s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:16:42 | 200 |  7.481955717s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:16:48 | 200 |  5.483058252s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:16:53 | 200 |  4.995897522s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:17:08 | 200 | 15.382816936s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:17:20 | 200 | 12.043443425s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:17:25 | 200 |  4.796630174s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:17:31 | 200 |  6.377201459s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:17:34 | 200 |  2.472573607s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:17:36 | 200 |  2.098746846s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:17:39 | 200 |  2.850513602s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:17:41 | 200 |  2.142034936s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:17:43 | 200 |  2.533466451s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:17:46 | 200 |   2.42903571s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:17:48 | 200 |  1.911485605s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:17:49 | 200 |  1.797092918s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:17:52 | 200 |  2.411186354s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:17:55 | 200 |  3.409144726s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:17:58 | 200 |  2.804101148s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:18:00 | 200 |  2.070429906s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:18:02 | 200 |  2.018454624s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:18:06 | 200 |   3.29688698s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:18:09 | 200 |  3.643143615s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:18:10 | 200 |  1.206699432s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:18:13 | 200 |  2.713479661s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:18:15 | 200 |  1.651649906s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:18:18 | 200 |  2.913638046s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:18:20 | 200 |  2.709480828s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:18:23 | 200 |  2.408522107s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:18:24 | 200 |  1.144048736s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:18:27 | 200 |  3.015707376s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:18:41 | 200 | 14.099714455s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:18:46 | 200 |  4.678132344s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:18:50 | 200 |  4.583167738s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:18:52 | 200 |  1.318021122s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:18:55 | 200 |  2.933800492s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:18:57 | 200 |  2.027002414s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:18:59 | 200 |  2.666222951s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:19:01 | 200 |  2.064009503s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:19:04 | 200 |  2.180971602s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:19:06 | 200 |  2.200695655s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:19:16 | 200 |  9.973285618s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:19:18 | 200 |  2.071208487s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:19:20 | 200 |  2.722078242s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:19:31 | 200 | 10.881502193s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:19:32 | 200 |   1.06910403s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:19:35 | 200 |  2.207824435s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:19:37 | 200 |  2.235493352s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:19:40 | 200 |  3.056605305s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:19:44 | 200 |  3.805003731s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:19:45 | 200 |  1.580720186s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:19:54 | 200 |  8.329411733s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:19:56 | 200 |  2.379034537s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:19:58 | 200 |  2.370998268s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:20:00 | 200 |  2.008264885s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:20:08 | 200 |   7.44934837s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:20:13 | 200 |  4.832152936s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:20:15 | 200 |  2.250989164s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:20:18 | 200 |  2.667110788s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:20:19 | 200 |  1.765753854s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:20:22 | 200 |  2.164928394s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:20:29 | 200 |  7.406147739s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:20:32 | 200 |   3.09505317s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:20:34 | 200 |  1.912045895s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:20:38 | 200 |  4.122927079s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:20:40 | 200 |  2.063305695s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:20:41 | 200 |   1.27847782s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:20:45 | 200 |  3.910714633s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:20:47 | 200 |  1.942525496s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:20:50 | 200 |  2.172717176s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:20:52 | 200 |  2.249343762s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:20:53 | 200 |  1.493341646s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:20:56 | 200 |  2.344118816s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:20:57 | 200 |    1.7836873s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:20:59 | 200 |  1.183912797s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:21:00 | 200 |  1.293929852s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:21:03 | 200 |  3.433316203s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:21:06 | 200 |  2.375084556s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:21:08 | 200 |  2.104699836s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:21:09 | 200 |  750.659702ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:21:11 | 200 |  2.418776203s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:21:12 | 200 |  1.362308239s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:21:15 | 200 |  2.517101269s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:21:16 | 200 |  749.495637ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:21:18 | 200 |  2.438980158s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:21:22 | 200 |  4.145238221s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:21:24 | 200 |  1.602063817s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:21:26 | 200 |  2.517655218s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:21:28 | 200 |  1.446896577s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:21:29 | 200 |  981.564909ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:21:31 | 200 |  2.253051853s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:21:33 | 200 |  2.273626777s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:21:42 | 200 |  8.398527196s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:21:43 | 200 |  1.617683028s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:21:45 | 200 |  1.571613504s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:21:53 | 200 |  7.665106722s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:21:55 | 200 |   2.56366692s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:21:56 | 200 |  1.298685092s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:22:00 | 200 |  3.713417967s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:22:04 | 200 |  4.097854337s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:22:07 | 200 |  2.600416654s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:22:10 | 200 |  3.264871808s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:22:15 | 200 |  4.513189868s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:22:19 | 200 |  3.942012504s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:22:24 | 200 |  5.063183824s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:22:33 | 200 |  9.048060803s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:22:39 | 200 |  6.434401408s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:22:41 | 200 |   2.09580918s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:22:43 | 200 |  1.777152454s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:22:46 | 200 |  3.504172118s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:22:50 | 200 |   3.88560553s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:22:52 | 200 |  1.741236995s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:22:55 | 200 |  3.299164041s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:22:58 | 200 |  3.021251197s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:23:00 | 200 |  1.619478082s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:23:25 | 200 | 24.539202278s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:23:28 | 200 |  3.033498777s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:23:31 | 200 |  3.566761013s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:23:41 | 200 |  9.900280809s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:23:43 | 200 |  2.394631061s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:23:49 | 200 |  5.347860842s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:23:52 | 200 |  3.034327789s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:23:53 | 200 |  1.253842207s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:24:34 | 500 | 40.794193804s |       127.0.0.1 | POST     "/api/generate"
time=2026-01-14T13:29:34.438-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 44491"
time=2026-01-14T13:29:34.881-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 37391"
time=2026-01-14T13:30:56.591-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 45493"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-01-14T13:30:57.088-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --model /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --port 39265"
time=2026-01-14T13:30:57.088-05:00 level=INFO source=sched.go:443 msg="system memory" total="251.4 GiB" free="233.5 GiB" free_swap="12.0 GiB"
time=2026-01-14T13:30:57.088-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b library=CUDA available="43.9 GiB" free="44.4 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-01-14T13:30:57.088-05:00 level=INFO source=server.go:496 msg="loading model" "model layers"=33 requested=-1
time=2026-01-14T13:30:57.089-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="4.3 GiB"
time=2026-01-14T13:30:57.089-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="11.0 GiB"
time=2026-01-14T13:30:57.089-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="5.7 GiB"
time=2026-01-14T13:30:57.089-05:00 level=INFO source=device.go:272 msg="total memory" size="21.0 GiB"
time=2026-01-14T13:30:57.099-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /cm/shared/apps/ollama/0.13.5/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA L40S, compute capability 8.9, VMM: yes, ID: GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b
load_backend: loaded CUDA backend from /cm/shared/apps/ollama/0.13.5/lib/ollama/cuda_v13/libggml-cuda.so
time=2026-01-14T13:30:57.224-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2026-01-14T13:30:57.224-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:39265"
time=2026-01-14T13:30:57.226-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:90000 KvCacheType: NumThreads:32 GPULayers:33[ID:GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
time=2026-01-14T13:30:57.226-05:00 level=INFO source=server.go:1338 msg="waiting for llama runner to start responding"
time=2026-01-14T13:30:57.226-05:00 level=INFO source=server.go:1372 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory device GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b utilizing NVML memory reporting free: 47664726016 total: 48305799168
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA L40S) (0000:e1:00.0) - 45456 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_embd_inp       = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
load_tensors:        CUDA0 model buffer size =  4403.49 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 90112
llama_context: n_ctx_seq     = 90112
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (90112) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.50 MiB
llama_kv_cache:      CUDA0 KV buffer size = 11264.00 MiB
llama_kv_cache: size = 11264.00 MiB ( 90112 cells,  32 layers,  1/1 seqs), K (f16): 5632.00 MiB, V (f16): 5632.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   296.01 MiB
llama_context:  CUDA_Host compute buffer size =   184.01 MiB
llama_context: graph nodes  = 999
llama_context: graph splits = 2
time=2026-01-14T13:30:58.982-05:00 level=INFO source=server.go:1376 msg="llama runner started in 1.89 seconds"
time=2026-01-14T13:30:58.982-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=1
time=2026-01-14T13:30:58.982-05:00 level=INFO source=server.go:1338 msg="waiting for llama runner to start responding"
time=2026-01-14T13:30:58.982-05:00 level=INFO source=server.go:1376 msg="llama runner started in 1.89 seconds"
[GIN] 2026/01/14 - 13:31:06 | 200 | 10.499926874s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:37:41 | 500 |         6m34s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:39:58 | 200 |  919.995265ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:39:59 | 200 |  715.860194ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:40:01 | 200 |  1.387313834s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:40:05 | 200 |  4.538576526s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:40:08 | 200 |  3.165128848s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:40:10 | 200 |  1.860250892s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:40:21 | 200 |   10.6656343s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:40:25 | 200 |  4.147325619s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:40:30 | 200 |  5.011552842s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:40:35 | 200 |  4.594357087s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:40:37 | 200 |  2.186401805s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:40:37 | 200 |  484.102604ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:40:40 | 200 |  2.401003066s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:40:43 | 200 |  3.324645201s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:40:45 | 200 |  1.968420818s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:40:47 | 200 |  1.639775237s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:40:51 | 200 |  3.942386487s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:40:54 | 200 |  3.715085583s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:41:01 | 200 |  7.218129412s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:41:04 | 200 |  2.976558384s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:41:10 | 200 |  5.925999884s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:41:14 | 200 |  3.356327464s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:41:24 | 200 | 10.219905345s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:41:32 | 200 |  7.862311054s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:41:34 | 200 |  2.596252251s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:41:44 | 200 |  9.729724655s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:42:00 | 200 | 15.894403606s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:42:13 | 200 | 13.093196395s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:42:16 | 200 |  3.132445173s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:42:35 | 200 |  18.86527017s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:42:50 | 200 | 14.571375431s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:43:12 | 200 | 21.852473899s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:43:26 | 200 | 13.920263117s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:43:39 | 200 | 13.900834501s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:43:40 | 200 |  709.662829ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:43:43 | 200 |  3.339913138s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:43:50 | 200 |  6.215371418s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:43:53 | 200 |  3.716230599s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:43:54 | 200 |  823.100378ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:43:55 | 200 |  592.301616ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:43:55 | 200 |  375.780476ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:43:57 | 200 |  1.858342431s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:43:58 | 200 |  378.206452ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:43:59 | 200 |  1.773167565s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:44:02 | 200 |  2.420888548s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:44:04 | 200 |  2.436559444s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:44:05 | 200 |  975.479662ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:44:06 | 200 |  1.203091471s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:44:09 | 200 |  2.551114964s |       127.0.0.1 | POST     "/api/generate"
time=2026-01-14T13:49:09.435-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 34853"
time=2026-01-14T13:49:09.829-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 35147"
time=2026-01-14T13:57:02.245-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 37329"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-01-14T13:57:02.680-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --model /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --port 34787"
time=2026-01-14T13:57:02.680-05:00 level=INFO source=sched.go:443 msg="system memory" total="251.4 GiB" free="233.1 GiB" free_swap="12.0 GiB"
time=2026-01-14T13:57:02.680-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b library=CUDA available="43.9 GiB" free="44.4 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-01-14T13:57:02.680-05:00 level=INFO source=server.go:496 msg="loading model" "model layers"=33 requested=-1
time=2026-01-14T13:57:02.681-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="4.3 GiB"
time=2026-01-14T13:57:02.681-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="11.0 GiB"
time=2026-01-14T13:57:02.681-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="5.7 GiB"
time=2026-01-14T13:57:02.681-05:00 level=INFO source=device.go:272 msg="total memory" size="21.0 GiB"
time=2026-01-14T13:57:02.689-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /cm/shared/apps/ollama/0.13.5/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA L40S, compute capability 8.9, VMM: yes, ID: GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b
load_backend: loaded CUDA backend from /cm/shared/apps/ollama/0.13.5/lib/ollama/cuda_v13/libggml-cuda.so
time=2026-01-14T13:57:02.794-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2026-01-14T13:57:02.794-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:34787"
time=2026-01-14T13:57:02.795-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:90000 KvCacheType: NumThreads:32 GPULayers:33[ID:GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
time=2026-01-14T13:57:02.795-05:00 level=INFO source=server.go:1338 msg="waiting for llama runner to start responding"
time=2026-01-14T13:57:02.796-05:00 level=INFO source=server.go:1372 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory device GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b utilizing NVML memory reporting free: 47664726016 total: 48305799168
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA L40S) (0000:e1:00.0) - 45456 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_embd_inp       = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
load_tensors:        CUDA0 model buffer size =  4403.49 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 90112
llama_context: n_ctx_seq     = 90112
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (90112) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.50 MiB
llama_kv_cache:      CUDA0 KV buffer size = 11264.00 MiB
llama_kv_cache: size = 11264.00 MiB ( 90112 cells,  32 layers,  1/1 seqs), K (f16): 5632.00 MiB, V (f16): 5632.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   296.01 MiB
llama_context:  CUDA_Host compute buffer size =   184.01 MiB
llama_context: graph nodes  = 999
llama_context: graph splits = 2
time=2026-01-14T13:57:04.300-05:00 level=INFO source=server.go:1376 msg="llama runner started in 1.62 seconds"
time=2026-01-14T13:57:04.300-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=1
time=2026-01-14T13:57:04.300-05:00 level=INFO source=server.go:1338 msg="waiting for llama runner to start responding"
time=2026-01-14T13:57:04.300-05:00 level=INFO source=server.go:1376 msg="llama runner started in 1.62 seconds"
[GIN] 2026/01/14 - 13:57:05 | 200 |  3.319156693s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:57:06 | 200 |  768.611505ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:57:07 | 200 |   1.57192429s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:57:11 | 200 |  3.754671508s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:57:13 | 200 |  1.803978002s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:57:15 | 200 |  1.757317418s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:57:22 | 200 |  7.328244137s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:57:23 | 200 |   1.12301838s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:57:25 | 200 |   1.81003642s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:57:28 | 200 |  2.730925866s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:57:32 | 200 |  4.255776649s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:57:32 | 200 |    511.7611ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:57:34 | 200 |  1.545217291s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:57:37 | 200 |  3.294366654s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:57:39 | 200 |   1.37421208s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:57:40 | 200 |  1.703228639s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:57:44 | 200 |  3.656734265s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:57:48 | 200 |  3.520716711s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:57:52 | 200 |  4.555676872s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:57:56 | 200 |  3.814079809s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:58:02 | 200 |   6.38728902s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:58:05 | 200 |  3.072779977s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:58:15 | 200 |   9.38991588s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:58:21 | 200 |  5.986588072s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:58:27 | 200 |   6.77128964s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:58:34 | 200 |  6.302705972s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:58:41 | 200 |  7.147381727s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:58:45 | 200 |  4.136757155s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:58:49 | 200 |     3.719469s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:59:02 | 200 | 12.920379327s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:59:06 | 200 |   4.24247047s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:59:19 | 200 | 13.011299068s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:59:23 | 200 |  4.441351339s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:59:34 | 200 | 10.148841272s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:59:35 | 200 |  1.518073411s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:59:39 | 200 |  3.503477383s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:59:41 | 200 |  2.086865131s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:59:49 | 200 |  8.145057962s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:59:50 | 200 |  896.932614ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:59:51 | 200 |  1.067804774s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:59:51 | 200 |  424.343625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:59:53 | 200 |  1.812786297s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:59:53 | 200 |   436.54754ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:59:56 | 200 |  2.034471638s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 13:59:58 | 200 |  2.682969138s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:00:00 | 200 |  2.030588205s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:00:04 | 200 |  3.421779918s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:00:05 | 200 |  1.214330119s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:00:08 | 200 |  3.221571617s |       127.0.0.1 | POST     "/api/generate"
time=2026-01-14T14:05:08.593-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 33937"
time=2026-01-14T14:05:08.947-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 40799"
time=2026-01-14T14:05:29.666-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 34869"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-01-14T14:05:30.104-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --model /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --port 45995"
time=2026-01-14T14:05:30.105-05:00 level=INFO source=sched.go:443 msg="system memory" total="251.4 GiB" free="228.4 GiB" free_swap="12.0 GiB"
time=2026-01-14T14:05:30.105-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b library=CUDA available="43.9 GiB" free="44.4 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-01-14T14:05:30.105-05:00 level=INFO source=server.go:496 msg="loading model" "model layers"=33 requested=-1
time=2026-01-14T14:05:30.105-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="4.3 GiB"
time=2026-01-14T14:05:30.105-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="11.0 GiB"
time=2026-01-14T14:05:30.105-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="5.7 GiB"
time=2026-01-14T14:05:30.105-05:00 level=INFO source=device.go:272 msg="total memory" size="21.0 GiB"
time=2026-01-14T14:05:30.114-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /cm/shared/apps/ollama/0.13.5/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA L40S, compute capability 8.9, VMM: yes, ID: GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b
load_backend: loaded CUDA backend from /cm/shared/apps/ollama/0.13.5/lib/ollama/cuda_v13/libggml-cuda.so
time=2026-01-14T14:05:30.218-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2026-01-14T14:05:30.218-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:45995"
time=2026-01-14T14:05:30.221-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:90000 KvCacheType: NumThreads:32 GPULayers:33[ID:GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
time=2026-01-14T14:05:30.221-05:00 level=INFO source=server.go:1338 msg="waiting for llama runner to start responding"
time=2026-01-14T14:05:30.221-05:00 level=INFO source=server.go:1372 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory device GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b utilizing NVML memory reporting free: 47664726016 total: 48305799168
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA L40S) (0000:e1:00.0) - 45456 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_embd_inp       = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
load_tensors:        CUDA0 model buffer size =  4403.49 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 90112
llama_context: n_ctx_seq     = 90112
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (90112) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.50 MiB
llama_kv_cache:      CUDA0 KV buffer size = 11264.00 MiB
llama_kv_cache: size = 11264.00 MiB ( 90112 cells,  32 layers,  1/1 seqs), K (f16): 5632.00 MiB, V (f16): 5632.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   296.01 MiB
llama_context:  CUDA_Host compute buffer size =   184.01 MiB
llama_context: graph nodes  = 999
llama_context: graph splits = 2
time=2026-01-14T14:05:31.976-05:00 level=INFO source=server.go:1376 msg="llama runner started in 1.87 seconds"
time=2026-01-14T14:05:31.976-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=1
time=2026-01-14T14:05:31.976-05:00 level=INFO source=server.go:1338 msg="waiting for llama runner to start responding"
time=2026-01-14T14:05:31.976-05:00 level=INFO source=server.go:1376 msg="llama runner started in 1.87 seconds"
[GIN] 2026/01/14 - 14:05:32 | 200 |  3.126886161s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:05:33 | 200 |  718.305332ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:05:34 | 200 |  1.430529566s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:05:37 | 200 |  2.251077249s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:05:40 | 200 |  3.186543669s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:05:46 | 200 |  6.310131127s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:05:55 | 200 |  9.016665388s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:05:56 | 200 |   854.61132ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:05:58 | 200 |  1.998407649s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:06:01 | 200 |  3.018074264s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:06:03 | 200 |  1.806466502s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:06:03 | 200 |  458.737708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:06:04 | 200 |  1.200736016s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:06:08 | 200 |  3.302095286s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:06:13 | 200 |  5.358881133s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:06:15 | 200 |  2.051491921s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:06:17 | 200 |  1.784565412s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:06:22 | 200 |  4.939915662s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:06:27 | 200 |  5.359631472s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:06:30 | 200 |  2.494217247s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:06:34 | 200 |  4.151322328s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:06:41 | 200 |  7.363549623s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:06:43 | 200 |  1.568068777s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:06:46 | 200 |  3.193220186s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:06:49 | 200 |  2.494800504s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:06:57 | 200 |  8.294700258s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:07:02 | 200 |  5.609154441s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:07:05 | 200 |   2.06109649s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:07:08 | 200 |  3.365622523s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:07:18 | 200 | 10.063948545s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:07:25 | 200 |  6.812610753s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:07:34 | 200 |   9.02796896s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:07:57 | 200 | 23.359947279s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:08:08 | 200 | 11.150361829s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:08:09 | 200 |  940.379159ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:08:14 | 200 |  4.304315709s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:08:27 | 200 | 13.893253871s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:08:29 | 200 |  1.786784927s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:08:30 | 200 |  831.458608ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:08:31 | 200 |  529.369523ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:08:31 | 200 |  653.739227ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:08:33 | 200 |  1.741879638s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:08:33 | 200 |  144.129176ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:08:36 | 200 |   2.67675812s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:08:38 | 200 |  2.559742509s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:08:43 | 200 |  4.508852313s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:08:45 | 200 |  2.510008143s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:08:52 | 200 |  6.469185309s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:08:55 | 200 |  3.176848527s |       127.0.0.1 | POST     "/api/generate"
time=2026-01-14T14:13:55.551-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 38999"
time=2026-01-14T14:13:55.961-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 42011"
time=2026-01-14T14:23:11.688-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 36361"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-01-14T14:23:12.122-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --model /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --port 35961"
time=2026-01-14T14:23:12.123-05:00 level=INFO source=sched.go:443 msg="system memory" total="251.4 GiB" free="233.4 GiB" free_swap="12.0 GiB"
time=2026-01-14T14:23:12.123-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b library=CUDA available="43.9 GiB" free="44.4 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-01-14T14:23:12.123-05:00 level=INFO source=server.go:496 msg="loading model" "model layers"=33 requested=-1
time=2026-01-14T14:23:12.123-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="4.3 GiB"
time=2026-01-14T14:23:12.123-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="11.0 GiB"
time=2026-01-14T14:23:12.123-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="5.7 GiB"
time=2026-01-14T14:23:12.123-05:00 level=INFO source=device.go:272 msg="total memory" size="21.0 GiB"
time=2026-01-14T14:23:12.132-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /cm/shared/apps/ollama/0.13.5/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA L40S, compute capability 8.9, VMM: yes, ID: GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b
load_backend: loaded CUDA backend from /cm/shared/apps/ollama/0.13.5/lib/ollama/cuda_v13/libggml-cuda.so
time=2026-01-14T14:23:12.236-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2026-01-14T14:23:12.236-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:35961"
time=2026-01-14T14:23:12.239-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:90000 KvCacheType: NumThreads:32 GPULayers:33[ID:GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
time=2026-01-14T14:23:12.239-05:00 level=INFO source=server.go:1338 msg="waiting for llama runner to start responding"
time=2026-01-14T14:23:12.239-05:00 level=INFO source=server.go:1372 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory device GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b utilizing NVML memory reporting free: 47664726016 total: 48305799168
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA L40S) (0000:e1:00.0) - 45456 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_embd_inp       = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
load_tensors:        CUDA0 model buffer size =  4403.49 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 90112
llama_context: n_ctx_seq     = 90112
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (90112) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.50 MiB
llama_kv_cache:      CUDA0 KV buffer size = 11264.00 MiB
llama_kv_cache: size = 11264.00 MiB ( 90112 cells,  32 layers,  1/1 seqs), K (f16): 5632.00 MiB, V (f16): 5632.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   296.01 MiB
llama_context:  CUDA_Host compute buffer size =   184.01 MiB
llama_context: graph nodes  = 999
llama_context: graph splits = 2
time=2026-01-14T14:23:13.992-05:00 level=INFO source=server.go:1376 msg="llama runner started in 1.87 seconds"
time=2026-01-14T14:23:13.992-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=1
time=2026-01-14T14:23:13.992-05:00 level=INFO source=server.go:1338 msg="waiting for llama runner to start responding"
time=2026-01-14T14:23:13.992-05:00 level=INFO source=server.go:1376 msg="llama runner started in 1.87 seconds"
[GIN] 2026/01/14 - 14:23:15 | 200 |  4.135542404s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:23:20 | 200 |  4.683605946s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:23:22 | 200 |  1.892499975s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:23:23 | 200 |  1.612550207s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:23:25 | 200 |  1.442452622s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:23:28 | 200 |  3.446696357s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:23:31 | 200 |  2.254457922s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:23:31 | 200 |  636.064685ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:23:33 | 200 |  1.511594151s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:23:36 | 200 |  3.630826353s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:23:37 | 200 |  683.221806ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:23:38 | 200 |  1.207106135s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:23:40 | 200 |  1.599633357s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:23:42 | 200 |  1.969319097s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:23:44 | 200 |  1.706429893s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:23:47 | 200 |  3.917713808s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:23:52 | 200 |   4.42779252s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:23:57 | 200 |   4.66381687s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:23:57 | 200 |  166.302022ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:24:05 | 200 |  8.266566214s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:24:10 | 200 |  4.865966115s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:24:10 | 200 |  140.752335ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:24:10 | 200 |  370.401929ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:24:11 | 200 |  744.960604ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:24:12 | 200 |  352.467881ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:24:13 | 200 |  941.518765ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:24:14 | 200 |  1.741943429s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:24:15 | 200 |  573.346324ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:24:18 | 200 |  2.745279017s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:24:18 | 200 |  561.528541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:24:20 | 200 |  1.443717798s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:24:22 | 200 |  1.960286604s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:24:23 | 200 |  1.808464679s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:24:26 | 200 |  2.468089127s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:24:30 | 200 |  4.171931472s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:24:31 | 200 |   549.76287ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:24:33 | 200 |  2.551987229s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:24:34 | 200 |  1.064393277s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:24:37 | 200 |  2.272898406s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:24:38 | 200 |  1.218543027s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:24:43 | 200 |  5.449319314s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:24:49 | 200 |  5.931726666s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:24:51 | 200 |  2.277047877s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:24:56 | 200 |  4.118629192s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:24:57 | 200 |  1.249667274s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:24:58 | 200 |  835.481577ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:25:05 | 200 |  7.177411399s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:25:19 | 200 | 14.643442747s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:25:24 | 200 |   4.39091218s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:25:29 | 200 |  4.750765674s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:25:32 | 200 |  3.558161175s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:25:35 | 200 |  3.010186904s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:25:37 | 200 |  2.103989653s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:25:40 | 200 |   2.95220516s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:25:42 | 200 |  1.463233825s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:25:45 | 200 |   3.47335892s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:25:49 | 200 |  3.973739337s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:25:53 | 200 |  4.110056861s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:25:55 | 200 |  1.637304774s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:25:56 | 200 |  765.508708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:25:56 | 200 |  792.654813ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:25:58 | 200 |  1.292320369s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:02 | 200 |  4.305738504s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:08 | 200 |  6.209087268s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:09 | 200 |  980.763716ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:11 | 200 |  1.951884554s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:14 | 200 |  2.875829061s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:16 | 200 |  1.669100911s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:18 | 200 |  2.401579411s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:19 | 200 |  682.476457ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:20 | 200 |  660.903546ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:22 | 200 |  1.968029383s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:25 | 200 |  3.050071475s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:25 | 200 |   340.44564ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:26 | 200 |  683.830455ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:27 | 200 |  1.189406161s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:29 | 200 |  2.375875072s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:30 | 200 |  767.664721ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:34 | 200 |  4.099676133s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:39 | 200 |  4.911370349s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:40 | 200 |  738.593535ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:41 | 200 |  1.021216031s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:43 | 200 |  1.876960499s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:44 | 200 |  783.995454ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:46 | 200 |  2.064274579s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:46 | 200 |   371.69731ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:48 | 200 |  1.563749415s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:48 | 200 |  763.050028ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:50 | 200 |  1.347387408s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:51 | 200 |  834.072573ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:54 | 200 |  2.915396918s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:54 | 200 |   791.80639ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:56 | 200 |  2.131288404s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:57 | 200 |  817.627161ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:26:59 | 200 |  1.335321195s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:27:00 | 200 |  1.729600264s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:27:04 | 200 |  3.950494691s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:27:05 | 200 |   550.97434ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:27:05 | 200 |  388.664759ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:27:05 | 200 |  128.645295ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:03 | 200 |  2.060869892s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:08 | 200 |  4.642773024s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:10 | 200 |  1.664288627s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:11 | 200 |  1.616802829s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:13 | 200 |  1.535758238s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:16 | 200 |  2.731395839s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:16 | 200 |  790.252142ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:19 | 200 |  2.289530156s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:20 | 200 |  943.240911ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:24 | 200 |  3.869023868s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:24 | 200 |  463.364374ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:25 | 200 |  1.176454051s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:27 | 200 |  1.478570642s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:29 | 200 |  1.878450154s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:30 | 200 |    1.4580216s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:34 | 200 |  3.937295575s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:39 | 200 |  4.702865205s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:43 | 200 |  4.649341236s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:43 | 200 |  163.374572ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:48 | 200 |   4.70161049s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:52 | 200 |  3.969027402s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:52 | 200 |  135.083913ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:52 | 200 |  133.957265ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:53 | 200 |  794.297013ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:54 | 200 |  630.640824ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:55 | 200 |  1.126400816s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:56 | 200 |  1.178801937s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:30:57 | 200 |  614.836385ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:31:01 | 200 |  3.916432824s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:31:01 | 200 |  488.203619ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:31:03 | 200 |  1.373626487s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:31:04 | 200 |  1.763691918s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:31:06 | 200 |  1.827332418s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:31:08 | 200 |  2.065296367s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:31:12 | 200 |  4.192165887s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:31:13 | 200 |   637.12675ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:31:16 | 200 |  2.479871827s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:31:17 | 200 |  1.226810384s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:31:19 | 200 |  2.134906267s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:31:20 | 200 |  1.057260867s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:31:26 | 200 |  5.820051416s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:31:33 | 200 |  6.868887587s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:31:35 | 200 |  1.989715672s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:31:39 | 200 |  3.951088268s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:31:43 | 200 |  4.201790909s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:31:44 | 200 |  1.026883445s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:31:53 | 200 |  9.228286867s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:32:08 | 200 | 14.789906183s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:32:35 | 200 | 27.015566259s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:32:38 | 200 |   3.26410874s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:32:44 | 200 |  6.220045781s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:33:10 | 200 | 25.417536419s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:33:11 | 200 |  1.699592186s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:33:15 | 200 |  3.283093888s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:33:17 | 200 |   2.31539209s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:33:21 | 200 |  3.602127382s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:33:25 | 200 |   4.24006556s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:33:29 | 200 |  4.132843881s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:33:30 | 200 |  1.412852714s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:33:33 | 200 |  2.113670148s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:33:33 | 200 |  582.171264ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:33:35 | 200 |   1.28645621s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:33:37 | 200 |  2.691681834s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:33:43 | 200 |  6.054665635s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:33:44 | 200 |  867.843118ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:33:46 | 200 |  1.696183713s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:33:48 | 200 |  2.509613757s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:33:51 | 200 |  2.823895985s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:33:54 | 200 |  2.678979898s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:33:55 | 200 |  711.435103ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:33:55 | 200 |   833.42507ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:33:58 | 200 |  2.210897177s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:00 | 200 |  2.139927851s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:00 | 200 |  322.246568ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:01 | 200 |  947.346577ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:03 | 200 |  2.087238617s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:05 | 200 |  2.106963929s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:08 | 200 |  2.707115955s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:12 | 200 |  4.216124446s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:20 | 200 |  7.425284871s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:24 | 200 |   3.96510277s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:25 | 200 |   1.04353339s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:27 | 200 |  1.931431707s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:27 | 200 |   491.88714ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:29 | 200 |  1.794188754s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:30 | 200 |  575.527575ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:30 | 200 |  893.919517ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:31 | 200 |  766.697326ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:33 | 200 |  1.705736488s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:34 | 200 |  756.001234ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:37 | 200 |  3.392639489s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:38 | 200 |  774.582543ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:40 | 200 |  2.038221452s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:41 | 200 |  707.916325ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:42 | 200 |  1.522982773s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:44 | 200 |  1.677180047s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:48 | 200 |  4.096610711s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:49 | 200 |  706.470554ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:49 | 200 |  380.974934ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:34:49 | 200 |  199.410281ms |       127.0.0.1 | POST     "/api/generate"
time=2026-01-14T14:39:49.670-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 37979"
time=2026-01-14T14:39:50.016-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 34515"
time=2026-01-14T14:42:38.283-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 41531"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-01-14T14:42:38.740-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --model /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --port 43123"
time=2026-01-14T14:42:38.741-05:00 level=INFO source=sched.go:443 msg="system memory" total="251.4 GiB" free="230.8 GiB" free_swap="12.0 GiB"
time=2026-01-14T14:42:38.741-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b library=CUDA available="43.9 GiB" free="44.4 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-01-14T14:42:38.741-05:00 level=INFO source=server.go:496 msg="loading model" "model layers"=33 requested=-1
time=2026-01-14T14:42:38.741-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="4.3 GiB"
time=2026-01-14T14:42:38.741-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="11.0 GiB"
time=2026-01-14T14:42:38.741-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="5.7 GiB"
time=2026-01-14T14:42:38.741-05:00 level=INFO source=device.go:272 msg="total memory" size="21.0 GiB"
time=2026-01-14T14:42:38.750-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /cm/shared/apps/ollama/0.13.5/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA L40S, compute capability 8.9, VMM: yes, ID: GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b
load_backend: loaded CUDA backend from /cm/shared/apps/ollama/0.13.5/lib/ollama/cuda_v13/libggml-cuda.so
time=2026-01-14T14:42:38.855-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2026-01-14T14:42:38.856-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:43123"
time=2026-01-14T14:42:38.858-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:90000 KvCacheType: NumThreads:32 GPULayers:33[ID:GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
time=2026-01-14T14:42:38.858-05:00 level=INFO source=server.go:1338 msg="waiting for llama runner to start responding"
time=2026-01-14T14:42:38.858-05:00 level=INFO source=server.go:1372 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory device GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b utilizing NVML memory reporting free: 47664726016 total: 48305799168
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA L40S) (0000:e1:00.0) - 45456 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_embd_inp       = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
load_tensors:        CUDA0 model buffer size =  4403.49 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 90112
llama_context: n_ctx_seq     = 90112
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (90112) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.50 MiB
llama_kv_cache:      CUDA0 KV buffer size = 11264.00 MiB
llama_kv_cache: size = 11264.00 MiB ( 90112 cells,  32 layers,  1/1 seqs), K (f16): 5632.00 MiB, V (f16): 5632.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   296.01 MiB
llama_context:  CUDA_Host compute buffer size =   184.01 MiB
llama_context: graph nodes  = 999
llama_context: graph splits = 2
time=2026-01-14T14:42:40.614-05:00 level=INFO source=server.go:1376 msg="llama runner started in 1.87 seconds"
time=2026-01-14T14:42:40.614-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=1
time=2026-01-14T14:42:40.614-05:00 level=INFO source=server.go:1338 msg="waiting for llama runner to start responding"
time=2026-01-14T14:42:40.614-05:00 level=INFO source=server.go:1376 msg="llama runner started in 1.87 seconds"
[GIN] 2026/01/14 - 14:42:42 | 200 |  4.315116698s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:42:43 | 200 |  1.185953163s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:42:45 | 200 |  1.775687553s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:42:46 | 200 |  1.527873115s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:42:48 | 200 |  1.545939635s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:42:51 | 200 |  3.331241874s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:42:52 | 200 |  674.671636ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:42:54 | 200 |  1.570198347s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:42:55 | 200 |  1.617032667s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:42:58 | 200 |  3.212924303s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:42:59 | 200 |  470.080517ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:00 | 200 |  1.399748069s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:03 | 200 |  2.896607746s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:05 | 200 |  1.506514109s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:06 | 200 |  1.722644578s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:10 | 200 |  3.914927407s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:11 | 200 |  1.029176761s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:15 | 200 |  3.988315491s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:16 | 200 |  172.366179ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:23 | 200 |   7.80238615s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:31 | 200 |  7.730311258s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:32 | 200 |  505.758236ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:32 | 200 |  132.225334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:32 | 200 |  419.192766ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:33 | 200 |  781.202172ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:34 | 200 |  785.433395ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:36 | 200 |  1.788347963s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:36 | 200 |  562.405422ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:39 | 200 |  3.094069585s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:39 | 200 |  178.617744ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:41 | 200 |  1.562595367s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:42 | 200 |  1.541134956s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:44 | 200 |  1.518865213s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:45 | 200 |   679.16317ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:49 | 200 |  4.002562327s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:49 | 200 |  502.417969ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:50 | 200 |  602.730663ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:51 | 200 |  812.142335ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:53 | 200 |  2.300207956s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:56 | 200 |  2.735615045s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:43:56 | 200 |  743.251471ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:44:03 | 200 |  6.279667436s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:44:04 | 200 |  1.131775617s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:44:08 | 200 |  4.498515349s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:44:09 | 200 |  1.172305145s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:44:15 | 200 |  5.689838074s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:44:22 | 200 |    6.4271541s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:44:39 | 200 | 17.525415752s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:44:58 | 200 | 18.405435901s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:44:58 | 200 |  884.880594ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:45:04 | 200 |  5.339438338s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:45:29 | 200 | 24.852004669s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:45:29 | 200 |   829.78187ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:45:32 | 200 |  2.487893348s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:45:37 | 200 |  5.089115628s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:45:41 | 200 |  3.612637587s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:45:44 | 200 |  3.549115314s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:45:49 | 200 |  5.216645054s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:45:50 | 200 |  842.949339ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:45:52 | 200 |  1.634695387s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:45:52 | 200 |  582.637541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:45:54 | 200 |  1.161529005s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:45:58 | 200 |  3.916790178s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:03 | 200 |  5.333759213s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:04 | 200 |  732.815382ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:05 | 200 |   1.58387942s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:08 | 200 |  3.194415141s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:10 | 200 |  1.250236502s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:12 | 200 |  2.518944343s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:13 | 200 |  719.251107ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:14 | 200 |   1.50245144s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:16 | 200 |  2.024307189s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:19 | 200 |  2.985770549s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:21 | 200 |  1.162571366s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:21 | 200 |  682.221261ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:23 | 200 |  1.382833893s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:25 | 200 |  2.363315796s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:26 | 200 |  736.620097ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:30 | 200 |  3.888731192s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:34 | 200 |  3.873073764s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:34 | 200 |  887.270169ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:36 | 200 |  1.192087234s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:37 | 200 |  1.573761885s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:37 | 200 |  164.460032ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:39 | 200 |  2.130055134s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:40 | 200 |  322.164881ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:41 | 200 |  1.603176668s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:42 | 200 |  741.623135ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:44 | 200 |  1.612848001s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:44 | 200 |  727.649374ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:47 | 200 |   2.90677703s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:48 | 200 |  823.793983ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:50 | 200 |  2.073608189s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:51 | 200 |  798.748508ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:53 | 200 |  1.457573994s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:54 | 200 |  1.213690435s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:58 | 200 |  3.972222382s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:58 | 200 |  547.440951ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:58 | 200 |  166.989466ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:46:59 | 200 |  133.438323ms |       127.0.0.1 | POST     "/api/generate"
time=2026-01-14T14:51:59.086-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 42051"
time=2026-01-14T14:51:59.421-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 46441"
time=2026-01-14T14:55:09.278-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 43707"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-01-14T14:55:09.761-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --model /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --port 46549"
time=2026-01-14T14:55:09.762-05:00 level=INFO source=sched.go:443 msg="system memory" total="251.4 GiB" free="233.3 GiB" free_swap="12.0 GiB"
time=2026-01-14T14:55:09.762-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b library=CUDA available="43.9 GiB" free="44.4 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-01-14T14:55:09.762-05:00 level=INFO source=server.go:496 msg="loading model" "model layers"=33 requested=-1
time=2026-01-14T14:55:09.762-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="4.3 GiB"
time=2026-01-14T14:55:09.762-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="11.0 GiB"
time=2026-01-14T14:55:09.762-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="5.7 GiB"
time=2026-01-14T14:55:09.762-05:00 level=INFO source=device.go:272 msg="total memory" size="21.0 GiB"
time=2026-01-14T14:55:09.771-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /cm/shared/apps/ollama/0.13.5/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA L40S, compute capability 8.9, VMM: yes, ID: GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b
load_backend: loaded CUDA backend from /cm/shared/apps/ollama/0.13.5/lib/ollama/cuda_v13/libggml-cuda.so
time=2026-01-14T14:55:09.876-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2026-01-14T14:55:09.876-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:46549"
time=2026-01-14T14:55:09.878-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:90000 KvCacheType: NumThreads:32 GPULayers:33[ID:GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
time=2026-01-14T14:55:09.878-05:00 level=INFO source=server.go:1338 msg="waiting for llama runner to start responding"
time=2026-01-14T14:55:09.879-05:00 level=INFO source=server.go:1372 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory device GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b utilizing NVML memory reporting free: 47664726016 total: 48305799168
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA L40S) (0000:e1:00.0) - 45456 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_embd_inp       = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
load_tensors:        CUDA0 model buffer size =  4403.49 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 90112
llama_context: n_ctx_seq     = 90112
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (90112) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.50 MiB
llama_kv_cache:      CUDA0 KV buffer size = 11264.00 MiB
llama_kv_cache: size = 11264.00 MiB ( 90112 cells,  32 layers,  1/1 seqs), K (f16): 5632.00 MiB, V (f16): 5632.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   296.01 MiB
llama_context:  CUDA_Host compute buffer size =   184.01 MiB
llama_context: graph nodes  = 999
llama_context: graph splits = 2
time=2026-01-14T14:55:11.635-05:00 level=INFO source=server.go:1376 msg="llama runner started in 1.87 seconds"
time=2026-01-14T14:55:11.635-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=1
time=2026-01-14T14:55:11.635-05:00 level=INFO source=server.go:1338 msg="waiting for llama runner to start responding"
time=2026-01-14T14:55:11.635-05:00 level=INFO source=server.go:1376 msg="llama runner started in 1.87 seconds"
[GIN] 2026/01/14 - 14:55:13 | 200 |  4.403953484s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:55:16 | 200 |  2.513190428s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:55:17 | 200 |  1.617334454s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:55:19 | 200 |   1.56604177s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:55:20 | 200 |  1.390355035s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:55:23 | 200 |  3.011988797s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:55:24 | 200 |  504.763037ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:55:24 | 200 |  506.920083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:55:26 | 200 |  1.972747367s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:55:29 | 200 |  3.087447292s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:55:30 | 200 |  656.487802ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:55:31 | 200 |  804.646158ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:55:32 | 200 |   872.77649ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:55:34 | 200 |  2.485723291s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:55:36 | 200 |  1.728629801s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:55:40 | 200 |  4.000320475s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:55:42 | 200 |  2.082167691s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:55:46 | 200 |  4.280780496s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:55:46 | 200 |  173.617742ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:55:52 | 200 |   5.66552172s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:56:00 | 200 |  8.014296403s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:56:01 | 200 |  504.411899ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:56:01 | 200 |  136.651149ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:56:01 | 200 |  673.756572ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:56:02 | 200 |  920.251653ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:56:03 | 200 |  328.631483ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:56:04 | 200 |  1.511758169s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:56:05 | 200 |   573.48368ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:56:07 | 200 |   2.71264986s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:56:08 | 200 |  518.051551ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:56:09 | 200 |  1.533241477s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:56:11 | 200 |  1.800103766s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:56:13 | 200 |  1.811750047s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:56:16 | 200 |  2.504342991s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:56:20 | 200 |  4.080542789s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:56:20 | 200 |  575.503413ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:56:21 | 200 |  786.814888ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:56:22 | 200 |  1.036251851s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:56:25 | 200 |  2.431083184s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:56:26 | 200 |  1.083425439s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:56:33 | 200 |  7.489049607s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:56:38 | 200 |   4.91768054s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:56:39 | 200 |  1.060399304s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:56:46 | 200 |  6.444292679s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:56:50 | 200 |  4.143731091s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:56:57 | 200 |  6.956939419s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:57:03 | 200 |  6.361088283s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:57:22 | 200 | 19.287337734s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:57:38 | 200 | 15.699832704s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:57:43 | 200 |  5.172923817s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:57:47 | 200 |  3.888533797s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:58:39 | 200 |  51.68972993s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:58:41 | 200 |  1.781195154s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:58:43 | 200 |   2.57136788s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:58:46 | 200 |  2.809020633s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:58:49 | 200 |  2.940978802s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:58:54 | 200 |  4.822999722s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:58:59 | 200 |   5.63301953s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:00 | 200 |   922.43622ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:01 | 200 |  760.135159ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:02 | 200 |  970.524148ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:03 | 200 |  1.323760999s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:04 | 200 |  681.514237ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:10 | 200 |  6.057034551s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:11 | 200 |  738.837619ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:12 | 200 |  1.671935224s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:15 | 200 |  2.492564081s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:18 | 200 |  3.000342533s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:20 | 200 |  2.485351699s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:21 | 200 |  666.969574ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:23 | 200 |  1.861555427s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:25 | 200 |  1.832171068s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:27 | 200 |  1.834461219s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:27 | 200 |  833.793286ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:28 | 200 |  901.290517ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:30 | 200 |  1.572070433s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:32 | 200 |  2.133495271s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:33 | 200 |  645.757127ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:36 | 200 |  3.020456463s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:43 | 200 |  6.971473442s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:44 | 200 |  1.002398044s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:45 | 200 |  964.765286ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:46 | 200 |  1.012121482s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:46 | 200 |  259.352008ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:48 | 200 |  2.006360799s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:48 | 200 |  453.801256ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:50 | 200 |  1.617623173s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:51 | 200 |   762.02357ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:52 | 200 |  1.623705894s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:53 | 200 |  849.936694ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:56 | 200 |  2.788906832s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:57 | 200 |  787.620249ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 14:59:59 | 200 |  1.958081819s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:00:00 | 200 |   768.08924ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:00:01 | 200 |  1.459430877s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:00:03 | 200 |  1.690806971s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:00:06 | 200 |  3.658645694s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:00:07 | 200 |  708.434612ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:00:08 | 200 |  353.197611ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:00:08 | 200 |  175.575344ms |       127.0.0.1 | POST     "/api/generate"
time=2026-01-14T15:05:08.209-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 33107"
time=2026-01-14T15:05:08.541-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 40293"
time=2026-01-14T15:09:40.057-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 47023"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-01-14T15:09:40.459-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --model /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --port 45125"
time=2026-01-14T15:09:40.460-05:00 level=INFO source=sched.go:443 msg="system memory" total="251.4 GiB" free="235.5 GiB" free_swap="12.0 GiB"
time=2026-01-14T15:09:40.460-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b library=CUDA available="43.9 GiB" free="44.4 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-01-14T15:09:40.460-05:00 level=INFO source=server.go:496 msg="loading model" "model layers"=33 requested=-1
time=2026-01-14T15:09:40.460-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="4.3 GiB"
time=2026-01-14T15:09:40.460-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="11.0 GiB"
time=2026-01-14T15:09:40.460-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="5.7 GiB"
time=2026-01-14T15:09:40.460-05:00 level=INFO source=device.go:272 msg="total memory" size="21.0 GiB"
time=2026-01-14T15:09:40.469-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /cm/shared/apps/ollama/0.13.5/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA L40S, compute capability 8.9, VMM: yes, ID: GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b
load_backend: loaded CUDA backend from /cm/shared/apps/ollama/0.13.5/lib/ollama/cuda_v13/libggml-cuda.so
time=2026-01-14T15:09:40.572-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2026-01-14T15:09:40.572-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:45125"
time=2026-01-14T15:09:40.574-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:90000 KvCacheType: NumThreads:32 GPULayers:33[ID:GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
time=2026-01-14T15:09:40.575-05:00 level=INFO source=server.go:1338 msg="waiting for llama runner to start responding"
time=2026-01-14T15:09:40.575-05:00 level=INFO source=server.go:1372 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory device GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b utilizing NVML memory reporting free: 47664726016 total: 48305799168
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA L40S) (0000:e1:00.0) - 45456 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_embd_inp       = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
load_tensors:        CUDA0 model buffer size =  4403.49 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 90112
llama_context: n_ctx_seq     = 90112
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (90112) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.50 MiB
llama_kv_cache:      CUDA0 KV buffer size = 11264.00 MiB
llama_kv_cache: size = 11264.00 MiB ( 90112 cells,  32 layers,  1/1 seqs), K (f16): 5632.00 MiB, V (f16): 5632.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   296.01 MiB
llama_context:  CUDA_Host compute buffer size =   184.01 MiB
llama_context: graph nodes  = 999
llama_context: graph splits = 2
time=2026-01-14T15:09:42.079-05:00 level=INFO source=server.go:1376 msg="llama runner started in 1.62 seconds"
time=2026-01-14T15:09:42.079-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=1
time=2026-01-14T15:09:42.079-05:00 level=INFO source=server.go:1338 msg="waiting for llama runner to start responding"
time=2026-01-14T15:09:42.079-05:00 level=INFO source=server.go:1376 msg="llama runner started in 1.62 seconds"
[GIN] 2026/01/14 - 15:09:43 | 200 |  3.807195714s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:09:48 | 200 |  4.350705808s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:09:49 | 200 |  1.659230706s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:09:51 | 200 |  1.594080451s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:09:52 | 200 |  1.559072409s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:09:56 | 200 |  3.195848745s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:09:56 | 200 |  550.177244ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:09:57 | 200 |   904.51558ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:09:58 | 200 |  997.905298ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:01 | 200 |  3.337289354s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:02 | 200 |  445.739802ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:04 | 200 |  1.745024976s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:06 | 200 |  2.373693331s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:07 | 200 |  1.437329399s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:09 | 200 |  1.698824733s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:13 | 200 |  3.863431248s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:17 | 200 |  4.435112326s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:22 | 200 |  4.078957268s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:22 | 200 |  164.348767ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:32 | 200 | 10.646059427s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:35 | 200 |  3.031714661s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:36 | 200 |  666.759601ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:37 | 200 |  661.456379ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:37 | 200 |  800.578511ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:38 | 200 |  964.811742ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:40 | 200 |  1.491299551s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:41 | 200 |  1.405761185s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:42 | 200 |  569.252037ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:43 | 200 |  1.000600807s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:43 | 200 |  505.863181ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:45 | 200 |  1.740015624s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:47 | 200 |  1.525187311s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:48 | 200 |  1.648363234s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:49 | 200 |  194.107623ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:53 | 200 |  3.876139717s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:53 | 200 |  467.562102ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:54 | 200 |  810.681013ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:55 | 200 |  1.369864486s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:58 | 200 |  2.217066808s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:10:58 | 200 |  866.449312ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:11:06 | 200 |  7.337685317s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:11:14 | 200 |  8.442682304s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:11:16 | 200 |    1.8484285s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:11:20 | 200 |   4.04312526s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:11:24 | 200 |   4.34973592s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:11:31 | 200 |   6.39307232s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:11:37 | 200 |  6.342883964s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:11:53 | 200 |  16.23252212s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:12:09 | 200 | 15.635816282s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:12:15 | 200 |  5.713845342s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:12:21 | 200 |  5.849870949s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:13:12 | 200 | 50.941751997s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:13:13 | 200 |   1.47521865s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:13:16 | 200 |  3.027128516s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:13:20 | 200 |  4.295179676s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:13:24 | 200 |  3.534419552s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:13:28 | 200 |  3.909404088s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:13:32 | 200 |  4.353302963s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:13:33 | 200 |  427.395912ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:13:34 | 200 |  1.693008724s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:13:35 | 200 |  430.272695ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:13:36 | 200 |  1.222996441s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:13:40 | 200 |  3.752908818s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:13:46 | 200 |  6.204307925s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:13:47 | 200 |  582.775192ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:13:48 | 200 |  1.616421189s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:13:50 | 200 |  1.792603041s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:13:51 | 200 |  791.117098ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:13:53 | 200 |  2.529758075s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:13:54 | 200 |  371.440386ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:13:55 | 200 |  1.849700497s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:13:57 | 200 |  1.970350691s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:02 | 200 |  4.067083358s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:02 | 200 |  414.312519ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:02 | 200 |  484.869404ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:04 | 200 |  1.948492069s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:06 | 200 |  2.040786924s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:07 | 200 |  743.407606ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:11 | 200 |  3.912047892s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:15 | 200 |  4.164393774s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:19 | 200 |  3.933475244s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:20 | 200 |  610.863815ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:22 | 200 |  1.846045842s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:22 | 200 |   317.18111ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:24 | 200 |  2.063510588s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:25 | 200 |  588.948455ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:26 | 200 |   1.68145856s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:27 | 200 |  763.643114ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:28 | 200 |  1.413303553s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:29 | 200 |  762.555452ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:33 | 200 |  3.363540763s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:33 | 200 |  777.047551ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:35 | 200 |  2.012180779s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:36 | 200 |  199.128456ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:37 | 200 |    1.4632491s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:38 | 200 |  1.391221072s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:42 | 200 |  4.019028316s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:43 | 200 |  593.239722ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:43 | 200 |  338.337793ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:14:44 | 200 |  133.268627ms |       127.0.0.1 | POST     "/api/generate"
time=2026-01-14T15:19:44.046-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 43453"
time=2026-01-14T15:19:44.382-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 42453"
time=2026-01-14T15:42:13.074-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 42535"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-01-14T15:42:13.502-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --model /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --port 34881"
time=2026-01-14T15:42:13.503-05:00 level=INFO source=sched.go:443 msg="system memory" total="251.4 GiB" free="235.4 GiB" free_swap="12.0 GiB"
time=2026-01-14T15:42:13.503-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b library=CUDA available="43.9 GiB" free="44.4 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-01-14T15:42:13.503-05:00 level=INFO source=server.go:496 msg="loading model" "model layers"=33 requested=-1
time=2026-01-14T15:42:13.503-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="4.3 GiB"
time=2026-01-14T15:42:13.503-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="11.0 GiB"
time=2026-01-14T15:42:13.503-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="5.7 GiB"
time=2026-01-14T15:42:13.503-05:00 level=INFO source=device.go:272 msg="total memory" size="21.0 GiB"
time=2026-01-14T15:42:13.512-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /cm/shared/apps/ollama/0.13.5/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA L40S, compute capability 8.9, VMM: yes, ID: GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b
load_backend: loaded CUDA backend from /cm/shared/apps/ollama/0.13.5/lib/ollama/cuda_v13/libggml-cuda.so
time=2026-01-14T15:42:13.614-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2026-01-14T15:42:13.614-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:34881"
time=2026-01-14T15:42:13.617-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:90000 KvCacheType: NumThreads:32 GPULayers:33[ID:GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
time=2026-01-14T15:42:13.617-05:00 level=INFO source=server.go:1338 msg="waiting for llama runner to start responding"
time=2026-01-14T15:42:13.617-05:00 level=INFO source=server.go:1372 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory device GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b utilizing NVML memory reporting free: 47664726016 total: 48305799168
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA L40S) (0000:e1:00.0) - 45456 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_embd_inp       = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
load_tensors:        CUDA0 model buffer size =  4403.49 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 90112
llama_context: n_ctx_seq     = 90112
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (90112) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.50 MiB
llama_kv_cache:      CUDA0 KV buffer size = 11264.00 MiB
llama_kv_cache: size = 11264.00 MiB ( 90112 cells,  32 layers,  1/1 seqs), K (f16): 5632.00 MiB, V (f16): 5632.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   296.01 MiB
llama_context:  CUDA_Host compute buffer size =   184.01 MiB
llama_context: graph nodes  = 999
llama_context: graph splits = 2
time=2026-01-14T15:42:15.371-05:00 level=INFO source=server.go:1376 msg="llama runner started in 1.87 seconds"
time=2026-01-14T15:42:15.371-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=1
time=2026-01-14T15:42:15.371-05:00 level=INFO source=server.go:1338 msg="waiting for llama runner to start responding"
time=2026-01-14T15:42:15.371-05:00 level=INFO source=server.go:1376 msg="llama runner started in 1.87 seconds"
[GIN] 2026/01/14 - 15:42:17 | 200 |  4.212892807s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:42:20 | 200 |  3.086546691s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:42:22 | 200 |  2.044654233s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:42:23 | 200 |   1.23189398s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:42:25 | 200 |  1.526367935s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:42:28 | 200 |   3.11037083s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:42:29 | 200 |  1.394109009s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:42:32 | 200 |  3.251828347s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:42:34 | 200 |  2.076395703s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:42:38 | 200 |  3.835436698s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:42:39 | 200 |  632.799133ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:42:41 | 200 |     2.160041s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:42:43 | 200 |  1.757377034s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:42:45 | 200 |  2.577098835s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:42:47 | 200 |  1.718466845s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:42:51 | 200 |  3.923322402s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:42:53 | 200 |  2.392533814s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:42:58 | 200 |  4.094952248s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:42:58 | 200 |  160.311967ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:43:04 | 200 |  6.675842546s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:43:11 | 200 |  6.186071989s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:43:11 | 200 |  511.904885ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:43:11 | 200 |  392.256429ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:43:12 | 200 |  739.404844ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:43:13 | 200 |  769.832164ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:43:15 | 200 |  1.963026443s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:43:16 | 200 |   1.41202427s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:43:17 | 200 |  804.807993ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:43:21 | 200 |  3.899995495s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:43:22 | 200 |  1.182467927s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:43:24 | 200 |  1.770152046s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:43:26 | 200 |  1.482750738s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:43:27 | 200 |   1.77445856s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:43:30 | 200 |  2.710632935s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:43:34 | 200 |  4.192096434s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:43:35 | 200 |  910.029457ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:43:37 | 200 |  1.906880234s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:43:38 | 200 |  798.495101ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:43:40 | 200 |  2.271062558s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:43:44 | 200 |   3.16181548s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:43:51 | 200 |  7.134418974s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:43:55 | 200 |   4.15675833s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:44:06 | 200 | 11.056298984s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:44:19 | 200 | 12.980399709s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:44:23 | 200 |  4.051598447s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:44:30 | 200 |  7.117153554s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:44:37 | 200 |  6.507209709s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:44:54 | 200 | 17.002065302s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:45:08 | 200 | 14.555175864s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:45:12 | 200 |  3.461111237s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:45:17 | 200 |  5.363688934s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:46:08 | 200 | 51.104317328s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:46:10 | 200 |  1.765763762s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:46:12 | 200 |  2.326870049s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:46:15 | 200 |  3.173530208s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:46:19 | 200 |  3.755922615s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:46:24 | 200 |  4.951830979s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:46:29 | 200 |  5.500560063s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:46:30 | 200 |  729.858885ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:46:32 | 200 |  1.617523681s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:46:33 | 200 |  922.390139ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:46:34 | 200 |  1.610604502s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:46:39 | 200 |  4.458814878s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:46:42 | 200 |  2.745194635s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:46:42 | 200 |  902.310484ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:46:44 | 200 |  1.862509504s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:46:47 | 200 |  2.337115312s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:46:49 | 200 |  2.733055803s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:46:52 | 200 |  2.425520647s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:46:53 | 200 |  681.811545ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:46:55 | 200 |  2.109910433s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:46:57 | 200 |  2.514479602s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:02 | 200 |  4.471654638s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:02 | 200 |  714.269532ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:03 | 200 |  991.303204ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:05 | 200 |  1.434779802s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:07 | 200 |   2.07226357s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:10 | 200 |  3.148412132s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:14 | 200 |  4.075656284s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:22 | 200 |  8.031788567s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:26 | 200 |  4.238755176s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:27 | 200 |  1.044994421s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:29 | 200 |  1.883427125s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:30 | 200 |  740.302707ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:32 | 200 |  1.866804332s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:33 | 200 |    481.3915ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:34 | 200 |  1.578743882s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:35 | 200 |  794.987993ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:37 | 200 |  1.948220138s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:38 | 200 |  1.110469436s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:41 | 200 |   3.04314089s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:42 | 200 |  1.042413495s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:44 | 200 |  2.032503326s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:47 | 200 |  2.845785942s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:48 | 200 |   1.26166705s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:50 | 200 |  1.493193931s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:54 | 200 |  3.945079566s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:55 | 200 |   874.13026ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:55 | 200 |  671.816167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:47:56 | 200 |  170.785449ms |       127.0.0.1 | POST     "/api/generate"
time=2026-01-14T15:52:56.069-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 36535"
time=2026-01-14T15:52:56.405-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 33987"
time=2026-01-14T15:59:24.479-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 46683"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-01-14T15:59:24.904-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --model /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --port 44641"
time=2026-01-14T15:59:24.905-05:00 level=INFO source=sched.go:443 msg="system memory" total="251.4 GiB" free="233.3 GiB" free_swap="12.0 GiB"
time=2026-01-14T15:59:24.905-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b library=CUDA available="43.9 GiB" free="44.4 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-01-14T15:59:24.905-05:00 level=INFO source=server.go:496 msg="loading model" "model layers"=33 requested=-1
time=2026-01-14T15:59:24.905-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="4.3 GiB"
time=2026-01-14T15:59:24.905-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="11.0 GiB"
time=2026-01-14T15:59:24.905-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="5.7 GiB"
time=2026-01-14T15:59:24.905-05:00 level=INFO source=device.go:272 msg="total memory" size="21.0 GiB"
time=2026-01-14T15:59:24.914-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /cm/shared/apps/ollama/0.13.5/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA L40S, compute capability 8.9, VMM: yes, ID: GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b
load_backend: loaded CUDA backend from /cm/shared/apps/ollama/0.13.5/lib/ollama/cuda_v13/libggml-cuda.so
time=2026-01-14T15:59:25.017-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2026-01-14T15:59:25.018-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:44641"
time=2026-01-14T15:59:25.020-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:90000 KvCacheType: NumThreads:32 GPULayers:33[ID:GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
time=2026-01-14T15:59:25.020-05:00 level=INFO source=server.go:1338 msg="waiting for llama runner to start responding"
time=2026-01-14T15:59:25.021-05:00 level=INFO source=server.go:1372 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory device GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b utilizing NVML memory reporting free: 47664726016 total: 48305799168
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA L40S) (0000:e1:00.0) - 45456 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_embd_inp       = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
load_tensors:        CUDA0 model buffer size =  4403.49 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 90112
llama_context: n_ctx_seq     = 90112
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (90112) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.50 MiB
llama_kv_cache:      CUDA0 KV buffer size = 11264.00 MiB
llama_kv_cache: size = 11264.00 MiB ( 90112 cells,  32 layers,  1/1 seqs), K (f16): 5632.00 MiB, V (f16): 5632.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   296.01 MiB
llama_context:  CUDA_Host compute buffer size =   184.01 MiB
llama_context: graph nodes  = 999
llama_context: graph splits = 2
time=2026-01-14T15:59:26.775-05:00 level=INFO source=server.go:1376 msg="llama runner started in 1.87 seconds"
time=2026-01-14T15:59:26.775-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=1
time=2026-01-14T15:59:26.775-05:00 level=INFO source=server.go:1338 msg="waiting for llama runner to start responding"
time=2026-01-14T15:59:26.775-05:00 level=INFO source=server.go:1376 msg="llama runner started in 1.87 seconds"
[GIN] 2026/01/14 - 15:59:28 | 200 |  4.400273608s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:59:33 | 200 |  4.369058684s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:59:35 | 200 |  1.972392634s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:59:36 | 200 |  1.421629797s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:59:38 | 200 |  1.702643573s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:59:41 | 200 |  3.755633109s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:59:44 | 200 |  2.368132263s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:59:46 | 200 |  2.138315541s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:59:49 | 200 |  2.594889544s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:59:52 | 200 |  3.673515745s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:59:54 | 200 |  1.415297677s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:59:56 | 200 |  2.399090317s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 15:59:58 | 200 |  1.695912869s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:00:00 | 200 |  2.451935451s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:00:02 | 200 |  1.792283269s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:00:07 | 200 |  5.241164459s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:00:10 | 200 |  2.551446615s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:00:15 | 200 |  4.550796836s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:00:15 | 200 |  545.583017ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:00:19 | 200 |  3.984840959s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:00:27 | 200 |  7.939134022s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:00:28 | 200 |  585.845169ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:00:28 | 200 |  451.021626ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:00:30 | 200 |  1.880169908s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:00:31 | 200 |  852.487156ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:00:32 | 200 |  1.116391204s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:00:34 | 200 |  2.183905848s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:00:35 | 200 |  1.025189998s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:00:39 | 200 |   3.36924264s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:00:40 | 200 |  1.118285993s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:00:41 | 200 |  1.722980373s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:00:43 | 200 |  2.033549949s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:00:44 | 200 |  836.216308ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:00:47 | 200 |  2.617927929s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:00:51 | 200 |  4.209212656s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:00:53 | 200 |  1.668271183s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:00:55 | 200 |  2.738401606s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:00:57 | 200 |  1.281537454s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:01:00 | 200 |  2.986106576s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:01:02 | 200 |  1.781690442s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:01:09 | 200 |  7.911805933s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:01:13 | 200 |  3.725329732s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:01:16 | 200 |  3.020316208s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:01:21 | 200 |  4.499975032s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:01:26 | 200 |  5.342855101s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:01:31 | 200 |  5.261088553s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:01:37 | 200 |  5.885873582s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:01:40 | 200 |  3.050010491s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:02:02 | 200 | 22.001840394s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:02:04 | 200 |  1.276147096s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:02:08 | 200 |  3.981527249s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:02:59 | 200 | 51.455248894s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:03:02 | 200 |  2.534129836s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:03:04 | 200 |   2.68675729s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:03:16 | 200 | 11.607445664s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:03:20 | 200 |  3.827245455s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:03:24 | 200 |  4.100485107s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:03:25 | 200 |  1.185602284s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:03:27 | 200 |  1.323952084s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:03:29 | 200 |  2.063227182s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:03:30 | 200 |  952.002352ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:03:31 | 200 |  1.398257232s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:03:35 | 200 |   3.58937799s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:03:41 | 200 |  5.753558572s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:03:42 | 200 |  1.536415403s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:03:44 | 200 |  1.581677729s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:03:46 | 200 |  2.497830874s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:03:52 | 200 |  5.296625571s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:03:54 | 200 |  2.447700002s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:03:56 | 200 |  1.552420078s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:03:58 | 200 |  2.154765883s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:04:00 | 200 |   2.51308158s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:04:03 | 200 |  2.668892081s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:04:04 | 200 |  1.371146454s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:04:06 | 200 |  1.485396766s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:04:07 | 200 |    1.5829212s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:04:09 | 200 |  1.827217179s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:04:12 | 200 |  3.105677884s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:04:17 | 200 |  4.401011926s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:04:24 | 200 |  7.487505146s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:04:28 | 200 |  4.010457332s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:04:30 | 200 |  2.050533056s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:04:33 | 200 |  2.400584543s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:04:34 | 200 |  842.150833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:04:36 | 200 |  2.096457292s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:04:37 | 200 |  922.315456ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:04:39 | 200 |  2.258934276s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:04:41 | 200 |  1.601522238s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:04:43 | 200 |  2.096614557s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:04:44 | 200 |  1.565197333s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:04:48 | 200 |  3.521552556s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:04:49 | 200 |   1.16214479s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:04:51 | 200 |  2.609134461s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:04:55 | 200 |  3.113568768s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:04:56 | 200 |  1.715984909s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:04:58 | 200 |  1.880782479s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:05:02 | 200 |  4.254045128s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:05:03 | 200 |  854.871064ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:05:04 | 200 |  817.269802ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:05:05 | 200 |  536.841577ms |       127.0.0.1 | POST     "/api/generate"
time=2026-01-14T16:10:05.164-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 35415"
time=2026-01-14T16:10:05.501-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 45881"
time=2026-01-14T16:12:20.313-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 36837"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: no_alloc         = 0
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2026-01-14T16:12:20.738-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --model /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --port 41015"
time=2026-01-14T16:12:20.738-05:00 level=INFO source=sched.go:443 msg="system memory" total="251.4 GiB" free="234.1 GiB" free_swap="12.0 GiB"
time=2026-01-14T16:12:20.738-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b library=CUDA available="43.9 GiB" free="44.4 GiB" minimum="457.0 MiB" overhead="0 B"
time=2026-01-14T16:12:20.738-05:00 level=INFO source=server.go:496 msg="loading model" "model layers"=33 requested=-1
time=2026-01-14T16:12:20.739-05:00 level=INFO source=device.go:240 msg="model weights" device=CUDA0 size="4.3 GiB"
time=2026-01-14T16:12:20.739-05:00 level=INFO source=device.go:251 msg="kv cache" device=CUDA0 size="11.0 GiB"
time=2026-01-14T16:12:20.739-05:00 level=INFO source=device.go:262 msg="compute graph" device=CUDA0 size="5.7 GiB"
time=2026-01-14T16:12:20.739-05:00 level=INFO source=device.go:272 msg="total memory" size="21.0 GiB"
time=2026-01-14T16:12:20.747-05:00 level=INFO source=runner.go:965 msg="starting go runner"
load_backend: loaded CPU backend from /cm/shared/apps/ollama/0.13.5/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA L40S, compute capability 8.9, VMM: yes, ID: GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b
load_backend: loaded CUDA backend from /cm/shared/apps/ollama/0.13.5/lib/ollama/cuda_v13/libggml-cuda.so
time=2026-01-14T16:12:20.850-05:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2026-01-14T16:12:20.850-05:00 level=INFO source=runner.go:1001 msg="Server listening on 127.0.0.1:41015"
time=2026-01-14T16:12:20.853-05:00 level=INFO source=runner.go:895 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:90000 KvCacheType: NumThreads:32 GPULayers:33[ID:GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
time=2026-01-14T16:12:20.853-05:00 level=INFO source=server.go:1338 msg="waiting for llama runner to start responding"
time=2026-01-14T16:12:20.853-05:00 level=INFO source=server.go:1372 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory device GPU-05589cd8-b6d1-316d-b2dd-ebfc6237db1b utilizing NVML memory reporting free: 47664726016 total: 48305799168
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA L40S) (0000:e1:00.0) - 45456 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/elliotg2/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: no_alloc         = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_embd_inp       = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_yarn_log_mul= 0.0000
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
load_tensors:        CUDA0 model buffer size =  4403.49 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 90112
llama_context: n_ctx_seq     = 90112
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (90112) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.50 MiB
llama_kv_cache:      CUDA0 KV buffer size = 11264.00 MiB
llama_kv_cache: size = 11264.00 MiB ( 90112 cells,  32 layers,  1/1 seqs), K (f16): 5632.00 MiB, V (f16): 5632.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      CUDA0 compute buffer size =   296.01 MiB
llama_context:  CUDA_Host compute buffer size =   184.01 MiB
llama_context: graph nodes  = 999
llama_context: graph splits = 2
time=2026-01-14T16:12:22.357-05:00 level=INFO source=server.go:1376 msg="llama runner started in 1.62 seconds"
time=2026-01-14T16:12:22.357-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=1
time=2026-01-14T16:12:22.357-05:00 level=INFO source=server.go:1338 msg="waiting for llama runner to start responding"
time=2026-01-14T16:12:22.357-05:00 level=INFO source=server.go:1376 msg="llama runner started in 1.62 seconds"
[GIN] 2026/01/14 - 16:12:24 | 200 |  4.087051616s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:12:28 | 200 |  4.217797917s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:12:30 | 200 |  1.932783497s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:12:31 | 200 |  1.460116219s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:12:33 | 200 |  1.470121808s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:12:37 | 200 |  3.785216158s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:12:39 | 200 |  2.307300897s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:12:43 | 200 |  3.737023301s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:12:45 | 200 |  2.675886002s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:12:49 | 200 |  3.808479003s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:12:51 | 200 |  1.544262293s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:12:54 | 200 |  3.381703048s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:12:57 | 200 |  3.058329103s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:13:00 | 200 |  2.875924858s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:13:02 | 200 |  1.862287669s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:13:06 | 200 |  3.981512112s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:13:11 | 200 |   4.94885574s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:13:16 | 200 |  4.827101469s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:13:16 | 200 |  166.520974ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:13:22 | 200 |   6.27428578s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:13:30 | 200 |  7.899198423s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:13:31 | 200 |  947.957826ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:13:32 | 200 |  881.020363ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:13:33 | 200 |  1.087200151s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:13:34 | 200 |  1.125660455s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:13:36 | 200 |   1.60287102s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:13:38 | 200 |  1.923656423s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:13:38 | 200 |  588.079106ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:13:41 | 200 |  2.950135087s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:13:42 | 200 |  1.122055352s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:13:44 | 200 |  1.661497025s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:13:46 | 200 |  1.959739957s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:13:48 | 200 |  1.864239785s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:13:51 | 200 |  2.769996494s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:13:56 | 200 |  4.937665458s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:13:57 | 200 |  968.707619ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:13:59 | 200 |  2.700173398s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:14:01 | 200 |  2.128612085s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:14:04 | 200 |  2.758982311s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:14:07 | 200 |  3.177791272s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:14:15 | 200 |  7.771051581s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:14:20 | 200 |  4.916586333s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:14:23 | 200 |  2.659086425s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:14:24 | 200 |  1.721446086s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:14:29 | 200 |  4.963997482s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:14:36 | 200 |   6.34493176s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:14:46 | 200 | 10.440079013s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:15:05 | 200 | 19.165953765s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:15:40 | 200 | 34.729595074s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:15:44 | 200 |  4.223039233s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:15:48 | 200 |   3.99062962s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:16:40 | 200 | 51.586563041s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:16:42 | 200 |  2.199827368s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:16:45 | 200 |  2.765288942s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:16:48 | 200 |  3.088653326s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:16:55 | 200 |  7.348169698s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:16:59 | 200 |  4.159548629s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:17:05 | 200 |  5.646325363s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:17:07 | 200 |  2.023307998s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:17:09 | 200 |  1.825237143s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:17:10 | 200 |  930.575825ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:17:11 | 200 |  1.473617396s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:17:16 | 200 |  4.360786731s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:17:19 | 200 |   3.72610277s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:17:21 | 200 |  1.135972921s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:17:22 | 200 |  1.577662115s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:17:25 | 200 |  3.100199849s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:17:28 | 200 |  2.750695677s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:17:31 | 200 |  3.520072808s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:17:32 | 200 |   745.69707ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:17:34 | 200 |  2.129777403s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:17:36 | 200 |  1.972356496s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:17:41 | 200 |  4.599505142s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:17:42 | 200 |  986.935695ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:17:43 | 200 |  1.178582877s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:17:46 | 200 |   2.81106286s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:17:48 | 200 |  2.381467443s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:17:51 | 200 |  2.706731869s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:17:55 | 200 |  4.229589642s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:17:58 | 200 |  2.792062509s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:18:02 | 200 |  4.008122282s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:18:03 | 200 |  1.080663379s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:18:05 | 200 |  2.023966759s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:18:06 | 200 |  901.565612ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:18:08 | 200 |  1.862652215s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:18:09 | 200 |  896.521812ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:18:10 | 200 |   1.58982436s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:18:12 | 200 |   1.15485288s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:18:14 | 200 |  2.157499058s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:18:15 | 200 |  1.187929017s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:18:19 | 200 |    3.7127604s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:18:20 | 200 |  1.133234559s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:18:23 | 200 |  2.791566868s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:18:26 | 200 |  3.034530332s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:18:27 | 200 |  1.558684328s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:18:29 | 200 |  1.525489038s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:18:30 | 200 |  1.133490438s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:18:31 | 200 |  851.448272ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:18:31 | 200 |  691.977745ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:18:32 | 200 |  233.669068ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:22:07 | 200 |  1.962527794s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:22:12 | 200 |  4.455537673s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:22:14 | 200 |  1.885499194s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:22:15 | 200 |  1.629444199s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:22:17 | 200 |  1.343748063s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:22:20 | 200 |  3.652998921s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:22:23 | 200 |  2.269702418s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:22:25 | 200 |  2.404540378s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:22:28 | 200 |  3.096470539s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:22:32 | 200 |  3.841767421s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:22:33 | 200 |  781.948675ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:22:35 | 200 |  2.288533007s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:22:39 | 200 |   4.03852413s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:22:42 | 200 |  2.768850305s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:22:44 | 200 |  1.816109807s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:22:49 | 200 |  4.935483909s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:22:53 | 200 |  4.819213268s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:22:58 | 200 |  4.453338131s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:22:58 | 200 |  315.478648ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:23:06 | 200 |   7.40043886s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:23:14 | 200 |  8.209611792s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:23:15 | 200 |  744.818687ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:23:15 | 200 |  544.634332ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:23:16 | 200 |  1.143821134s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:23:19 | 200 |  2.398898937s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:23:19 | 200 |  705.325738ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:23:21 | 200 |  1.300507425s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:23:22 | 200 |   849.68581ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:23:24 | 200 |  2.760364404s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:23:25 | 200 |  1.119941049s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:23:27 | 200 |   1.70980519s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:23:29 | 200 |  1.468542975s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:23:30 | 200 |  1.541033931s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:23:33 | 200 |   2.66300204s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:23:38 | 200 |  4.926072568s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:23:39 | 200 |  948.653794ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:23:41 | 200 |  2.220581974s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:23:43 | 200 |  1.689559808s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:23:45 | 200 |  2.603929541s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:23:48 | 200 |  3.038600676s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:23:54 | 200 |  6.089326071s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:24:00 | 200 |  5.186182624s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:24:02 | 200 |  2.913039459s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:24:09 | 200 |  6.596597697s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:24:21 | 200 | 11.721756781s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:24:27 | 200 |  5.992086314s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:24:33 | 200 |  6.301233973s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:24:53 | 200 | 20.048925199s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:25:35 | 200 | 42.001152832s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:25:39 | 200 |  4.025082852s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:25:46 | 200 |  6.718308197s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:26:37 | 200 | 51.490139192s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:26:40 | 200 |  2.216999772s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:26:43 | 200 |  3.296547604s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:26:46 | 200 |  3.411252003s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:26:51 | 200 |    4.4949752s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:26:57 | 200 |  6.219178366s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:27:02 | 200 |  5.097603904s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:27:04 | 200 |  1.659353485s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:27:06 | 200 |  2.295706756s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:27:07 | 200 |  906.114909ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:27:09 | 200 |  1.920946326s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:27:13 | 200 |   4.31831242s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:27:19 | 200 |   6.06370981s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:27:20 | 200 |  729.320077ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:27:22 | 200 |  2.157236536s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:27:26 | 200 |  3.531994982s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:27:32 | 200 |  6.589137023s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:27:35 | 200 |  2.176483195s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:27:36 | 200 |  1.011195696s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:27:37 | 200 |  1.774325575s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:27:39 | 200 |  1.991353217s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:27:44 | 200 |  5.116965265s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:27:46 | 200 |  1.799515173s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:27:48 | 200 |  1.692065965s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:27:50 | 200 |  2.130955151s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:27:53 | 200 |  2.545736216s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:27:56 | 200 |  3.178549961s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:28:00 | 200 |  4.207415927s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:28:06 | 200 |   5.96126607s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:28:10 | 200 |  4.050141673s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:28:11 | 200 |  1.058300593s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:28:13 | 200 |  2.071198412s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:28:14 | 200 |  782.156006ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:28:16 | 200 |  2.053093944s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:28:17 | 200 |  895.267623ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:28:19 | 200 |  1.771587308s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:28:20 | 200 |  1.094644274s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:28:22 | 200 |  2.135351616s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:28:23 | 200 |  1.162365551s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:28:27 | 200 |  3.829813207s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:28:28 | 200 |  1.072400764s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:28:31 | 200 |  2.912807109s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:28:34 | 200 |  3.001025634s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:28:37 | 200 |  2.862203478s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:28:40 | 200 |  3.136386504s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:28:44 | 200 |  4.305533703s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:28:45 | 200 |  671.770118ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:28:46 | 200 |  771.471223ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2026/01/14 - 16:28:46 | 200 |  210.253731ms |       127.0.0.1 | POST     "/api/generate"
time=2026-01-14T16:33:46.319-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 33539"
time=2026-01-14T16:33:46.656-05:00 level=INFO source=server.go:429 msg="starting runner" cmd="/cm/shared/apps/ollama/0.13.5/bin/ollama runner --ollama-engine --port 38813"
