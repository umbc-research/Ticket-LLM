TicketID,TransactionID,TransactionType,QueueName,CreatedDate,SubjectNoHTML,TicketStatus,TicketOwner,TicketOwnerUsername,Requestor,RequestorUsername,RequestorEmail,TransactionName,TransactionEmail,TransactionContent
3280927,71946187,Correspond,DoIT-Research-Computing,2025-09-25 16:34:21.0000000,HPC Slurm/Software Issue: GPU Job QoS not Permitted,resolved,Danielle Esposito,desposi1,Bharg Barot,bhargvb1,bhargvb1@umbc.edu,Bharg Barot,bhargvb1@umbc.edu,"<div dir=3D""ltr"">Hey Daniela,<div><br></div><div>The error is back, unfortu= nately.</div><div><br></div><div>#!/bin/bash<br>#SBATCH --job-name=3DKcolle= ctLVL<br>#SBATCH --output=3Dklog/collect_%A_%a.out<br>#SBATCH --error=3Dklo= g/collect_%A_%a.err<br>#SBATCH --mem=3D64G<br>#SBATCH --time=3D72:00:00<br>= #SBATCH --constraint=3Drtx_6000<br>#SBATCH --gres=3Dgpu:4<br>#SBATCH --arra= y=3D6,7,12,13,20<br>#SBATCH --mail-user=3D<a href=3D""mailto:bhargvb1@umbc.e= du"">bhargvb1@umbc.edu</a><br>#SBATCH --mail-type=3DEND,FAIL<br>#SBATCH --pa= rtition=3Dgpu-general<br><br>It worked initially, but when I tried again, i= t failed.</div><div>100150_[6-7,12-13, gpu-gener Kcollect bhargvb1 PD =C2= =A0 =C2=A0 =C2=A0 0:00 =C2=A0 =C2=A0 =C2=A01 (Nodes required for job are DO= WN, DRAINED or reserved for jobs in higher priority partitions)<br></div></= div><br><div class=3D""gmail_quote gmail_quote_container""><div dir=3D""ltr"" c= lass=3D""gmail_attr"">On Thu, Sep 25, 2025 at 12:05=E2=80=AFPM Bharg Barot &l= t;<a href=3D""mailto:bhargvb1@umbc.edu"">bhargvb1@umbc.edu</a>&gt; wrote:<br>= </div><blockquote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;b= order-left:1px solid rgb(204,204,204);padding-left:1ex""><div dir=3D""ltr"">Th= ank you, it worked!</div><br><div class=3D""gmail_quote""><div dir=3D""ltr"" cl= ass=3D""gmail_attr"">On Thu, Sep 25, 2025 at 11:38=E2=80=AFAM Danielle Esposi= to via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMB= CHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote""=  style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);p= adding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Disp= lay.html?id=3D3280927"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc= .edu/Ticket/Display.html?id=3D3280927</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Bharg,<br> <br> Please try to specify the partition you want to run on. Before, there was o= nly<br> one gpu partition, so you would not need to specify the partition. However = this<br> has since changed. In this case, you would probably want to use &#39;gpu-ge= neral&#39;.<br> The &#39;gpu-contrib&#39; partition does not allow jobs to run on it, it is=  purely for<br> information purposes. If you are a part of a group that contributed GPU nod= es,<br> your group would have its own partition for those contributed nodes. You ca= n<br> still run on contributed nodes (if you use the &#39;gpu&#39; partition), ho= wever your<br> job could be preempted by the contributing group.<br> <br> You can add &#39;#SBATCH ---partition=3Dgpu-general&#39; to your sbatch hea= ders. On<br> gpu-general, your job will not be preempted (until after 3 days of runtime)= <br> since these nodes are shared by everyone. For more information on the new G= PU<br> model, please checkout this wiki page:<br> <a href=3D""https://umbc.atlassian.net/wiki/spaces/faq/pages/1249509377/chip= +Partitions+and+Usage"" rel=3D""noreferrer"" target=3D""_blank"">https://umbc.at= lassian.net/wiki/spaces/faq/pages/1249509377/chip+Partitions+and+Usage</a><= br> <br> Let me know if this works! Have a nice day!<br> <br> --<br> <br> Kind regards,<br> Danielle Esposito (she/her/hers)<br> DoIT Unix Infra Student Worker<br> <br> On Thu Sep 25 11:01:57 2025, SW91938 wrote:<br> <br> &gt; Am I missing something here? Here are the headers of my Slurm script, = which<br> &gt; I have been using for a long time. They used to work fine as recently = as<br> &gt; yesterday morning. #!/bin/bash<br> &gt; #SBATCH --job-name=3DKcollectLVL<br> &gt; #SBATCH --output=3Dklog/collect_%A_%a.out<br> &gt; #SBATCH --error=3Dklog/collect_%A_%a.err<br> &gt; #SBATCH --mem=3D64G<br> &gt; #SBATCH --time=3D72:00:00<br> &gt; #SBATCH --constraint=3Drtx_6000<br> &gt; #SBATCH --gres=3Dgpu:4<br> &gt; #SBATCH --array=3D6,7,12,13,20<br> &gt; #SBATCH --mail-user=3D<a href=3D""mailto:bhargvb1@umbc.edu"" target=3D""_= blank"">bhargvb1@umbc.edu</a><br> &gt; #SBATCH --mail-type=3DEND,FAIL<br> &gt; On Thu, Sep 25, 2025 at 10:56 AM Elliot Gobbert via RT<br> &gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp= @rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html= ?id=3D3280927"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tic= ket/Display.html?id=3D3280927</a> &gt;<br> <br> &gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt; Hello there,<br> <br> &gt;&gt; Yes, we recently made some changes to the GPU partitions on Chip. = This<br> &gt;&gt; is a<br> &gt;&gt; very common mistake we&#39;re seeing.<br> <br> &gt;&gt; The gpu-contrib partition cannot be charged by users; it&#39;s onl= y<br> &gt;&gt; available so<br> &gt;&gt; that users can see the list of preemptible nodes.<br> <br> &gt;&gt; The partitions wiki page mentions more about this:<br> <br> &gt;&gt; <a href=3D""https://umbc.atlassian.net/wiki/spaces/faq/pages/124950= 9377/chip+Partitions+and+Usage"" rel=3D""noreferrer"" target=3D""_blank"">https:= //umbc.atlassian.net/wiki/spaces/faq/pages/1249509377/chip+Partitions+and+U= sage</a><br> <br> &gt;&gt; Let me know if that helps,<br> <br> &gt;&gt; Elliot Gobbert<br> <br> </blockquote></div> </blockquote></div> "
3280927,71946472,Correspond,DoIT-Research-Computing,2025-09-25 16:45:56.0000000,HPC Slurm/Software Issue: GPU Job QoS not Permitted,resolved,Danielle Esposito,desposi1,Bharg Barot,bhargvb1,bhargvb1@umbc.edu,Bharg Barot,bhargvb1@umbc.edu,"Danielle,  Update: It is back up and working again.  On Thu, Sep 25, 2025 at 12:34=E2=80=AFPM Bharg Barot <bhargvb1@umbc.edu> wr= ote:  > Hey Daniela, > > The error is back, unfortunately. > > #!/bin/bash > #SBATCH --job-name=3DKcollectLVL > #SBATCH --output=3Dklog/collect_%A_%a.out > #SBATCH --error=3Dklog/collect_%A_%a.err > #SBATCH --mem=3D64G > #SBATCH --time=3D72:00:00 > #SBATCH --constraint=3Drtx_6000 > #SBATCH --gres=3Dgpu:4 > #SBATCH --array=3D6,7,12,13,20 > #SBATCH --mail-user=3Dbhargvb1@umbc.edu > #SBATCH --mail-type=3DEND,FAIL > #SBATCH --partition=3Dgpu-general > > It worked initially, but when I tried again, it failed. > 100150_[6-7,12-13, gpu-gener Kcollect bhargvb1 PD       0:00      1 (Nodes > required for job are DOWN, DRAINED or reserved for jobs in higher priority > partitions) > > On Thu, Sep 25, 2025 at 12:05=E2=80=AFPM Bharg Barot <bhargvb1@umbc.edu> = wrote: > >> Thank you, it worked! >> >> On Thu, Sep 25, 2025 at 11:38=E2=80=AFAM Danielle Esposito via RT < >> UMBCHelp@rt.umbc.edu> wrote: >> >>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3280927 > >>> >>> Last Update From Ticket: >>> >>> Hi Bharg, >>> >>> Please try to specify the partition you want to run on. Before, there >>> was only >>> one gpu partition, so you would not need to specify the partition. >>> However this >>> has since changed. In this case, you would probably want to use >>> 'gpu-general'. >>> The 'gpu-contrib' partition does not allow jobs to run on it, it is >>> purely for >>> information purposes. If you are a part of a group that contributed GPU >>> nodes, >>> your group would have its own partition for those contributed nodes. You >>> can >>> still run on contributed nodes (if you use the 'gpu' partition), however >>> your >>> job could be preempted by the contributing group. >>> >>> You can add '#SBATCH ---partition=3Dgpu-general' to your sbatch headers= . On >>> gpu-general, your job will not be preempted (until after 3 days of >>> runtime) >>> since these nodes are shared by everyone. For more information on the >>> new GPU >>> model, please checkout this wiki page: >>> >>> https://umbc.atlassian.net/wiki/spaces/faq/pages/1249509377/chip+Partit= ions+and+Usage >>> >>> Let me know if this works! Have a nice day! >>> >>> -- >>> >>> Kind regards, >>> Danielle Esposito (she/her/hers) >>> DoIT Unix Infra Student Worker >>> >>> On Thu Sep 25 11:01:57 2025, SW91938 wrote: >>> >>> > Am I missing something here? Here are the headers of my Slurm script, >>> which >>> > I have been using for a long time. They used to work fine as recently >>> as >>> > yesterday morning. #!/bin/bash >>> > #SBATCH --job-name=3DKcollectLVL >>> > #SBATCH --output=3Dklog/collect_%A_%a.out >>> > #SBATCH --error=3Dklog/collect_%A_%a.err >>> > #SBATCH --mem=3D64G >>> > #SBATCH --time=3D72:00:00 >>> > #SBATCH --constraint=3Drtx_6000 >>> > #SBATCH --gres=3Dgpu:4 >>> > #SBATCH --array=3D6,7,12,13,20 >>> > #SBATCH --mail-user=3Dbhargvb1@umbc.edu >>> > #SBATCH --mail-type=3DEND,FAIL >>> > On Thu, Sep 25, 2025 at 10:56 AM Elliot Gobbert via RT >>> > <UMBCHelp@rt.umbc.edu> wrote: >>> >>> >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3280927 > >>> >>> >> Last Update From Ticket: >>> >>> >> Hello there, >>> >>> >> Yes, we recently made some changes to the GPU partitions on Chip. Th= is >>> >> is a >>> >> very common mistake we're seeing. >>> >>> >> The gpu-contrib partition cannot be charged by users; it's only >>> >> available so >>> >> that users can see the list of preemptible nodes. >>> >>> >> The partitions wiki page mentions more about this: >>> >>> >> >>> https://umbc.atlassian.net/wiki/spaces/faq/pages/1249509377/chip+Partit= ions+and+Usage >>> >>> >> Let me know if that helps, >>> >>> >> Elliot Gobbert >>> >>> "
3280927,71946472,Correspond,DoIT-Research-Computing,2025-09-25 16:45:56.0000000,HPC Slurm/Software Issue: GPU Job QoS not Permitted,resolved,Danielle Esposito,desposi1,Bharg Barot,bhargvb1,bhargvb1@umbc.edu,Bharg Barot,bhargvb1@umbc.edu,"<div dir=3D""ltr"">Danielle,<div><br></div><div>Update: It is back up and wor= king again.</div></div><br><div class=3D""gmail_quote gmail_quote_container""= ><div dir=3D""ltr"" class=3D""gmail_attr"">On Thu, Sep 25, 2025 at 12:34=E2=80= =AFPM Bharg Barot &lt;<a href=3D""mailto:bhargvb1@umbc.edu"">bhargvb1@umbc.ed= u</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""margin= :0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex""= ><div dir=3D""ltr"">Hey Daniela,<div><br></div><div>The error is back, unfort= unately.</div><div><br></div><div>#!/bin/bash<br>#SBATCH --job-name=3DKcoll= ectLVL<br>#SBATCH --output=3Dklog/collect_%A_%a.out<br>#SBATCH --error=3Dkl= og/collect_%A_%a.err<br>#SBATCH --mem=3D64G<br>#SBATCH --time=3D72:00:00<br= >#SBATCH --constraint=3Drtx_6000<br>#SBATCH --gres=3Dgpu:4<br>#SBATCH --arr= ay=3D6,7,12,13,20<br>#SBATCH --mail-user=3D<a href=3D""mailto:bhargvb1@umbc.= edu"" target=3D""_blank"">bhargvb1@umbc.edu</a><br>#SBATCH --mail-type=3DEND,F= AIL<br>#SBATCH --partition=3Dgpu-general<br><br>It worked initially, but wh= en I tried again, it failed.</div><div>100150_[6-7,12-13, gpu-gener Kcollec= t bhargvb1 PD =C2=A0 =C2=A0 =C2=A0 0:00 =C2=A0 =C2=A0 =C2=A01 (Nodes requir= ed for job are DOWN, DRAINED or reserved for jobs in higher priority partit= ions)<br></div></div><br><div class=3D""gmail_quote""><div dir=3D""ltr"" class= =3D""gmail_attr"">On Thu, Sep 25, 2025 at 12:05=E2=80=AFPM Bharg Barot &lt;<a=  href=3D""mailto:bhargvb1@umbc.edu"" target=3D""_blank"">bhargvb1@umbc.edu</a>&= gt; wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""margin:0px 0= px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex""><div = dir=3D""ltr"">Thank you, it worked!</div><br><div class=3D""gmail_quote""><div = dir=3D""ltr"" class=3D""gmail_attr"">On Thu, Sep 25, 2025 at 11:38=E2=80=AFAM D= anielle Esposito via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target= =3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class= =3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rg= b(204,204,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc= .edu/Ticket/Display.html?id=3D3280927"" rel=3D""noreferrer"" target=3D""_blank""= >https://rt.umbc.edu/Ticket/Display.html?id=3D3280927</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Bharg,<br> <br> Please try to specify the partition you want to run on. Before, there was o= nly<br> one gpu partition, so you would not need to specify the partition. However = this<br> has since changed. In this case, you would probably want to use &#39;gpu-ge= neral&#39;.<br> The &#39;gpu-contrib&#39; partition does not allow jobs to run on it, it is=  purely for<br> information purposes. If you are a part of a group that contributed GPU nod= es,<br> your group would have its own partition for those contributed nodes. You ca= n<br> still run on contributed nodes (if you use the &#39;gpu&#39; partition), ho= wever your<br> job could be preempted by the contributing group.<br> <br> You can add &#39;#SBATCH ---partition=3Dgpu-general&#39; to your sbatch hea= ders. On<br> gpu-general, your job will not be preempted (until after 3 days of runtime)= <br> since these nodes are shared by everyone. For more information on the new G= PU<br> model, please checkout this wiki page:<br> <a href=3D""https://umbc.atlassian.net/wiki/spaces/faq/pages/1249509377/chip= +Partitions+and+Usage"" rel=3D""noreferrer"" target=3D""_blank"">https://umbc.at= lassian.net/wiki/spaces/faq/pages/1249509377/chip+Partitions+and+Usage</a><= br> <br> Let me know if this works! Have a nice day!<br> <br> --<br> <br> Kind regards,<br> Danielle Esposito (she/her/hers)<br> DoIT Unix Infra Student Worker<br> <br> On Thu Sep 25 11:01:57 2025, SW91938 wrote:<br> <br> &gt; Am I missing something here? Here are the headers of my Slurm script, = which<br> &gt; I have been using for a long time. They used to work fine as recently = as<br> &gt; yesterday morning. #!/bin/bash<br> &gt; #SBATCH --job-name=3DKcollectLVL<br> &gt; #SBATCH --output=3Dklog/collect_%A_%a.out<br> &gt; #SBATCH --error=3Dklog/collect_%A_%a.err<br> &gt; #SBATCH --mem=3D64G<br> &gt; #SBATCH --time=3D72:00:00<br> &gt; #SBATCH --constraint=3Drtx_6000<br> &gt; #SBATCH --gres=3Dgpu:4<br> &gt; #SBATCH --array=3D6,7,12,13,20<br> &gt; #SBATCH --mail-user=3D<a href=3D""mailto:bhargvb1@umbc.edu"" target=3D""_= blank"">bhargvb1@umbc.edu</a><br> &gt; #SBATCH --mail-type=3DEND,FAIL<br> &gt; On Thu, Sep 25, 2025 at 10:56 AM Elliot Gobbert via RT<br> &gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp= @rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html= ?id=3D3280927"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tic= ket/Display.html?id=3D3280927</a> &gt;<br> <br> &gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt; Hello there,<br> <br> &gt;&gt; Yes, we recently made some changes to the GPU partitions on Chip. = This<br> &gt;&gt; is a<br> &gt;&gt; very common mistake we&#39;re seeing.<br> <br> &gt;&gt; The gpu-contrib partition cannot be charged by users; it&#39;s onl= y<br> &gt;&gt; available so<br> &gt;&gt; that users can see the list of preemptible nodes.<br> <br> &gt;&gt; The partitions wiki page mentions more about this:<br> <br> &gt;&gt; <a href=3D""https://umbc.atlassian.net/wiki/spaces/faq/pages/124950= 9377/chip+Partitions+and+Usage"" rel=3D""noreferrer"" target=3D""_blank"">https:= //umbc.atlassian.net/wiki/spaces/faq/pages/1249509377/chip+Partitions+and+U= sage</a><br> <br> &gt;&gt; Let me know if that helps,<br> <br> &gt;&gt; Elliot Gobbert<br> <br> </blockquote></div> </blockquote></div> </blockquote></div> "
3280927,71946774,Correspond,DoIT-Research-Computing,2025-09-25 16:56:10.0000000,HPC Slurm/Software Issue: GPU Job QoS not Permitted,resolved,Danielle Esposito,desposi1,Bharg Barot,bhargvb1,bhargvb1@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Bharg,</p>  <p>I see that your job is current running without issue. Did you get that output from squeue? If so, you can see that the job status is &quot;PD&quot; which means pending. I am not 100% sure why it is showing this exact error, but it seems the nodes were allocated shortly after the start of the job. Are there any actual errors with the job itself?&nbsp;</p>  <p>We are looking into the cause of this message, however it does not seem to actually be causing any errors.&nbsp;</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Thu Sep 25 12:34:21 2025, SW91938 wrote: <blockquote>#!/bin/bash<br /> #SBATCH --job-name=KcollectLVL<br /> #SBATCH --output=klog/collect_%A_%a.out<br /> #SBATCH --error=klog/collect_%A_%a.err<br /> #SBATCH --mem=64G<br /> #SBATCH --time=72:00:00<br /> #SBATCH --constraint=rtx_6000<br /> #SBATCH --gres=gpu:4<br /> #SBATCH --array=6,7,12,13,20<br /> #SBATCH --mail-user=bhargvb1@umbc.edu<br /> #SBATCH --mail-type=END,FAIL<br /> #SBATCH --partition=gpu-general</blockquote> </div> "
3281158,71949342,Create,DoIT-Research-Computing,2025-09-25 18:18:07.0000000,HPC User Account: mcham2 in Student Group,resolved,Elliot Gobbert,elliotg2,Mostafa Cham,mcham2,mcham2@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Mostafa Last Name:                 Cham Email:                     mcham2@umbc.edu Campus ID:                 QT51576  Request Type:              High Performance Cluster  Create/Modify account in Student group  Hi,  I need access to the iHARP research group on the chip cluster. Could you please add me to the research group with the permission of Drs. Wang and Janeja. I really appreciate any help you can provide.  Best regards, Mostafa  "
3281158,72012766,Correspond,DoIT-Research-Computing,2025-09-30 13:45:57.0000000,HPC User Account: mcham2 in Student Group,resolved,Elliot Gobbert,elliotg2,Mostafa Cham,mcham2,mcham2@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hello,&nbsp;</p>  <p>We just need the written permission from Drs. Wang and Janeja. They are cc&#39;d.&nbsp;</p>  <p>Then I can add you to the iHarp group as usual.</p>  <p>No rush,</p>  <p>Elliot Gobbert</p> "
3281158,72012893,Correspond,DoIT-Research-Computing,2025-09-30 13:50:19.0000000,HPC User Account: mcham2 in Student Group,resolved,Elliot Gobbert,elliotg2,Mostafa Cham,mcham2,mcham2@umbc.edu,Jianwu Wang,jianwu@umbc.edu,"<div dir=3D""ltr"">I confirm=C2=A0Mostafa is also an iHARP student besides hi= s current group setup. Thanks!</div><br><div class=3D""gmail_quote gmail_quo= te_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Tue, Sep 30, 2025 at=  9:46=E2=80=AFAM Elliot Gobbert via RT &lt;<a href=3D""mailto:UMBCHelp@rt.um= bc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""g= mail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204= ,204,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/= Ticket/Display.html?id=3D3281158"" rel=3D""noreferrer"" target=3D""_blank"">http= s://rt.umbc.edu/Ticket/Display.html?id=3D3281158</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hello,<br> <br> We just need the written permission from Drs. Wang and Janeja. They are cc&= #39;d.<br> <br> Then I can add you to the iHarp group as usual.<br> <br> No rush,<br> <br> Elliot Gobbert<br> <br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><div style=3D""color:rgb(0,0,0)"">Best wishes<br>S= incerely yours<br><br>Jianwu Wang, Ph.D. (he/him)<br><a href=3D""mailto:jian= wu@umbc.edu"" style=3D""color:rgb(17,85,204)"" target=3D""_blank"">jianwu@umbc.e= du</a><br>Website:=C2=A0<a href=3D""https://userpages.umbc.edu/~jianwu/"" sty= le=3D""color:rgb(17,85,204)"" target=3D""_blank"">https://bdal.umbc.edu/people/= jianwu/</a><br>WebEx:=C2=A0<a href=3D""https://umbc.webex.com/meet/jianwu"" s= tyle=3D""color:rgb(17,85,204)"" target=3D""_blank"">https://umbc.webex.com/meet= /jianwu</a><br><br>Professor of Data Science,=C2=A0<a href=3D""http://inform= ationsystems.umbc.edu/"" style=3D""color:rgb(17,85,204)"" target=3D""_blank"">De= partment of Information Systems</a></div><div style=3D""color:rgb(0,0,0)"">Di= rector,=C2=A0<a href=3D""https://bdal.umbc.edu/"" style=3D""color:rgb(17,85,20= 4)"" target=3D""_blank"">Big Data Analytics Lab</a></div><div style=3D""color:r= gb(0,0,0)"">Director,=C2=A0<a href=3D""https://scales.umbc.edu/"" target=3D""_b= lank"">Center for Scalable Data and Computational Science (ScaleS)</a></div>= <div style=3D""color:rgb(0,0,0)"">Co-Director,=C2=A0<a href=3D""https://bigdat= areu.umbc.edu/"" target=3D""_blank"">NSF REU Site on Online Big Data=C2=A0Anal= ytics</a></div><div style=3D""color:rgb(0,0,0)"">Co-Lead,=C2=A0<a href=3D""htt= ps://iharp.umbc.edu/"" style=3D""color:rgb(17,85,204)"" target=3D""_blank"">NSF = HDR institute for Data=C2=A0and Model Revolution in the=C2=A0Polar Regions = (iHARP)</a><br>University of Maryland, Baltimore=C2=A0County<br>410-455-388= 3<br>ITE 423=C2=A0</div></div></div> "
3281158,72012893,Correspond,DoIT-Research-Computing,2025-09-30 13:50:19.0000000,HPC User Account: mcham2 in Student Group,resolved,Elliot Gobbert,elliotg2,Mostafa Cham,mcham2,mcham2@umbc.edu,Jianwu Wang,jianwu@umbc.edu,"I confirm Mostafa is also an iHARP student besides his current group setup. Thanks!  On Tue, Sep 30, 2025 at 9:46=E2=80=AFAM Elliot Gobbert via RT <UMBCHelp@rt.= umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3281158 > > > Last Update From Ticket: > > Hello, > > We just need the written permission from Drs. Wang and Janeja. They are > cc'd. > > Then I can add you to the iHarp group as usual. > > No rush, > > Elliot Gobbert > > >  --=20 Best wishes Sincerely yours  Jianwu Wang, Ph.D. (he/him) jianwu@umbc.edu Website: https://bdal.umbc.edu/people/jianwu/ <https://userpages.umbc.edu/~jianwu/> WebEx: https://umbc.webex.com/meet/jianwu  Professor of Data Science, Department of Information Systems <http://informationsystems.umbc.edu/> Director, Big Data Analytics Lab <https://bdal.umbc.edu/> Director, Center for Scalable Data and Computational Science (ScaleS) <https://scales.umbc.edu/> Co-Director, NSF REU Site on Online Big Data Analytics <https://bigdatareu.umbc.edu/> Co-Lead, NSF HDR institute for Data and Model Revolution in the Polar Regions (iHARP) <https://iharp.umbc.edu/> University of Maryland, Baltimore County 410-455-3883 ITE 423 "
3281158,72015701,Correspond,DoIT-Research-Computing,2025-09-30 14:38:35.0000000,HPC User Account: mcham2 in Student Group,resolved,Elliot Gobbert,elliotg2,Mostafa Cham,mcham2,mcham2@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hi Mostafa,<br /> Your user has been added to iharp as a secondary group.<br /> Your home directory has additional symbolic links to your group storage space.<br /> Please read through the documentation found at hpcf.umbc.edu.<br /> Please submit additional questions or issues as separate tickets via the following link.<br /> (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/)<br /> <br /> &nbsp;</p>  <p>Let me know if you have any more questions or concerns,</p>  <p>Elliot</p> "
3281158,72021159,Correspond,DoIT-Research-Computing,2025-09-30 17:10:20.0000000,HPC User Account: mcham2 in Student Group,resolved,Elliot Gobbert,elliotg2,Mostafa Cham,mcham2,mcham2@umbc.edu,Vandana Janeja,vjaneja@umbc.edu,"approved  ------------------------------------------------------------ Vandana Janeja, Ph.D.  Associate Dean for Research and Faculty Development College of Engineering and Information Technology Professor, Information Systems Department, Director NSF HDR Institute-iHARP  University of Maryland, Baltimore County A Carnegie R1 Institution Lab : https://mdata.umbc.edu/ NSF Institute: https://iharp.umbc.edu/ webex: https://umbc.webex.com/join/vjaneja  *Land Acknowledgement: UMBC was established upon the land of the Piscataway Conoy and Susquehannock peoples. Over time, citizens of many more Indigenous nations have come to reside in this region. We humbly offer our respects to all past, present, and future Indigenous people connected to this place.*   On Tue, Sep 30, 2025 at 9:50=E2=80=AFAM Jianwu Wang via RT <UMBCHelp@rt.umb= c.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3281158 > > > Last Update From Ticket: > > I confirm Mostafa is also an iHARP student besides his current group setu= p. > Thanks! > > On Tue, Sep 30, 2025 at 9:46=E2=80=AFAM Elliot Gobbert via RT < > UMBCHelp@rt.umbc.edu> > wrote: > > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3281158 > > > > > Last Update From Ticket: > > > > Hello, > > > > We just need the written permission from Drs. Wang and Janeja. They are > > cc'd. > > > > Then I can add you to the iHarp group as usual. > > > > No rush, > > > > Elliot Gobbert > > > > > > > > -- > Best wishes > Sincerely yours > > Jianwu Wang, Ph.D. (he/him) > jianwu@umbc.edu > Website: https://bdal.umbc.edu/people/jianwu/ > <https://userpages.umbc.edu/~jianwu/> > WebEx: https://umbc.webex.com/meet/jianwu > > Professor of Data Science, Department of Information Systems > <http://informationsystems.umbc.edu/> > Director, Big Data Analytics Lab <https://bdal.umbc.edu/> > Director, Center for Scalable Data and Computational Science (ScaleS) > <https://scales.umbc.edu/> > Co-Director, NSF REU Site on Online Big Data Analytics > <https://bigdatareu.umbc.edu/> > Co-Lead, NSF HDR institute for Data and Model Revolution in the Polar > Regions (iHARP) <https://iharp.umbc.edu/> > University of Maryland, Baltimore County > 410-455-3883 > ITE 423 > > "
3281158,72021159,Correspond,DoIT-Research-Computing,2025-09-30 17:10:20.0000000,HPC User Account: mcham2 in Student Group,resolved,Elliot Gobbert,elliotg2,Mostafa Cham,mcham2,mcham2@umbc.edu,Vandana Janeja,vjaneja@umbc.edu,"<div dir=3D""ltr""><div><div style=3D""font-size:small"" class=3D""gmail_default= "">approved</div><br clear=3D""all""></div><div><div dir=3D""ltr"" class=3D""gmai= l_signature"" data-smartmail=3D""gmail_signature""><div dir=3D""ltr"">----------= --------------------------------------------------<br><div>Vandana Janeja, = Ph.D.</div><div><br></div><div>Associate Dean for Research and Faculty Deve= lopment</div><div>College of Engineering and Information Technology=C2=A0</= div><div>Professor, Information Systems Department,</div><div>Director NSF = HDR Institute-iHARP</div><div><br></div><span>University of Maryland, Balti= more County</span><div>A Carnegie R1 Institution</div><div></div><div>Lab := <a href=3D""https://mdata.umbc.edu/"" target=3D""_blank""> https://mdata.umbc.e= du/ <br></a></div><div>NSF Institute: <a href=3D""https://iharp.umbc.edu/"" t= arget=3D""_blank"">https://iharp.umbc.edu/</a>=C2=A0 <br></div><div>webex: <a=  href=3D""https://umbc.webex.com/join/vjaneja"" target=3D""_blank"">https://umb= c.webex.com/join/vjaneja</a></div><div><br></div><div></div><font size=3D""2= ""><i>Land Acknowledgement: <span>UMBC was established upon the land=20 of the Piscataway Conoy and Susquehannock peoples. Over time, citizens=20 of many more Indigenous nations have come to reside in this region. We=20 humbly offer our respects to all past, present, and future Indigenous=20 people connected to this place.</span></i></font></div></div></div><br></di= v><br><div class=3D""gmail_quote gmail_quote_container""><div dir=3D""ltr"" cla= ss=3D""gmail_attr"">On Tue, Sep 30, 2025 at 9:50=E2=80=AFAM Jianwu Wang via R= T &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; = wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""margin:0px 0px 0= px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"">Ticket &l= t;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D3281158"" rel= =3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?i= d=3D3281158</a> &gt;<br> <br> Last Update From Ticket:<br> <br> I confirm Mostafa is also an iHARP student besides his current group setup.= <br> Thanks!<br> <br> On Tue, Sep 30, 2025 at 9:46=E2=80=AFAM Elliot Gobbert via RT &lt;<a href= =3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>= &gt;<br> wrote:<br> <br> &gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id= =3D3281158"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket= /Display.html?id=3D3281158</a> &gt;<br> &gt;<br> &gt; Last Update From Ticket:<br> &gt;<br> &gt; Hello,<br> &gt;<br> &gt; We just need the written permission from Drs. Wang and Janeja. They ar= e<br> &gt; cc&#39;d.<br> &gt;<br> &gt; Then I can add you to the iHarp group as usual.<br> &gt;<br> &gt; No rush,<br> &gt;<br> &gt; Elliot Gobbert<br> &gt;<br> &gt;<br> &gt;<br> <br> -- <br> Best wishes<br> Sincerely yours<br> <br> Jianwu Wang, Ph.D. (he/him)<br> <a href=3D""mailto:jianwu@umbc.edu"" target=3D""_blank"">jianwu@umbc.edu</a><br> Website: <a href=3D""https://bdal.umbc.edu/people/jianwu/"" rel=3D""noreferrer= "" target=3D""_blank"">https://bdal.umbc.edu/people/jianwu/</a><br> &lt;<a href=3D""https://userpages.umbc.edu/~jianwu/"" rel=3D""noreferrer"" targ= et=3D""_blank"">https://userpages.umbc.edu/~jianwu/</a>&gt;<br> WebEx: <a href=3D""https://umbc.webex.com/meet/jianwu"" rel=3D""noreferrer"" ta= rget=3D""_blank"">https://umbc.webex.com/meet/jianwu</a><br> <br> Professor of Data Science, Department of Information Systems<br> &lt;<a href=3D""http://informationsystems.umbc.edu/"" rel=3D""noreferrer"" targ= et=3D""_blank"">http://informationsystems.umbc.edu/</a>&gt;<br> Director, Big Data Analytics Lab &lt;<a href=3D""https://bdal.umbc.edu/"" rel= =3D""noreferrer"" target=3D""_blank"">https://bdal.umbc.edu/</a>&gt;<br> Director, Center for Scalable Data and Computational Science (ScaleS)<br> &lt;<a href=3D""https://scales.umbc.edu/"" rel=3D""noreferrer"" target=3D""_blan= k"">https://scales.umbc.edu/</a>&gt;<br> Co-Director, NSF REU Site on Online Big Data Analytics<br> &lt;<a href=3D""https://bigdatareu.umbc.edu/"" rel=3D""noreferrer"" target=3D""_= blank"">https://bigdatareu.umbc.edu/</a>&gt;<br> Co-Lead, NSF HDR institute for Data and Model Revolution in the Polar<br> Regions (iHARP) &lt;<a href=3D""https://iharp.umbc.edu/"" rel=3D""noreferrer"" = target=3D""_blank"">https://iharp.umbc.edu/</a>&gt;<br> University of Maryland, Baltimore County<br> 410-455-3883<br> ITE 423<br> <br> </blockquote></div> "
3281243,71952656,Create,DoIT-Research-Computing,2025-09-25 19:55:41.0000000,HPC Other Issue: Nodes on partition pi_bennettj being used in partition 2024,resolved,Greg Ballantine,gballan1,Anthony Casale,acasale1,acasale1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Anthony<br /> Last Name:                 Casale<br /> Email:                     acasale1@umbc.edu<br /> Campus ID:                 UM80933<br /> <br /> Request Type:              High Performance Cluster<br /> <br /> Hello, <br />  <br /> I'm trying to run jobs on the pi_bennettj partition, however, 4 of our nodes are being used on partition 2024. <br />  <br /> I was under the impression that the pi researchers would preempt anyone using our nodes. <br />  <br /> I've attached a list of the nodes being used. <br />  <br /> Thank you, <br /> Anthony Casale<br /> <br /> Attachment 1: <a href=""https://umbc.box.com/s/47j9k67x88i2csgwh6xmhawplao6ni0l"" target=""_blank"">nodes.txt</a><br /> "
3281243,71953873,Correspond,DoIT-Research-Computing,2025-09-25 20:41:44.0000000,HPC Other Issue: Nodes on partition pi_bennettj being used in partition 2024,resolved,Greg Ballantine,gballan1,Anthony Casale,acasale1,acasale1@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Anthony,<br /> <br /> That is correct - nodes in the pi_bennettj partition will peempt jobs from users that are not in the pi_bennettj group after a 10 minute grace period.<br /> <br /> It looks like your jobs were submitted at 3:48pm EST and started ~10 minutes later at 3:58pm EST. Can you confirm that your job is running now?<br /> <br /> Thank you,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Thu Sep 25 15:55:41 2025, ZZ99999 wrote:</p>  <blockquote>First Name: Anthony<br /> Last Name: Casale<br /> Email: acasale1@umbc.edu<br /> Campus ID: UM80933<br /> <br /> Request Type: High Performance Cluster<br /> <br /> Hello,<br /> <br /> I&#39;m trying to run jobs on the pi_bennettj partition, however, 4 of our nodes are being used on partition 2024.<br /> <br /> I was under the impression that the pi researchers would preempt anyone using our nodes.<br /> <br /> I&#39;ve attached a list of the nodes being used.<br /> <br /> Thank you,<br /> Anthony Casale<br /> <br /> Attachment 1: nodes.txt</blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3281243,71954925,Correspond,DoIT-Research-Computing,2025-09-26 00:00:06.0000000,HPC Other Issue: Nodes on partition pi_bennettj being used in partition 2024,resolved,Greg Ballantine,gballan1,Anthony Casale,acasale1,acasale1@umbc.edu,Anthony Casale,acasale1@umbc.edu,"I was unaware of the 10 minute grace period.  Many apologies.  My calculations did run.  I appreciate your help and information on this.  Thank you! Anthony Casale  On Thu, Sep 25, 2025 at 4:41=E2=80=AFPM Greg Ballantine via RT <UMBCHelp@rt= .umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3281243 > > > Last Update From Ticket: > > Hello Anthony, > > That is correct - nodes in the pi_bennettj partition will peempt jobs from > users that are not in the pi_bennettj group after a 10 minute grace perio= d. > > It looks like your jobs were submitted at 3:48pm EST and started ~10 > minutes > later at 3:58pm EST. Can you confirm that your job is running now? > > Thank you, > Greg > > On Thu Sep 25 15:55:41 2025, ZZ99999 wrote: > > > First Name: Anthony > > Last Name: Casale > > Email: acasale1@umbc.edu > > Campus ID: UM80933 > > > Request Type: High Performance Cluster > > > Hello, > > > I'm trying to run jobs on the pi_bennettj partition, however, 4 of our > > nodes are being used on partition 2024. > > > I was under the impression that the pi researchers would preempt anyone > > using our nodes. > > > I've attached a list of the nodes being used. > > > Thank you, > > Anthony Casale > > > Attachment 1: nodes.txt > > -- > > Gregory BallantineSystem Administrator for Research and Enterprise > ComputingUMBC > - DoIT > > "
3281243,71954925,Correspond,DoIT-Research-Computing,2025-09-26 00:00:06.0000000,HPC Other Issue: Nodes on partition pi_bennettj being used in partition 2024,resolved,Greg Ballantine,gballan1,Anthony Casale,acasale1,acasale1@umbc.edu,Anthony Casale,acasale1@umbc.edu,"<div dir=3D""ltr"">I was unaware of the 10 minute grace period.=C2=A0 Many ap= ologies.=C2=A0 My calculations did run.<div><br></div><div>I appreciate you= r help and information on this.</div><div><br></div><div>Thank you!</div><d= iv>Anthony Casale</div></div><br><div class=3D""gmail_quote gmail_quote_cont= ainer""><div dir=3D""ltr"" class=3D""gmail_attr"">On Thu, Sep 25, 2025 at 4:41= =E2=80=AFPM Greg Ballantine via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.e= du"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail= _quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204= ,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Tick= et/Display.html?id=3D3281243"" rel=3D""noreferrer"" target=3D""_blank"">https://= rt.umbc.edu/Ticket/Display.html?id=3D3281243</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hello Anthony,<br> <br> That is correct - nodes in the pi_bennettj partition will peempt jobs from<= br> users that are not in the pi_bennettj group after a 10 minute grace period.= <br> <br> It looks like your jobs were submitted at 3:48pm EST and started ~10 minute= s<br> later at 3:58pm EST. Can you confirm that your job is running now?<br> <br> Thank you,<br> Greg<br> <br> On Thu Sep 25 15:55:41 2025, ZZ99999 wrote:<br> <br> &gt; First Name: Anthony<br> &gt; Last Name: Casale<br> &gt; Email: <a href=3D""mailto:acasale1@umbc.edu"" target=3D""_blank"">acasale1= @umbc.edu</a><br> &gt; Campus ID: UM80933<br> <br> &gt; Request Type: High Performance Cluster<br> <br> &gt; Hello,<br> <br> &gt; I&#39;m trying to run jobs on the pi_bennettj partition, however, 4 of=  our<br> &gt; nodes are being used on partition 2024.<br> <br> &gt; I was under the impression that the pi researchers would preempt anyon= e<br> &gt; using our nodes.<br> <br> &gt; I&#39;ve attached a list of the nodes being used.<br> <br> &gt; Thank you,<br> &gt; Anthony Casale<br> <br> &gt; Attachment 1: nodes.txt<br> <br> --<br> <br> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br> - DoIT<br> <br> </blockquote></div> "
3281293,71954223,Create,DoIT-Research-Computing,2025-09-25 20:56:36.0000000,HPC Other Issue: jobs getting killed on 2024 queue,resolved,Danielle Esposito,desposi1,Sergio De souza-machad,sergio,sergio@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Sergio Last Name:                 De souza-machad Email:                     sergio@umbc.edu Campus ID:                 VR64161  Request Type:              High Performance Cluster  For some reason many of my jobs are getting killed/preempted (slurmsteps). I thought I had an issue with the code but seems fine to me.   Haven't had that before  Is there a way to limit the jobs only to Larrabee's computer?   Wierd this is suppose the slurm job ID is 402067, I keep getting told eg 402166 has been killed?? makes no sense?  [sergio@chip-login1 AI_RTA]$ grep -in slurmst slurm*  slurm-402067_90.out:17:.........+slurmstepd: error: *** JOB 402157 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** slurm-402067_91.out:17:.........+slurmstepd: error: *** JOB 402158 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** slurm-402067_92.out:17:.........slurmstepd: error: *** JOB 402159 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** slurm-402067_93.out:17:.........+.slurmstepd: error: *** JOB 402160 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** slurm-402067_94.out:17:.........+..slurmstepd: error: *** JOB 402161 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** slurm-402067_95.out:17:.........+.slurmstepd: error: *** JOB 402162 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** slurm-402067_96.out:17:.........slurmstepd: error: *** JOB 402163 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** slurm-402067_97.out:17:.........+slurmstepd: error: *** JOB 402164 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** slurm-402067_98.out:17:.........+.........+...slurmstepd: error: *** JOB 402165 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO PREEMPTION *** slurm-402067_99.out:17:.........+.........slurmstepd: error: *** JOB 402166 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO PREEMPTION ***   "
3281293,71959428,Correspond,DoIT-Research-Computing,2025-09-26 13:52:22.0000000,HPC Other Issue: jobs getting killed on 2024 queue,resolved,Danielle Esposito,desposi1,Sergio De souza-machad,sergio,sergio@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Sergio,</p>  <p>First off, let me elaborate on the slurm job ID&#39;s you are seeing. When submitting an array job, there is the main job ID (which is 402067 in this case), along with the array tasks for each (which are like, 402067_90). However, each individual array task also gets its own actual job ID. For example, the actual job ID for array task 402067_90 is&nbsp;402157.</p>  <p>Next, your job was likely canceled due to someone in pi_bennettj attempting to run a job on their nodes. Nodes c24-[01-10] are all nodes contributed from pi_bennettj, therefore they have priority access and the ability to preempt other users not in their group. If you want to run your jobs on 2024 nodes, but do not want to risk preemption, use the &#39;match&#39; partition.&nbsp;</p>  <p>If you would like more information on which partitions allow/disallow preemption, check out this wiki page:&nbsp;https://umbc.atlassian.net/wiki/spaces/faq/pages/1249509377/chip+Partitions+and+Usage#Partitions</p>  <p>Let me know if that helps! Have a nice day!</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Thu Sep 25 16:56:36 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Sergio Last Name:                 De souza-machad Email:                     sergio@umbc.edu Campus ID:                 VR64161  Request Type:              High Performance Cluster  For some reason many of my jobs are getting killed/preempted (slurmsteps). I thought I had an issue with the code but seems fine to me.   Haven&#39;t had that before  Is there a way to limit the jobs only to Larrabee&#39;s computer?   Wierd this is suppose the slurm job ID is 402067, I keep getting told eg 402166 has been killed?? makes no sense?  [sergio@chip-login1 AI_RTA]$ grep -in slurmst slurm*  slurm-402067_90.out:17:.........+slurmstepd: error: *** JOB 402157 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** slurm-402067_91.out:17:.........+slurmstepd: error: *** JOB 402158 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** slurm-402067_92.out:17:.........slurmstepd: error: *** JOB 402159 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** slurm-402067_93.out:17:.........+.slurmstepd: error: *** JOB 402160 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** slurm-402067_94.out:17:.........+..slurmstepd: error: *** JOB 402161 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** slurm-402067_95.out:17:.........+.slurmstepd: error: *** JOB 402162 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** slurm-402067_96.out:17:.........slurmstepd: error: *** JOB 402163 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** slurm-402067_97.out:17:.........+slurmstepd: error: *** JOB 402164 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** slurm-402067_98.out:17:.........+.........+...slurmstepd: error: *** JOB 402165 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO PREEMPTION *** slurm-402067_99.out:17:.........+.........slurmstepd: error: *** JOB 402166 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO PREEMPTION ***   </pre> </blockquote> </div> "
3281293,71973406,Correspond,DoIT-Research-Computing,2025-09-26 20:38:45.0000000,HPC Other Issue: jobs getting killed on 2024 queue,resolved,Danielle Esposito,desposi1,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"Hi Danielle,  Thanks for the info  I tried this just now   #SBATCH --job-name=3DFIT_TILE_TRENDS    ## name #SBATCH -N1                           ## number of job step is to be allocated per instance of matlab #SBATCH --cpus-per-task 1             ## tasks per node/number of cores per matlab session will be  ##SBATCH --partition=3D2024              ## desired partition #SBATCH --partition=3Dmatch             ## desired partition #SBATCH --cluster=3Dchip-cpu            ## desired cluster #SBATCH --account=3Dpi_strow #SBATCH --qos=3Dshared                  ## qos to get as many cpu2024 as possible, else put pi_strow  and get  bin/rm: cannot remove '*~': No such file or directory /bin/rm: cannot remove 'slurm*.err': No such file or directory sbatch: error: Missing: '--gres' sbatch: error:   You must specify a Generic RESource to use in your job. sbatch: error: See this webpage for more details: https://hpcf.umbc.edu/compute/overview/. sbatch: error: Batch job submission failed: Unspecified error  which according to the webpage you pointed me to, should be used for gpu processors?  Thanks  Sergio   On Fri, Sep 26, 2025 at 9:52=E2=80=AFAM Danielle Esposito via RT < UMBCHelp@rt.umbc.edu> wrote:  > If you agree your issue is resolved, please give us feedback on your > experience by completing a brief satisfaction survey: > > > https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&customeremail= =3Dsergio%40umbc.edu&groupid=3DEIS&ticketid=3D3281293&ticketowner=3Ddesposi= 1%40umbc.edu&ticketsubject=3DHPC%20Other%20Issue%3A%20jobs%20getting%20kill= ed%20on%202024%20queue > > If you believe your issue has not been resolved, please respond to this > message, which will reopen your ticket. Note: A full record of your reque= st > can be found at: > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3281293 > > > Thank You > > _________________________________________ > > R e s o l u t i o n: > =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D = =3D > > Hi Sergio, > > First off, let me elaborate on the slurm job ID's you are seeing. When > submitting an array job, there is the main job ID (which is 402067 in this > case), along with the array tasks for each (which are like, 402067_90). > However, each individual array task also gets its own actual job ID. For > example, the actual job ID for array task 402067_90 is 402157. > > Next, your job was likely canceled due to someone in pi_bennettj > attempting to > run a job on their nodes. Nodes c24-[01-10] are all nodes contributed from > pi_bennettj, therefore they have priority access and the ability to preem= pt > other users not in their group. If you want to run your jobs on 2024 > nodes, but > do not want to risk preemption, use the 'match' partition. > > If you would like more information on which partitions allow/disallow > preemption, check out this wiki page: > > https://umbc.atlassian.net/wiki/spaces/faq/pages/1249509377/chip+Partitio= ns+and+Usage#Partitions > > Let me know if that helps! Have a nice day! > > -- > > Kind regards, > Danielle Esposito (she/her/hers) > DoIT Unix Infra Student Worker > > On Thu Sep 25 16:56:36 2025, ZZ99999 wrote: > > > First Name:                Sergio > > Last Name:                 De souza-machad > > Email:                     sergio@umbc.edu > > Campus ID:                 VR64161 > > > > Request Type:              High Performance Cluster > > > > For some reason many of my jobs are getting killed/preempted > (slurmsteps). I thought I had an issue with the code but seems fine to me. > > > > Haven't had that before > > > > Is there a way to limit the jobs only to Larrabee's computer? > > > > Wierd this is suppose the slurm job ID is 402067, I keep getting told eg > 402166 has been killed?? makes no sense? > > > > [sergio@chip-login1 AI_RTA]$ grep -in slurmst slurm* > > > > slurm-402067_90.out:17:.........+slurmstepd: error: *** JOB 402157 ON > c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** > > slurm-402067_91.out:17:.........+slurmstepd: error: *** JOB 402158 ON > c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** > > slurm-402067_92.out:17:.........slurmstepd: error: *** JOB 402159 ON > c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** > > slurm-402067_93.out:17:.........+.slurmstepd: error: *** JOB 402160 ON > c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** > > slurm-402067_94.out:17:.........+..slurmstepd: error: *** JOB 402161 ON > c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** > > slurm-402067_95.out:17:.........+.slurmstepd: error: *** JOB 402162 ON > c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** > > slurm-402067_96.out:17:.........slurmstepd: error: *** JOB 402163 ON > c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** > > slurm-402067_97.out:17:.........+slurmstepd: error: *** JOB 402164 ON > c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** > > slurm-402067_98.out:17:.........+.........+...slurmstepd: error: *** JOB > 402165 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO PREEMPTION *** > > slurm-402067_99.out:17:.........+.........slurmstepd: error: *** JOB > 402166 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO PREEMPTION *** > > > > > > > ______________________________________ > > Original Request: > > Requestors: Sergio De souza-machad > > First Name:                Sergio > Last Name:                 De souza-machad > Email:                     sergio@umbc.edu > Campus ID:                 VR64161 > > Request Type:              High Performance Cluster > > For some reason many of my jobs are getting killed/preempted (slurmsteps). > I thought I had an issue with the code but seems fine to me. > > Haven't had that before > > Is there a way to limit the jobs only to Larrabee's computer? > > Wierd this is suppose the slurm job ID is 402067, I keep getting told eg > 402166 has been killed?? makes no sense? > > [sergio@chip-login1 AI_RTA]$ grep -in slurmst slurm* > > slurm-402067_90.out:17:.........+slurmstepd: error: *** JOB 402157 ON > c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** > slurm-402067_91.out:17:.........+slurmstepd: error: *** JOB 402158 ON > c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** > slurm-402067_92.out:17:.........slurmstepd: error: *** JOB 402159 ON > c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** > slurm-402067_93.out:17:.........+.slurmstepd: error: *** JOB 402160 ON > c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** > slurm-402067_94.out:17:.........+..slurmstepd: error: *** JOB 402161 ON > c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** > slurm-402067_95.out:17:.........+.slurmstepd: error: *** JOB 402162 ON > c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** > slurm-402067_96.out:17:.........slurmstepd: error: *** JOB 402163 ON > c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** > slurm-402067_97.out:17:.........+slurmstepd: error: *** JOB 402164 ON > c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** > slurm-402067_98.out:17:.........+.........+...slurmstepd: error: *** JOB > 402165 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO PREEMPTION *** > slurm-402067_99.out:17:.........+.........slurmstepd: error: *** JOB > 402166 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO PREEMPTION *** > > > >  --=20 ---------------------------------------------------------------------------= ------------------------------------------ Sergio DeSouza-Machado sergio@umbc.edu Research Assoc. Professor,                                              (W) 410-455-1944 JCET/Dept of Physics (F) 410-455-1072 UMBC, Baltimore MD 21250 "
3281293,71973406,Correspond,DoIT-Research-Computing,2025-09-26 20:38:45.0000000,HPC Other Issue: jobs getting killed on 2024 queue,resolved,Danielle Esposito,desposi1,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"<div dir=3D""ltr"">Hi Danielle,<div><br></div><div>Thanks for the info<br><br= >I tried this just now<br><blockquote style=3D""margin:0 0 0 40px;border:non= e;padding:0px""><div><br></div></blockquote></div><div><span style=3D""backgr= ound-color:rgb(204,204,204)"">#SBATCH --job-name=3DFIT_TILE_TRENDS =C2=A0 = =C2=A0## name<br>#SBATCH -N1 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 ## number of job step is to b= e allocated per instance of matlab<br>#SBATCH --cpus-per-task 1 =C2=A0 =C2= =A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 ## tasks per node/number of cores per matla= b session will be<br><br>##SBATCH --partition=3D2024 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 =C2=A0 =C2=A0 =C2=A0## desired partition<br>#SBATCH --partition=3Dma= tch =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 ## desired partition<br>#SBAT= CH --cluster=3Dchip-cpu =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0## desired=  cluster<br>#SBATCH --account=3Dpi_strow<br>#SBATCH --qos=3Dshared =C2=A0 = =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0## qos to get as man= y cpu2024 as possible, else put pi_strow<br></span></div><div><br></div><di= v>and get<br><br><span style=3D""background-color:rgb(204,204,204)"">bin/rm: = cannot remove &#39;*~&#39;: No such file or directory<br>/bin/rm: cannot re= move &#39;slurm*.err&#39;: No such file or directory<br>sbatch: error: Miss= ing: &#39;--gres&#39;<br>sbatch: error: =C2=A0	 You must specify a Generic = RESource to use in your job.<br>sbatch: error: See this webpage for more de= tails: <a href=3D""https://hpcf.umbc.edu/compute/overview/"">https://hpcf.umb= c.edu/compute/overview/</a>.<br>sbatch: error: Batch job submission failed:=  Unspecified error<br></span><br>which according to the webpage you pointed=  me to, should be used for gpu processors?</div><div><br></div><div>Thanks<= /div><div><br></div><div>Sergio<br><br></div></div><br><div class=3D""gmail_= quote gmail_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Fri, = Sep 26, 2025 at 9:52=E2=80=AFAM Danielle Esposito via RT &lt;<a href=3D""mai= lto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blo= ckquote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left= :1px solid rgb(204,204,204);padding-left:1ex"">If you agree your issue is re= solved, please give us feedback on your experience by completing a brief sa= tisfaction survey: <br> <br> <a href=3D""https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&amp;= customeremail=3Dsergio%40umbc.edu&amp;groupid=3DEIS&amp;ticketid=3D3281293&= amp;ticketowner=3Ddesposi1%40umbc.edu&amp;ticketsubject=3DHPC%20Other%20Iss= ue%3A%20jobs%20getting%20killed%20on%202024%20queue"" rel=3D""noreferrer"" tar= get=3D""_blank"">https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&= amp;customeremail=3Dsergio%40umbc.edu&amp;groupid=3DEIS&amp;ticketid=3D3281= 293&amp;ticketowner=3Ddesposi1%40umbc.edu&amp;ticketsubject=3DHPC%20Other%2= 0Issue%3A%20jobs%20getting%20killed%20on%202024%20queue</a><br> <br> If you believe your issue has not been resolved, please respond to this mes= sage, which will reopen your ticket. Note: A full record of your request ca= n be found at:=C2=A0 <br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D328= 1293"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3281293</a> &gt;<br> <br> Thank You<br> <br> _________________________________________<br> <br> R e s o l u t i o n:<br> =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D= =C2=A0 <br> <br> Hi Sergio,<br> <br> First off, let me elaborate on the slurm job ID&#39;s you are seeing. When<= br> submitting an array job, there is the main job ID (which is 402067 in this<= br> case), along with the array tasks for each (which are like, 402067_90).<br> However, each individual array task also gets its own actual job ID. For<br> example, the actual job ID for array task 402067_90 is 402157.<br> <br> Next, your job was likely canceled due to someone in pi_bennettj attempting=  to<br> run a job on their nodes. Nodes c24-[01-10] are all nodes contributed from<= br> pi_bennettj, therefore they have priority access and the ability to preempt= <br> other users not in their group. If you want to run your jobs on 2024 nodes,=  but<br> do not want to risk preemption, use the &#39;match&#39; partition.<br> <br> If you would like more information on which partitions allow/disallow<br> preemption, check out this wiki page:<br> <a href=3D""https://umbc.atlassian.net/wiki/spaces/faq/pages/1249509377/chip= +Partitions+and+Usage#Partitions"" rel=3D""noreferrer"" target=3D""_blank"">http= s://umbc.atlassian.net/wiki/spaces/faq/pages/1249509377/chip+Partitions+and= +Usage#Partitions</a><br> <br> Let me know if that helps! Have a nice day!<br> <br> --<br> <br> Kind regards,<br> Danielle Esposito (she/her/hers)<br> DoIT Unix Infra Student Worker<br> <br> On Thu Sep 25 16:56:36 2025, ZZ99999 wrote:<br> <br> &gt; First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Ser= gio<br> &gt; Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0De souza-machad<br> &gt; Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 =C2=A0<a href=3D""mailto:sergio@umbc.edu"" target=3D""_blank"">sergio@um= bc.edu</a><br> &gt; Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0VR64161<br> &gt; <br> &gt; Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Per= formance Cluster<br> &gt; <br> &gt; For some reason many of my jobs are getting killed/preempted (slurmste= ps). I thought I had an issue with the code but seems fine to me. <br> &gt; <br> &gt; Haven&#39;t had that before<br> &gt; <br> &gt; Is there a way to limit the jobs only to Larrabee&#39;s computer? <br> &gt; <br> &gt; Wierd this is suppose the slurm job ID is 402067, I keep getting told = eg 402166 has been killed?? makes no sense?<br> &gt; <br> &gt; [sergio@chip-login1 AI_RTA]$ grep -in slurmst slurm*<br> &gt; <br> &gt; slurm-402067_90.out:17:.........+slurmstepd: error: *** JOB 402157 ON = c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> &gt; slurm-402067_91.out:17:.........+slurmstepd: error: *** JOB 402158 ON = c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> &gt; slurm-402067_92.out:17:.........slurmstepd: error: *** JOB 402159 ON c= 24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> &gt; slurm-402067_93.out:17:.........+.slurmstepd: error: *** JOB 402160 ON=  c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> &gt; slurm-402067_94.out:17:.........+..slurmstepd: error: *** JOB 402161 O= N c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> &gt; slurm-402067_95.out:17:.........+.slurmstepd: error: *** JOB 402162 ON=  c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> &gt; slurm-402067_96.out:17:.........slurmstepd: error: *** JOB 402163 ON c= 24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> &gt; slurm-402067_97.out:17:.........+slurmstepd: error: *** JOB 402164 ON = c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> &gt; slurm-402067_98.out:17:.........+.........+...slurmstepd: error: *** J= OB 402165 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO PREEMPTION ***<= br> &gt; slurm-402067_99.out:17:.........+.........slurmstepd: error: *** JOB 4= 02166 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO PREEMPTION ***<br> &gt; <br> <br> <br> <br> <br> ______________________________________<br> <br> Original Request:<br> <br> Requestors: Sergio De souza-machad<br> <br> First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Sergio<b= r> Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0De = souza-machad<br> Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=  =C2=A0<a href=3D""mailto:sergio@umbc.edu"" target=3D""_blank"">sergio@umbc.edu= </a><br> Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0VR6= 4161<br> <br> Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Performa= nce Cluster<br> <br> For some reason many of my jobs are getting killed/preempted (slurmsteps). = I thought I had an issue with the code but seems fine to me. <br> <br> Haven&#39;t had that before<br> <br> Is there a way to limit the jobs only to Larrabee&#39;s computer? <br> <br> Wierd this is suppose the slurm job ID is 402067, I keep getting told eg 40= 2166 has been killed?? makes no sense?<br> <br> [sergio@chip-login1 AI_RTA]$ grep -in slurmst slurm*<br> <br> slurm-402067_90.out:17:.........+slurmstepd: error: *** JOB 402157 ON c24-0= 2 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> slurm-402067_91.out:17:.........+slurmstepd: error: *** JOB 402158 ON c24-0= 2 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> slurm-402067_92.out:17:.........slurmstepd: error: *** JOB 402159 ON c24-02=  CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> slurm-402067_93.out:17:.........+.slurmstepd: error: *** JOB 402160 ON c24-= 02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> slurm-402067_94.out:17:.........+..slurmstepd: error: *** JOB 402161 ON c24= -02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> slurm-402067_95.out:17:.........+.slurmstepd: error: *** JOB 402162 ON c24-= 02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> slurm-402067_96.out:17:.........slurmstepd: error: *** JOB 402163 ON c24-02=  CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> slurm-402067_97.out:17:.........+slurmstepd: error: *** JOB 402164 ON c24-0= 2 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> slurm-402067_98.out:17:.........+.........+...slurmstepd: error: *** JOB 40= 2165 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO PREEMPTION ***<br> slurm-402067_99.out:17:.........+.........slurmstepd: error: *** JOB 402166=  ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO PREEMPTION ***<br> <br> <br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><div><div dir=3D""ltr""><div><div dir=3D""ltr""><div= >--------------------------------------------------------------------------= -------------------------------------------<br>Sergio DeSouza-Machado=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 <a href=3D""mailto:sergio@umbc.edu"" target= =3D""_blank"">sergio@umbc.edu</a><br>Research Assoc. Professor,=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 (W) 410-455-1944<br>JCET/Dept of Physics= =C2=A0=C2=A0 =C2=A0=C2=A0 =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0 (F) 410-455-1072<br></div><div>UMBC, Baltimore MD 21250<br>= </div></div></div></div></div></div></div> "
3281293,71973530,Correspond,DoIT-Research-Computing,2025-09-26 20:41:53.0000000,HPC Other Issue: jobs getting killed on 2024 queue,resolved,Danielle Esposito,desposi1,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"Whoops, sorry, was submitting the batch file designed for taki ...  -Sergio  On Fri, Sep 26, 2025 at 4:38=E2=80=AFPM Sergio De souza-machad <sergio@umbc= .edu> wrote:  > Hi Danielle, > > Thanks for the info > > I tried this just now > > > #SBATCH --job-name=3DFIT_TILE_TRENDS    ## name > #SBATCH -N1                           ## number of job step is to be > allocated per instance of matlab > #SBATCH --cpus-per-task 1             ## tasks per node/number of cores > per matlab session will be > > ##SBATCH --partition=3D2024              ## desired partition > #SBATCH --partition=3Dmatch             ## desired partition > #SBATCH --cluster=3Dchip-cpu            ## desired cluster > #SBATCH --account=3Dpi_strow > #SBATCH --qos=3Dshared                  ## qos to get as many cpu2024 as > possible, else put pi_strow > > and get > > bin/rm: cannot remove '*~': No such file or directory > /bin/rm: cannot remove 'slurm*.err': No such file or directory > sbatch: error: Missing: '--gres' > sbatch: error:   You must specify a Generic RESource to use in your job. > sbatch: error: See this webpage for more details: > https://hpcf.umbc.edu/compute/overview/. > sbatch: error: Batch job submission failed: Unspecified error > > which according to the webpage you pointed me to, should be used for gpu > processors? > > Thanks > > Sergio > > > On Fri, Sep 26, 2025 at 9:52=E2=80=AFAM Danielle Esposito via RT < > UMBCHelp@rt.umbc.edu> wrote: > >> If you agree your issue is resolved, please give us feedback on your >> experience by completing a brief satisfaction survey: >> >> >> https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&customeremai= l=3Dsergio%40umbc.edu&groupid=3DEIS&ticketid=3D3281293&ticketowner=3Ddespos= i1%40umbc.edu&ticketsubject=3DHPC%20Other%20Issue%3A%20jobs%20getting%20kil= led%20on%202024%20queue >> >> If you believe your issue has not been resolved, please respond to this >> message, which will reopen your ticket. Note: A full record of your requ= est >> can be found at: >> >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3281293 > >> >> Thank You >> >> _________________________________________ >> >> R e s o l u t i o n: >> =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D = =3D >> >> Hi Sergio, >> >> First off, let me elaborate on the slurm job ID's you are seeing. When >> submitting an array job, there is the main job ID (which is 402067 in th= is >> case), along with the array tasks for each (which are like, 402067_90). >> However, each individual array task also gets its own actual job ID. For >> example, the actual job ID for array task 402067_90 is 402157. >> >> Next, your job was likely canceled due to someone in pi_bennettj >> attempting to >> run a job on their nodes. Nodes c24-[01-10] are all nodes contributed fr= om >> pi_bennettj, therefore they have priority access and the ability to >> preempt >> other users not in their group. If you want to run your jobs on 2024 >> nodes, but >> do not want to risk preemption, use the 'match' partition. >> >> If you would like more information on which partitions allow/disallow >> preemption, check out this wiki page: >> >> https://umbc.atlassian.net/wiki/spaces/faq/pages/1249509377/chip+Partiti= ons+and+Usage#Partitions >> >> Let me know if that helps! Have a nice day! >> >> -- >> >> Kind regards, >> Danielle Esposito (she/her/hers) >> DoIT Unix Infra Student Worker >> >> On Thu Sep 25 16:56:36 2025, ZZ99999 wrote: >> >> > First Name:                Sergio >> > Last Name:                 De souza-machad >> > Email:                     sergio@umbc.edu >> > Campus ID:                 VR64161 >> > >> > Request Type:              High Performance Cluster >> > >> > For some reason many of my jobs are getting killed/preempted >> (slurmsteps). I thought I had an issue with the code but seems fine to m= e. >> > >> > Haven't had that before >> > >> > Is there a way to limit the jobs only to Larrabee's computer? >> > >> > Wierd this is suppose the slurm job ID is 402067, I keep getting told >> eg 402166 has been killed?? makes no sense? >> > >> > [sergio@chip-login1 AI_RTA]$ grep -in slurmst slurm* >> > >> > slurm-402067_90.out:17:.........+slurmstepd: error: *** JOB 402157 ON >> c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** >> > slurm-402067_91.out:17:.........+slurmstepd: error: *** JOB 402158 ON >> c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** >> > slurm-402067_92.out:17:.........slurmstepd: error: *** JOB 402159 ON >> c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** >> > slurm-402067_93.out:17:.........+.slurmstepd: error: *** JOB 402160 ON >> c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** >> > slurm-402067_94.out:17:.........+..slurmstepd: error: *** JOB 402161 ON >> c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** >> > slurm-402067_95.out:17:.........+.slurmstepd: error: *** JOB 402162 ON >> c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** >> > slurm-402067_96.out:17:.........slurmstepd: error: *** JOB 402163 ON >> c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** >> > slurm-402067_97.out:17:.........+slurmstepd: error: *** JOB 402164 ON >> c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** >> > slurm-402067_98.out:17:.........+.........+...slurmstepd: error: *** >> JOB 402165 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO PREEMPTION = *** >> > slurm-402067_99.out:17:.........+.........slurmstepd: error: *** JOB >> 402166 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO PREEMPTION *** >> > >> >> >> >> >> ______________________________________ >> >> Original Request: >> >> Requestors: Sergio De souza-machad >> >> First Name:                Sergio >> Last Name:                 De souza-machad >> Email:                     sergio@umbc.edu >> Campus ID:                 VR64161 >> >> Request Type:              High Performance Cluster >> >> For some reason many of my jobs are getting killed/preempted >> (slurmsteps). I thought I had an issue with the code but seems fine to m= e. >> >> Haven't had that before >> >> Is there a way to limit the jobs only to Larrabee's computer? >> >> Wierd this is suppose the slurm job ID is 402067, I keep getting told eg >> 402166 has been killed?? makes no sense? >> >> [sergio@chip-login1 AI_RTA]$ grep -in slurmst slurm* >> >> slurm-402067_90.out:17:.........+slurmstepd: error: *** JOB 402157 ON >> c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** >> slurm-402067_91.out:17:.........+slurmstepd: error: *** JOB 402158 ON >> c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** >> slurm-402067_92.out:17:.........slurmstepd: error: *** JOB 402159 ON >> c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** >> slurm-402067_93.out:17:.........+.slurmstepd: error: *** JOB 402160 ON >> c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** >> slurm-402067_94.out:17:.........+..slurmstepd: error: *** JOB 402161 ON >> c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** >> slurm-402067_95.out:17:.........+.slurmstepd: error: *** JOB 402162 ON >> c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** >> slurm-402067_96.out:17:.........slurmstepd: error: *** JOB 402163 ON >> c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** >> slurm-402067_97.out:17:.........+slurmstepd: error: *** JOB 402164 ON >> c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** >> slurm-402067_98.out:17:.........+.........+...slurmstepd: error: *** JOB >> 402165 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO PREEMPTION *** >> slurm-402067_99.out:17:.........+.........slurmstepd: error: *** JOB >> 402166 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO PREEMPTION *** >> >> >> >> > > -- > > -------------------------------------------------------------------------= -------------------------------------------- > Sergio DeSouza-Machado > sergio@umbc.edu > Research Assoc. Professor, > (W) 410-455-1944 > JCET/Dept of Physics > (F) 410-455-1072 > UMBC, Baltimore MD 21250 >   --=20 ---------------------------------------------------------------------------= ------------------------------------------ Sergio DeSouza-Machado sergio@umbc.edu Research Assoc. Professor,                                              (W) 410-455-1944 JCET/Dept of Physics (F) 410-455-1072 UMBC, Baltimore MD 21250 "
3281293,71973530,Correspond,DoIT-Research-Computing,2025-09-26 20:41:53.0000000,HPC Other Issue: jobs getting killed on 2024 queue,resolved,Danielle Esposito,desposi1,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"<div dir=3D""ltr"">Whoops, sorry, was submitting the batch file designed for = taki ...<div><br></div><div>-Sergio</div></div><br><div class=3D""gmail_quot= e gmail_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Fri, Sep = 26, 2025 at 4:38=E2=80=AFPM Sergio De souza-machad &lt;<a href=3D""mailto:se= rgio@umbc.edu"">sergio@umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D= ""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(2= 04,204,204);padding-left:1ex""><div dir=3D""ltr"">Hi Danielle,<div><br></div><= div>Thanks for the info<br><br>I tried this just now<br><blockquote style= =3D""margin:0px 0px 0px 40px;border:none;padding:0px""><div><br></div></block= quote></div><div><span style=3D""background-color:rgb(204,204,204)"">#SBATCH = --job-name=3DFIT_TILE_TRENDS =C2=A0 =C2=A0## name<br>#SBATCH -N1 =C2=A0 =C2= =A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 ## number of job step is to be allocated per instance of matlab<br>#= SBATCH --cpus-per-task 1 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 ## tasks=  per node/number of cores per matlab session will be<br><br>##SBATCH --part= ition=3D2024 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0## desired par= tition<br>#SBATCH --partition=3Dmatch =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 ## desired partition<br>#SBATCH --cluster=3Dchip-cpu =C2=A0 =C2=A0 = =C2=A0 =C2=A0 =C2=A0 =C2=A0## desired cluster<br>#SBATCH --account=3Dpi_str= ow<br>#SBATCH --qos=3Dshared =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 =C2=A0## qos to get as many cpu2024 as possible, else put pi_str= ow<br></span></div><div><br></div><div>and get<br><br><span style=3D""backgr= ound-color:rgb(204,204,204)"">bin/rm: cannot remove &#39;*~&#39;: No such fi= le or directory<br>/bin/rm: cannot remove &#39;slurm*.err&#39;: No such fil= e or directory<br>sbatch: error: Missing: &#39;--gres&#39;<br>sbatch: error= : =C2=A0	 You must specify a Generic RESource to use in your job.<br>sbatch= : error: See this webpage for more details: <a href=3D""https://hpcf.umbc.ed= u/compute/overview/"" target=3D""_blank"">https://hpcf.umbc.edu/compute/overvi= ew/</a>.<br>sbatch: error: Batch job submission failed: Unspecified error<b= r></span><br>which according to the webpage you pointed me to, should be us= ed for gpu processors?</div><div><br></div><div>Thanks</div><div><br></div>= <div>Sergio<br><br></div></div><br><div class=3D""gmail_quote""><div dir=3D""l= tr"" class=3D""gmail_attr"">On Fri, Sep 26, 2025 at 9:52=E2=80=AFAM Danielle E= sposito via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank= "">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_q= uote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,2= 04);padding-left:1ex"">If you agree your issue is resolved, please give us f= eedback on your experience by completing a brief satisfaction survey: <br> <br> <a href=3D""https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&amp;= customeremail=3Dsergio%40umbc.edu&amp;groupid=3DEIS&amp;ticketid=3D3281293&= amp;ticketowner=3Ddesposi1%40umbc.edu&amp;ticketsubject=3DHPC%20Other%20Iss= ue%3A%20jobs%20getting%20killed%20on%202024%20queue"" rel=3D""noreferrer"" tar= get=3D""_blank"">https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&= amp;customeremail=3Dsergio%40umbc.edu&amp;groupid=3DEIS&amp;ticketid=3D3281= 293&amp;ticketowner=3Ddesposi1%40umbc.edu&amp;ticketsubject=3DHPC%20Other%2= 0Issue%3A%20jobs%20getting%20killed%20on%202024%20queue</a><br> <br> If you believe your issue has not been resolved, please respond to this mes= sage, which will reopen your ticket. Note: A full record of your request ca= n be found at:=C2=A0 <br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D328= 1293"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3281293</a> &gt;<br> <br> Thank You<br> <br> _________________________________________<br> <br> R e s o l u t i o n:<br> =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D= =C2=A0 <br> <br> Hi Sergio,<br> <br> First off, let me elaborate on the slurm job ID&#39;s you are seeing. When<= br> submitting an array job, there is the main job ID (which is 402067 in this<= br> case), along with the array tasks for each (which are like, 402067_90).<br> However, each individual array task also gets its own actual job ID. For<br> example, the actual job ID for array task 402067_90 is 402157.<br> <br> Next, your job was likely canceled due to someone in pi_bennettj attempting=  to<br> run a job on their nodes. Nodes c24-[01-10] are all nodes contributed from<= br> pi_bennettj, therefore they have priority access and the ability to preempt= <br> other users not in their group. If you want to run your jobs on 2024 nodes,=  but<br> do not want to risk preemption, use the &#39;match&#39; partition.<br> <br> If you would like more information on which partitions allow/disallow<br> preemption, check out this wiki page:<br> <a href=3D""https://umbc.atlassian.net/wiki/spaces/faq/pages/1249509377/chip= +Partitions+and+Usage#Partitions"" rel=3D""noreferrer"" target=3D""_blank"">http= s://umbc.atlassian.net/wiki/spaces/faq/pages/1249509377/chip+Partitions+and= +Usage#Partitions</a><br> <br> Let me know if that helps! Have a nice day!<br> <br> --<br> <br> Kind regards,<br> Danielle Esposito (she/her/hers)<br> DoIT Unix Infra Student Worker<br> <br> On Thu Sep 25 16:56:36 2025, ZZ99999 wrote:<br> <br> &gt; First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Ser= gio<br> &gt; Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0De souza-machad<br> &gt; Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 =C2=A0<a href=3D""mailto:sergio@umbc.edu"" target=3D""_blank"">sergio@um= bc.edu</a><br> &gt; Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0VR64161<br> &gt; <br> &gt; Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Per= formance Cluster<br> &gt; <br> &gt; For some reason many of my jobs are getting killed/preempted (slurmste= ps). I thought I had an issue with the code but seems fine to me. <br> &gt; <br> &gt; Haven&#39;t had that before<br> &gt; <br> &gt; Is there a way to limit the jobs only to Larrabee&#39;s computer? <br> &gt; <br> &gt; Wierd this is suppose the slurm job ID is 402067, I keep getting told = eg 402166 has been killed?? makes no sense?<br> &gt; <br> &gt; [sergio@chip-login1 AI_RTA]$ grep -in slurmst slurm*<br> &gt; <br> &gt; slurm-402067_90.out:17:.........+slurmstepd: error: *** JOB 402157 ON = c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> &gt; slurm-402067_91.out:17:.........+slurmstepd: error: *** JOB 402158 ON = c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> &gt; slurm-402067_92.out:17:.........slurmstepd: error: *** JOB 402159 ON c= 24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> &gt; slurm-402067_93.out:17:.........+.slurmstepd: error: *** JOB 402160 ON=  c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> &gt; slurm-402067_94.out:17:.........+..slurmstepd: error: *** JOB 402161 O= N c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> &gt; slurm-402067_95.out:17:.........+.slurmstepd: error: *** JOB 402162 ON=  c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> &gt; slurm-402067_96.out:17:.........slurmstepd: error: *** JOB 402163 ON c= 24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> &gt; slurm-402067_97.out:17:.........+slurmstepd: error: *** JOB 402164 ON = c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> &gt; slurm-402067_98.out:17:.........+.........+...slurmstepd: error: *** J= OB 402165 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO PREEMPTION ***<= br> &gt; slurm-402067_99.out:17:.........+.........slurmstepd: error: *** JOB 4= 02166 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO PREEMPTION ***<br> &gt; <br> <br> <br> <br> <br> ______________________________________<br> <br> Original Request:<br> <br> Requestors: Sergio De souza-machad<br> <br> First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Sergio<b= r> Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0De = souza-machad<br> Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=  =C2=A0<a href=3D""mailto:sergio@umbc.edu"" target=3D""_blank"">sergio@umbc.edu= </a><br> Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0VR6= 4161<br> <br> Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Performa= nce Cluster<br> <br> For some reason many of my jobs are getting killed/preempted (slurmsteps). = I thought I had an issue with the code but seems fine to me. <br> <br> Haven&#39;t had that before<br> <br> Is there a way to limit the jobs only to Larrabee&#39;s computer? <br> <br> Wierd this is suppose the slurm job ID is 402067, I keep getting told eg 40= 2166 has been killed?? makes no sense?<br> <br> [sergio@chip-login1 AI_RTA]$ grep -in slurmst slurm*<br> <br> slurm-402067_90.out:17:.........+slurmstepd: error: *** JOB 402157 ON c24-0= 2 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> slurm-402067_91.out:17:.........+slurmstepd: error: *** JOB 402158 ON c24-0= 2 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> slurm-402067_92.out:17:.........slurmstepd: error: *** JOB 402159 ON c24-02=  CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> slurm-402067_93.out:17:.........+.slurmstepd: error: *** JOB 402160 ON c24-= 02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> slurm-402067_94.out:17:.........+..slurmstepd: error: *** JOB 402161 ON c24= -02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> slurm-402067_95.out:17:.........+.slurmstepd: error: *** JOB 402162 ON c24-= 02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> slurm-402067_96.out:17:.........slurmstepd: error: *** JOB 402163 ON c24-02=  CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> slurm-402067_97.out:17:.........+slurmstepd: error: *** JOB 402164 ON c24-0= 2 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br> slurm-402067_98.out:17:.........+.........+...slurmstepd: error: *** JOB 40= 2165 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO PREEMPTION ***<br> slurm-402067_99.out:17:.........+.........slurmstepd: error: *** JOB 402166=  ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO PREEMPTION ***<br> <br> <br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><div><div dir=3D""ltr""><div><div dir=3D""ltr""><div= >--------------------------------------------------------------------------= -------------------------------------------<br>Sergio DeSouza-Machado=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 <a href=3D""mailto:sergio@umbc.edu"" target= =3D""_blank"">sergio@umbc.edu</a><br>Research Assoc. Professor,=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 (W) 410-455-1944<br>JCET/Dept of Physics= =C2=A0=C2=A0 =C2=A0=C2=A0 =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0 (F) 410-455-1072<br></div><div>UMBC, Baltimore MD 21250<br>= </div></div></div></div></div></div></div> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><div><div dir=3D""ltr""><div><div dir=3D""ltr""><div= >--------------------------------------------------------------------------= -------------------------------------------<br>Sergio DeSouza-Machado=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 <a href=3D""mailto:sergio@umbc.edu"" target= =3D""_blank"">sergio@umbc.edu</a><br>Research Assoc. Professor,=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 (W) 410-455-1944<br>JCET/Dept of Physics= =C2=A0=C2=A0 =C2=A0=C2=A0 =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0 (F) 410-455-1072<br></div><div>UMBC, Baltimore MD 21250<br>= </div></div></div></div></div></div></div> "
3281293,71990940,Correspond,DoIT-Research-Computing,2025-09-29 16:02:57.0000000,HPC Other Issue: jobs getting killed on 2024 queue,resolved,Danielle Esposito,desposi1,Sergio De souza-machad,sergio,sergio@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>I see. Is everything working as intended now?&nbsp;</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Fri Sep 26 16:41:53 2025, VR64161 wrote: <blockquote> <div>Whoops, sorry, was submitting the batch file designed for taki ... <div>&nbsp;</div>  <div>-Sergio</div> </div> &nbsp;  <div> <div>On Fri, Sep 26, 2025 at 4:38=E2=80=AFPM Sergio De souza-machad &lt;ser= gio@umbc.edu&gt; wrote:</div>  <blockquote> <div>Hi Danielle, <div>&nbsp;</div>  <div>Thanks for the info<br /> <br /> I tried this just now <blockquote> <div>&nbsp;</div> </blockquote> </div>  <div><span style=3D""background-color:#cccccc"">#SBATCH --job-name=3DFIT_TILE= _TRENDS &nbsp; &nbsp;## name<br /> #SBATCH -N1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = &nbsp; &nbsp; &nbsp; &nbsp; ## number of job step is to be allocated per in= stance of matlab<br /> #SBATCH --cpus-per-task 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ## task= s per node/number of cores per matlab session will be<br /> <br /> ##SBATCH --partition=3D2024 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp= ;## desired partition<br /> #SBATCH --partition=3Dmatch &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ## de= sired partition<br /> #SBATCH --cluster=3Dchip-cpu &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;## de= sired cluster<br /> #SBATCH --account=3Dpi_strow<br /> #SBATCH --qos=3Dshared &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nb= sp; &nbsp;## qos to get as many cpu2024 as possible, else put pi_strow</spa= n></div>  <div>&nbsp;</div>  <div>and get<br /> <br /> <span style=3D""background-color:#cccccc"">bin/rm: cannot remove &#39;*~&#39;= : No such file or directory<br /> /bin/rm: cannot remove &#39;slurm*.err&#39;: No such file or directory<br /> sbatch: error: Missing: &#39;--gres&#39;<br /> sbatch: error: &nbsp; You must specify a Generic RESource to use in your jo= b.<br /> sbatch: error: See this webpage for more details: https://hpcf.umbc.edu/com= pute/overview/.<br /> sbatch: error: Batch job submission failed: Unspecified error</span><br /> <br /> which according to the webpage you pointed me to, should be used for gpu pr= ocessors?</div>  <div>&nbsp;</div>  <div>Thanks</div>  <div>&nbsp;</div>  <div>Sergio<br /> &nbsp;</div> </div> &nbsp;  <div> <div>On Fri, Sep 26, 2025 at 9:52=E2=80=AFAM Danielle Esposito via RT &lt;U= MBCHelp@rt.umbc.edu&gt; wrote:</div>  <blockquote>If you agree your issue is resolved, please give us feedback on=  your experience by completing a brief satisfaction survey:<br /> <br /> https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&amp;customerema= il=3Dsergio%40umbc.edu&amp;groupid=3DEIS&amp;ticketid=3D3281293&amp;ticketo= wner=3Ddesposi1%40umbc.edu&amp;ticketsubject=3DHPC%20Other%20Issue%3A%20job= s%20getting%20killed%20on%202024%20queue<br /> <br /> If you believe your issue has not been resolved, please respond to this mes= sage, which will reopen your ticket. Note: A full record of your request ca= n be found at:&nbsp;<br /> <br /> Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3281293 &gt;<b= r /> <br /> Thank You<br /> <br /> _________________________________________<br /> <br /> R e s o l u t i o n:<br /> =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D= &nbsp;<br /> <br /> Hi Sergio,<br /> <br /> First off, let me elaborate on the slurm job ID&#39;s you are seeing. When<= br /> submitting an array job, there is the main job ID (which is 402067 in this<= br /> case), along with the array tasks for each (which are like, 402067_90).<br = /> However, each individual array task also gets its own actual job ID. For<br=  /> example, the actual job ID for array task 402067_90 is 402157.<br /> <br /> Next, your job was likely canceled due to someone in pi_bennettj attempting=  to<br /> run a job on their nodes. Nodes c24-[01-10] are all nodes contributed from<= br /> pi_bennettj, therefore they have priority access and the ability to preempt= <br /> other users not in their group. If you want to run your jobs on 2024 nodes,=  but<br /> do not want to risk preemption, use the &#39;match&#39; partition.<br /> <br /> If you would like more information on which partitions allow/disallow<br /> preemption, check out this wiki page:<br /> https://umbc.atlassian.net/wiki/spaces/faq/pages/1249509377/chip+Partitions= +and+Usage#Partitions<br /> <br /> Let me know if that helps! Have a nice day!<br /> <br /> --<br /> <br /> Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker<br /> <br /> On Thu Sep 25 16:56:36 2025, ZZ99999 wrote:<br /> <br /> &gt; First Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ser= gio<br /> &gt; Last Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p;De souza-machad<br /> &gt; Email:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &= nbsp; &nbsp;sergio@umbc.edu<br /> &gt; Campus ID:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p;VR64161<br /> &gt;<br /> &gt; Request Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; High Per= formance Cluster<br /> &gt;<br /> &gt; For some reason many of my jobs are getting killed/preempted (slurmste= ps). I thought I had an issue with the code but seems fine to me.<br /> &gt;<br /> &gt; Haven&#39;t had that before<br /> &gt;<br /> &gt; Is there a way to limit the jobs only to Larrabee&#39;s computer?<br /> &gt;<br /> &gt; Wierd this is suppose the slurm job ID is 402067, I keep getting told = eg 402166 has been killed?? makes no sense?<br /> &gt;<br /> &gt; [sergio@chip-login1 AI_RTA]$ grep -in slurmst slurm*<br /> &gt;<br /> &gt; slurm-402067_90.out:17:.........+slurmstepd: error: *** JOB 402157 ON = c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br /> &gt; slurm-402067_91.out:17:.........+slurmstepd: error: *** JOB 402158 ON = c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br /> &gt; slurm-402067_92.out:17:.........slurmstepd: error: *** JOB 402159 ON c= 24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br /> &gt; slurm-402067_93.out:17:.........+.slurmstepd: error: *** JOB 402160 ON=  c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br /> &gt; slurm-402067_94.out:17:.........+..slurmstepd: error: *** JOB 402161 O= N c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br /> &gt; slurm-402067_95.out:17:.........+.slurmstepd: error: *** JOB 402162 ON=  c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br /> &gt; slurm-402067_96.out:17:.........slurmstepd: error: *** JOB 402163 ON c= 24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br /> &gt; slurm-402067_97.out:17:.........+slurmstepd: error: *** JOB 402164 ON = c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br /> &gt; slurm-402067_98.out:17:.........+.........+...slurmstepd: error: *** J= OB 402165 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO PREEMPTION ***<= br /> &gt; slurm-402067_99.out:17:.........+.........slurmstepd: error: *** JOB 4= 02166 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO PREEMPTION ***<br /> &gt;<br /> <br /> <br /> <br /> <br /> ______________________________________<br /> <br /> Original Request:<br /> <br /> Requestors: Sergio De souza-machad<br /> <br /> First Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Sergio<b= r /> Last Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;De = souza-machad<br /> Email:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;=  &nbsp;sergio@umbc.edu<br /> Campus ID:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;VR6= 4161<br /> <br /> Request Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; High Performa= nce Cluster<br /> <br /> For some reason many of my jobs are getting killed/preempted (slurmsteps). = I thought I had an issue with the code but seems fine to me.<br /> <br /> Haven&#39;t had that before<br /> <br /> Is there a way to limit the jobs only to Larrabee&#39;s computer?<br /> <br /> Wierd this is suppose the slurm job ID is 402067, I keep getting told eg 40= 2166 has been killed?? makes no sense?<br /> <br /> [sergio@chip-login1 AI_RTA]$ grep -in slurmst slurm*<br /> <br /> slurm-402067_90.out:17:.........+slurmstepd: error: *** JOB 402157 ON c24-0= 2 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br /> slurm-402067_91.out:17:.........+slurmstepd: error: *** JOB 402158 ON c24-0= 2 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br /> slurm-402067_92.out:17:.........slurmstepd: error: *** JOB 402159 ON c24-02=  CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br /> slurm-402067_93.out:17:.........+.slurmstepd: error: *** JOB 402160 ON c24-= 02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br /> slurm-402067_94.out:17:.........+..slurmstepd: error: *** JOB 402161 ON c24= -02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br /> slurm-402067_95.out:17:.........+.slurmstepd: error: *** JOB 402162 ON c24-= 02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br /> slurm-402067_96.out:17:.........slurmstepd: error: *** JOB 402163 ON c24-02=  CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br /> slurm-402067_97.out:17:.........+slurmstepd: error: *** JOB 402164 ON c24-0= 2 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION ***<br /> slurm-402067_98.out:17:.........+.........+...slurmstepd: error: *** JOB 40= 2165 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO PREEMPTION ***<br /> slurm-402067_99.out:17:.........+.........slurmstepd: error: *** JOB 402166=  ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO PREEMPTION ***<br /> <br /> <br /> &nbsp;</blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --  <div> <div> <div> <div> <div> <div> <div>----------------------------------------------------------------------= -----------------------------------------------<br /> Sergio DeSouza-Machado&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sergio@umbc.e= du<br /> Research Assoc. Professor,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&= nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbs= p;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&= nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (W)=  410-455-1944<br /> JCET/Dept of Physics&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (F) 410-455-1072</div>  <div>UMBC, Baltimore MD 21250</div> </div> </div> </div> </div> </div> </div> </blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --  <div> <div> <div> <div> <div> <div> <div>----------------------------------------------------------------------= -----------------------------------------------<br /> Sergio DeSouza-Machado&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sergio@umbc.e= du<br /> Research Assoc. Professor,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&= nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbs= p;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&= nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (W)=  410-455-1944<br /> JCET/Dept of Physics&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (F) 410-455-1072</div>  <div>UMBC, Baltimore MD 21250</div> </div> </div> </div> </div> </div> </div> </blockquote> </div> "
3281293,71992213,Correspond,DoIT-Research-Computing,2025-09-29 16:34:21.0000000,HPC Other Issue: jobs getting killed on 2024 queue,resolved,Danielle Esposito,desposi1,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"Yes, had much better success over the weekend,  Thanks,  Sergio   On Mon, Sep 29, 2025 at 12:03=E2=80=AFPM Danielle Esposito via RT < UMBCHelp@rt.umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3281293 > > > Last Update From Ticket: > > I see. Is everything working as intended now? > > -- > > Kind regards, > Danielle Esposito (she/her/hers) > DoIT Unix Infra Student Worker > > On Fri Sep 26 16:41:53 2025, VR64161 wrote: > > > Whoops, sorry, was submitting the batch file designed for taki ... > -Sergio > > On Fri, Sep 26, 2025 at 4:38 PM Sergio De souza-machad <sergio@umbc.edu> > > wrote: > > >> Hi Danielle, Thanks for the info > > >> I tried this just now > > >> #SBATCH --job-name=3DFIT_TILE_TRENDS ## name > >> #SBATCH -N1 ## number of job step is to be allocated per instance of > >> matlab > >> #SBATCH --cpus-per-task 1 ## tasks per node/number of cores per matlab > >> session will be > > >> ##SBATCH --partition=3D2024 ## desired partition > >> #SBATCH --partition=3Dmatch ## desired partition > >> #SBATCH --cluster=3Dchip-cpu ## desired cluster > >> #SBATCH --account=3Dpi_strow > >> #SBATCH --qos=3Dshared ## qos to get as many cpu2024 as possible, else > >> put pi_strow and get > > >> bin/rm: cannot remove '*~': No such file or directory > >> /bin/rm: cannot remove 'slurm*.err': No such file or directory > >> sbatch: error: Missing: '--gres' > >> sbatch: error: You must specify a Generic RESource to use in your job. > >> sbatch: error: See this webpage for more details: > >> https://hpcf.umbc.edu/compute/overview/. > >> sbatch: error: Batch job submission failed: Unspecified error > > >> which according to the webpage you pointed me to, should be used for > >> gpu processors? Thanks Sergio > >> On Fri, Sep 26, 2025 at 9:52 AM Danielle Esposito via RT > >> <UMBCHelp@rt.umbc.edu> wrote: > > >>> If you agree your issue is resolved, please give us feedback on > >>> your experience by completing a brief satisfaction survey: > > >>> > https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&customeremail= =3Dsergio%40umbc.edu&groupid=3DEIS&ticketid=3D3281293&ticketowner=3Ddesposi= 1%40umbc.edu&ticketsubject=3DHPC%20Other%20Issue%3A%20jobs%20getting%20kill= ed%20on%202024%20queue > > >>> If you believe your issue has not been resolved, please respond to > >>> this message, which will reopen your ticket. Note: A full record of > >>> your request can be found at: > > >>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3281293 > > > >>> Thank You > > >>> _________________________________________ > > >>> R e s o l u t i o n: > >>> =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D = =3D =3D > > >>> Hi Sergio, > > >>> First off, let me elaborate on the slurm job ID's you are seeing. > >>> When > >>> submitting an array job, there is the main job ID (which is 402067 > >>> in this > >>> case), along with the array tasks for each (which are like, > >>> 402067_90). > >>> However, each individual array task also gets its own actual job > >>> ID. For > >>> example, the actual job ID for array task 402067_90 is 402157. > > >>> Next, your job was likely canceled due to someone in pi_bennettj > >>> attempting to > >>> run a job on their nodes. Nodes c24-[01-10] are all nodes > >>> contributed from > >>> pi_bennettj, therefore they have priority access and the ability to > >>> preempt > >>> other users not in their group. If you want to run your jobs on > >>> 2024 nodes, but > >>> do not want to risk preemption, use the 'match' partition. > > >>> If you would like more information on which partitions > >>> allow/disallow > >>> preemption, check out this wiki page: > >>> > https://umbc.atlassian.net/wiki/spaces/faq/pages/1249509377/chip+Partitio= ns+and+Usage#Partitions > > >>> Let me know if that helps! Have a nice day! > > >>> -- > > >>> Kind regards, > >>> Danielle Esposito (she/her/hers) > >>> DoIT Unix Infra Student Worker > > >>> On Thu Sep 25 16:56:36 2025, ZZ99999 wrote: > > >>> > First Name: Sergio > >>> > Last Name: De souza-machad > >>> > Email: sergio@umbc.edu > >>> > Campus ID: VR64161 > >>> > > >>> > Request Type: High Performance Cluster > >>> > > >>> > For some reason many of my jobs are getting killed/preempted > >>> (slurmsteps). I thought I had an issue with the code but seems fine > >>> to me. > >>> > > >>> > Haven't had that before > >>> > > >>> > Is there a way to limit the jobs only to Larrabee's computer? > >>> > > >>> > Wierd this is suppose the slurm job ID is 402067, I keep getting > >>> told eg 402166 has been killed?? makes no sense? > >>> > > >>> > [sergio@chip-login1 AI_RTA]$ grep -in slurmst slurm* > >>> > > >>> > slurm-402067_90.out:17:.........+slurmstepd: error: *** JOB > >>> 402157 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION > >>> *** > >>> > slurm-402067_91.out:17:.........+slurmstepd: error: *** JOB > >>> 402158 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION > >>> *** > >>> > slurm-402067_92.out:17:.........slurmstepd: error: *** JOB 402159 > >>> ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** > >>> > slurm-402067_93.out:17:.........+.slurmstepd: error: *** JOB > >>> 402160 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION > >>> *** > >>> > slurm-402067_94.out:17:.........+..slurmstepd: error: *** JOB > >>> 402161 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION > >>> *** > >>> > slurm-402067_95.out:17:.........+.slurmstepd: error: *** JOB > >>> 402162 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION > >>> *** > >>> > slurm-402067_96.out:17:.........slurmstepd: error: *** JOB 402163 > >>> ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** > >>> > slurm-402067_97.out:17:.........+slurmstepd: error: *** JOB > >>> 402164 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION > >>> *** > >>> > slurm-402067_98.out:17:.........+.........+...slurmstepd: error: > >>> *** JOB 402165 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO > >>> PREEMPTION *** > >>> > slurm-402067_99.out:17:.........+.........slurmstepd: error: *** > >>> JOB 402166 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO > >>> PREEMPTION *** > >>> > > > > > > >>> ______________________________________ > > >>> Original Request: > > >>> Requestors: Sergio De souza-machad > > >>> First Name: Sergio > >>> Last Name: De souza-machad > >>> Email: sergio@umbc.edu > >>> Campus ID: VR64161 > > >>> Request Type: High Performance Cluster > > >>> For some reason many of my jobs are getting killed/preempted > >>> (slurmsteps). I thought I had an issue with the code but seems fine > >>> to me. > > >>> Haven't had that before > > >>> Is there a way to limit the jobs only to Larrabee's computer? > > >>> Wierd this is suppose the slurm job ID is 402067, I keep getting > >>> told eg 402166 has been killed?? makes no sense? > > >>> [sergio@chip-login1 AI_RTA]$ grep -in slurmst slurm* > > >>> slurm-402067_90.out:17:.........+slurmstepd: error: *** JOB 402157 > >>> ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** > >>> slurm-402067_91.out:17:.........+slurmstepd: error: *** JOB 402158 > >>> ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** > >>> slurm-402067_92.out:17:.........slurmstepd: error: *** JOB 402159 > >>> ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** > >>> slurm-402067_93.out:17:.........+.slurmstepd: error: *** JOB 402160 > >>> ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** > >>> slurm-402067_94.out:17:.........+..slurmstepd: error: *** JOB > >>> 402161 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION > >>> *** > >>> slurm-402067_95.out:17:.........+.slurmstepd: error: *** JOB 402162 > >>> ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** > >>> slurm-402067_96.out:17:.........slurmstepd: error: *** JOB 402163 > >>> ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** > >>> slurm-402067_97.out:17:.........+slurmstepd: error: *** JOB 402164 > >>> ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *** > >>> slurm-402067_98.out:17:.........+.........+...slurmstepd: error: > >>> *** JOB 402165 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO > >>> PREEMPTION *** > >>> slurm-402067_99.out:17:.........+.........slurmstepd: error: *** > >>> JOB 402166 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO > >>> PREEMPTION *** > > > >> -- > >> > -------------------------------------------------------------------------= -------------------------------------------- > >> Sergio DeSouza-Machado sergio@umbc.edu > >> Research Assoc. Professor, (W) 410-455-1944 > >> JCET/Dept of Physics (F) 410-455-1072UMBC, Baltimore MD 21250 > > > -- > > > -------------------------------------------------------------------------= -------------------------------------------- > > Sergio DeSouza-Machado sergio@umbc.edu > > Research Assoc. Professor, (W) 410-455-1944 > > JCET/Dept of Physics (F) 410-455-1072UMBC, Baltimore MD 21250 > >  --=20 ---------------------------------------------------------------------------= ------------------------------------------ Sergio DeSouza-Machado sergio@umbc.edu Research Assoc. Professor,                                              (W) 410-455-1944 JCET/Dept of Physics (F) 410-455-1072 UMBC, Baltimore MD 21250 "
3281293,71992213,Correspond,DoIT-Research-Computing,2025-09-29 16:34:21.0000000,HPC Other Issue: jobs getting killed on 2024 queue,resolved,Danielle Esposito,desposi1,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"<div dir=3D""ltr"">Yes, had much better success over the weekend,<div><br></d= iv><div>Thanks,</div><div><br></div><div>Sergio</div><div><br></div></div><= br><div class=3D""gmail_quote gmail_quote_container""><div dir=3D""ltr"" class= =3D""gmail_attr"">On Mon, Sep 29, 2025 at 12:03=E2=80=AFPM Danielle Esposito = via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>= &gt; wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""margin:0px = 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"">Tick= et &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D3281293= "" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.h= tml?id=3D3281293</a> &gt;<br> <br> Last Update From Ticket:<br> <br> I see. Is everything working as intended now?<br> <br> --<br> <br> Kind regards,<br> Danielle Esposito (she/her/hers)<br> DoIT Unix Infra Student Worker<br> <br> On Fri Sep 26 16:41:53 2025, VR64161 wrote:<br> <br> &gt; Whoops, sorry, was submitting the batch file designed for taki ... -Se= rgio<br> &gt; On Fri, Sep 26, 2025 at 4:38 PM Sergio De souza-machad &lt;<a href=3D""= mailto:sergio@umbc.edu"" target=3D""_blank"">sergio@umbc.edu</a>&gt;<br> &gt; wrote:<br> <br> &gt;&gt; Hi Danielle, Thanks for the info<br> <br> &gt;&gt; I tried this just now<br> <br> &gt;&gt; #SBATCH --job-name=3DFIT_TILE_TRENDS ## name<br> &gt;&gt; #SBATCH -N1 ## number of job step is to be allocated per instance = of<br> &gt;&gt; matlab<br> &gt;&gt; #SBATCH --cpus-per-task 1 ## tasks per node/number of cores per ma= tlab<br> &gt;&gt; session will be<br> <br> &gt;&gt; ##SBATCH --partition=3D2024 ## desired partition<br> &gt;&gt; #SBATCH --partition=3Dmatch ## desired partition<br> &gt;&gt; #SBATCH --cluster=3Dchip-cpu ## desired cluster<br> &gt;&gt; #SBATCH --account=3Dpi_strow<br> &gt;&gt; #SBATCH --qos=3Dshared ## qos to get as many cpu2024 as possible, = else<br> &gt;&gt; put pi_strow and get<br> <br> &gt;&gt; bin/rm: cannot remove &#39;*~&#39;: No such file or directory<br> &gt;&gt; /bin/rm: cannot remove &#39;slurm*.err&#39;: No such file or direc= tory<br> &gt;&gt; sbatch: error: Missing: &#39;--gres&#39;<br> &gt;&gt; sbatch: error: You must specify a Generic RESource to use in your = job.<br> &gt;&gt; sbatch: error: See this webpage for more details:<br> &gt;&gt; <a href=3D""https://hpcf.umbc.edu/compute/overview/"" rel=3D""norefer= rer"" target=3D""_blank"">https://hpcf.umbc.edu/compute/overview/</a>.<br> &gt;&gt; sbatch: error: Batch job submission failed: Unspecified error<br> <br> &gt;&gt; which according to the webpage you pointed me to, should be used f= or<br> &gt;&gt; gpu processors? Thanks Sergio<br> &gt;&gt; On Fri, Sep 26, 2025 at 9:52 AM Danielle Esposito via RT<br> &gt;&gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBC= Help@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt;&gt; If you agree your issue is resolved, please give us feedback o= n<br> &gt;&gt;&gt; your experience by completing a brief satisfaction survey:<br> <br> &gt;&gt;&gt; <a href=3D""https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3= MTISF6Ly&amp;customeremail=3Dsergio%40umbc.edu&amp;groupid=3DEIS&amp;ticket= id=3D3281293&amp;ticketowner=3Ddesposi1%40umbc.edu&amp;ticketsubject=3DHPC%= 20Other%20Issue%3A%20jobs%20getting%20killed%20on%202024%20queue"" rel=3D""no= referrer"" target=3D""_blank"">https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etf= DUq3MTISF6Ly&amp;customeremail=3Dsergio%40umbc.edu&amp;groupid=3DEIS&amp;ti= cketid=3D3281293&amp;ticketowner=3Ddesposi1%40umbc.edu&amp;ticketsubject=3D= HPC%20Other%20Issue%3A%20jobs%20getting%20killed%20on%202024%20queue</a><br> <br> &gt;&gt;&gt; If you believe your issue has not been resolved, please respon= d to<br> &gt;&gt;&gt; this message, which will reopen your ticket. Note: A full reco= rd of<br> &gt;&gt;&gt; your request can be found at:<br> <br> &gt;&gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.= html?id=3D3281293"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu= /Ticket/Display.html?id=3D3281293</a> &gt;<br> <br> &gt;&gt;&gt; Thank You<br> <br> &gt;&gt;&gt; _________________________________________<br> <br> &gt;&gt;&gt; R e s o l u t i o n:<br> &gt;&gt;&gt; =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D = =3D =3D =3D =3D<br> <br> &gt;&gt;&gt; Hi Sergio,<br> <br> &gt;&gt;&gt; First off, let me elaborate on the slurm job ID&#39;s you are = seeing.<br> &gt;&gt;&gt; When<br> &gt;&gt;&gt; submitting an array job, there is the main job ID (which is 40= 2067<br> &gt;&gt;&gt; in this<br> &gt;&gt;&gt; case), along with the array tasks for each (which are like,<br> &gt;&gt;&gt; 402067_90).<br> &gt;&gt;&gt; However, each individual array task also gets its own actual j= ob<br> &gt;&gt;&gt; ID. For<br> &gt;&gt;&gt; example, the actual job ID for array task 402067_90 is 402157.= <br> <br> &gt;&gt;&gt; Next, your job was likely canceled due to someone in pi_bennet= tj<br> &gt;&gt;&gt; attempting to<br> &gt;&gt;&gt; run a job on their nodes. Nodes c24-[01-10] are all nodes<br> &gt;&gt;&gt; contributed from<br> &gt;&gt;&gt; pi_bennettj, therefore they have priority access and the abili= ty to<br> &gt;&gt;&gt; preempt<br> &gt;&gt;&gt; other users not in their group. If you want to run your jobs o= n<br> &gt;&gt;&gt; 2024 nodes, but<br> &gt;&gt;&gt; do not want to risk preemption, use the &#39;match&#39; partit= ion.<br> <br> &gt;&gt;&gt; If you would like more information on which partitions<br> &gt;&gt;&gt; allow/disallow<br> &gt;&gt;&gt; preemption, check out this wiki page:<br> &gt;&gt;&gt; <a href=3D""https://umbc.atlassian.net/wiki/spaces/faq/pages/12= 49509377/chip+Partitions+and+Usage#Partitions"" rel=3D""noreferrer"" target=3D= ""_blank"">https://umbc.atlassian.net/wiki/spaces/faq/pages/1249509377/chip+P= artitions+and+Usage#Partitions</a><br> <br> &gt;&gt;&gt; Let me know if that helps! Have a nice day!<br> <br> &gt;&gt;&gt; --<br> <br> &gt;&gt;&gt; Kind regards,<br> &gt;&gt;&gt; Danielle Esposito (she/her/hers)<br> &gt;&gt;&gt; DoIT Unix Infra Student Worker<br> <br> &gt;&gt;&gt; On Thu Sep 25 16:56:36 2025, ZZ99999 wrote:<br> <br> &gt;&gt;&gt; &gt; First Name: Sergio<br> &gt;&gt;&gt; &gt; Last Name: De souza-machad<br> &gt;&gt;&gt; &gt; Email: <a href=3D""mailto:sergio@umbc.edu"" target=3D""_blan= k"">sergio@umbc.edu</a><br> &gt;&gt;&gt; &gt; Campus ID: VR64161<br> &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt; Request Type: High Performance Cluster<br> &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt; For some reason many of my jobs are getting killed/preemp= ted<br> &gt;&gt;&gt; (slurmsteps). I thought I had an issue with the code but seems=  fine<br> &gt;&gt;&gt; to me.<br> &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt; Haven&#39;t had that before<br> &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt; Is there a way to limit the jobs only to Larrabee&#39;s c= omputer?<br> &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt; Wierd this is suppose the slurm job ID is 402067, I keep = getting<br> &gt;&gt;&gt; told eg 402166 has been killed?? makes no sense?<br> &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt; [sergio@chip-login1 AI_RTA]$ grep -in slurmst slurm*<br> &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt; slurm-402067_90.out:17:.........+slurmstepd: error: *** J= OB<br> &gt;&gt;&gt; 402157 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEM= PTION<br> &gt;&gt;&gt; ***<br> &gt;&gt;&gt; &gt; slurm-402067_91.out:17:.........+slurmstepd: error: *** J= OB<br> &gt;&gt;&gt; 402158 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEM= PTION<br> &gt;&gt;&gt; ***<br> &gt;&gt;&gt; &gt; slurm-402067_92.out:17:.........slurmstepd: error: *** JO= B 402159<br> &gt;&gt;&gt; ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *= **<br> &gt;&gt;&gt; &gt; slurm-402067_93.out:17:.........+.slurmstepd: error: *** = JOB<br> &gt;&gt;&gt; 402160 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEM= PTION<br> &gt;&gt;&gt; ***<br> &gt;&gt;&gt; &gt; slurm-402067_94.out:17:.........+..slurmstepd: error: ***=  JOB<br> &gt;&gt;&gt; 402161 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEM= PTION<br> &gt;&gt;&gt; ***<br> &gt;&gt;&gt; &gt; slurm-402067_95.out:17:.........+.slurmstepd: error: *** = JOB<br> &gt;&gt;&gt; 402162 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEM= PTION<br> &gt;&gt;&gt; ***<br> &gt;&gt;&gt; &gt; slurm-402067_96.out:17:.........slurmstepd: error: *** JO= B 402163<br> &gt;&gt;&gt; ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *= **<br> &gt;&gt;&gt; &gt; slurm-402067_97.out:17:.........+slurmstepd: error: *** J= OB<br> &gt;&gt;&gt; 402164 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEM= PTION<br> &gt;&gt;&gt; ***<br> &gt;&gt;&gt; &gt; slurm-402067_98.out:17:.........+.........+...slurmstepd:=  error:<br> &gt;&gt;&gt; *** JOB 402165 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE = TO<br> &gt;&gt;&gt; PREEMPTION ***<br> &gt;&gt;&gt; &gt; slurm-402067_99.out:17:.........+.........slurmstepd: err= or: ***<br> &gt;&gt;&gt; JOB 402166 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO<b= r> &gt;&gt;&gt; PREEMPTION ***<br> &gt;&gt;&gt; &gt;<br> <br> <br> <br> <br> &gt;&gt;&gt; ______________________________________<br> <br> &gt;&gt;&gt; Original Request:<br> <br> &gt;&gt;&gt; Requestors: Sergio De souza-machad<br> <br> &gt;&gt;&gt; First Name: Sergio<br> &gt;&gt;&gt; Last Name: De souza-machad<br> &gt;&gt;&gt; Email: <a href=3D""mailto:sergio@umbc.edu"" target=3D""_blank"">se= rgio@umbc.edu</a><br> &gt;&gt;&gt; Campus ID: VR64161<br> <br> &gt;&gt;&gt; Request Type: High Performance Cluster<br> <br> &gt;&gt;&gt; For some reason many of my jobs are getting killed/preempted<b= r> &gt;&gt;&gt; (slurmsteps). I thought I had an issue with the code but seems=  fine<br> &gt;&gt;&gt; to me.<br> <br> &gt;&gt;&gt; Haven&#39;t had that before<br> <br> &gt;&gt;&gt; Is there a way to limit the jobs only to Larrabee&#39;s comput= er?<br> <br> &gt;&gt;&gt; Wierd this is suppose the slurm job ID is 402067, I keep getti= ng<br> &gt;&gt;&gt; told eg 402166 has been killed?? makes no sense?<br> <br> &gt;&gt;&gt; [sergio@chip-login1 AI_RTA]$ grep -in slurmst slurm*<br> <br> &gt;&gt;&gt; slurm-402067_90.out:17:.........+slurmstepd: error: *** JOB 40= 2157<br> &gt;&gt;&gt; ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *= **<br> &gt;&gt;&gt; slurm-402067_91.out:17:.........+slurmstepd: error: *** JOB 40= 2158<br> &gt;&gt;&gt; ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *= **<br> &gt;&gt;&gt; slurm-402067_92.out:17:.........slurmstepd: error: *** JOB 402= 159<br> &gt;&gt;&gt; ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *= **<br> &gt;&gt;&gt; slurm-402067_93.out:17:.........+.slurmstepd: error: *** JOB 4= 02160<br> &gt;&gt;&gt; ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *= **<br> &gt;&gt;&gt; slurm-402067_94.out:17:.........+..slurmstepd: error: *** JOB<= br> &gt;&gt;&gt; 402161 ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEM= PTION<br> &gt;&gt;&gt; ***<br> &gt;&gt;&gt; slurm-402067_95.out:17:.........+.slurmstepd: error: *** JOB 4= 02162<br> &gt;&gt;&gt; ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *= **<br> &gt;&gt;&gt; slurm-402067_96.out:17:.........slurmstepd: error: *** JOB 402= 163<br> &gt;&gt;&gt; ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *= **<br> &gt;&gt;&gt; slurm-402067_97.out:17:.........+slurmstepd: error: *** JOB 40= 2164<br> &gt;&gt;&gt; ON c24-02 CANCELLED AT 2025-09-25T15:58:40 DUE TO PREEMPTION *= **<br> &gt;&gt;&gt; slurm-402067_98.out:17:.........+.........+...slurmstepd: erro= r:<br> &gt;&gt;&gt; *** JOB 402165 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE = TO<br> &gt;&gt;&gt; PREEMPTION ***<br> &gt;&gt;&gt; slurm-402067_99.out:17:.........+.........slurmstepd: error: *= **<br> &gt;&gt;&gt; JOB 402166 ON c24-03 CANCELLED AT 2025-09-25T16:00:57 DUE TO<b= r> &gt;&gt;&gt; PREEMPTION ***<br> <br> <br> &gt;&gt; --<br> &gt;&gt; ------------------------------------------------------------------= ---------------------------------------------------<br> &gt;&gt; Sergio DeSouza-Machado <a href=3D""mailto:sergio@umbc.edu"" target= =3D""_blank"">sergio@umbc.edu</a><br> &gt;&gt; Research Assoc. Professor, (W) 410-455-1944<br> &gt;&gt; JCET/Dept of Physics (F) 410-455-1072UMBC, Baltimore MD 21250<br> <br> &gt; --<br> &gt; ----------------------------------------------------------------------= -----------------------------------------------<br> &gt; Sergio DeSouza-Machado <a href=3D""mailto:sergio@umbc.edu"" target=3D""_b= lank"">sergio@umbc.edu</a><br> &gt; Research Assoc. Professor, (W) 410-455-1944<br> &gt; JCET/Dept of Physics (F) 410-455-1072UMBC, Baltimore MD 21250<br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><div><div dir=3D""ltr""><div><div dir=3D""ltr""><div= >--------------------------------------------------------------------------= -------------------------------------------<br>Sergio DeSouza-Machado=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 <a href=3D""mailto:sergio@umbc.edu"" target= =3D""_blank"">sergio@umbc.edu</a><br>Research Assoc. Professor,=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 (W) 410-455-1944<br>JCET/Dept of Physics= =C2=A0=C2=A0 =C2=A0=C2=A0 =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0 (F) 410-455-1072<br></div><div>UMBC, Baltimore MD 21250<br>= </div></div></div></div></div></div></div> "
3281491,71960738,Create,DoIT-Research-Computing,2025-09-26 14:38:19.0000000,Srun not working for a parallel task,resolved,Beamlak Bekele,bbekele1,Yifan Hu,yifanh1,yifanh1@umbc.edu,Beamlak Bekele,bbekele1@umbc.edu,"<p>Yifan had an office hour with Beamlak and Tartela. We weren&#39;t able to provide an immediate solution. Yifan, please add the errors you get, your slurm script, and the working directory.&nbsp;&nbsp;</p>  <p>&nbsp;</p>  <p>&nbsp;</p> "
3281491,71963619,Correspond,DoIT-Research-Computing,2025-09-26 16:00:58.0000000,Srun not working for a parallel task,resolved,Beamlak Bekele,bbekele1,Yifan Hu,yifanh1,yifanh1@umbc.edu,Yifan Hu,yifanh1@umbc.edu,"Hi,  I have listed the 6 files (SLURM scripts, job outputs, executable outputs) = in the next section. In short, the ""mpirun"" launches my program successfull= y, while ""srun"" leads to several invalid memory access issues. Both SLURM s= cripts launch 31 executables at the same time (on 1 CPU node with 8 CPU cor= es). The only difference is the MPI launcher ""srun"" v.s. ""mpirun"".  ""srun"" version: - Work directory for the ""srun"" version: /umbc/rs/pi_dreynold/users/yifanh1= /20250926-genex-test-srun/ - ""srun"" job script: /umbc/rs/pi_dreynold/users/yifanh1/20250926-genex-test= -srun/job.sh - ""srun"" job output: /umbc/rs/pi_dreynold/users/yifanh1/20250926-genex-test= -srun/409289.genex-test-srun.out - Actual ""srun"" commands and output: /umbc/rs/pi_dreynold/users/yifanh1/MPC= DF/phoenix/genex/build-20250926-111830/src/Testing/Temporary/LastTest.log  ""mpirun"" version: - Work directory for the ""mpirun"" version: /umbc/rs/pi_dreynold/users/yifan= h1/20250926-genex-test-mpirun/ - ""mpirun"" job script: /umbc/rs/pi_dreynold/users/yifanh1/20250926-genex-te= st-mpirun/job.sh - ""mpirun"" job output: /umbc/rs/pi_dreynold/users/yifanh1/20250926-genex-te= st-mpirun/409290.genex-test-mpirun.out - Actual ""mpirun"" commands and output: /umbc/rs/pi_dreynold/users/yifanh1/M= PCDF/phoenix/genex/build-20250926-111842/src/Testing/Temporary/LastTest.log  The ""mpirun"" command I use for this test is from a Spack build of OpenMPI w= ith the following spec: -- linux-rhel9-cascadelake / %c,cxx,fortran=3Dgcc@13.3.0 ---------- openmpi@5.0.8+atomics~cuda~debug+fortran~gpfs~internal-hwloc~internal-libev= ent~internal-pmix~ipv6~java~lustre~memchecker~openshmem~rocm~romio+rsh~stat= ic~two_level_namespace+vt+wrapper-rpath build_system=3Dautotools fabrics:= =3Dnone romio-filesystem:=3Dnone schedulers:=3Dnone  Best, Yifan  > On Sep 26, 2025, at 10:38=E2=80=AFAM, via RT <UMBCHelp@rt.umbc.edu> wrote: >=20 > Greetings, >=20 > This message has been automatically generated in response to the > creation of a ticket regarding: >=20 > ------------------------------------------------------------------------- > Subject: ""Srun not working for a parallel task "" >=20 > Message:=20 >=20 > Yifan had an office hour with Beamlak and Tartela. We weren't able to pro= vide > an immediate solution. Yifan, please add the errors you get, your slurm s= cript, > and the working directory. >=20 >=20 > ------------------------------------------------------------------------- >=20 > There is no need to reply to this message right now.=20=20 >=20 > Your ticket has been assigned an ID of [Research Computing #3281491] or y= ou can go there directly by clicking the link below. >=20 > Ticket <URL: https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Di= splay.html?id%3D3281491&source=3Dgmail-imap&ust=3D1759502303000000&usg=3DAO= vVaw1BhV3ToX1x16wyeHk2LQlO > >=20 > You can login to view your open tickets at any time by visiting https://w= ww.google.com/url?q=3Dhttp://my.umbc.edu&source=3Dgmail-imap&ust=3D17595023= 03000000&usg=3DAOvVaw0xuQYMPyzRNQkSErohTnsE and clicking on ""Help"" and ""Req= uest Help"".=20 >=20 > Alternately you can click on https://www.google.com/url?q=3Dhttp://my.umb= c.edu/help&source=3Dgmail-imap&ust=3D1759502303000000&usg=3DAOvVaw0m_jvA-yD= KTyCKcX8QkLj3 >=20 >                        Thank you >=20   "
3281491,72083445,Correspond,DoIT-Research-Computing,2025-10-03 15:01:47.0000000,Srun not working for a parallel task,resolved,Beamlak Bekele,bbekele1,Yifan Hu,yifanh1,yifanh1@umbc.edu,Beamlak Bekele,bbekele1@umbc.edu,"<div> <p>Hi&nbsp;</p>  <p>I apologize for the late reply.&nbsp;</p>  <p>I wanted to suggest trying to add <code>--mpi=3Dpmix</code> to <code>sru= n</code>&nbsp;commands(<code>srun --mpi=3Dpmix -n 8 /path/to/your/test --ve= rbose</code>). I think <code>srun</code> might be defaulting to a single ra= nk, which could explain why you&#39;re seeing the &quot;Insufficient proces= ses&quot; error even when requesting multiple tasks and srun working when t= ask is set to 1.</p>  <p><code>pmix</code> (Process Management Interface for Exascale) is the plu= gin that enables Slurm to correctly launch and manage MPI tasks across mult= iple ranks. Without specifying it, <code>srun</code> may not properly initi= alize the MPI environment,</p>  <pre>  &nbsp;</pre>  <p>Let me know if that doesn&rsquo;t resolve the issue.</p>  <p>&nbsp;</p>  <p>On Fri Sep 26 12:00:58 2025, SN59577 wrote:</p>  <blockquote> <pre> Hi,  I have listed the 6 files (SLURM scripts, job outputs, executable outputs) = in the next section. In short, the &quot;mpirun&quot; launches my program s= uccessfully, while &quot;srun&quot; leads to several invalid memory access = issues. Both SLURM scripts launch 31 executables at the same time (on 1 CPU=  node with 8 CPU cores). The only difference is the MPI launcher &quot;srun= &quot; v.s. &quot;mpirun&quot;.  &quot;srun&quot; version: - Work directory for the &quot;srun&quot; version: /umbc/rs/pi_dreynold/use= rs/yifanh1/20250926-genex-test-srun/ - &quot;srun&quot; job script: /umbc/rs/pi_dreynold/users/yifanh1/20250926-= genex-test-srun/job.sh - &quot;srun&quot; job output: /umbc/rs/pi_dreynold/users/yifanh1/20250926-= genex-test-srun/409289.genex-test-srun.out - Actual &quot;srun&quot; commands and output: /umbc/rs/pi_dreynold/users/y= ifanh1/MPCDF/phoenix/genex/build-20250926-111830/src/Testing/Temporary/Last= Test.log  &quot;mpirun&quot; version: - Work directory for the &quot;mpirun&quot; version: /umbc/rs/pi_dreynold/u= sers/yifanh1/20250926-genex-test-mpirun/ - &quot;mpirun&quot; job script: /umbc/rs/pi_dreynold/users/yifanh1/2025092= 6-genex-test-mpirun/job.sh - &quot;mpirun&quot; job output: /umbc/rs/pi_dreynold/users/yifanh1/2025092= 6-genex-test-mpirun/409290.genex-test-mpirun.out - Actual &quot;mpirun&quot; commands and output: /umbc/rs/pi_dreynold/users= /yifanh1/MPCDF/phoenix/genex/build-20250926-111842/src/Testing/Temporary/La= stTest.log  The &quot;mpirun&quot; command I use for this test is from a Spack build of=  OpenMPI with the following spec: -- linux-rhel9-cascadelake / %c,cxx,fortran=3Dgcc@13.3.0 ---------- openmpi@5.0.8+atomics~cuda~debug+fortran~gpfs~internal-hwloc~internal-libev= ent~internal-pmix~ipv6~java~lustre~memchecker~openshmem~rocm~romio+rsh~stat= ic~two_level_namespace+vt+wrapper-rpath build_system=3Dautotools fabrics:= =3Dnone romio-filesystem:=3Dnone schedulers:=3Dnone  Best, Yifan  &gt; On Sep 26, 2025, at 10:38=E2=80=AFAM, via RT &lt;UMBCHelp@rt.umbc.edu&= gt; wrote: &gt;=20 &gt; Greetings, &gt;=20 &gt; This message has been automatically generated in response to the &gt; creation of a ticket regarding: &gt;=20 &gt; ----------------------------------------------------------------------= --- &gt; Subject: &quot;Srun not working for a parallel task &quot; &gt;=20 &gt; Message:=20 &gt;=20 &gt; Yifan had an office hour with Beamlak and Tartela. We weren&#39;t able=  to provide &gt; an immediate solution. Yifan, please add the errors you get, your slur= m script, &gt; and the working directory. &gt;=20 &gt;=20 &gt; ----------------------------------------------------------------------= --- &gt;=20 &gt; There is no need to reply to this message right now.=20=20 &gt;=20 &gt; Your ticket has been assigned an ID of [Research Computing #3281491] o= r you can go there directly by clicking the link below. &gt;=20 &gt; Ticket &lt;URL: https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Tic= ket/Display.html?id%3D3281491&amp;source=3Dgmail-imap&amp;ust=3D17595023030= 00000&amp;usg=3DAOvVaw1BhV3ToX1x16wyeHk2LQlO &gt; &gt;=20 &gt; You can login to view your open tickets at any time by visiting https:= //www.google.com/url?q=3Dhttp://my.umbc.edu&amp;source=3Dgmail-imap&amp;ust= =3D1759502303000000&amp;usg=3DAOvVaw0xuQYMPyzRNQkSErohTnsE and clicking on = &quot;Help&quot; and &quot;Request Help&quot;.=20 &gt;=20 &gt; Alternately you can click on https://www.google.com/url?q=3Dhttp://my.= umbc.edu/help&amp;source=3Dgmail-imap&amp;ust=3D1759502303000000&amp;usg=3D= AOvVaw0m_jvA-yDKTyCKcX8QkLj3 &gt;=20 &gt;                        Thank you &gt;=20   </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> "
3281714,71967302,Create,DoIT-Research-Computing,2025-09-26 17:36:48.0000000,HPC Slurm/Software Issue: Cannot create slurm job,resolved,Max Breitmeyer,mb17,Paul Abili,pabili1,pabili1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Paul Last Name:                 Abili Email:                     pabili1@umbc.edu Campus ID:                 QD36016  Request Type:              High Performance Cluster   Hello!  I attempted to run a slurm job: srun --cluster=chip-gpu --account=pi_laramar --mem=20000 --time=12:00:00 --gres=gpu:1 --pty $SHELL  Recieved: srun: Requested partition configuration not available now  I am pretty sure chip is down, would it be possible to recieve an update when chip is available again?  Thank you!  "
3281714,71968028,Correspond,DoIT-Research-Computing,2025-09-26 17:55:02.0000000,HPC Slurm/Software Issue: Cannot create slurm job,resolved,Max Breitmeyer,mb17,Paul Abili,pabili1,pabili1@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Paul,</p>  <p>Chip is not down. That being said, I did notice that the configuration was acting strange, and it turns out that it was trying to submit jobs to the &quot;gpu-contrib&quot; partition which is non-submittable partition. I&#39;ve changed that the &quot;gpu&quot; partition is back to being the default partition and should work. In the future you can always check to see if chip is really down by using the &quot;sinfo&quot; command. Additionally you can also specify the partition using the &quot;--partition=${gpu_partition}&quot;. Let me know if you have more questions about this, I&#39;ll leave the ticket open for a few days.&nbsp;</p>  <p>On Fri Sep 26 13:36:48 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Paul Last Name:                 Abili Email:                     pabili1@umbc.edu Campus ID:                 QD36016  Request Type:              High Performance Cluster   Hello!  I attempted to run a slurm job: srun --cluster=chip-gpu --account=pi_laramar --mem=20000 --time=12:00:00 --gres=gpu:1 --pty $SHELL  Recieved: srun: Requested partition configuration not available now  I am pretty sure chip is down, would it be possible to recieve an update when chip is available again?  Thank you!  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3281714,71968970,Correspond,DoIT-Research-Computing,2025-09-26 18:20:20.0000000,HPC Slurm/Software Issue: Cannot create slurm job,resolved,Max Breitmeyer,mb17,Paul Abili,pabili1,pabili1@umbc.edu,Paul Abili,pabili1@umbc.edu,"Thank you!  On Fri, Sep 26, 2025 at 1:55=E2=80=AFPM Max Breitmeyer via RT <UMBCHelp@rt.= umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3281714 > > > Last Update From Ticket: > > Hi Paul, > > Chip is not down. That being said, I did notice that the configuration was > acting strange, and it turns out that it was trying to submit jobs to the > ""gpu-contrib"" partition which is non-submittable partition. I've changed > that > the ""gpu"" partition is back to being the default partition and should > work. In > the future you can always check to see if chip is really down by using the > ""sinfo"" command. Additionally you can also specify the partition using the > ""--partition=3D${gpu_partition}"". Let me know if you have more questions > about > this, I'll leave the ticket open for a few days. > > On Fri Sep 26 13:36:48 2025, ZZ99999 wrote: > > > First Name:                Paul > > Last Name:                 Abili > > Email:                     pabili1@umbc.edu > > Campus ID:                 QD36016 > > > > Request Type:              High Performance Cluster > > > > > > Hello! > > > > I attempted to run a slurm job: srun --cluster=3Dchip-gpu > --account=3Dpi_laramar --mem=3D20000 --time=3D12:00:00 --gres=3Dgpu:1 --p= ty $SHELL > > > > Recieved: srun: Requested partition configuration not available now > > > > I am pretty sure chip is down, would it be possible to recieve an update > when chip is available again? > > > > Thank you! > > > -- > > Best, > Max Breitmeyer > DOIT HPC System Administrator > > "
3281714,71968970,Correspond,DoIT-Research-Computing,2025-09-26 18:20:20.0000000,HPC Slurm/Software Issue: Cannot create slurm job,resolved,Max Breitmeyer,mb17,Paul Abili,pabili1,pabili1@umbc.edu,Paul Abili,pabili1@umbc.edu,"<div dir=3D""ltr"">Thank you!</div><br><div class=3D""gmail_quote gmail_quote_= container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Fri, Sep 26, 2025 at 1:= 55=E2=80=AFPM Max Breitmeyer via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.= edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmai= l_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,20= 4,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Tic= ket/Display.html?id=3D3281714"" rel=3D""noreferrer"" target=3D""_blank"">https:/= /rt.umbc.edu/Ticket/Display.html?id=3D3281714</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Paul,<br> <br> Chip is not down. That being said, I did notice that the configuration was<= br> acting strange, and it turns out that it was trying to submit jobs to the<b= r> &quot;gpu-contrib&quot; partition which is non-submittable partition. I&#39= ;ve changed that<br> the &quot;gpu&quot; partition is back to being the default partition and sh= ould work. In<br> the future you can always check to see if chip is really down by using the<= br> &quot;sinfo&quot; command. Additionally you can also specify the partition = using the<br> &quot;--partition=3D${gpu_partition}&quot;. Let me know if you have more qu= estions about<br> this, I&#39;ll leave the ticket open for a few days.<br> <br> On Fri Sep 26 13:36:48 2025, ZZ99999 wrote:<br> <br> &gt; First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Pau= l<br> &gt; Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0Abili<br> &gt; Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 =C2=A0<a href=3D""mailto:pabili1@umbc.edu"" target=3D""_blank"">pabili1@= umbc.edu</a><br> &gt; Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0QD36016<br> &gt; <br> &gt; Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Per= formance Cluster<br> &gt; <br> &gt; <br> &gt; Hello!<br> &gt; <br> &gt; I attempted to run a slurm job: srun --cluster=3Dchip-gpu --account=3D= pi_laramar --mem=3D20000 --time=3D12:00:00 --gres=3Dgpu:1 --pty $SHELL<br> &gt; <br> &gt; Recieved: srun: Requested partition configuration not available now<br> &gt; <br> &gt; I am pretty sure chip is down, would it be possible to recieve an upda= te when chip is available again?<br> &gt; <br> &gt; Thank you!<br> <br> <br> --<br> <br> Best,<br> Max Breitmeyer<br> DOIT HPC System Administrator<br> <br> </blockquote></div> "
3282078,71976338,Create,DoIT-Research-Computing,2025-09-27 20:32:05.0000000,Cloud: Google Vision,stalled,Tim Champ,champ,Enrico Berkes,eberkes,eberkes@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Enrico Last Name:                 Berkes Email:                     eberkes@umbc.edu Campus ID:                 RV06612  Request Type:              Cloud Computing  Cloud Service:            GCP  Prof. Andrews and I have a new project in which we plan to OCR historical patent documents. We currently have high-quality images of each page and would like to evaluate the performance of cloud-based tools such as Google Vision and Amazon Textract.  If the quality meets our standards, our goal is to apply this process to the entire repository of historical patent documents (over 6 million documents, each with multiple pages).  Could you please let us know what steps we need to follow to get this effort started, including any pre-approvals or technical requirements on our end?  Thank you for your guidance and support.  "
3282078,72036141,Correspond,DoIT-Research-Computing,2025-10-01 14:04:52.0000000,Cloud: Google Vision,stalled,Tim Champ,champ,Enrico Berkes,eberkes,eberkes@umbc.edu,Tim Champ,champ@umbc.edu,"<div> <p>Hello!<br /> <br /> I believe we can get you an AWS account and a Google Cloud project in place to do this.&nbsp; I had a few questions to help get this going:<br /> <br /> 1 - do you have experience using both of these tools in the past?</p>  <p>2 - Both of these services have cost associated with them, do you have a pcard (or a departmental person with one) that can handle the billing and allocation of charges?</p>  <p>3 - How many people would need access?</p>  <p>4 - I assume based on the data you mentioned that it isn&#39;t sensitive or covered under any regulations of note, but can you confirm that?</p>  <p>If you can let me know the answers to those, we can get started on the process.&nbsp; If it would be easier to set up a call and talk this over, please let me know.<br /> <br /> Thanks!<br /> Tim</p>  <p>&nbsp;</p>  <p>On Sat Sep 27 16:32:05 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Enrico Last Name:                 Berkes Email:                     eberkes@umbc.edu Campus ID:                 RV06612  Request Type:              Cloud Computing  Cloud Service:            GCP  Prof. Andrews and I have a new project in which we plan to OCR historical patent documents. We currently have high-quality images of each page and would like to evaluate the performance of cloud-based tools such as Google Vision and Amazon Textract.  If the quality meets our standards, our goal is to apply this process to the entire repository of historical patent documents (over 6 million documents, each with multiple pages).  Could you please let us know what steps we need to follow to get this effort started, including any pre-approvals or technical requirements on our end?  Thank you for your guidance and support.  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;<br /> Tim Champ<br /> Associate Director of Research and Enterprise Computing<br /> Enterprise Infrastructure Solutions<br /> UMBC - DoIT</p> "
3282078,72061475,Correspond,DoIT-Research-Computing,2025-10-02 16:00:59.0000000,Cloud: Google Vision,stalled,Tim Champ,champ,Enrico Berkes,eberkes,eberkes@umbc.edu,Johnnie Fries,jfries2@umbc.edu,"Hi,  The department does have  P-card and I looked at the rules and see:  * Software =E2=80=93 all software must first go through the Software/Cloud = Services request process before purchase. *  How would that work in this case?  Thank you,  Johnnie  Johnnie Fries Office Supervisor, Economics Department jfries2@umbc.edu 410-455-2172 PUP #338 M-F 8am - Noon; 12:30pm to 4:30pm Pronouns: She/her/hers  *UMBC was established upon the land of the Piscataway and Susquehannock peoples. <https://oei.umbc.edu/land-acknowledgement-statement/>*      On Wed, Oct 1, 2025 at 10:04=E2=80=AFAM Tim Champ via RT <UMBCHelp@rt.umbc.= edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3282078 > > > Last Update From Ticket: > > Hello! > > I believe we can get you an AWS account and a Google Cloud project in > place to > do this. I had a few questions to help get this going: > > 1 - do you have experience using both of these tools in the past? > > 2 - Both of these services have cost associated with them, do you have a > pcard > (or a departmental person with one) that can handle the billing and > allocation > of charges? > > 3 - How many people would need access? > > 4 - I assume based on the data you mentioned that it isn't sensitive or > covered > under any regulations of note, but can you confirm that? > > If you can let me know the answers to those, we can get started on the > process. > If it would be easier to set up a call and talk this over, please let me > know. > > Thanks! > Tim > > On Sat Sep 27 16:32:05 2025, ZZ99999 wrote: > > > First Name:                Enrico > > Last Name:                 Berkes > > Email:                     eberkes@umbc.edu > > Campus ID:                 RV06612 > > > > Request Type:              Cloud Computing > > > > Cloud Service:            GCP > > > > Prof. Andrews and I have a new project in which we plan to OCR > historical patent documents. We currently have high-quality images of each > page and would like to evaluate the performance of cloud-based tools such > as Google Vision and Amazon Textract. > > > > If the quality meets our standards, our goal is to apply this process to > the entire repository of historical patent documents (over 6 million > documents, each with multiple pages). > > > > Could you please let us know what steps we need to follow to get this > effort started, including any pre-approvals or technical requirements on > our end? > > > > Thank you for your guidance and support. > > > -- > Tim Champ > Associate Director of Research and Enterprise Computing > Enterprise Infrastructure Solutions > UMBC - DoIT > > > "
3282078,72061475,Correspond,DoIT-Research-Computing,2025-10-02 16:00:59.0000000,Cloud: Google Vision,stalled,Tim Champ,champ,Enrico Berkes,eberkes,eberkes@umbc.edu,Johnnie Fries,jfries2@umbc.edu,"<div dir=3D""ltr""><div dir=3D""ltr""><div><div class=3D""gmail_default"" style= =3D""font-family:tahoma,sans-serif"">Hi,</div><div class=3D""gmail_default"" st= yle=3D""font-family:tahoma,sans-serif""><br></div><div class=3D""gmail_default= "" style=3D""font-family:tahoma,sans-serif"">The department does have=C2=A0 P-= card and I looked at the rules and see:</div><div class=3D""gmail_default"" s= tyle=3D""font-family:tahoma,sans-serif""><br></div><div class=3D""gmail_defaul= t"" style=3D""font-family:tahoma,sans-serif""><b><i>=C2=A0Software =E2=80=93 a= ll software must first go through the Software/Cloud Services request proce= ss before=20 purchase.=C2=A0</i></b></div><br clear=3D""all""></div><div><div class=3D""gma= il_default"" style=3D""font-family:tahoma,sans-serif"">How would that work in = this case?</div><div class=3D""gmail_default"" style=3D""font-family:tahoma,sa= ns-serif""><br></div><div class=3D""gmail_default"" style=3D""font-family:tahom= a,sans-serif"">Thank you,</div><div class=3D""gmail_default"" style=3D""font-fa= mily:tahoma,sans-serif""><br></div><div class=3D""gmail_default"" style=3D""fon= t-family:tahoma,sans-serif"">Johnnie</div><br></div><div><div dir=3D""ltr"" cl= ass=3D""gmail_signature""><div dir=3D""ltr"">Johnnie Fries<div>Office Superviso= r, Economics Department</div><div><a href=3D""mailto:jfries2@umbc.edu"" targe= t=3D""_blank"">jfries2@umbc.edu</a></div><div>410-455-2172</div><div>PUP #338= </div><div>M-F 8am - Noon; 12:30pm to 4:30pm<br></div><div>Pronouns: She/he= r/hers</div><div><br></div><div><i style=3D""color:rgb(34,34,34);font-family= :Roboto,RobotoDraft,Helvetica,Arial,sans-serif;font-size:12.8px""><a href=3D= ""https://oei.umbc.edu/land-acknowledgement-statement/"" style=3D""color:rgb(1= 7,85,204)"" target=3D""_blank"">UMBC was established upon the land of the Pisc= ataway and Susquehannock peoples.</a></i><br></div><div><br></div><div><br>= </div><div><br></div></div></div></div><br></div></div><br><div class=3D""gm= ail_quote gmail_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On W= ed, Oct 1, 2025 at 10:04=E2=80=AFAM Tim Champ via RT &lt;<a href=3D""mailto:= UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockqu= ote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px=  solid rgb(204,204,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https:= //rt.umbc.edu/Ticket/Display.html?id=3D3282078"" rel=3D""noreferrer"" target= =3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3282078</a> &gt;<b= r> <br> Last Update From Ticket:<br> <br> Hello!<br> <br> I believe we can get you an AWS account and a Google Cloud project in place=  to<br> do this. I had a few questions to help get this going:<br> <br> 1 - do you have experience using both of these tools in the past?<br> <br> 2 - Both of these services have cost associated with them, do you have a pc= ard<br> (or a departmental person with one) that can handle the billing and allocat= ion<br> of charges?<br> <br> 3 - How many people would need access?<br> <br> 4 - I assume based on the data you mentioned that it isn&#39;t sensitive or=  covered<br> under any regulations of note, but can you confirm that?<br> <br> If you can let me know the answers to those, we can get started on the proc= ess.<br> If it would be easier to set up a call and talk this over, please let me kn= ow.<br> <br> Thanks!<br> Tim<br> <br> On Sat Sep 27 16:32:05 2025, ZZ99999 wrote:<br> <br> &gt; First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Enr= ico<br> &gt; Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0Berkes<br> &gt; Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 =C2=A0<a href=3D""mailto:eberkes@umbc.edu"" target=3D""_blank"">eberkes@= umbc.edu</a><br> &gt; Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0RV06612<br> &gt; <br> &gt; Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Cloud Co= mputing<br> &gt; <br> &gt; Cloud Service:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 GCP<br> &gt; <br> &gt; Prof. Andrews and I have a new project in which we plan to OCR histori= cal patent documents. We currently have high-quality images of each page an= d would like to evaluate the performance of cloud-based tools such as Googl= e Vision and Amazon Textract.<br> &gt; <br> &gt; If the quality meets our standards, our goal is to apply this process = to the entire repository of historical patent documents (over 6 million doc= uments, each with multiple pages).<br> &gt; <br> &gt; Could you please let us know what steps we need to follow to get this = effort started, including any pre-approvals or technical requirements on ou= r end?<br> &gt; <br> &gt; Thank you for your guidance and support.<br> <br> <br> --<br> Tim Champ<br> Associate Director of Research and Enterprise Computing<br> Enterprise Infrastructure Solutions<br> UMBC - DoIT<br> <br> <br> </blockquote></div> "
3282078,72123637,Correspond,DoIT-Research-Computing,2025-10-06 20:20:29.0000000,Cloud: Google Vision,stalled,Tim Champ,champ,Enrico Berkes,eberkes,eberkes@umbc.edu,Enrico Berkes,eberkes@umbc.edu,"Hi Tim,  Let me try to answer your questions in order:     1. We do not have extensive experience using these tools. We have an RA    who got a Master's degree in data science at UMBC who will help us out    throughout the process, but I think it's going to be a learning    experience for everyone of us.    2. Johnnie Fries (in copy) sent you an email about the P-Card last week.    In the past we were told that the best way for similar charges was to use a    personal credit card and ask for a reimbursement. Is the P-Card suggested    for these services?    3. It will be Prof. Michael Andrews (in copy), me, and Shirisha Biyyala (    shirisb1@umbc.edu) our RA.    4. All the data we'll use are publicly available on the USPTO website.  Thank you so much for all your help!  Enrico    On Wed, Oct 01, 2025 at 10:04 AM, Tim Champ <UMBCHelp@rt.umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3282078 > > > Last Update From Ticket: > > Hello! > > I believe we can get you an AWS account and a Google Cloud project in > place to do this. I had a few questions to help get this going: > > 1 - do you have experience using both of these tools in the past? > > 2 - Both of these services have cost associated with them, do you have a > pcard > (or a departmental person with one) that can handle the billing and > allocation of charges? > > 3 - How many people would need access? > > 4 - I assume based on the data you mentioned that it isn't sensitive or > covered under any regulations of note, but can you confirm that? > > If you can let me know the answers to those, we can get started on the > process. If it would be easier to set up a call and talk this over, please > let me know. > > Thanks! > Tim > > On Sat Sep 27 16:32:05 2025, ZZ99999 wrote: > > First Name: Enrico > Last Name: Berkes > Email: eberkes@umbc.edu > Campus ID: RV06612 > > Request Type: Cloud Computing > > Cloud Service: GCP > > Prof. Andrews and I have a new project in which we plan to OCR historical > patent documents. We currently have high-quality images of each page and > would like to evaluate the performance of cloud-based tools such as Google > Vision and Amazon Textract. > > If the quality meets our standards, our goal is to apply this process to > the entire repository of historical patent documents (over 6 million > documents, each with multiple pages). > > Could you please let us know what steps we need to follow to get this > effort started, including any pre-approvals or technical requirements on > our end? > > Thank you for your guidance and support. > > -- > Tim Champ > Associate Director of Research and Enterprise Computing Enterprise > Infrastructure Solutions > UMBC - DoIT > "
3282078,72123637,Correspond,DoIT-Research-Computing,2025-10-06 20:20:29.0000000,Cloud: Google Vision,stalled,Tim Champ,champ,Enrico Berkes,eberkes,eberkes@umbc.edu,Enrico Berkes,eberkes@umbc.edu,"<html><head></head><body><div><div><div><div class=3D"""">Hi Tim,<br></div><d= iv class=3D""""><br></div><div class=3D"""">Let me try to answer your questions=  in order:<br></div><ol><li value=3D""1"">We do not have extensive experience=  using these tools. We have an RA who got a Master&#39;s degree in data sci= ence at UMBC who will help us out throughout the process, but I think it&#3= 9;s going to be a learning <span>experience</span>=C2=A0for everyone of us.= <br></li><li>Johnnie Fries (in copy) sent you an email about the P-Card las= t week. In the past we were told that the best way for similar charges was = to use a personal credit card and ask for a reimbursement. Is the P-Card su= ggested for these services?<br></li><li>It will be Prof. Michael Andrews (i= n copy), me, and=C2=A0Shirisha Biyyala (<a href=3D""mailto:shirisb1@umbc.edu= "">shirisb1@umbc.edu</a>) our RA.<br></li><li>All the data we&#39;ll use are=  publicly available on the USPTO website.<br></li></ol><div>Thank you so mu= ch for all your help!<br></div><div><br></div><div>Enrico</div><div><br></d= iv></div><div></div><br><div class=3D""gmail_signature""></div></div><br><div= ><div class=3D""gmail_quote"">On Wed, Oct 01, 2025 at 10:04 AM, Tim Champ <sp= an dir=3D""ltr"">&lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank= "">UMBCHelp@rt.umbc.edu</a>&gt;</span> wrote:<br><blockquote class=3D""gmail_= quote"" style=3D""margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1= ex""><div class=3D""gmail_extra""><div class=3D""gmail_quote sh-color-black sh-= color""><p class=3D""sh-color-black sh-color"">Ticket &lt;URL: <a target=3D""_b= lank"" rel=3D""noopener noreferrer"" href=3D""https://rt.umbc.edu/Ticket/Displa= y.html?id=3D3282078"" class=3D""sh-color-blue sh-color"">https:/<wbr>/<wbr>rt.= <wbr>umbc.<wbr>edu/<wbr>Ticket/<wbr>Display.<wbr>html?id=3D3282078</a> &gt; </p><p class=3D""sh-color-black sh-color""> Last Update From Ticket: </p><p class=3D""sh-color-black sh-color""> Hello! </p><p class=3D""sh-color-black sh-color""> I believe we can get you an AWS account and a Google Cloud project in place=  to do this. I had a few questions to help get this going: </p><p class=3D""sh-color-black sh-color""> 1 - do you have experience using both of these tools in the past? </p><p class=3D""sh-color-black sh-color""> 2 - Both of these services have cost associated with them, do you have a pc= ard <br> (or a departmental person with one) that can handle the billing and allocat= ion of charges? </p><p class=3D""sh-color-black sh-color""> 3 - How many people would need access? </p><p class=3D""sh-color-black sh-color""> 4 - I assume based on the data you mentioned that it isn&#39;t sensitive or=  covered under any regulations of note, but can you confirm that? </p><p class=3D""sh-color-black sh-color""> If you can let me know the answers to those, we can get started on the proc= ess. If it would be easier to set up a call and talk this over, please let me kn= ow. </p><p class=3D""sh-color-black sh-color""> Thanks! <br> Tim </p><p class=3D""sh-color-black sh-color""> On Sat Sep 27 16:32:05 2025, ZZ99999 wrote: </p><blockquote class=3D""sh-color-black sh-color""><p class=3D""sh-color-blac= k sh-color""> First Name:                Enrico <br> Last Name:                 Berkes <br> Email:                     <a target=3D""_blank"" rel=3D""noopener noreferrer""=  href=3D""mailto:eberkes@umbc.edu"" class=3D""sh-color-blue sh-color"">eberkes@= <wbr>umbc.<wbr>edu</a> <br> Campus ID:                 RV06612 </p><p class=3D""sh-color-black sh-color""> Request Type:              Cloud Computing </p><p class=3D""sh-color-black sh-color""> Cloud Service:            GCP </p><p class=3D""sh-color-black sh-color""> Prof. Andrews and I have a new project in which we plan to OCR historical p= atent documents. We currently have high-quality images of each page and wou= ld like to evaluate the performance of cloud-based tools such as Google Vis= ion and Amazon Textract. </p><p class=3D""sh-color-black sh-color""> If the quality meets our standards, our goal is to apply this process to th= e entire repository of historical patent documents (over 6 million document= s, each with multiple pages). </p><p class=3D""sh-color-black sh-color""> Could you please let us know what steps we need to follow to get this effor= t started, including any pre-approvals or technical requirements on our end? </p><p class=3D""sh-color-black sh-color""> Thank you for your guidance and support. </p></blockquote><p class=3D""sh-color-black sh-color""> -- <br> Tim Champ <br> Associate Director of Research and Enterprise Computing Enterprise Infrastructure Solutions <br> UMBC - DoIT</p></div></div></blockquote></div></div><br></div></body></html> "
3282078,72141536,Correspond,DoIT-Research-Computing,2025-10-07 16:46:39.0000000,Cloud: Google Vision,stalled,Tim Champ,champ,Enrico Berkes,eberkes,eberkes@umbc.edu,Tim Champ,champ@umbc.edu,"<div> <p>Hello all.&nbsp;</p>  <p>Per the pcard questions - we do not need a cloud services review for a new AWS account.&nbsp; Since it is already through existing contracts, we can do this if you have the grant/department money to cover it.&nbsp; It&#39;s recommended to use the Pcard, and not do reimbursement as we already have existing language in place to cover this.&nbsp; I will plan to let our reseller know to contact Johnnie to get those details.</p>  <p>It sounds like the data is public, so that removes any concern about how to secure it.&nbsp; I can get these accounts created with admin abilities for the three people listed.&nbsp; You should hear back in a couple days.&nbsp; Thanks!<br /> <br /> Tim</p>  <p>&nbsp;</p>  <p>On Mon Oct 06 16:20:29 2025, RV06612 wrote:</p>  <blockquote> <div> <div> <div> <div>Hi Tim,</div>  <div>&nbsp;</div>  <div>Let me try to answer your questions in order:</div>  <ol> 	<li>We do not have extensive experience using these tools. We have an RA who got a Master&#39;s degree in data science at UMBC who will help us out throughout the process, but I think it&#39;s going to be a learning experience&nbsp;for everyone of us.</li> 	<li>Johnnie Fries (in copy) sent you an email about the P-Card last week. In the past we were told that the best way for similar charges was to use a personal credit card and ask for a reimbursement. Is the P-Card suggested for these services?</li> 	<li>It will be Prof. Michael Andrews (in copy), me, and&nbsp;Shirisha Biyyala (shirisb1@umbc.edu) our RA.</li> 	<li>All the data we&#39;ll use are publicly available on the USPTO website.</li> </ol>  <div>Thank you so much for all your help!</div>  <div>&nbsp;</div>  <div>Enrico</div>  <div>&nbsp;</div> </div>  <div>&nbsp;</div> &nbsp;  <div>&nbsp;</div> </div> &nbsp;  <div> <div>On Wed, Oct 01, 2025 at 10:04 AM, Tim Champ <span dir=""ltr"">&lt;UMBCHelp@rt.umbc.edu&gt;</span> wrote:  <blockquote> <div> <div> <p>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3282078 &gt;</p>  <p>Last Update From Ticket:</p>  <p>Hello!</p>  <p>I believe we can get you an AWS account and a Google Cloud project in place to do this. I had a few questions to help get this going:</p>  <p>1 - do you have experience using both of these tools in the past?</p>  <p>2 - Both of these services have cost associated with them, do you have a pcard<br /> (or a departmental person with one) that can handle the billing and allocation of charges?</p>  <p>3 - How many people would need access?</p>  <p>4 - I assume based on the data you mentioned that it isn&#39;t sensitive or covered under any regulations of note, but can you confirm that?</p>  <p>If you can let me know the answers to those, we can get started on the process. If it would be easier to set up a call and talk this over, please let me know.</p>  <p>Thanks!<br /> Tim</p>  <p>On Sat Sep 27 16:32:05 2025, ZZ99999 wrote:</p>  <blockquote> <p>First Name: Enrico<br /> Last Name: Berkes<br /> Email: eberkes@umbc.edu<br /> Campus ID: RV06612</p>  <p>Request Type: Cloud Computing</p>  <p>Cloud Service: GCP</p>  <p>Prof. Andrews and I have a new project in which we plan to OCR historical patent documents. We currently have high-quality images of each page and would like to evaluate the performance of cloud-based tools such as Google Vision and Amazon Textract.</p>  <p>If the quality meets our standards, our goal is to apply this process to the entire repository of historical patent documents (over 6 million documents, each with multiple pages).</p>  <p>Could you please let us know what steps we need to follow to get this effort started, including any pre-approvals or technical requirements on our end?</p>  <p>Thank you for your guidance and support.</p> </blockquote>  <p>--<br /> Tim Champ<br /> Associate Director of Research and Enterprise Computing Enterprise Infrastructure Solutions<br /> UMBC - DoIT</p> </div> </div> </blockquote> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;<br /> Tim Champ<br /> Associate Director of Research and Enterprise Computing<br /> Enterprise Infrastructure Solutions<br /> UMBC - DoIT</p> "
3282078,72141673,Correspond,DoIT-Research-Computing,2025-10-07 16:52:52.0000000,Cloud: Google Vision,stalled,Tim Champ,champ,Enrico Berkes,eberkes,eberkes@umbc.edu,Enrico Berkes,eberkes@umbc.edu,"Awesome! Thank you for the quick reply. Would it be possible to also get a Google Cloud account?  Thank you,  Enrico   On Tue, Oct 07, 2025 at 12:46 PM, Tim Champ <UMBCHelp@rt.umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3282078 > > > Last Update From Ticket: > > Hello all. > > Per the pcard questions - we do not need a cloud services review for a new > AWS account. Since it is already through existing contracts, we can do this > if you have the grant/department money to cover it. It's recommended to use > the Pcard, and not do reimbursement as we already have existing language in > place to cover this. I will plan to let our reseller know to contact > Johnnie to get those details. > > It sounds like the data is public, so that removes any concern about how > to secure it. I can get these accounts created with admin abilities for the > three people listed. You should hear back in a couple days. Thanks! > > Tim > > On Mon Oct 06 16:20:29 2025, RV06612 wrote: > > Hi Tim, Let me try to answer your questions in order: > > 1. We do not have extensive experience using these tools. We have an RA > > who got a Master's degree in data science at UMBC who will help us out > throughout the process, but I think it's going to be a learning experience > for everyone of us. > > 2. Johnnie Fries (in copy) sent you an email about the P-Card last week. > > In the past we were told that the best way for similar charges was to use > a personal credit card and ask for a reimbursement. Is the P-Card suggested > for these services? > > 3. It will be Prof. Michael Andrews (in copy), me, and Shirisha Biyyala > > (shirisb1@umbc.edu) our RA. > > 4. All the data we'll use are publicly available on the USPTO website. > > Thank you so much for all your help! Enrico On Wed, Oct 01, 2025 at 10:04 > AM, Tim Champ <UMBCHelp@rt.umbc.edu> wrote: > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3282078 > > > Last Update From Ticket: > > Hello! > > I believe we can get you an AWS account and a Google Cloud project in > place to do this. I had a few questions to help get this going: > > 1 - do you have experience using both of these tools in the past? > > 2 - Both of these services have cost associated with them, do you have a > pcard > (or a departmental person with one) that can handle the billing and > allocation of charges? > > 3 - How many people would need access? > > 4 - I assume based on the data you mentioned that it isn't sensitive or > covered under any regulations of note, but can you confirm that? > > If you can let me know the answers to those, we can get started on the > process. If it would be easier to set up a call and talk this over, please > let me know. > > Thanks! > Tim > > On Sat Sep 27 16:32:05 2025, ZZ99999 wrote: > > First Name: Enrico > Last Name: Berkes > Email: eberkes@umbc.edu > Campus ID: RV06612 > > Request Type: Cloud Computing > > Cloud Service: GCP > > Prof. Andrews and I have a new project in which we plan to OCR historical > patent documents. We currently have high-quality images of each page and > would like to evaluate the performance of cloud-based tools such as Google > Vision and Amazon Textract. > > If the quality meets our standards, our goal is to apply this process to > the entire repository of historical patent documents > (over 6 million documents, each with multiple pages). > > Could you please let us know what steps we need to follow to get this > effort started, including any pre-approvals or technical requirements on > our end? > > Thank you for your guidance and support. > > -- > Tim Champ > Associate Director of Research and Enterprise Computing Enterprise > Infrastructure Solutions > UMBC - DoIT > > -- > Tim Champ > Associate Director of Research and Enterprise Computing Enterprise > Infrastructure Solutions > UMBC - DoIT > "
3282078,72141673,Correspond,DoIT-Research-Computing,2025-10-07 16:52:52.0000000,Cloud: Google Vision,stalled,Tim Champ,champ,Enrico Berkes,eberkes,eberkes@umbc.edu,Enrico Berkes,eberkes@umbc.edu,"<html><head></head><body><div><div><div><div class="""">Awesome! Thank you for the quick reply. Would it be possible to also get a Google Cloud account?<br></div><div class=""""><br></div><div class="""">Thank you,<br></div><div class=""""><br></div><div class="""">Enrico</div></div><div></div><br><div class=""gmail_signature""></div></div><br><div><div class=""gmail_quote"">On Tue, Oct 07, 2025 at 12:46 PM, Tim Champ <span dir=""ltr"">&lt;<a href=""mailto:UMBCHelp@rt.umbc.edu"" target=""_blank"">UMBCHelp@rt.umbc.edu</a>&gt;</span> wrote:<br><blockquote class=""gmail_quote"" style=""margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex""><div class=""gmail_extra""><div class=""gmail_quote sh-color-black sh-color""><p class=""sh-color-black sh-color"">Ticket &lt;URL: <a target=""_blank"" rel=""noopener noreferrer"" href=""https://rt.umbc.edu/Ticket/Display.html?id=3282078"" class=""sh-color-blue sh-color"">https:/<wbr>/<wbr>rt.<wbr>umbc.<wbr>edu/<wbr>Ticket/<wbr>Display.<wbr>html?id=3282078</a> &gt; </p><p class=""sh-color-black sh-color""> Last Update From Ticket: </p><p class=""sh-color-black sh-color""> Hello all. </p><p class=""sh-color-black sh-color""> Per the pcard questions - we do not need a cloud services review for a new AWS account. Since it is already through existing contracts, we can do this if you have the grant/department money to cover it. It&#39;s recommended to use the Pcard, and not do reimbursement as we already have existing language in place to cover this. I will plan to let our reseller know to contact Johnnie to get those details. </p><p class=""sh-color-black sh-color""> It sounds like the data is public, so that removes any concern about how to secure it. I can get these accounts created with admin abilities for the three people listed. You should hear back in a couple days. Thanks! </p><p class=""sh-color-black sh-color""> Tim </p><p class=""sh-color-black sh-color""> On Mon Oct 06 16:20:29 2025, RV06612 wrote: </p><blockquote class=""sh-color-black sh-color""><p class=""sh-color-black sh-color""> Hi Tim, Let me try to answer your questions in order: </p><blockquote class=""sh-color-black sh-color""><p class=""sh-color-black sh-color""> 1. We do not have extensive experience using these tools. We have an RA </p><blockquote class=""sh-color-black sh-color""><p class=""sh-color-black sh-color""> who got a Master&#39;s degree in data science at UMBC who will help us out throughout the process, but I think it&#39;s going to be a learning experience for everyone of us. </p></blockquote><p class=""sh-color-black sh-color""> 2. Johnnie Fries (in copy) sent you an email about the P-Card last week. </p><blockquote class=""sh-color-black sh-color""><p class=""sh-color-black sh-color""> In the past we were told that the best way for similar charges was to use a personal credit card and ask for a reimbursement. Is the P-Card suggested for these services? </p></blockquote><p class=""sh-color-black sh-color""> 3. It will be Prof. Michael Andrews (in copy), me, and Shirisha Biyyala </p><blockquote class=""sh-color-black sh-color""><p class=""sh-color-black sh-color""> (<a target=""_blank"" rel=""noopener noreferrer"" href=""mailto:shirisb1@umbc.edu"" class=""sh-color-blue sh-color"">shirisb1@<wbr>umbc.<wbr>edu</a>) our RA. </p></blockquote><p class=""sh-color-black sh-color""> 4. All the data we&#39;ll use are publicly available on the USPTO website. </p></blockquote><p class=""sh-color-black sh-color""> Thank you so much for all your help! Enrico <span class=""sh-date sh-color-black sh-color"">On Wed, Oct 01</span>, 2025 at 10:04 AM, Tim Champ &lt;<a target=""_blank"" rel=""noopener noreferrer"" href=""mailto:UMBCHelp@rt.umbc.edu"" class=""sh-color-blue sh-color"">UMBCHelp@<wbr>rt.<wbr>umbc.<wbr>edu</a>&gt; wrote: </p><blockquote class=""sh-color-black sh-color""><p class=""sh-color-black sh-color""> Ticket &lt;URL: <a target=""_blank"" rel=""noopener noreferrer"" href=""https://rt.umbc.edu/Ticket/Display.html?id=3282078"" class=""sh-color-blue sh-color"">https:/<wbr>/<wbr>rt.<wbr>umbc.<wbr>edu/<wbr>Ticket/<wbr>Display.<wbr>html?id=3282078</a> &gt; </p><p class=""sh-color-black sh-color""> Last Update From Ticket: </p><p class=""sh-color-black sh-color""> Hello! </p><p class=""sh-color-black sh-color""> I believe we can get you an AWS account and a Google Cloud project in place to do this. I had a few questions to help get this going: </p><p class=""sh-color-black sh-color""> 1 - do you have experience using both of these tools in the past? </p><p class=""sh-color-black sh-color""> 2 - Both of these services have cost associated with them, do you have a pcard <br> (or a departmental person with one) that can handle the billing and allocation of charges? </p><p class=""sh-color-black sh-color""> 3 - How many people would need access? </p><p class=""sh-color-black sh-color""> 4 - I assume based on the data you mentioned that it isn&#39;t sensitive or covered under any regulations of note, but can you confirm that? </p><p class=""sh-color-black sh-color""> If you can let me know the answers to those, we can get started on the process. If it would be easier to set up a call and talk this over, please let me know. </p><p class=""sh-color-black sh-color""> Thanks! <br> Tim </p><p class=""sh-color-black sh-color""> On Sat Sep 27 16:32:05 2025, ZZ99999 wrote: </p><blockquote class=""sh-color-black sh-color""><p class=""sh-color-black sh-color""> First Name: Enrico <br> Last Name: Berkes <br> Email: <a target=""_blank"" rel=""noopener noreferrer"" href=""mailto:eberkes@umbc.edu"" class=""sh-color-blue sh-color"">eberkes@<wbr>umbc.<wbr>edu</a> <br> Campus ID: RV06612 </p><p class=""sh-color-black sh-color""> Request Type: Cloud Computing </p><p class=""sh-color-black sh-color""> Cloud Service: GCP </p><p class=""sh-color-black sh-color""> Prof. Andrews and I have a new project in which we plan to OCR historical patent documents. We currently have high-quality images of each page and would like to evaluate the performance of cloud-based tools such as Google Vision and Amazon Textract. </p><p class=""sh-color-black sh-color""> If the quality meets our standards, our goal is to apply this process to the entire repository of historical patent documents <br> (over 6 million documents, each with multiple pages). </p><p class=""sh-color-black sh-color""> Could you please let us know what steps we need to follow to get this effort started, including any pre-approvals or technical requirements on our end? </p><p class=""sh-color-black sh-color""> Thank you for your guidance and support. </p></blockquote><p class=""sh-color-black sh-color""> -- <br> Tim Champ <br> Associate Director of Research and Enterprise Computing Enterprise Infrastructure Solutions <br> UMBC - DoIT </p></blockquote></blockquote><p class=""sh-color-black sh-color""> -- <br> Tim Champ <br> Associate Director of Research and Enterprise Computing Enterprise Infrastructure Solutions <br> UMBC - DoIT</p></div></div></blockquote></div></div><br></div></body></html> "
3282078,72158224,Correspond,DoIT-Research-Computing,2025-10-08 14:10:55.0000000,Cloud: Google Vision,stalled,Tim Champ,champ,Enrico Berkes,eberkes,eberkes@umbc.edu,Tim Champ,champ@umbc.edu,"<div> <p>Yes indeed - I meant to say that as well.&nbsp; I&#39;ll work through this asap.<br /> <br /> Tim</p>  <p>&nbsp;</p>  <p>On Tue Oct 07 12:52:52 2025, RV06612 wrote:</p>  <blockquote> <div> <div> <div> <div>Awesome! Thank you for the quick reply. Would it be possible to also get a Google Cloud account?</div>  <div>&nbsp;</div>  <div>Thank you,</div>  <div>&nbsp;</div>  <div>Enrico</div> </div>  <div>&nbsp;</div> &nbsp;  <div>&nbsp;</div> </div> &nbsp;  <div> <div>On Tue, Oct 07, 2025 at 12:46 PM, Tim Champ <span dir=""ltr"">&lt;UMBCHelp@rt.umbc.edu&gt;</span> wrote:  <blockquote> <div> <div> <p>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3282078 &gt;</p>  <p>Last Update From Ticket:</p>  <p>Hello all.</p>  <p>Per the pcard questions - we do not need a cloud services review for a new AWS account. Since it is already through existing contracts, we can do this if you have the grant/department money to cover it. It&#39;s recommended to use the Pcard, and not do reimbursement as we already have existing language in place to cover this. I will plan to let our reseller know to contact Johnnie to get those details.</p>  <p>It sounds like the data is public, so that removes any concern about how to secure it. I can get these accounts created with admin abilities for the three people listed. You should hear back in a couple days. Thanks!</p>  <p>Tim</p>  <p>On Mon Oct 06 16:20:29 2025, RV06612 wrote:</p>  <blockquote> <p>Hi Tim, Let me try to answer your questions in order:</p>  <blockquote> <p>1. We do not have extensive experience using these tools. We have an RA</p>  <blockquote> <p>who got a Master&#39;s degree in data science at UMBC who will help us out throughout the process, but I think it&#39;s going to be a learning experience for everyone of us.</p> </blockquote>  <p>2. Johnnie Fries (in copy) sent you an email about the P-Card last week.</p>  <blockquote> <p>In the past we were told that the best way for similar charges was to use a personal credit card and ask for a reimbursement. Is the P-Card suggested for these services?</p> </blockquote>  <p>3. It will be Prof. Michael Andrews (in copy), me, and Shirisha Biyyala</p>  <blockquote> <p>(shirisb1@umbc.edu) our RA.</p> </blockquote>  <p>4. All the data we&#39;ll use are publicly available on the USPTO website.</p> </blockquote>  <p>Thank you so much for all your help! Enrico On Wed, Oct 01, 2025 at 10:04 AM, Tim Champ &lt;UMBCHelp@rt.umbc.edu&gt; wrote:</p>  <blockquote> <p>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3282078 &gt;</p>  <p>Last Update From Ticket:</p>  <p>Hello!</p>  <p>I believe we can get you an AWS account and a Google Cloud project in place to do this. I had a few questions to help get this going:</p>  <p>1 - do you have experience using both of these tools in the past?</p>  <p>2 - Both of these services have cost associated with them, do you have a pcard<br /> (or a departmental person with one) that can handle the billing and allocation of charges?</p>  <p>3 - How many people would need access?</p>  <p>4 - I assume based on the data you mentioned that it isn&#39;t sensitive or covered under any regulations of note, but can you confirm that?</p>  <p>If you can let me know the answers to those, we can get started on the process. If it would be easier to set up a call and talk this over, please let me know.</p>  <p>Thanks!<br /> Tim</p>  <p>On Sat Sep 27 16:32:05 2025, ZZ99999 wrote:</p>  <blockquote> <p>First Name: Enrico<br /> Last Name: Berkes<br /> Email: eberkes@umbc.edu<br /> Campus ID: RV06612</p>  <p>Request Type: Cloud Computing</p>  <p>Cloud Service: GCP</p>  <p>Prof. Andrews and I have a new project in which we plan to OCR historical patent documents. We currently have high-quality images of each page and would like to evaluate the performance of cloud-based tools such as Google Vision and Amazon Textract.</p>  <p>If the quality meets our standards, our goal is to apply this process to the entire repository of historical patent documents<br /> (over 6 million documents, each with multiple pages).</p>  <p>Could you please let us know what steps we need to follow to get this effort started, including any pre-approvals or technical requirements on our end?</p>  <p>Thank you for your guidance and support.</p> </blockquote>  <p>--<br /> Tim Champ<br /> Associate Director of Research and Enterprise Computing Enterprise Infrastructure Solutions<br /> UMBC - DoIT</p> </blockquote> </blockquote>  <p>--<br /> Tim Champ<br /> Associate Director of Research and Enterprise Computing Enterprise Infrastructure Solutions<br /> UMBC - DoIT</p> </div> </div> </blockquote> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;<br /> Tim Champ<br /> Associate Director of Research and Enterprise Computing<br /> Enterprise Infrastructure Solutions<br /> UMBC - DoIT</p> "
3282078,72180127,Correspond,DoIT-Research-Computing,2025-10-09 13:06:02.0000000,Cloud: Google Vision,stalled,Tim Champ,champ,Enrico Berkes,eberkes,eberkes@umbc.edu,James Sonnichsen,jsonni2@umbc.edu,"<p>Hello,<br /> For the AWS account could you please let me know what you would like the name to be. We used a Dept-project format.&nbsp; So in this case we would use Econ-**** please let me know what the title of this project should be. Thank you.&nbsp;&nbsp;</p>  <p>--&nbsp;<br /> James Sonnichsen<br /> DoIT, Windows System Administrator<br /> jsonni2@Umbc.edu<br /> (410)-455-6295</p> "
3282078,72198025,Correspond,DoIT-Research-Computing,2025-10-10 01:54:52.0000000,Cloud: Google Vision,stalled,Tim Champ,champ,Enrico Berkes,eberkes,eberkes@umbc.edu,Enrico Berkes,eberkes@umbc.edu,"Hi James,  Thank you for getting back to us. We could call the project Econ-Historical_Patents or something along these lines.  Thank you!  Enrico   On Thu, Oct 09, 2025 at 9:06 AM, James Sonnichsen <UMBCHelp@rt.umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3282078 > > > Last Update From Ticket: > > Hello, > For the AWS account could you please let me know what you would like the > name to be. We used a Dept-project format. So in this case we would use > Econ-**** please let me know what the title of this project should be. > Thank you. > > -- > James Sonnichsen > DoIT, Windows System Administrator > jsonni2@Umbc.edu > (410)-455-6295 > "
3282078,72198025,Correspond,DoIT-Research-Computing,2025-10-10 01:54:52.0000000,Cloud: Google Vision,stalled,Tim Champ,champ,Enrico Berkes,eberkes,eberkes@umbc.edu,Enrico Berkes,eberkes@umbc.edu,"<html><head></head><body><div><div><div><div class=3D"""">Hi James,<br></div>= <div class=3D""""><br></div><div class=3D"""">Thank=C2=A0you for getting back t= o us. We could call the project Econ-Historical_Patents or something along = these lines.<br></div><div class=3D""""><br></div><div class=3D"""">Thank=C2=A0= you!<br></div><div class=3D""""><br></div><div class=3D"""">Enrico</div></div><= div></div><br><div class=3D""gmail_signature""></div></div><br><div><div clas= s=3D""gmail_quote"">On Thu, Oct 09, 2025 at 9:06 AM, James Sonnichsen <span d= ir=3D""ltr"">&lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UM= BCHelp@rt.umbc.edu</a>&gt;</span> wrote:<br><blockquote class=3D""gmail_quot= e"" style=3D""margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"">= <div class=3D""gmail_extra""><div class=3D""gmail_quote sh-color-black sh-colo= r""><p class=3D""sh-color-black sh-color"">Ticket &lt;URL: <a target=3D""_blank= "" rel=3D""noopener noreferrer"" href=3D""https://rt.umbc.edu/Ticket/Display.ht= ml?id=3D3282078"" class=3D""sh-color-blue sh-color"">https:/<wbr>/<wbr>rt.<wbr= >umbc.<wbr>edu/<wbr>Ticket/<wbr>Display.<wbr>html?id=3D3282078</a> &gt; </p><p class=3D""sh-color-black sh-color""> Last Update From Ticket: </p><p class=3D""sh-color-black sh-color""> Hello, <br> For the AWS account could you please let me know what you would like the na= me to be. We used a Dept-project format. So in this case we would use Econ-**** please let me know what the title of this project should be. Thank you. </p><p class=3D""sh-color-black sh-color""> -- <br> James Sonnichsen <br> DoIT, Windows System Administrator <br> <a target=3D""_blank"" rel=3D""noopener noreferrer"" href=3D""mailto:jsonni2@Umb= c.edu"" class=3D""sh-color-blue sh-color"">jsonni2@<wbr>Umbc.<wbr>edu</a> <br> (410)-455-6295</p></div></div></blockquote></div></div><br></div></body></h= tml> "
3282078,72245535,Correspond,DoIT-Research-Computing,2025-10-13 18:35:48.0000000,Cloud: Google Vision,stalled,Tim Champ,champ,Enrico Berkes,eberkes,eberkes@umbc.edu,James Sonnichsen,jsonni2@umbc.edu,"<p>Hello,&nbsp;<br /> The account has been created and I have sent an email to our reseller and CC&#39;d&nbsp; the staff member Johnnie.&nbsp; Once the payment method has been established I can turn over the account to you.&nbsp; Please let me know if you have any questions.&nbsp;</p>  <p>--&nbsp;<br /> James Sonnichsen<br /> DoIT, Windows System Administrator<br /> jsonni2@Umbc.edu<br /> (410)-455-6295</p> "
3282078,72399195,Correspond,DoIT-Research-Computing,2025-10-21 16:14:48.0000000,Cloud: Google Vision,stalled,Tim Champ,champ,Enrico Berkes,eberkes,eberkes@umbc.edu,Tim Champ,champ@umbc.edu,"<p>Hello again!<br /> <br /> We&#39;ve created the Google cloud project for you.&nbsp; I apologize for the delay.&nbsp; For now, the billing isn&#39;t configured, but should be within a week or so.&nbsp; You can use it now, though.&nbsp; If you go to console.cloud.google.com and login with your UMBC account, you should see a project named &quot;econhistoricalpatents&quot;.&nbsp; You can use that to run any services, etc.<br /> <br /> Once I can get the billing corrected, I&#39;ll reach out to Johnnie to get that in place.&nbsp; Please let us know if you have any questions.<br /> <br /> Tim</p>  <p>--&nbsp;<br /> Tim Champ<br /> Associate Director of Research and Enterprise Computing<br /> Enterprise Infrastructure Solutions<br /> UMBC - DoIT</p> "
3282078,72437972,Correspond,DoIT-Research-Computing,2025-10-22 18:55:55.0000000,Cloud: Google Vision,stalled,Tim Champ,champ,Enrico Berkes,eberkes,eberkes@umbc.edu,Enrico Berkes,eberkes@umbc.edu,"Hi Tim,  Thank you for letting me know. I will try to login and ask my collaborators to do the same!  Thank you again for all your help,  Enrico   On Tue, Oct 21, 2025 at 12:14 PM, Tim Champ <UMBCHelp@rt.umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3282078 > > > Last Update From Ticket: > > Hello again! > > We've created the Google cloud project for you. I apologize for the delay. > For now, the billing isn't configured, but should be within a week or so. > You can use it now, though. If you go to console.cloud.google.com and > login with your UMBC account, you should see a project named > ""econhistoricalpatents"". You can use that to run any services, etc. > > Once I can get the billing corrected, I'll reach out to Johnnie to get > that in place. Please let us know if you have any questions. > > Tim > > -- > Tim Champ > Associate Director of Research and Enterprise Computing Enterprise > Infrastructure Solutions > UMBC - DoIT > "
3282078,72437972,Correspond,DoIT-Research-Computing,2025-10-22 18:55:55.0000000,Cloud: Google Vision,stalled,Tim Champ,champ,Enrico Berkes,eberkes,eberkes@umbc.edu,Enrico Berkes,eberkes@umbc.edu,"<html><head></head><body><div><div><div><div class="""">Hi Tim,<br></div><div class=""""><br></div><div class="""">Thank you for letting me know. I will try to login and ask my collaborators to do the same!<br></div><div class=""""><br></div><div class="""">Thank you again for all your help,<br></div><div class=""""><br></div><div class="""">Enrico</div></div><div></div><br><div class=""gmail_signature""></div></div><br><div><div class=""gmail_quote"">On Tue, Oct 21, 2025 at 12:14 PM, Tim Champ <span dir=""ltr"">&lt;<a href=""mailto:UMBCHelp@rt.umbc.edu"" target=""_blank"">UMBCHelp@rt.umbc.edu</a>&gt;</span> wrote:<br><blockquote class=""gmail_quote"" style=""margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex""><div class=""gmail_extra""><div class=""gmail_quote sh-color-black sh-color""><p class=""sh-color-black sh-color"">Ticket &lt;URL: <a target=""_blank"" rel=""noopener noreferrer"" href=""https://rt.umbc.edu/Ticket/Display.html?id=3282078"" class=""sh-color-blue sh-color"">https:/<wbr>/<wbr>rt.<wbr>umbc.<wbr>edu/<wbr>Ticket/<wbr>Display.<wbr>html?id=3282078</a> &gt; </p><p class=""sh-color-black sh-color""> Last Update From Ticket: </p><p class=""sh-color-black sh-color""> Hello again! </p><p class=""sh-color-black sh-color""> We&#39;ve created the Google cloud project for you. I apologize for the delay. For now, the billing isn&#39;t configured, but should be within a week or so. You can use it now, though. If you go to <a target=""_blank"" rel=""noopener noreferrer"" href=""http://console.cloud.google.com/"" class=""sh-color-blue sh-color"">console.<wbr>cloud.<wbr>google.<wbr>com</a> and login with your UMBC account, you should see a project named &quot;econhistoricalpatents&quot;. You can use that to run any services, etc. </p><p class=""sh-color-black sh-color""> Once I can get the billing corrected, I&#39;ll reach out to Johnnie to get that in place. Please let us know if you have any questions. </p><p class=""sh-color-black sh-color""> Tim </p><p class=""sh-color-black sh-color""> -- <br> Tim Champ <br> Associate Director of Research and Enterprise Computing Enterprise Infrastructure Solutions <br> UMBC - DoIT</p></div></div></blockquote></div></div><br></div></body></html> "
3282078,72617448,Comment,DoIT-Research-Computing,2025-10-31 13:11:06.0000000,Cloud: Google Vision,stalled,Tim Champ,champ,Enrico Berkes,eberkes,eberkes@umbc.edu,Tim Champ,champ@umbc.edu,"<p>Waiting for info from Burwood to get GCP billing in place</p>  <p>--&nbsp;<br /> Tim Champ<br /> Associate Director of Research and Enterprise Computing<br /> Enterprise Infrastructure Solutions<br /> UMBC - DoIT</p> "
3282188,71978241,Create,DoIT-Research-Computing,2025-09-28 19:03:56.0000000,HPC Slurm/Software Issue: Please install newer Matlab,open,Max Breitmeyer,mb17,Matthias Gobbert,gobbert,gobbert@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Matthias Last Name:                 Gobbert Email:                     gobbert@umbc.edu Campus ID:                 AX68683  Request Type:              High Performance Cluster   Hi,  We are on R2023b on chip right now, which is two years old.  Could you please install R2025b? This came out recently.  If there are any concerns about doing such things, I would be interested to hear.  Thanks,  Matthias  "
3282188,72041798,Correspond,DoIT-Research-Computing,2025-10-01 16:34:02.0000000,HPC Slurm/Software Issue: Please install newer Matlab,open,Max Breitmeyer,mb17,Matthias Gobbert,gobbert,gobbert@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Matthias,</p>  <p>I&#39;ve added matlab 2025b to the 2024 partitions, and working on getting it installed in 2018 and 2021 partitions. Please try it out and let me know if you have any problems!</p>  <p>On Sun Sep 28 15:03:56 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Matthias Last Name:                 Gobbert Email:                     gobbert@umbc.edu Campus ID:                 AX68683  Request Type:              High Performance Cluster   Hi,  We are on R2023b on chip right now, which is two years old.  Could you please install R2025b? This came out recently.  If there are any concerns about doing such things, I would be interested to hear.  Thanks,  Matthias  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3282188,72043017,Correspond,DoIT-Research-Computing,2025-10-01 17:15:31.0000000,HPC Slurm/Software Issue: Please install newer Matlab,open,Max Breitmeyer,mb17,Matthias Gobbert,gobbert,gobbert@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Matthias,</p>  <p>Matlab 2025b should now be available on all partitions.&nbsp;</p>  <p>On Wed Oct 01 12:34:02 2025, OL73413 wrote:</p>  <blockquote> <div> <p>Hi Matthias,</p>  <p>I&#39;ve added matlab 2025b to the 2024 partitions, and working on getting it installed in 2018 and 2021 partitions. Please try it out and let me know if you have any problems!</p>  <p>On Sun Sep 28 15:03:56 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Matthias Last Name:                 Gobbert Email:                     gobbert@umbc.edu Campus ID:                 AX68683  Request Type:              High Performance Cluster   Hi,  We are on R2023b on chip right now, which is two years old.  Could you please install R2025b? This came out recently.  If there are any concerns about doing such things, I would be interested to hear.  Thanks,  Matthias  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3282188,72050057,Correspond,DoIT-Research-Computing,2025-10-01 20:36:51.0000000,HPC Slurm/Software Issue: Please install newer Matlab,open,Max Breitmeyer,mb17,Matthias Gobbert,gobbert,gobbert@umbc.edu,Matthias Gobbert,gobbert@umbc.edu,"Hi, Max,  Well, when I try running the analogous slurm script as in the past, I am getting an error. I am also getting those errors when trying an interactive session.  I am only using 2024.  In directory /umbc/rs/gobbert/common/research/FVM2DtestPollution/matlab/  there are subdirectories ver6.3matlabR2023b ver6.4matlabR2025b  As the names imply, the only difference is, which Matlab release is loaded in the slurm file run-matlab_parallel.slurm In ver6.3 you see what is supposed to happen for a job that took 1 or 2 minutes. In ver6.4 you see the error in the slurm....err file.  Does this help?  Feel free to create a new directory and try right there again.  Matthias  Matthias K. Gobbert, Ph.D., Professor of Mathematics Department of Mathematics and Statistics Center for Interdisciplinary Research and Consulting (circ.umbc.edu) UMBC High Performance Computing Facility (hpcf.umbc.edu) REU Site: Online Interdisciplinary Big Data Analytics (BigDataREU.umbc.edu <http://bigdatareu.umbc.edu>) University of Maryland, Baltimore County 1000 Hilltop Circle, Baltimore, MD 21250 http://www.umbc.edu/~gobbert   On Wed, Oct 1, 2025 at 1:15=E2=80=AFPM Max Breitmeyer via RT <UMBCHelp@rt.u= mbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3282188 > > > Last Update From Ticket: > > Hi Matthias, > > Matlab 2025b should now be available on all partitions. > > On Wed Oct 01 12:34:02 2025, OL73413 wrote: > > > Hi Matthias, > > > I've added matlab 2025b to the 2024 partitions, and working on getting = it > > installed in 2018 and 2021 partitions. Please try it out and let me know > if > > you have any problems! > > > On Sun Sep 28 15:03:56 2025, ZZ99999 wrote: > > >> First Name:                Matthias > >> Last Name:                 Gobbert > >> Email:                     gobbert@umbc.edu > >> Campus ID:                 AX68683 > >> > >> Request Type:              High Performance Cluster > >> > >> > >> Hi, > >> > >> We are on R2023b on chip right now, which is two years old. > >> > >> Could you please install R2025b? This came out recently. > >> > >> If there are any concerns about doing such things, I would be > interested to hear. > >> > >> Thanks, > >> > >> Matthias > > > > > -- > > > Best, > > Max Breitmeyer > > DOIT HPC System Administrator > > -- > > Best, > Max Breitmeyer > DOIT HPC System Administrator > > "
3282188,72050057,Correspond,DoIT-Research-Computing,2025-10-01 20:36:51.0000000,HPC Slurm/Software Issue: Please install newer Matlab,open,Max Breitmeyer,mb17,Matthias Gobbert,gobbert,gobbert@umbc.edu,Matthias Gobbert,gobbert@umbc.edu,"<div dir=3D""ltr""><div>Hi, Max,</div><div><br></div><div>Well, when I try ru= nning the analogous slurm script as in the past, I am getting an error. I a= m also getting those errors when trying an interactive session.</div><div><= br></div><div>I am only using 2024.</div><div><br></div><div>In directory</= div><div>/umbc/rs/gobbert/common/research/FVM2DtestPollution/matlab/</div><= div><br></div><div>there are subdirectories=C2=A0</div><div><div dir=3D""ltr= "" class=3D""gmail_signature"" data-smartmail=3D""gmail_signature""><div dir=3D""= ltr""><div>ver6.3matlabR2023b<br>ver6.4matlabR2025b<br></div><div><br></div>= <div>As the names imply, the only difference is, which Matlab release is lo= aded in the slurm file=C2=A0run-matlab_parallel.slurm</div><div>In ver6.3 y= ou see what is supposed to happen for a job that took 1 or 2 minutes.</div>= <div>In ver6.4 you see the error in the slurm....err file.</div><div><br></= div><div>Does this help?</div><div><br></div><div>Feel free to create a new=  directory and try right there again.<br><br>Matthias</div><div><br></div><= div>Matthias K. Gobbert, Ph.D., Professor of Mathematics</div><div>Departme= nt of Mathematics and Statistics</div><div>Center for Interdisciplinary Res= earch and Consulting (<a href=3D""http://circ.umbc.edu"" target=3D""_blank"">ci= rc.umbc.edu</a>)</div><div>UMBC High Performance Computing Facility (<a hre= f=3D""http://hpcf.umbc.edu"" target=3D""_blank"">hpcf.umbc.edu</a>)</div><div>R= EU Site: Online Interdisciplinary Big Data Analytics (<a href=3D""http://big= datareu.umbc.edu"" target=3D""_blank"">BigDataREU.umbc.edu</a>)</div><div>Univ= ersity of Maryland, Baltimore County</div><div>1000 Hilltop Circle, Baltimo= re, MD 21250</div><div><a href=3D""http://www.umbc.edu/~gobbert"" target=3D""_= blank"">http://www.umbc.edu/~gobbert</a></div></div></div></div><br></div><b= r><div class=3D""gmail_quote gmail_quote_container""><div dir=3D""ltr"" class= =3D""gmail_attr"">On Wed, Oct 1, 2025 at 1:15=E2=80=AFPM Max Breitmeyer via R= T &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; = wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""margin:0px 0px 0= px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"">Ticket &l= t;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D3282188"" rel= =3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?i= d=3D3282188</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Matthias,<br> <br> Matlab 2025b should now be available on all partitions.<br> <br> On Wed Oct 01 12:34:02 2025, OL73413 wrote:<br> <br> &gt; Hi Matthias,<br> <br> &gt; I&#39;ve added matlab 2025b to the 2024 partitions, and working on get= ting it<br> &gt; installed in 2018 and 2021 partitions. Please try it out and let me kn= ow if<br> &gt; you have any problems!<br> <br> &gt; On Sun Sep 28 15:03:56 2025, ZZ99999 wrote:<br> <br> &gt;&gt; First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=  Matthias<br> &gt;&gt; Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0Gobbert<br> &gt;&gt; Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 =C2=A0<a href=3D""mailto:gobbert@umbc.edu"" target=3D""_blank"">gobb= ert@umbc.edu</a><br> &gt;&gt; Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0AX68683<br> &gt;&gt; <br> &gt;&gt; Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High=  Performance Cluster<br> &gt;&gt; <br> &gt;&gt; <br> &gt;&gt; Hi,<br> &gt;&gt; <br> &gt;&gt; We are on R2023b on chip right now, which is two years old.<br> &gt;&gt; <br> &gt;&gt; Could you please install R2025b? This came out recently.<br> &gt;&gt; <br> &gt;&gt; If there are any concerns about doing such things, I would be inte= rested to hear.<br> &gt;&gt; <br> &gt;&gt; Thanks,<br> &gt;&gt; <br> &gt;&gt; Matthias<br> &gt; <br> <br> &gt; --<br> <br> &gt; Best,<br> &gt; Max Breitmeyer<br> &gt; DOIT HPC System Administrator<br> <br> --<br> <br> Best,<br> Max Breitmeyer<br> DOIT HPC System Administrator<br> <br> </blockquote></div> "
3282188,72050481,Correspond,DoIT-Research-Computing,2025-10-01 21:10:02.0000000,HPC Slurm/Software Issue: Please install newer Matlab,open,Max Breitmeyer,mb17,Matthias Gobbert,gobbert,gobbert@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Matthias,</p>  <p>Sorry about that. At first it looked like I just needed to point to the = updated license server, but after doing that and running it&#39;s saying we= &#39;re missing a license for this release. I&#39;m working with license ad= ministrator right now. Once I know more, I&#39;ll let you know.&nbsp;</p>  <p>On Wed Oct 01 16:36:51 2025, AX68683 wrote:</p>  <blockquote> <div> <div>Hi, Max,</div>  <div>&nbsp;</div>  <div>Well, when I try running the analogous slurm script as in the past, I = am getting an error. I am also getting those errors when trying an interact= ive session.</div>  <div>&nbsp;</div>  <div>I am only using 2024.</div>  <div>&nbsp;</div>  <div>In directory</div>  <div>/umbc/rs/gobbert/common/research/FVM2DtestPollution/matlab/</div>  <div>&nbsp;</div>  <div>there are subdirectories&nbsp;</div>  <div> <div> <div> <div>ver6.3matlabR2023b<br /> ver6.4matlabR2025b</div>  <div>&nbsp;</div>  <div>As the names imply, the only difference is, which Matlab release is lo= aded in the slurm file&nbsp;run-matlab_parallel.slurm</div>  <div>In ver6.3 you see what is supposed to happen for a job that took 1 or = 2 minutes.</div>  <div>In ver6.4 you see the error in the slurm....err file.</div>  <div>&nbsp;</div>  <div>Does this help?</div>  <div>&nbsp;</div>  <div>Feel free to create a new directory and try right there again.<br /> <br /> Matthias</div>  <div>&nbsp;</div>  <div>Matthias K. Gobbert, Ph.D., Professor of Mathematics</div>  <div>Department of Mathematics and Statistics</div>  <div>Center for Interdisciplinary Research and Consulting (circ.umbc.edu)</= div>  <div>UMBC High Performance Computing Facility (hpcf.umbc.edu)</div>  <div>REU Site: Online Interdisciplinary Big Data Analytics (BigDataREU.umbc= .edu)</div>  <div>University of Maryland, Baltimore County</div>  <div>1000 Hilltop Circle, Baltimore, MD 21250</div>  <div>http://www.umbc.edu/~gobbert</div> </div> </div> </div> </div> &nbsp;  <div> <div>On Wed, Oct 1, 2025 at 1:15=E2=80=AFPM Max Breitmeyer via RT &lt;UMBCH= elp@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 82188 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hi Matthias,<br /> <br /> Matlab 2025b should now be available on all partitions.<br /> <br /> On Wed Oct 01 12:34:02 2025, OL73413 wrote:<br /> <br /> &gt; Hi Matthias,<br /> <br /> &gt; I&#39;ve added matlab 2025b to the 2024 partitions, and working on get= ting it<br /> &gt; installed in 2018 and 2021 partitions. Please try it out and let me kn= ow if<br /> &gt; you have any problems!<br /> <br /> &gt; On Sun Sep 28 15:03:56 2025, ZZ99999 wrote:<br /> <br /> &gt;&gt; First Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;=  Matthias<br /> &gt;&gt; Last Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = &nbsp;Gobbert<br /> &gt;&gt; Email:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p; &nbsp; &nbsp;gobbert@umbc.edu<br /> &gt;&gt; Campus ID:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = &nbsp;AX68683<br /> &gt;&gt;<br /> &gt;&gt; Request Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; High=  Performance Cluster<br /> &gt;&gt;<br /> &gt;&gt;<br /> &gt;&gt; Hi,<br /> &gt;&gt;<br /> &gt;&gt; We are on R2023b on chip right now, which is two years old.<br /> &gt;&gt;<br /> &gt;&gt; Could you please install R2025b? This came out recently.<br /> &gt;&gt;<br /> &gt;&gt; If there are any concerns about doing such things, I would be inte= rested to hear.<br /> &gt;&gt;<br /> &gt;&gt; Thanks,<br /> &gt;&gt;<br /> &gt;&gt; Matthias<br /> &gt;<br /> <br /> &gt; --<br /> <br /> &gt; Best,<br /> &gt; Max Breitmeyer<br /> &gt; DOIT HPC System Administrator<br /> <br /> --<br /> <br /> Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator<br /> &nbsp;</blockquote> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3282188,72589225,Correspond,DoIT-Research-Computing,2025-10-29 21:43:15.0000000,HPC Slurm/Software Issue: Please install newer Matlab,open,Max Breitmeyer,mb17,Matthias Gobbert,gobbert,gobbert@umbc.edu,Matthias Gobbert,gobbert@umbc.edu,"<div> <p>Hi, Max,</p>  <p>Have you been able to find out anything?</p>  <p>A student has now hit upon the issue and asked me, hence my follow-up.</= p>  <p>I understand you guys are juggling many tickets.</p>  <p>Matthias</p>  <p>&nbsp;</p>  <p>On Wed Oct 01 17:10:02 2025, OL73413 wrote:</p>  <blockquote> <div> <p>Hi Matthias,</p>  <p>Sorry about that. At first it looked like I just needed to point to the = updated license server, but after doing that and running it&#39;s saying we= &#39;re missing a license for this release. I&#39;m working with license ad= ministrator right now. Once I know more, I&#39;ll let you know.&nbsp;</p>  <p>On Wed Oct 01 16:36:51 2025, AX68683 wrote:</p>  <blockquote> <div> <div>Hi, Max,</div>  <div>&nbsp;</div>  <div>Well, when I try running the analogous slurm script as in the past, I = am getting an error. I am also getting those errors when trying an interact= ive session.</div>  <div>&nbsp;</div>  <div>I am only using 2024.</div>  <div>&nbsp;</div>  <div>In directory</div>  <div>/umbc/rs/gobbert/common/research/FVM2DtestPollution/matlab/</div>  <div>&nbsp;</div>  <div>there are subdirectories&nbsp;</div>  <div> <div> <div> <div>ver6.3matlabR2023b<br /> ver6.4matlabR2025b</div>  <div>&nbsp;</div>  <div>As the names imply, the only difference is, which Matlab release is lo= aded in the slurm file&nbsp;run-matlab_parallel.slurm</div>  <div>In ver6.3 you see what is supposed to happen for a job that took 1 or = 2 minutes.</div>  <div>In ver6.4 you see the error in the slurm....err file.</div>  <div>&nbsp;</div>  <div>Does this help?</div>  <div>&nbsp;</div>  <div>Feel free to create a new directory and try right there again.<br /> <br /> Matthias</div>  <div>&nbsp;</div>  <div>Matthias K. Gobbert, Ph.D., Professor of Mathematics</div>  <div>Department of Mathematics and Statistics</div>  <div>Center for Interdisciplinary Research and Consulting (circ.umbc.edu)</= div>  <div>UMBC High Performance Computing Facility (hpcf.umbc.edu)</div>  <div>REU Site: Online Interdisciplinary Big Data Analytics (BigDataREU.umbc= .edu)</div>  <div>University of Maryland, Baltimore County</div>  <div>1000 Hilltop Circle, Baltimore, MD 21250</div>  <div>http://www.umbc.edu/~gobbert</div> </div> </div> </div> </div> &nbsp;  <div> <div>On Wed, Oct 1, 2025 at 1:15=E2=80=AFPM Max Breitmeyer via RT &lt;UMBCH= elp@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 82188 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hi Matthias,<br /> <br /> Matlab 2025b should now be available on all partitions.<br /> <br /> On Wed Oct 01 12:34:02 2025, OL73413 wrote:<br /> <br /> &gt; Hi Matthias,<br /> <br /> &gt; I&#39;ve added matlab 2025b to the 2024 partitions, and working on get= ting it<br /> &gt; installed in 2018 and 2021 partitions. Please try it out and let me kn= ow if<br /> &gt; you have any problems!<br /> <br /> &gt; On Sun Sep 28 15:03:56 2025, ZZ99999 wrote:<br /> <br /> &gt;&gt; First Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;=  Matthias<br /> &gt;&gt; Last Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = &nbsp;Gobbert<br /> &gt;&gt; Email:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p; &nbsp; &nbsp;gobbert@umbc.edu<br /> &gt;&gt; Campus ID:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = &nbsp;AX68683<br /> &gt;&gt;<br /> &gt;&gt; Request Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; High=  Performance Cluster<br /> &gt;&gt;<br /> &gt;&gt;<br /> &gt;&gt; Hi,<br /> &gt;&gt;<br /> &gt;&gt; We are on R2023b on chip right now, which is two years old.<br /> &gt;&gt;<br /> &gt;&gt; Could you please install R2025b? This came out recently.<br /> &gt;&gt;<br /> &gt;&gt; If there are any concerns about doing such things, I would be inte= rested to hear.<br /> &gt;&gt;<br /> &gt;&gt; Thanks,<br /> &gt;&gt;<br /> &gt;&gt; Matthias<br /> &gt;<br /> <br /> &gt; --<br /> <br /> &gt; Best,<br /> &gt; Max Breitmeyer<br /> &gt; DOIT HPC System Administrator<br /> <br /> --<br /> <br /> Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator<br /> &nbsp;</blockquote> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;<br /> Matthias Gobbert<br /> Professor, Mathematics and Statistics<br /> gobbert@umbc.edu</p> "
3282188,72612554,Correspond,DoIT-Research-Computing,2025-10-30 22:51:17.0000000,HPC Slurm/Software Issue: Please install newer Matlab,open,Max Breitmeyer,mb17,Matthias Gobbert,gobbert,gobbert@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Matthias,</p>  <p>We don&#39;t control when the licenses get purchased unfortunately. I kn= ow the plan is that the license will be replaced in the beginning of 2026, = but I&#39;m unsure if it will include a license for MATLAB 2025b. I can see=  if our license include an iso for MATLAB2024a/b if that will help you in t= he short-term?</p>  <p>On Wed Oct 29 17:43:15 2025, AX68683 wrote:</p>  <blockquote> <div> <p>Hi, Max,</p>  <p>Have you been able to find out anything?</p>  <p>A student has now hit upon the issue and asked me, hence my follow-up.</= p>  <p>I understand you guys are juggling many tickets.</p>  <p>Matthias</p>  <p>&nbsp;</p>  <p>On Wed Oct 01 17:10:02 2025, OL73413 wrote:</p>  <blockquote> <div> <p>Hi Matthias,</p>  <p>Sorry about that. At first it looked like I just needed to point to the = updated license server, but after doing that and running it&#39;s saying we= &#39;re missing a license for this release. I&#39;m working with license ad= ministrator right now. Once I know more, I&#39;ll let you know.&nbsp;</p>  <p>On Wed Oct 01 16:36:51 2025, AX68683 wrote:</p>  <blockquote> <div> <div>Hi, Max,</div>  <div>&nbsp;</div>  <div>Well, when I try running the analogous slurm script as in the past, I = am getting an error. I am also getting those errors when trying an interact= ive session.</div>  <div>&nbsp;</div>  <div>I am only using 2024.</div>  <div>&nbsp;</div>  <div>In directory</div>  <div>/umbc/rs/gobbert/common/research/FVM2DtestPollution/matlab/</div>  <div>&nbsp;</div>  <div>there are subdirectories&nbsp;</div>  <div> <div> <div> <div>ver6.3matlabR2023b<br /> ver6.4matlabR2025b</div>  <div>&nbsp;</div>  <div>As the names imply, the only difference is, which Matlab release is lo= aded in the slurm file&nbsp;run-matlab_parallel.slurm</div>  <div>In ver6.3 you see what is supposed to happen for a job that took 1 or = 2 minutes.</div>  <div>In ver6.4 you see the error in the slurm....err file.</div>  <div>&nbsp;</div>  <div>Does this help?</div>  <div>&nbsp;</div>  <div>Feel free to create a new directory and try right there again.<br /> <br /> Matthias</div>  <div>&nbsp;</div>  <div>Matthias K. Gobbert, Ph.D., Professor of Mathematics</div>  <div>Department of Mathematics and Statistics</div>  <div>Center for Interdisciplinary Research and Consulting (circ.umbc.edu)</= div>  <div>UMBC High Performance Computing Facility (hpcf.umbc.edu)</div>  <div>REU Site: Online Interdisciplinary Big Data Analytics (BigDataREU.umbc= .edu)</div>  <div>University of Maryland, Baltimore County</div>  <div>1000 Hilltop Circle, Baltimore, MD 21250</div>  <div>http://www.umbc.edu/~gobbert</div> </div> </div> </div> </div> &nbsp;  <div> <div>On Wed, Oct 1, 2025 at 1:15=E2=80=AFPM Max Breitmeyer via RT &lt;UMBCH= elp@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 82188 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hi Matthias,<br /> <br /> Matlab 2025b should now be available on all partitions.<br /> <br /> On Wed Oct 01 12:34:02 2025, OL73413 wrote:<br /> <br /> &gt; Hi Matthias,<br /> <br /> &gt; I&#39;ve added matlab 2025b to the 2024 partitions, and working on get= ting it<br /> &gt; installed in 2018 and 2021 partitions. Please try it out and let me kn= ow if<br /> &gt; you have any problems!<br /> <br /> &gt; On Sun Sep 28 15:03:56 2025, ZZ99999 wrote:<br /> <br /> &gt;&gt; First Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;=  Matthias<br /> &gt;&gt; Last Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = &nbsp;Gobbert<br /> &gt;&gt; Email:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p; &nbsp; &nbsp;gobbert@umbc.edu<br /> &gt;&gt; Campus ID:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = &nbsp;AX68683<br /> &gt;&gt;<br /> &gt;&gt; Request Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; High=  Performance Cluster<br /> &gt;&gt;<br /> &gt;&gt;<br /> &gt;&gt; Hi,<br /> &gt;&gt;<br /> &gt;&gt; We are on R2023b on chip right now, which is two years old.<br /> &gt;&gt;<br /> &gt;&gt; Could you please install R2025b? This came out recently.<br /> &gt;&gt;<br /> &gt;&gt; If there are any concerns about doing such things, I would be inte= rested to hear.<br /> &gt;&gt;<br /> &gt;&gt; Thanks,<br /> &gt;&gt;<br /> &gt;&gt; Matthias<br /> &gt;<br /> <br /> &gt; --<br /> <br /> &gt; Best,<br /> &gt; Max Breitmeyer<br /> &gt; DOIT HPC System Administrator<br /> <br /> --<br /> <br /> Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator<br /> &nbsp;</blockquote> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;<br /> Matthias Gobbert<br /> Professor, Mathematics and Statistics<br /> gobbert@umbc.edu</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3282188,72613560,Correspond,DoIT-Research-Computing,2025-10-31 01:46:11.0000000,HPC Slurm/Software Issue: Please install newer Matlab,open,Max Breitmeyer,mb17,Matthias Gobbert,gobbert,gobbert@umbc.edu,Matthias Gobbert,gobbert@umbc.edu,"Hi, Max,  Well, this sounds too odd. We certainly have a current campus license, otherwise no Matlab of any kind would work. And with a current license, we should have the right to a 2025 release. In fact, on my laptop I have 2025. Could you forward this to someone who works with the licenses? If it is too much work on your end, I can of course live with it until January, but this just does not sound right.  Matthias  Matthias K. Gobbert, Ph.D., Professor of Mathematics Department of Mathematics and Statistics Center for Interdisciplinary Research and Consulting (circ.umbc.edu) UMBC High Performance Computing Facility (hpcf.umbc.edu) REU Site: Online Interdisciplinary Big Data Analytics (BigDataREU.umbc.edu <http://bigdatareu.umbc.edu>) University of Maryland, Baltimore County 1000 Hilltop Circle, Baltimore, MD 21250 http://www.umbc.edu/~gobbert   On Thu, Oct 30, 2025 at 6:51=E2=80=AFPM Max Breitmeyer via RT <UMBCHelp@rt.= umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3282188 > > > Last Update From Ticket: > > Hi Matthias, > > We don't control when the licenses get purchased unfortunately. I know the > plan > is that the license will be replaced in the beginning of 2026, but I'm > unsure > if it will include a license for MATLAB 2025b. I can see if our license > include > an iso for MATLAB2024a/b if that will help you in the short-term? > > On Wed Oct 29 17:43:15 2025, AX68683 wrote: > > > Hi, Max, > > > Have you been able to find out anything? > > > A student has now hit upon the issue and asked me, hence my follow-up. > > > I understand you guys are juggling many tickets. > > > Matthias > > > On Wed Oct 01 17:10:02 2025, OL73413 wrote: > > >> Hi Matthias, > > >> Sorry about that. At first it looked like I just needed to point to the > >> updated license server, but after doing that and running it's saying > >> we're missing a license for this release. I'm working with license > >> administrator right now. Once I know more, I'll let you know. > > >> On Wed Oct 01 16:36:51 2025, AX68683 wrote: > > >>> Hi, Max, Well, when I try running the analogous slurm script as in > >>> the past, I am getting an error. I am also getting those errors > >>> when trying an interactive session. I am only using 2024. In > >>> directory/umbc/rs/gobbert/common/research/FVM2DtestPollution/matlab/ > >>> there are subdirectories ver6.3matlabR2023b > >>> ver6.4matlabR2025b As the names imply, the only difference is, > >>> which Matlab release is loaded in the slurm file > >>> run-matlab_parallel.slurmIn ver6.3 you see what is supposed to > >>> happen for a job that took 1 or 2 minutes.In ver6.4 you see the > >>> error in the slurm....err file. Does this help? Feel free to create > >>> a new directory and try right there again. > > >>> Matthias Matthias K. Gobbert, Ph.D., Professor of MathematicsDepartme= nt > >>> of Mathematics and StatisticsCenter for Interdisciplinary Research > >>> and Consulting (circ.umbc.edu)UMBC High Performance Computing > >>> Facility (hpcf.umbc.edu)REU Site: Online Interdisciplinary Big Data > >>> Analytics (BigDataREU.umbc.edu)University of Maryland, Baltimore > >>> County1000 Hilltop Circle, Baltimore, MD 21250 > http://www.umbc.edu/~gobbert > >>> On Wed, Oct 1, 2025 at 1:15 PM Max Breitmeyer via RT > >>> <UMBCHelp@rt.umbc.edu> wrote: > > >>>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3282188 > >>>> > > > >>>> Last Update From Ticket: > > >>>> Hi Matthias, > > >>>> Matlab 2025b should now be available on all partitions. > > >>>> On Wed Oct 01 12:34:02 2025, OL73413 wrote: > > >>>> > Hi Matthias, > > >>>> > I've added matlab 2025b to the 2024 partitions, and working > >>>> on getting it > >>>> > installed in 2018 and 2021 partitions. Please try it out and > >>>> let me know if > >>>> > you have any problems! > > >>>> > On Sun Sep 28 15:03:56 2025, ZZ99999 wrote: > > >>>> >> First Name: Matthias > >>>> >> Last Name: Gobbert > >>>> >> Email: gobbert@umbc.edu > >>>> >> Campus ID: AX68683 > >>>> >> > >>>> >> Request Type: High Performance Cluster > >>>> >> > >>>> >> > >>>> >> Hi, > >>>> >> > >>>> >> We are on R2023b on chip right now, which is two years old. > >>>> >> > >>>> >> Could you please install R2025b? This came out recently. > >>>> >> > >>>> >> If there are any concerns about doing such things, I would > >>>> be interested to hear. > >>>> >> > >>>> >> Thanks, > >>>> >> > >>>> >> Matthias > >>>> > > > >>>> > -- > > >>>> > Best, > >>>> > Max Breitmeyer > >>>> > DOIT HPC System Administrator > > >>>> -- > > >>>> Best, > >>>> Max Breitmeyer > >>>> DOIT HPC System Administrator > > >> -- > > >> Best, > >> Max Breitmeyer > >> DOIT HPC System Administrator > > > -- > > Matthias Gobbert > > Professor, Mathematics and Statistics > > gobbert@umbc.edu > > -- > > Best, > Max Breitmeyer > DOIT HPC System Administrator > > "
3282188,72613560,Correspond,DoIT-Research-Computing,2025-10-31 01:46:11.0000000,HPC Slurm/Software Issue: Please install newer Matlab,open,Max Breitmeyer,mb17,Matthias Gobbert,gobbert,gobbert@umbc.edu,Matthias Gobbert,gobbert@umbc.edu,"<div dir=3D""ltr""><div>Hi, Max,</div><div><br></div><div>Well, this sounds t= oo odd. We certainly have a current campus license, otherwise no Matlab of = any kind would work. And with a current license, we should have the right t= o a 2025 release. In fact, on my laptop I have 2025.</div><div>Could you fo= rward this to someone who works with the licenses? If it is too much work o= n your end, I can of course live with it until January, but this just does = not sound right.</div><div><br></div><div>Matthias</div><div><div dir=3D""lt= r"" class=3D""gmail_signature"" data-smartmail=3D""gmail_signature""><div dir=3D= ""ltr""><div><br></div><div>Matthias K. Gobbert, Ph.D., Professor of Mathemat= ics</div><div>Department of Mathematics and Statistics</div><div>Center for=  Interdisciplinary Research and Consulting (<a href=3D""http://circ.umbc.edu= "" target=3D""_blank"">circ.umbc.edu</a>)</div><div>UMBC High Performance Comp= uting Facility (<a href=3D""http://hpcf.umbc.edu"" target=3D""_blank"">hpcf.umb= c.edu</a>)</div><div>REU Site: Online Interdisciplinary Big Data Analytics = (<a href=3D""http://bigdatareu.umbc.edu"" target=3D""_blank"">BigDataREU.umbc.e= du</a>)</div><div>University of Maryland, Baltimore County</div><div>1000 H= illtop Circle, Baltimore, MD 21250</div><div><a href=3D""http://www.umbc.edu= /~gobbert"" target=3D""_blank"">http://www.umbc.edu/~gobbert</a></div></div></= div></div><br></div><br><div class=3D""gmail_quote gmail_quote_container""><d= iv dir=3D""ltr"" class=3D""gmail_attr"">On Thu, Oct 30, 2025 at 6:51=E2=80=AFPM=  Max Breitmeyer via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp= @rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" styl= e=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);paddin= g-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.h= tml?id=3D3282188"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/= Ticket/Display.html?id=3D3282188</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Matthias,<br> <br> We don&#39;t control when the licenses get purchased unfortunately. I know = the plan<br> is that the license will be replaced in the beginning of 2026, but I&#39;m = unsure<br> if it will include a license for MATLAB 2025b. I can see if our license inc= lude<br> an iso for MATLAB2024a/b if that will help you in the short-term?<br> <br> On Wed Oct 29 17:43:15 2025, AX68683 wrote:<br> <br> &gt; Hi, Max,<br> <br> &gt; Have you been able to find out anything?<br> <br> &gt; A student has now hit upon the issue and asked me, hence my follow-up.= <br> <br> &gt; I understand you guys are juggling many tickets.<br> <br> &gt; Matthias<br> <br> &gt; On Wed Oct 01 17:10:02 2025, OL73413 wrote:<br> <br> &gt;&gt; Hi Matthias,<br> <br> &gt;&gt; Sorry about that. At first it looked like I just needed to point t= o the<br> &gt;&gt; updated license server, but after doing that and running it&#39;s = saying<br> &gt;&gt; we&#39;re missing a license for this release. I&#39;m working with=  license<br> &gt;&gt; administrator right now. Once I know more, I&#39;ll let you know.<= br> <br> &gt;&gt; On Wed Oct 01 16:36:51 2025, AX68683 wrote:<br> <br> &gt;&gt;&gt; Hi, Max, Well, when I try running the analogous slurm script a= s in<br> &gt;&gt;&gt; the past, I am getting an error. I am also getting those error= s<br> &gt;&gt;&gt; when trying an interactive session. I am only using 2024. In<b= r> &gt;&gt;&gt; directory/umbc/rs/gobbert/common/research/FVM2DtestPollution/m= atlab/<br> &gt;&gt;&gt; there are subdirectories ver6.3matlabR2023b<br> &gt;&gt;&gt; ver6.4matlabR2025b As the names imply, the only difference is,= <br> &gt;&gt;&gt; which Matlab release is loaded in the slurm file<br> &gt;&gt;&gt; run-matlab_parallel.slurmIn ver6.3 you see what is supposed to= <br> &gt;&gt;&gt; happen for a job that took 1 or 2 minutes.In ver6.4 you see th= e<br> &gt;&gt;&gt; error in the slurm....err file. Does this help? Feel free to c= reate<br> &gt;&gt;&gt; a new directory and try right there again.<br> <br> &gt;&gt;&gt; Matthias Matthias K. Gobbert, Ph.D., Professor of MathematicsD= epartment<br> &gt;&gt;&gt; of Mathematics and StatisticsCenter for Interdisciplinary Rese= arch<br> &gt;&gt;&gt; and Consulting (<a href=3D""http://circ.umbc.edu"" rel=3D""norefe= rrer"" target=3D""_blank"">circ.umbc.edu</a>)UMBC High Performance Computing<b= r> &gt;&gt;&gt; Facility (<a href=3D""http://hpcf.umbc.edu"" rel=3D""noreferrer"" = target=3D""_blank"">hpcf.umbc.edu</a>)REU Site: Online Interdisciplinary Big = Data<br> &gt;&gt;&gt; Analytics (<a href=3D""http://BigDataREU.umbc.edu"" rel=3D""noref= errer"" target=3D""_blank"">BigDataREU.umbc.edu</a>)University of Maryland, Ba= ltimore<br> &gt;&gt;&gt; County1000 Hilltop Circle, Baltimore, MD 21250<a href=3D""http:= //www.umbc.edu/~gobbert"" rel=3D""noreferrer"" target=3D""_blank"">http://www.um= bc.edu/~gobbert</a><br> &gt;&gt;&gt; On Wed, Oct 1, 2025 at 1:15 PM Max Breitmeyer via RT<br> &gt;&gt;&gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">= UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt;&gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Disp= lay.html?id=3D3282188"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc= .edu/Ticket/Display.html?id=3D3282188</a><br> &gt;&gt;&gt;&gt; &gt;<br> <br> &gt;&gt;&gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt;&gt;&gt; Hi Matthias,<br> <br> &gt;&gt;&gt;&gt; Matlab 2025b should now be available on all partitions.<br> <br> &gt;&gt;&gt;&gt; On Wed Oct 01 12:34:02 2025, OL73413 wrote:<br> <br> &gt;&gt;&gt;&gt; &gt; Hi Matthias,<br> <br> &gt;&gt;&gt;&gt; &gt; I&#39;ve added matlab 2025b to the 2024 partitions, a= nd working<br> &gt;&gt;&gt;&gt; on getting it<br> &gt;&gt;&gt;&gt; &gt; installed in 2018 and 2021 partitions. Please try it = out and<br> &gt;&gt;&gt;&gt; let me know if<br> &gt;&gt;&gt;&gt; &gt; you have any problems!<br> <br> &gt;&gt;&gt;&gt; &gt; On Sun Sep 28 15:03:56 2025, ZZ99999 wrote:<br> <br> &gt;&gt;&gt;&gt; &gt;&gt; First Name: Matthias<br> &gt;&gt;&gt;&gt; &gt;&gt; Last Name: Gobbert<br> &gt;&gt;&gt;&gt; &gt;&gt; Email: <a href=3D""mailto:gobbert@umbc.edu"" target= =3D""_blank"">gobbert@umbc.edu</a><br> &gt;&gt;&gt;&gt; &gt;&gt; Campus ID: AX68683<br> &gt;&gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt;&gt; &gt;&gt; Request Type: High Performance Cluster<br> &gt;&gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt;&gt; &gt;&gt; Hi,<br> &gt;&gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt;&gt; &gt;&gt; We are on R2023b on chip right now, which is two = years old.<br> &gt;&gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt;&gt; &gt;&gt; Could you please install R2025b? This came out re= cently.<br> &gt;&gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt;&gt; &gt;&gt; If there are any concerns about doing such things= , I would<br> &gt;&gt;&gt;&gt; be interested to hear.<br> &gt;&gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt;&gt; &gt;&gt; Thanks,<br> &gt;&gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt;&gt; &gt;&gt; Matthias<br> &gt;&gt;&gt;&gt; &gt;<br> <br> &gt;&gt;&gt;&gt; &gt; --<br> <br> &gt;&gt;&gt;&gt; &gt; Best,<br> &gt;&gt;&gt;&gt; &gt; Max Breitmeyer<br> &gt;&gt;&gt;&gt; &gt; DOIT HPC System Administrator<br> <br> &gt;&gt;&gt;&gt; --<br> <br> &gt;&gt;&gt;&gt; Best,<br> &gt;&gt;&gt;&gt; Max Breitmeyer<br> &gt;&gt;&gt;&gt; DOIT HPC System Administrator<br> <br> &gt;&gt; --<br> <br> &gt;&gt; Best,<br> &gt;&gt; Max Breitmeyer<br> &gt;&gt; DOIT HPC System Administrator<br> <br> &gt; --<br> &gt; Matthias Gobbert<br> &gt; Professor, Mathematics and Statistics<br> &gt; <a href=3D""mailto:gobbert@umbc.edu"" target=3D""_blank"">gobbert@umbc.edu= </a><br> <br> --<br> <br> Best,<br> Max Breitmeyer<br> DOIT HPC System Administrator<br> <br> </blockquote></div> "
3282188,72704805,Correspond,DoIT-Research-Computing,2025-11-04 14:18:17.0000000,HPC Slurm/Software Issue: Please install newer Matlab,open,Max Breitmeyer,mb17,Matthias Gobbert,gobbert,gobbert@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Matthias,</p>  <p>While it is true we have a current campus license, there is a difference=  between a &quot;site license&quot; which is what we need for MATLAB to ope= rate on shared resources such as the HPC or on in computer labs, and then t= here are individual licenses handed out by the university that lets people = use it on their own equipment (such as your laptop). However, the procureme= nt of the new site-license which includes 2025b, was recently completed, an= d our license manager put it into place yesterday. As such, MATLAB2025b is = now available for use on the cluster.</p>  <p>On Thu Oct 30 21:46:11 2025, AX68683 wrote:</p>  <blockquote> <div> <div>Hi, Max,</div>  <div>&nbsp;</div>  <div>Well, this sounds too odd. We certainly have a current campus license,=  otherwise no Matlab of any kind would work. And with a current license, we=  should have the right to a 2025 release. In fact, on my laptop I have 2025= .</div>  <div>Could you forward this to someone who works with the licenses? If it i= s too much work on your end, I can of course live with it until January, bu= t this just does not sound right.</div>  <div>&nbsp;</div>  <div>Matthias</div>  <div> <div> <div> <div>&nbsp;</div>  <div>Matthias K. Gobbert, Ph.D., Professor of Mathematics</div>  <div>Department of Mathematics and Statistics</div>  <div>Center for Interdisciplinary Research and Consulting (circ.umbc.edu)</= div>  <div>UMBC High Performance Computing Facility (hpcf.umbc.edu)</div>  <div>REU Site: Online Interdisciplinary Big Data Analytics (BigDataREU.umbc= .edu)</div>  <div>University of Maryland, Baltimore County</div>  <div>1000 Hilltop Circle, Baltimore, MD 21250</div>  <div>http://www.umbc.edu/~gobbert</div> </div> </div> </div> </div> &nbsp;  <div> <div>On Thu, Oct 30, 2025 at 6:51=E2=80=AFPM Max Breitmeyer via RT &lt;UMBC= Help@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 82188 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hi Matthias,<br /> <br /> We don&#39;t control when the licenses get purchased unfortunately. I know = the plan<br /> is that the license will be replaced in the beginning of 2026, but I&#39;m = unsure<br /> if it will include a license for MATLAB 2025b. I can see if our license inc= lude<br /> an iso for MATLAB2024a/b if that will help you in the short-term?<br /> <br /> On Wed Oct 29 17:43:15 2025, AX68683 wrote:<br /> <br /> &gt; Hi, Max,<br /> <br /> &gt; Have you been able to find out anything?<br /> <br /> &gt; A student has now hit upon the issue and asked me, hence my follow-up.= <br /> <br /> &gt; I understand you guys are juggling many tickets.<br /> <br /> &gt; Matthias<br /> <br /> &gt; On Wed Oct 01 17:10:02 2025, OL73413 wrote:<br /> <br /> &gt;&gt; Hi Matthias,<br /> <br /> &gt;&gt; Sorry about that. At first it looked like I just needed to point t= o the<br /> &gt;&gt; updated license server, but after doing that and running it&#39;s = saying<br /> &gt;&gt; we&#39;re missing a license for this release. I&#39;m working with=  license<br /> &gt;&gt; administrator right now. Once I know more, I&#39;ll let you know.<= br /> <br /> &gt;&gt; On Wed Oct 01 16:36:51 2025, AX68683 wrote:<br /> <br /> &gt;&gt;&gt; Hi, Max, Well, when I try running the analogous slurm script a= s in<br /> &gt;&gt;&gt; the past, I am getting an error. I am also getting those error= s<br /> &gt;&gt;&gt; when trying an interactive session. I am only using 2024. In<b= r /> &gt;&gt;&gt; directory/umbc/rs/gobbert/common/research/FVM2DtestPollution/m= atlab/<br /> &gt;&gt;&gt; there are subdirectories ver6.3matlabR2023b<br /> &gt;&gt;&gt; ver6.4matlabR2025b As the names imply, the only difference is,= <br /> &gt;&gt;&gt; which Matlab release is loaded in the slurm file<br /> &gt;&gt;&gt; run-matlab_parallel.slurmIn ver6.3 you see what is supposed to= <br /> &gt;&gt;&gt; happen for a job that took 1 or 2 minutes.In ver6.4 you see th= e<br /> &gt;&gt;&gt; error in the slurm....err file. Does this help? Feel free to c= reate<br /> &gt;&gt;&gt; a new directory and try right there again.<br /> <br /> &gt;&gt;&gt; Matthias Matthias K. Gobbert, Ph.D., Professor of MathematicsD= epartment<br /> &gt;&gt;&gt; of Mathematics and StatisticsCenter for Interdisciplinary Rese= arch<br /> &gt;&gt;&gt; and Consulting (circ.umbc.edu)UMBC High Performance Computing<= br /> &gt;&gt;&gt; Facility (hpcf.umbc.edu)REU Site: Online Interdisciplinary Big=  Data<br /> &gt;&gt;&gt; Analytics (BigDataREU.umbc.edu)University of Maryland, Baltimo= re<br /> &gt;&gt;&gt; County1000 Hilltop Circle, Baltimore, MD 21250http://www.umbc.= edu/~gobbert<br /> &gt;&gt;&gt; On Wed, Oct 1, 2025 at 1:15 PM Max Breitmeyer via RT<br /> &gt;&gt;&gt; &lt;UMBCHelp@rt.umbc.edu&gt; wrote:<br /> <br /> &gt;&gt;&gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id= =3D3282188<br /> &gt;&gt;&gt;&gt; &gt;<br /> <br /> &gt;&gt;&gt;&gt; Last Update From Ticket:<br /> <br /> &gt;&gt;&gt;&gt; Hi Matthias,<br /> <br /> &gt;&gt;&gt;&gt; Matlab 2025b should now be available on all partitions.<br=  /> <br /> &gt;&gt;&gt;&gt; On Wed Oct 01 12:34:02 2025, OL73413 wrote:<br /> <br /> &gt;&gt;&gt;&gt; &gt; Hi Matthias,<br /> <br /> &gt;&gt;&gt;&gt; &gt; I&#39;ve added matlab 2025b to the 2024 partitions, a= nd working<br /> &gt;&gt;&gt;&gt; on getting it<br /> &gt;&gt;&gt;&gt; &gt; installed in 2018 and 2021 partitions. Please try it = out and<br /> &gt;&gt;&gt;&gt; let me know if<br /> &gt;&gt;&gt;&gt; &gt; you have any problems!<br /> <br /> &gt;&gt;&gt;&gt; &gt; On Sun Sep 28 15:03:56 2025, ZZ99999 wrote:<br /> <br /> &gt;&gt;&gt;&gt; &gt;&gt; First Name: Matthias<br /> &gt;&gt;&gt;&gt; &gt;&gt; Last Name: Gobbert<br /> &gt;&gt;&gt;&gt; &gt;&gt; Email: gobbert@umbc.edu<br /> &gt;&gt;&gt;&gt; &gt;&gt; Campus ID: AX68683<br /> &gt;&gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt;&gt; &gt;&gt; Request Type: High Performance Cluster<br /> &gt;&gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt;&gt; &gt;&gt; Hi,<br /> &gt;&gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt;&gt; &gt;&gt; We are on R2023b on chip right now, which is two = years old.<br /> &gt;&gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt;&gt; &gt;&gt; Could you please install R2025b? This came out re= cently.<br /> &gt;&gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt;&gt; &gt;&gt; If there are any concerns about doing such things= , I would<br /> &gt;&gt;&gt;&gt; be interested to hear.<br /> &gt;&gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt;&gt; &gt;&gt; Thanks,<br /> &gt;&gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt;&gt; &gt;&gt; Matthias<br /> &gt;&gt;&gt;&gt; &gt;<br /> <br /> &gt;&gt;&gt;&gt; &gt; --<br /> <br /> &gt;&gt;&gt;&gt; &gt; Best,<br /> &gt;&gt;&gt;&gt; &gt; Max Breitmeyer<br /> &gt;&gt;&gt;&gt; &gt; DOIT HPC System Administrator<br /> <br /> &gt;&gt;&gt;&gt; --<br /> <br /> &gt;&gt;&gt;&gt; Best,<br /> &gt;&gt;&gt;&gt; Max Breitmeyer<br /> &gt;&gt;&gt;&gt; DOIT HPC System Administrator<br /> <br /> &gt;&gt; --<br /> <br /> &gt;&gt; Best,<br /> &gt;&gt; Max Breitmeyer<br /> &gt;&gt; DOIT HPC System Administrator<br /> <br /> &gt; --<br /> &gt; Matthias Gobbert<br /> &gt; Professor, Mathematics and Statistics<br /> &gt; gobbert@umbc.edu<br /> <br /> --<br /> <br /> Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator<br /> &nbsp;</blockquote> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3282188,72712996,Correspond,DoIT-Research-Computing,2025-11-04 16:43:13.0000000,HPC Slurm/Software Issue: Please install newer Matlab,open,Max Breitmeyer,mb17,Matthias Gobbert,gobbert,gobbert@umbc.edu,Matthias Gobbert,gobbert@umbc.edu,"Hi, Max,  Interesting. I wonder what Procurement was negotiating. Anyhow, so should it just work?  In directory  /umbc/rs/pi_gobbert/common/research/FVM2DtestPollution/matlab/ver6.4matlabR= 2025b  it works, and it gives correct results, but the stderr file shows the following message below. Of course, I did not log out and in again or anything like that.  [gobbert@chip-login2 ver6.4matlabR2025b]$ more slurm_matlab_595955.err An error occurred while extracting product files. Try rerunning the installer.  For help, see this <a href=3D""https://www.mathworks.com/pi_ee_mci_glnxa64 "">support  article</a>. An unexpected error has occurred. To resolve this issue, contact <a href=3D"" https://www.mathworks.com/pi_gene_mci_gl nxa64"">Technical Support</a>. An error occurred while extracting product files. Try rerunning the installer.  For help, see this <a href=3D""https://www.mathworks.com/pi_ee_mci_glnxa64 "">support  article</a>. An unexpected error has occurred. To resolve this issue, contact <a href=3D"" https://www.mathworks.com/pi_gene_mci_gl nxa64"">Technical Support</a>. An error occurred while extracting product files. Try rerunning the installer.  For help, see this <a href=3D""https://www.mathworks.com/pi_ee_mci_glnxa64 "">support  article</a>. An unexpected error has occurred. To resolve this issue, contact <a href=3D"" https://www.mathworks.com/pi_gene_mci_gl nxa64"">Technical Support</a>.  Matthias K. Gobbert, Ph.D., Professor of Mathematics Department of Mathematics and Statistics Center for Interdisciplinary Research and Consulting (circ.umbc.edu) UMBC High Performance Computing Facility (hpcf.umbc.edu) REU Site: Online Interdisciplinary Big Data Analytics (BigDataREU.umbc.edu <http://bigdatareu.umbc.edu>) University of Maryland, Baltimore County 1000 Hilltop Circle, Baltimore, MD 21250 http://www.umbc.edu/~gobbert   On Tue, Nov 4, 2025 at 9:18=E2=80=AFAM Max Breitmeyer via RT <UMBCHelp@rt.u= mbc.edu> wrote:  > If you agree your issue is resolved, please give us feedback on your > experience by completing a brief satisfaction survey: > > > https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&customeremail= =3Dgobbert%40umbc.edu&groupid=3DEIS&ticketid=3D3282188&ticketowner=3Dmb17%4= 0umbc.edu&ticketsubject=3DHPC%20Slurm%2FSoftware%20Issue%3A%20Please%20inst= all%20newer%20Matlab > > If you believe your issue has not been resolved, please respond to this > message, which will reopen your ticket. Note: A full record of your reque= st > can be found at: > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3282188 > > > Thank You > > _________________________________________ > > R e s o l u t i o n: > =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D = =3D > > Hi Matthias, > > While it is true we have a current campus license, there is a difference > between a ""site license"" which is what we need for MATLAB to operate on > shared > resources such as the HPC or on in computer labs, and then there are > individual > licenses handed out by the university that lets people use it on their own > equipment (such as your laptop). However, the procurement of the new > site-license which includes 2025b, was recently completed, and our license > manager put it into place yesterday. As such, MATLAB2025b is now available > for > use on the cluster. > > On Thu Oct 30 21:46:11 2025, AX68683 wrote: > > > Hi, Max, Well, this sounds too odd. We certainly have a current campus > > license, otherwise no Matlab of any kind would work. And with a current > > license, we should have the right to a 2025 release. In fact, on my > laptop > > I have 2025.Could you forward this to someone who works with the > licenses? > > If it is too much work on your end, I can of course live with it until > > January, but this just does not sound right. Matthias Matthias K. > Gobbert, > > Ph.D., Professor of MathematicsDepartment of Mathematics and > StatisticsCenter > > for Interdisciplinary Research and Consulting (circ.umbc.edu)UMBC High > > Performance Computing Facility (hpcf.umbc.edu)REU Site: Online > > Interdisciplinary Big Data Analytics (BigDataREU.umbc.edu)University of > > Maryland, Baltimore County1000 Hilltop Circle, Baltimore, MD 21250 > http://www.umbc.edu/~gobbert > > On Thu, Oct 30, 2025 at 6:51 PM Max Breitmeyer via RT > > <UMBCHelp@rt.umbc.edu> wrote: > > >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3282188 > > > >> Last Update From Ticket: > > >> Hi Matthias, > > >> We don't control when the licenses get purchased unfortunately. I know > >> the plan > >> is that the license will be replaced in the beginning of 2026, but I'm > >> unsure > >> if it will include a license for MATLAB 2025b. I can see if our license > >> include > >> an iso for MATLAB2024a/b if that will help you in the short-term? > > >> On Wed Oct 29 17:43:15 2025, AX68683 wrote: > > >> > Hi, Max, > > >> > Have you been able to find out anything? > > >> > A student has now hit upon the issue and asked me, hence my > >> follow-up. > > >> > I understand you guys are juggling many tickets. > > >> > Matthias > > >> > On Wed Oct 01 17:10:02 2025, OL73413 wrote: > > >> >> Hi Matthias, > > >> >> Sorry about that. At first it looked like I just needed to point to > >> the > >> >> updated license server, but after doing that and running it's saying > >> >> we're missing a license for this release. I'm working with license > >> >> administrator right now. Once I know more, I'll let you know. > > >> >> On Wed Oct 01 16:36:51 2025, AX68683 wrote: > > >> >>> Hi, Max, Well, when I try running the analogous slurm script as in > >> >>> the past, I am getting an error. I am also getting those errors > >> >>> when trying an interactive session. I am only using 2024. In > >> >>> > >> directory/umbc/rs/gobbert/common/research/FVM2DtestPollution/matlab/ > >> >>> there are subdirectories ver6.3matlabR2023b > >> >>> ver6.4matlabR2025b As the names imply, the only difference is, > >> >>> which Matlab release is loaded in the slurm file > >> >>> run-matlab_parallel.slurmIn ver6.3 you see what is supposed to > >> >>> happen for a job that took 1 or 2 minutes.In ver6.4 you see the > >> >>> error in the slurm....err file. Does this help? Feel free to create > >> >>> a new directory and try right there again. > > >> >>> Matthias Matthias K. Gobbert, Ph.D., Professor of > >> MathematicsDepartment > >> >>> of Mathematics and StatisticsCenter for Interdisciplinary Research > >> >>> and Consulting (circ.umbc.edu)UMBC High Performance Computing > >> >>> Facility (hpcf.umbc.edu)REU Site: Online Interdisciplinary Big Data > >> >>> Analytics (BigDataREU.umbc.edu)University of Maryland, Baltimore > >> >>> County1000 Hilltop Circle, Baltimore, MD > >> 21250http://www.umbc.edu/~gobbert > >> >>> On Wed, Oct 1, 2025 at 1:15 PM Max Breitmeyer via RT > >> >>> <UMBCHelp@rt.umbc.edu> wrote: > > >> >>>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3282188 > >> >>>> > > > >> >>>> Last Update From Ticket: > > >> >>>> Hi Matthias, > > >> >>>> Matlab 2025b should now be available on all partitions. > > >> >>>> On Wed Oct 01 12:34:02 2025, OL73413 wrote: > > >> >>>> > Hi Matthias, > > >> >>>> > I've added matlab 2025b to the 2024 partitions, and working > >> >>>> on getting it > >> >>>> > installed in 2018 and 2021 partitions. Please try it out and > >> >>>> let me know if > >> >>>> > you have any problems! > > >> >>>> > On Sun Sep 28 15:03:56 2025, ZZ99999 wrote: > > >> >>>> >> First Name: Matthias > >> >>>> >> Last Name: Gobbert > >> >>>> >> Email: gobbert@umbc.edu > >> >>>> >> Campus ID: AX68683 > >> >>>> >> > >> >>>> >> Request Type: High Performance Cluster > >> >>>> >> > >> >>>> >> > >> >>>> >> Hi, > >> >>>> >> > >> >>>> >> We are on R2023b on chip right now, which is two years old. > >> >>>> >> > >> >>>> >> Could you please install R2025b? This came out recently. > >> >>>> >> > >> >>>> >> If there are any concerns about doing such things, I would > >> >>>> be interested to hear. > >> >>>> >> > >> >>>> >> Thanks, > >> >>>> >> > >> >>>> >> Matthias > >> >>>> > > > >> >>>> > -- > > >> >>>> > Best, > >> >>>> > Max Breitmeyer > >> >>>> > DOIT HPC System Administrator > > >> >>>> -- > > >> >>>> Best, > >> >>>> Max Breitmeyer > >> >>>> DOIT HPC System Administrator > > >> >> -- > > >> >> Best, > >> >> Max Breitmeyer > >> >> DOIT HPC System Administrator > > >> > -- > >> > Matthias Gobbert > >> > Professor, Mathematics and Statistics > >> > gobbert@umbc.edu > > >> -- > > >> Best, > >> Max Breitmeyer > >> DOIT HPC System Administrator > > -- > > Best, > Max Breitmeyer > DOIT HPC System Administrator > > > > ______________________________________ > > Original Request: > > Requestors: Matthias Gobbert > > First Name:                Matthias > Last Name:                 Gobbert > Email:                     gobbert@umbc.edu > Campus ID:                 AX68683 > > Request Type:              High Performance Cluster > > > Hi, > > We are on R2023b on chip right now, which is two years old. > > Could you please install R2025b? This came out recently. > > If there are any concerns about doing such things, I would be interested > to hear. > > Thanks, > > Matthias > > > "
3282188,72712996,Correspond,DoIT-Research-Computing,2025-11-04 16:43:13.0000000,HPC Slurm/Software Issue: Please install newer Matlab,open,Max Breitmeyer,mb17,Matthias Gobbert,gobbert,gobbert@umbc.edu,Matthias Gobbert,gobbert@umbc.edu,"<div dir=3D""ltr""><div>Hi, Max,</div><div><br></div><div>Interesting. I wond= er what Procurement was negotiating. Anyhow, so should it just work?</div><= div><br></div><div>In directory</div><div><br></div><div>/umbc/rs/pi_gobber= t/common/research/FVM2DtestPollution/matlab/ver6.4matlabR2025b</div><div><b= r></div><div>it works, and it gives correct results, but the stderr file sh= ows the following message below. Of course, I did not log out and in again = or anything like that.</div><div><br></div><div>[gobbert@chip-login2 ver6.4= matlabR2025b]$ more slurm_matlab_595955.err<br>An error occurred while extr= acting product files.<br>Try rerunning the installer.<br><br>For help, see = this &lt;a href=3D&quot;<a href=3D""https://www.mathworks.com/pi_ee_mci_glnx= a64"">https://www.mathworks.com/pi_ee_mci_glnxa64</a>&quot;&gt;support<br>= =C2=A0article&lt;/a&gt;.<br>An unexpected error has occurred.<br>To resolve=  this issue, contact &lt;a href=3D&quot;<a href=3D""https://www.mathworks.co= m/pi_gene_mci_gl"">https://www.mathworks.com/pi_gene_mci_gl</a><br>nxa64&quo= t;&gt;Technical Support&lt;/a&gt;.<br>An error occurred while extracting pr= oduct files.<br>Try rerunning the installer.<br><br>For help, see this &lt;= a href=3D&quot;<a href=3D""https://www.mathworks.com/pi_ee_mci_glnxa64"">http= s://www.mathworks.com/pi_ee_mci_glnxa64</a>&quot;&gt;support<br>=C2=A0artic= le&lt;/a&gt;.<br>An unexpected error has occurred.<br>To resolve this issue= , contact &lt;a href=3D&quot;<a href=3D""https://www.mathworks.com/pi_gene_m= ci_gl"">https://www.mathworks.com/pi_gene_mci_gl</a><br>nxa64&quot;&gt;Techn= ical Support&lt;/a&gt;.<br>An error occurred while extracting product files= .<br>Try rerunning the installer.<br><br>For help, see this &lt;a href=3D&q= uot;<a href=3D""https://www.mathworks.com/pi_ee_mci_glnxa64"">https://www.mat= hworks.com/pi_ee_mci_glnxa64</a>&quot;&gt;support<br>=C2=A0article&lt;/a&gt= ;.<br>An unexpected error has occurred.<br>To resolve this issue, contact &= lt;a href=3D&quot;<a href=3D""https://www.mathworks.com/pi_gene_mci_gl"">http= s://www.mathworks.com/pi_gene_mci_gl</a><br>nxa64&quot;&gt;Technical Suppor= t&lt;/a&gt;.<br></div><div><div dir=3D""ltr"" class=3D""gmail_signature"" data-= smartmail=3D""gmail_signature""><div dir=3D""ltr""><div><br></div><div>Matthias=  K. Gobbert, Ph.D., Professor of Mathematics</div><div>Department of Mathem= atics and Statistics</div><div>Center for Interdisciplinary Research and Co= nsulting (<a href=3D""http://circ.umbc.edu"" target=3D""_blank"">circ.umbc.edu<= /a>)</div><div>UMBC High Performance Computing Facility (<a href=3D""http://= hpcf.umbc.edu"" target=3D""_blank"">hpcf.umbc.edu</a>)</div><div>REU Site: Onl= ine Interdisciplinary Big Data Analytics (<a href=3D""http://bigdatareu.umbc= .edu"" target=3D""_blank"">BigDataREU.umbc.edu</a>)</div><div>University of Ma= ryland, Baltimore County</div><div>1000 Hilltop Circle, Baltimore, MD 21250= </div><div><a href=3D""http://www.umbc.edu/~gobbert"" target=3D""_blank"">http:= //www.umbc.edu/~gobbert</a></div></div></div></div><br></div><br><div class= =3D""gmail_quote gmail_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr= "">On Tue, Nov 4, 2025 at 9:18=E2=80=AFAM Max Breitmeyer via RT &lt;<a href= =3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></d= iv><blockquote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;bord= er-left:1px solid rgb(204,204,204);padding-left:1ex"">If you agree your issu= e is resolved, please give us feedback on your experience by completing a b= rief satisfaction survey: <br> <br> <a href=3D""https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&amp;= customeremail=3Dgobbert%40umbc.edu&amp;groupid=3DEIS&amp;ticketid=3D3282188= &amp;ticketowner=3Dmb17%40umbc.edu&amp;ticketsubject=3DHPC%20Slurm%2FSoftwa= re%20Issue%3A%20Please%20install%20newer%20Matlab"" rel=3D""noreferrer"" targe= t=3D""_blank"">https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&am= p;customeremail=3Dgobbert%40umbc.edu&amp;groupid=3DEIS&amp;ticketid=3D32821= 88&amp;ticketowner=3Dmb17%40umbc.edu&amp;ticketsubject=3DHPC%20Slurm%2FSoft= ware%20Issue%3A%20Please%20install%20newer%20Matlab</a><br> <br> If you believe your issue has not been resolved, please respond to this mes= sage, which will reopen your ticket. Note: A full record of your request ca= n be found at:=C2=A0 <br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D328= 2188"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3282188</a> &gt;<br> <br> Thank You<br> <br> _________________________________________<br> <br> R e s o l u t i o n:<br> =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D= =C2=A0 <br> <br> Hi Matthias,<br> <br> While it is true we have a current campus license, there is a difference<br> between a &quot;site license&quot; which is what we need for MATLAB to oper= ate on shared<br> resources such as the HPC or on in computer labs, and then there are indivi= dual<br> licenses handed out by the university that lets people use it on their own<= br> equipment (such as your laptop). However, the procurement of the new<br> site-license which includes 2025b, was recently completed, and our license<= br> manager put it into place yesterday. As such, MATLAB2025b is now available = for<br> use on the cluster.<br> <br> On Thu Oct 30 21:46:11 2025, AX68683 wrote:<br> <br> &gt; Hi, Max, Well, this sounds too odd. We certainly have a current campus= <br> &gt; license, otherwise no Matlab of any kind would work. And with a curren= t<br> &gt; license, we should have the right to a 2025 release. In fact, on my la= ptop<br> &gt; I have 2025.Could you forward this to someone who works with the licen= ses?<br> &gt; If it is too much work on your end, I can of course live with it until= <br> &gt; January, but this just does not sound right. Matthias Matthias K. Gobb= ert,<br> &gt; Ph.D., Professor of MathematicsDepartment of Mathematics and Statistic= sCenter<br> &gt; for Interdisciplinary Research and Consulting (<a href=3D""http://circ.= umbc.edu"" rel=3D""noreferrer"" target=3D""_blank"">circ.umbc.edu</a>)UMBC High<= br> &gt; Performance Computing Facility (<a href=3D""http://hpcf.umbc.edu"" rel= =3D""noreferrer"" target=3D""_blank"">hpcf.umbc.edu</a>)REU Site: Online<br> &gt; Interdisciplinary Big Data Analytics (<a href=3D""http://BigDataREU.umb= c.edu"" rel=3D""noreferrer"" target=3D""_blank"">BigDataREU.umbc.edu</a>)Univers= ity of<br> &gt; Maryland, Baltimore County1000 Hilltop Circle, Baltimore, MD 21250<a h= ref=3D""http://www.umbc.edu/~gobbert"" rel=3D""noreferrer"" target=3D""_blank"">h= ttp://www.umbc.edu/~gobbert</a><br> &gt; On Thu, Oct 30, 2025 at 6:51 PM Max Breitmeyer via RT<br> &gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp= @rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html= ?id=3D3282188"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tic= ket/Display.html?id=3D3282188</a> &gt;<br> <br> &gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt; Hi Matthias,<br> <br> &gt;&gt; We don&#39;t control when the licenses get purchased unfortunately= . I know<br> &gt;&gt; the plan<br> &gt;&gt; is that the license will be replaced in the beginning of 2026, but=  I&#39;m<br> &gt;&gt; unsure<br> &gt;&gt; if it will include a license for MATLAB 2025b. I can see if our li= cense<br> &gt;&gt; include<br> &gt;&gt; an iso for MATLAB2024a/b if that will help you in the short-term?<= br> <br> &gt;&gt; On Wed Oct 29 17:43:15 2025, AX68683 wrote:<br> <br> &gt;&gt; &gt; Hi, Max,<br> <br> &gt;&gt; &gt; Have you been able to find out anything?<br> <br> &gt;&gt; &gt; A student has now hit upon the issue and asked me, hence my<b= r> &gt;&gt; follow-up.<br> <br> &gt;&gt; &gt; I understand you guys are juggling many tickets.<br> <br> &gt;&gt; &gt; Matthias<br> <br> &gt;&gt; &gt; On Wed Oct 01 17:10:02 2025, OL73413 wrote:<br> <br> &gt;&gt; &gt;&gt; Hi Matthias,<br> <br> &gt;&gt; &gt;&gt; Sorry about that. At first it looked like I just needed t= o point to<br> &gt;&gt; the<br> &gt;&gt; &gt;&gt; updated license server, but after doing that and running = it&#39;s saying<br> &gt;&gt; &gt;&gt; we&#39;re missing a license for this release. I&#39;m wor= king with license<br> &gt;&gt; &gt;&gt; administrator right now. Once I know more, I&#39;ll let y= ou know.<br> <br> &gt;&gt; &gt;&gt; On Wed Oct 01 16:36:51 2025, AX68683 wrote:<br> <br> &gt;&gt; &gt;&gt;&gt; Hi, Max, Well, when I try running the analogous slurm=  script as in<br> &gt;&gt; &gt;&gt;&gt; the past, I am getting an error. I am also getting th= ose errors<br> &gt;&gt; &gt;&gt;&gt; when trying an interactive session. I am only using 2= 024. In<br> &gt;&gt; &gt;&gt;&gt;<br> &gt;&gt; directory/umbc/rs/gobbert/common/research/FVM2DtestPollution/matla= b/<br> &gt;&gt; &gt;&gt;&gt; there are subdirectories ver6.3matlabR2023b<br> &gt;&gt; &gt;&gt;&gt; ver6.4matlabR2025b As the names imply, the only diffe= rence is,<br> &gt;&gt; &gt;&gt;&gt; which Matlab release is loaded in the slurm file<br> &gt;&gt; &gt;&gt;&gt; run-matlab_parallel.slurmIn ver6.3 you see what is su= pposed to<br> &gt;&gt; &gt;&gt;&gt; happen for a job that took 1 or 2 minutes.In ver6.4 y= ou see the<br> &gt;&gt; &gt;&gt;&gt; error in the slurm....err file. Does this help? Feel = free to create<br> &gt;&gt; &gt;&gt;&gt; a new directory and try right there again.<br> <br> &gt;&gt; &gt;&gt;&gt; Matthias Matthias K. Gobbert, Ph.D., Professor of<br> &gt;&gt; MathematicsDepartment<br> &gt;&gt; &gt;&gt;&gt; of Mathematics and StatisticsCenter for Interdiscipli= nary Research<br> &gt;&gt; &gt;&gt;&gt; and Consulting (<a href=3D""http://circ.umbc.edu"" rel= =3D""noreferrer"" target=3D""_blank"">circ.umbc.edu</a>)UMBC High Performance C= omputing<br> &gt;&gt; &gt;&gt;&gt; Facility (<a href=3D""http://hpcf.umbc.edu"" rel=3D""nor= eferrer"" target=3D""_blank"">hpcf.umbc.edu</a>)REU Site: Online Interdiscipli= nary Big Data<br> &gt;&gt; &gt;&gt;&gt; Analytics (<a href=3D""http://BigDataREU.umbc.edu"" rel= =3D""noreferrer"" target=3D""_blank"">BigDataREU.umbc.edu</a>)University of Mar= yland, Baltimore<br> &gt;&gt; &gt;&gt;&gt; County1000 Hilltop Circle, Baltimore, MD<br> &gt;&gt; 21250<a href=3D""http://www.umbc.edu/~gobbert"" rel=3D""noreferrer"" t= arget=3D""_blank"">http://www.umbc.edu/~gobbert</a><br> &gt;&gt; &gt;&gt;&gt; On Wed, Oct 1, 2025 at 1:15 PM Max Breitmeyer via RT<= br> &gt;&gt; &gt;&gt;&gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D= ""_blank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; &gt;&gt;&gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ti= cket/Display.html?id=3D3282188"" rel=3D""noreferrer"" target=3D""_blank"">https:= //rt.umbc.edu/Ticket/Display.html?id=3D3282188</a><br> &gt;&gt; &gt;&gt;&gt;&gt; &gt;<br> <br> &gt;&gt; &gt;&gt;&gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt; &gt;&gt;&gt;&gt; Hi Matthias,<br> <br> &gt;&gt; &gt;&gt;&gt;&gt; Matlab 2025b should now be available on all parti= tions.<br> <br> &gt;&gt; &gt;&gt;&gt;&gt; On Wed Oct 01 12:34:02 2025, OL73413 wrote:<br> <br> &gt;&gt; &gt;&gt;&gt;&gt; &gt; Hi Matthias,<br> <br> &gt;&gt; &gt;&gt;&gt;&gt; &gt; I&#39;ve added matlab 2025b to the 2024 part= itions, and working<br> &gt;&gt; &gt;&gt;&gt;&gt; on getting it<br> &gt;&gt; &gt;&gt;&gt;&gt; &gt; installed in 2018 and 2021 partitions. Pleas= e try it out and<br> &gt;&gt; &gt;&gt;&gt;&gt; let me know if<br> &gt;&gt; &gt;&gt;&gt;&gt; &gt; you have any problems!<br> <br> &gt;&gt; &gt;&gt;&gt;&gt; &gt; On Sun Sep 28 15:03:56 2025, ZZ99999 wrote:<= br> <br> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; First Name: Matthias<br> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; Last Name: Gobbert<br> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; Email: <a href=3D""mailto:gobbert@umbc.ed= u"" target=3D""_blank"">gobbert@umbc.edu</a><br> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; Campus ID: AX68683<br> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; Request Type: High Performance Cluster<b= r> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; Hi,<br> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; We are on R2023b on chip right now, whic= h is two years old.<br> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; Could you please install R2025b? This ca= me out recently.<br> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; If there are any concerns about doing su= ch things, I would<br> &gt;&gt; &gt;&gt;&gt;&gt; be interested to hear.<br> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; Thanks,<br> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; Matthias<br> &gt;&gt; &gt;&gt;&gt;&gt; &gt;<br> <br> &gt;&gt; &gt;&gt;&gt;&gt; &gt; --<br> <br> &gt;&gt; &gt;&gt;&gt;&gt; &gt; Best,<br> &gt;&gt; &gt;&gt;&gt;&gt; &gt; Max Breitmeyer<br> &gt;&gt; &gt;&gt;&gt;&gt; &gt; DOIT HPC System Administrator<br> <br> &gt;&gt; &gt;&gt;&gt;&gt; --<br> <br> &gt;&gt; &gt;&gt;&gt;&gt; Best,<br> &gt;&gt; &gt;&gt;&gt;&gt; Max Breitmeyer<br> &gt;&gt; &gt;&gt;&gt;&gt; DOIT HPC System Administrator<br> <br> &gt;&gt; &gt;&gt; --<br> <br> &gt;&gt; &gt;&gt; Best,<br> &gt;&gt; &gt;&gt; Max Breitmeyer<br> &gt;&gt; &gt;&gt; DOIT HPC System Administrator<br> <br> &gt;&gt; &gt; --<br> &gt;&gt; &gt; Matthias Gobbert<br> &gt;&gt; &gt; Professor, Mathematics and Statistics<br> &gt;&gt; &gt; <a href=3D""mailto:gobbert@umbc.edu"" target=3D""_blank"">gobbert= @umbc.edu</a><br> <br> &gt;&gt; --<br> <br> &gt;&gt; Best,<br> &gt;&gt; Max Breitmeyer<br> &gt;&gt; DOIT HPC System Administrator<br> <br> --<br> <br> Best,<br> Max Breitmeyer<br> DOIT HPC System Administrator<br> <br> <br> <br> ______________________________________<br> <br> Original Request:<br> <br> Requestors: Matthias Gobbert<br> <br> First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Matthias= <br> Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0Gob= bert<br> Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=  =C2=A0<a href=3D""mailto:gobbert@umbc.edu"" target=3D""_blank"">gobbert@umbc.e= du</a><br> Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0AX6= 8683<br> <br> Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Performa= nce Cluster<br> <br> <br> Hi,<br> <br> We are on R2023b on chip right now, which is two years old.<br> <br> Could you please install R2025b? This came out recently.<br> <br> If there are any concerns about doing such things, I would be interested to=  hear.<br> <br> Thanks,<br> <br> Matthias<br> <br> <br> </blockquote></div> "
3282188,72713394,Correspond,DoIT-Research-Computing,2025-11-04 16:49:54.0000000,HPC Slurm/Software Issue: Please install newer Matlab,open,Max Breitmeyer,mb17,Matthias Gobbert,gobbert,gobbert@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>This is interesting. I didn&#39;t see this on my initial test when runni= ng. Let me get back to you on this. I may have to delete the program and re= run it.&nbsp;</p>  <p>On Tue Nov 04 11:43:13 2025, AX68683 wrote:</p>  <blockquote> <div> <div>Hi, Max,</div>  <div>&nbsp;</div>  <div>Interesting. I wonder what Procurement was negotiating. Anyhow, so sho= uld it just work?</div>  <div>&nbsp;</div>  <div>In directory</div>  <div>&nbsp;</div>  <div>/umbc/rs/pi_gobbert/common/research/FVM2DtestPollution/matlab/ver6.4ma= tlabR2025b</div>  <div>&nbsp;</div>  <div>it works, and it gives correct results, but the stderr file shows the = following message below. Of course, I did not log out and in again or anyth= ing like that.</div>  <div>&nbsp;</div>  <div>[gobbert@chip-login2 ver6.4matlabR2025b]$ more slurm_matlab_595955.err= <br /> An error occurred while extracting product files.<br /> Try rerunning the installer.<br /> <br /> For help, see this &lt;a href=3D&quot;https://www.mathworks.com/pi_ee_mci_g= lnxa64&quot;&gt;support<br /> &nbsp;article&lt;/a&gt;.<br /> An unexpected error has occurred.<br /> To resolve this issue, contact &lt;a href=3D&quot;https://www.mathworks.com= /pi_gene_mci_gl<br /> nxa64&quot;&gt;Technical Support&lt;/a&gt;.<br /> An error occurred while extracting product files.<br /> Try rerunning the installer.<br /> <br /> For help, see this &lt;a href=3D&quot;https://www.mathworks.com/pi_ee_mci_g= lnxa64&quot;&gt;support<br /> &nbsp;article&lt;/a&gt;.<br /> An unexpected error has occurred.<br /> To resolve this issue, contact &lt;a href=3D&quot;https://www.mathworks.com= /pi_gene_mci_gl<br /> nxa64&quot;&gt;Technical Support&lt;/a&gt;.<br /> An error occurred while extracting product files.<br /> Try rerunning the installer.<br /> <br /> For help, see this &lt;a href=3D&quot;https://www.mathworks.com/pi_ee_mci_g= lnxa64&quot;&gt;support<br /> &nbsp;article&lt;/a&gt;.<br /> An unexpected error has occurred.<br /> To resolve this issue, contact &lt;a href=3D&quot;https://www.mathworks.com= /pi_gene_mci_gl<br /> nxa64&quot;&gt;Technical Support&lt;/a&gt;.</div>  <div> <div> <div> <div>&nbsp;</div>  <div>Matthias K. Gobbert, Ph.D., Professor of Mathematics</div>  <div>Department of Mathematics and Statistics</div>  <div>Center for Interdisciplinary Research and Consulting (circ.umbc.edu)</= div>  <div>UMBC High Performance Computing Facility (hpcf.umbc.edu)</div>  <div>REU Site: Online Interdisciplinary Big Data Analytics (BigDataREU.umbc= .edu)</div>  <div>University of Maryland, Baltimore County</div>  <div>1000 Hilltop Circle, Baltimore, MD 21250</div>  <div>http://www.umbc.edu/~gobbert</div> </div> </div> </div> </div> &nbsp;  <div> <div>On Tue, Nov 4, 2025 at 9:18=E2=80=AFAM Max Breitmeyer via RT &lt;UMBCH= elp@rt.umbc.edu&gt; wrote:</div>  <blockquote>If you agree your issue is resolved, please give us feedback on=  your experience by completing a brief satisfaction survey:<br /> <br /> https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&amp;customerema= il=3Dgobbert%40umbc.edu&amp;groupid=3DEIS&amp;ticketid=3D3282188&amp;ticket= owner=3Dmb17%40umbc.edu&amp;ticketsubject=3DHPC%20Slurm%2FSoftware%20Issue%= 3A%20Please%20install%20newer%20Matlab<br /> <br /> If you believe your issue has not been resolved, please respond to this mes= sage, which will reopen your ticket. Note: A full record of your request ca= n be found at:&nbsp;<br /> <br /> Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3282188 &gt;<b= r /> <br /> Thank You<br /> <br /> _________________________________________<br /> <br /> R e s o l u t i o n:<br /> =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D= &nbsp;<br /> <br /> Hi Matthias,<br /> <br /> While it is true we have a current campus license, there is a difference<br=  /> between a &quot;site license&quot; which is what we need for MATLAB to oper= ate on shared<br /> resources such as the HPC or on in computer labs, and then there are indivi= dual<br /> licenses handed out by the university that lets people use it on their own<= br /> equipment (such as your laptop). However, the procurement of the new<br /> site-license which includes 2025b, was recently completed, and our license<= br /> manager put it into place yesterday. As such, MATLAB2025b is now available = for<br /> use on the cluster.<br /> <br /> On Thu Oct 30 21:46:11 2025, AX68683 wrote:<br /> <br /> &gt; Hi, Max, Well, this sounds too odd. We certainly have a current campus= <br /> &gt; license, otherwise no Matlab of any kind would work. And with a curren= t<br /> &gt; license, we should have the right to a 2025 release. In fact, on my la= ptop<br /> &gt; I have 2025.Could you forward this to someone who works with the licen= ses?<br /> &gt; If it is too much work on your end, I can of course live with it until= <br /> &gt; January, but this just does not sound right. Matthias Matthias K. Gobb= ert,<br /> &gt; Ph.D., Professor of MathematicsDepartment of Mathematics and Statistic= sCenter<br /> &gt; for Interdisciplinary Research and Consulting (circ.umbc.edu)UMBC High= <br /> &gt; Performance Computing Facility (hpcf.umbc.edu)REU Site: Online<br /> &gt; Interdisciplinary Big Data Analytics (BigDataREU.umbc.edu)University o= f<br /> &gt; Maryland, Baltimore County1000 Hilltop Circle, Baltimore, MD 21250http= ://www.umbc.edu/~gobbert<br /> &gt; On Thu, Oct 30, 2025 at 6:51 PM Max Breitmeyer via RT<br /> &gt; &lt;UMBCHelp@rt.umbc.edu&gt; wrote:<br /> <br /> &gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32821= 88 &gt;<br /> <br /> &gt;&gt; Last Update From Ticket:<br /> <br /> &gt;&gt; Hi Matthias,<br /> <br /> &gt;&gt; We don&#39;t control when the licenses get purchased unfortunately= . I know<br /> &gt;&gt; the plan<br /> &gt;&gt; is that the license will be replaced in the beginning of 2026, but=  I&#39;m<br /> &gt;&gt; unsure<br /> &gt;&gt; if it will include a license for MATLAB 2025b. I can see if our li= cense<br /> &gt;&gt; include<br /> &gt;&gt; an iso for MATLAB2024a/b if that will help you in the short-term?<= br /> <br /> &gt;&gt; On Wed Oct 29 17:43:15 2025, AX68683 wrote:<br /> <br /> &gt;&gt; &gt; Hi, Max,<br /> <br /> &gt;&gt; &gt; Have you been able to find out anything?<br /> <br /> &gt;&gt; &gt; A student has now hit upon the issue and asked me, hence my<b= r /> &gt;&gt; follow-up.<br /> <br /> &gt;&gt; &gt; I understand you guys are juggling many tickets.<br /> <br /> &gt;&gt; &gt; Matthias<br /> <br /> &gt;&gt; &gt; On Wed Oct 01 17:10:02 2025, OL73413 wrote:<br /> <br /> &gt;&gt; &gt;&gt; Hi Matthias,<br /> <br /> &gt;&gt; &gt;&gt; Sorry about that. At first it looked like I just needed t= o point to<br /> &gt;&gt; the<br /> &gt;&gt; &gt;&gt; updated license server, but after doing that and running = it&#39;s saying<br /> &gt;&gt; &gt;&gt; we&#39;re missing a license for this release. I&#39;m wor= king with license<br /> &gt;&gt; &gt;&gt; administrator right now. Once I know more, I&#39;ll let y= ou know.<br /> <br /> &gt;&gt; &gt;&gt; On Wed Oct 01 16:36:51 2025, AX68683 wrote:<br /> <br /> &gt;&gt; &gt;&gt;&gt; Hi, Max, Well, when I try running the analogous slurm=  script as in<br /> &gt;&gt; &gt;&gt;&gt; the past, I am getting an error. I am also getting th= ose errors<br /> &gt;&gt; &gt;&gt;&gt; when trying an interactive session. I am only using 2= 024. In<br /> &gt;&gt; &gt;&gt;&gt;<br /> &gt;&gt; directory/umbc/rs/gobbert/common/research/FVM2DtestPollution/matla= b/<br /> &gt;&gt; &gt;&gt;&gt; there are subdirectories ver6.3matlabR2023b<br /> &gt;&gt; &gt;&gt;&gt; ver6.4matlabR2025b As the names imply, the only diffe= rence is,<br /> &gt;&gt; &gt;&gt;&gt; which Matlab release is loaded in the slurm file<br /> &gt;&gt; &gt;&gt;&gt; run-matlab_parallel.slurmIn ver6.3 you see what is su= pposed to<br /> &gt;&gt; &gt;&gt;&gt; happen for a job that took 1 or 2 minutes.In ver6.4 y= ou see the<br /> &gt;&gt; &gt;&gt;&gt; error in the slurm....err file. Does this help? Feel = free to create<br /> &gt;&gt; &gt;&gt;&gt; a new directory and try right there again.<br /> <br /> &gt;&gt; &gt;&gt;&gt; Matthias Matthias K. Gobbert, Ph.D., Professor of<br = /> &gt;&gt; MathematicsDepartment<br /> &gt;&gt; &gt;&gt;&gt; of Mathematics and StatisticsCenter for Interdiscipli= nary Research<br /> &gt;&gt; &gt;&gt;&gt; and Consulting (circ.umbc.edu)UMBC High Performance C= omputing<br /> &gt;&gt; &gt;&gt;&gt; Facility (hpcf.umbc.edu)REU Site: Online Interdiscipl= inary Big Data<br /> &gt;&gt; &gt;&gt;&gt; Analytics (BigDataREU.umbc.edu)University of Maryland= , Baltimore<br /> &gt;&gt; &gt;&gt;&gt; County1000 Hilltop Circle, Baltimore, MD<br /> &gt;&gt; 21250http://www.umbc.edu/~gobbert<br /> &gt;&gt; &gt;&gt;&gt; On Wed, Oct 1, 2025 at 1:15 PM Max Breitmeyer via RT<= br /> &gt;&gt; &gt;&gt;&gt; &lt;UMBCHelp@rt.umbc.edu&gt; wrote:<br /> <br /> &gt;&gt; &gt;&gt;&gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Displa= y.html?id=3D3282188<br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt;<br /> <br /> &gt;&gt; &gt;&gt;&gt;&gt; Last Update From Ticket:<br /> <br /> &gt;&gt; &gt;&gt;&gt;&gt; Hi Matthias,<br /> <br /> &gt;&gt; &gt;&gt;&gt;&gt; Matlab 2025b should now be available on all parti= tions.<br /> <br /> &gt;&gt; &gt;&gt;&gt;&gt; On Wed Oct 01 12:34:02 2025, OL73413 wrote:<br /> <br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt; Hi Matthias,<br /> <br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt; I&#39;ve added matlab 2025b to the 2024 part= itions, and working<br /> &gt;&gt; &gt;&gt;&gt;&gt; on getting it<br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt; installed in 2018 and 2021 partitions. Pleas= e try it out and<br /> &gt;&gt; &gt;&gt;&gt;&gt; let me know if<br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt; you have any problems!<br /> <br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt; On Sun Sep 28 15:03:56 2025, ZZ99999 wrote:<= br /> <br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; First Name: Matthias<br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; Last Name: Gobbert<br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; Email: gobbert@umbc.edu<br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; Campus ID: AX68683<br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; Request Type: High Performance Cluster<b= r /> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; Hi,<br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; We are on R2023b on chip right now, whic= h is two years old.<br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; Could you please install R2025b? This ca= me out recently.<br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; If there are any concerns about doing su= ch things, I would<br /> &gt;&gt; &gt;&gt;&gt;&gt; be interested to hear.<br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; Thanks,<br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; Matthias<br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt;<br /> <br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt; --<br /> <br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt; Best,<br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt; Max Breitmeyer<br /> &gt;&gt; &gt;&gt;&gt;&gt; &gt; DOIT HPC System Administrator<br /> <br /> &gt;&gt; &gt;&gt;&gt;&gt; --<br /> <br /> &gt;&gt; &gt;&gt;&gt;&gt; Best,<br /> &gt;&gt; &gt;&gt;&gt;&gt; Max Breitmeyer<br /> &gt;&gt; &gt;&gt;&gt;&gt; DOIT HPC System Administrator<br /> <br /> &gt;&gt; &gt;&gt; --<br /> <br /> &gt;&gt; &gt;&gt; Best,<br /> &gt;&gt; &gt;&gt; Max Breitmeyer<br /> &gt;&gt; &gt;&gt; DOIT HPC System Administrator<br /> <br /> &gt;&gt; &gt; --<br /> &gt;&gt; &gt; Matthias Gobbert<br /> &gt;&gt; &gt; Professor, Mathematics and Statistics<br /> &gt;&gt; &gt; gobbert@umbc.edu<br /> <br /> &gt;&gt; --<br /> <br /> &gt;&gt; Best,<br /> &gt;&gt; Max Breitmeyer<br /> &gt;&gt; DOIT HPC System Administrator<br /> <br /> --<br /> <br /> Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator<br /> <br /> <br /> <br /> ______________________________________<br /> <br /> Original Request:<br /> <br /> Requestors: Matthias Gobbert<br /> <br /> First Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Matthias= <br /> Last Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Gob= bert<br /> Email:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;=  &nbsp;gobbert@umbc.edu<br /> Campus ID:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;AX6= 8683<br /> <br /> Request Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; High Performa= nce Cluster<br /> <br /> <br /> Hi,<br /> <br /> We are on R2023b on chip right now, which is two years old.<br /> <br /> Could you please install R2025b? This came out recently.<br /> <br /> If there are any concerns about doing such things, I would be interested to=  hear.<br /> <br /> Thanks,<br /> <br /> Matthias<br /> <br /> &nbsp;</blockquote> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3282261,71979530,Create,DoIT-Research-Computing,2025-09-29 09:51:37.0000000,HPC User Account: sumyaho1 in Pi_ksolaima,resolved,Danielle Esposito,desposi1,Sumya Hamid Oishe,sumyaho1,sumyaho1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Sumya Hamid Last Name:                 Oishe Email:                     sumyaho1@umbc.edu Campus ID:                 DL19814  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    ksolaima@umbc.edu Existing Group:       Pi_ksolaima Project Title:        Mutimodal information retriever Project Abstract:     This project focuses on the development of a robust system for anomaly detection in multivariate, irregularly-sampled time-series data, with a application to identifying electricity theft from the Smart Grid Generated Data (SGCC) dataset. The primary methodology involves the implementation and training of Neural Controlled Differential Equations, a deep learning architecture designed to capture complex temporal dependencies.   I am developing and training a series of deep learning models for my time-series analysis research. On my current resources, the training process is prohibitively slow, with each epoch taking approximately 40-45 minutes to complete. This makes iterative development, debugging, and essential hyperparameter tuning impractical. Access to the HPC cluster would significantly accelerate this research by enabling me to run multiple experiments in parallel and iterate on model architectures and feature engineering more efficiently. Thank you for your time and consideration.  "
3282261,71992091,Correspond,DoIT-Research-Computing,2025-09-29 16:32:05.0000000,HPC User Account: sumyaho1 in Pi_ksolaima,resolved,Danielle Esposito,desposi1,Sumya Hamid Oishe,sumyaho1,sumyaho1@umbc.edu,Khaled Solaiman,ksolaima@umbc.edu,"Yes, I approve.  On Mon, Sep 29, 2025 at 5:51=E2=80=AFAM RT API via RT <UMBCHelp@rt.umbc.edu= > wrote:  > This e-mail is a notification that a UMBC user: Sumya Hamid Oishe < > sumyaho1@umbc.edu> has requested an account within UMBC's HPC environment > in your group <Pi_ksolaima>. As the PI, we request that you acknowledge a= nd > approve this account creation by replying to this message. Alternatively > you can go to this link and review the ticket and indicate your decision > here: > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3282261 > > > Once we have your approval, we will create the account and you and the new > user will receive another e-mail notifying you that the account has been > created. If you have any other questions or concerns please contact us. > > - UMBC DoIT Research Computing Support Staff >   --=20 Best Regards, KMA Solaiman, PhD Assistant Teaching Professor Department of Computer Science and Electrical Engineering University of Maryland, Baltimore County (765) 775-8230 "
3282261,71992091,Correspond,DoIT-Research-Computing,2025-09-29 16:32:05.0000000,HPC User Account: sumyaho1 in Pi_ksolaima,resolved,Danielle Esposito,desposi1,Sumya Hamid Oishe,sumyaho1,sumyaho1@umbc.edu,Khaled Solaiman,ksolaima@umbc.edu,"<div dir=3D""ltr"">Yes, I approve.=C2=A0</div><br><div class=3D""gmail_quote g= mail_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Mon, Sep 29,=  2025 at 5:51=E2=80=AFAM RT API via RT &lt;<a href=3D""mailto:UMBCHelp@rt.um= bc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""g= mail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204= ,204,204);padding-left:1ex"">This e-mail is a notification that a UMBC user:=  Sumya Hamid Oishe &lt;<a href=3D""mailto:sumyaho1@umbc.edu"" target=3D""_blan= k"">sumyaho1@umbc.edu</a>&gt; has requested an account within UMBC&#39;s HPC=  environment in your group &lt;Pi_ksolaima&gt;. As the PI, we request that = you acknowledge and approve this account creation by replying to this messa= ge. Alternatively you can go to this link and review the ticket and indicat= e your decision here:<br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D328= 2261"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3282261</a> &gt;<br> <br> Once we have your approval, we will create the account and you and the new = user will receive another e-mail notifying you that the account has been cr= eated. If you have any other questions or concerns please contact us.<br> <br> - UMBC DoIT Research Computing Support Staff<br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><font color=3D""#000000"">Best Regards,<br>KMA Sol= aiman, PhD</font><div><div><font color=3D""#000000"">Assistant Teaching Profe= ssor</font></div><div><font color=3D""#000000"">Department of Computer Scienc= e and Electrical Engineering<br>University of Maryland, Baltimore County<br= >(765) 775-8230</font><br></div></div></div></div> "
3282261,71992977,Correspond,DoIT-Research-Computing,2025-09-29 16:59:49.0000000,HPC User Account: sumyaho1 in Pi_ksolaima,resolved,Danielle Esposito,desposi1,Sumya Hamid Oishe,sumyaho1,sumyaho1@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Sumya,<br /> Your account (sumyaho1) has been added to the pi_ksolaima group on chip.rs.umbc.edu .</p>  <p>Please read through the documentation found at hpcf.umbc.edu &gt; User Support.<br /> All available modules can be viewed using the command &#39;module avail&#39;.<br /> Please submit additional questions or issues as separate tickets via the following link.<br /> (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/)</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Mon Sep 29 05:51:37 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Sumya Hamid Last Name:                 Oishe Email:                     sumyaho1@umbc.edu Campus ID:                 DL19814  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    ksolaima@umbc.edu Existing Group:       Pi_ksolaima Project Title:        Mutimodal information retriever Project Abstract:     This project focuses on the development of a robust system for anomaly detection in multivariate, irregularly-sampled time-series data, with a application to identifying electricity theft from the Smart Grid Generated Data (SGCC) dataset. The primary methodology involves the implementation and training of Neural Controlled Differential Equations, a deep learning architecture designed to capture complex temporal dependencies.   I am developing and training a series of deep learning models for my time-series analysis research. On my current resources, the training process is prohibitively slow, with each epoch taking approximately 40-45 minutes to complete. This makes iterative development, debugging, and essential hyperparameter tuning impractical. Access to the HPC cluster would significantly accelerate this research by enabling me to run multiple experiments in parallel and iterate on model architectures and feature engineering more efficiently. Thank you for your time and consideration.  </pre> </blockquote> </div> "
3282435,71989443,Create,DoIT-Research-Computing,2025-09-29 15:30:23.0000000,Need to create account in the chip cluster under my advisors group,resolved,Beamlak Bekele,bbekele1,Pavan Raj Ravi,pravi1,pravi1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Pavan Raj Last Name:                 Ravi Email:                     pravi1@umbc.edu Campus ID:                 MJ24933  Request Type:                 2. How do I create my account?  Need to create an account in the chip cluster with Dr.Wangs group.  "
3282435,72016856,Comment,DoIT-Research-Computing,2025-09-30 15:09:21.0000000,Need to create account in the chip cluster under my advisors group,resolved,Beamlak Bekele,bbekele1,Pavan Raj Ravi,pravi1,pravi1@umbc.edu,Beamlak Bekele,bbekele1@umbc.edu,"<p>Correct request was submitted: https://rt.umbc.edu/Ticket/Display.html?id=3283387</p>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> "
3282455,71990209,Create,DoIT-Research-Computing,2025-09-29 15:46:36.0000000,HPC Other Issue: client_loop: send disconnect: Broken pipe happens every few minutes,resolved,Max Breitmeyer,mb17,Joseph Bennett,bennettj,bennettj@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Joseph Last Name:                 Bennett Email:                     bennettj@umbc.edu Campus ID:                 AZ21850  Request Type:              High Performance Cluster  Hello, since last week I have been disconnected repeatedly after logging into chip after only a few minutes of waiting for jobs to progress. I am using the standard ""ssh bennettj@chip.rs.umbc.edu"" command that has been working for months, but it is disconnecting me too frequently. Why is the Connection to chip.rs.umbc.edu closed by remote host closed so much while I'm working? This is interrupting our research. Is there a different way to login now of which I am unaware? Thank you!  "
3282455,71994037,Correspond,DoIT-Research-Computing,2025-09-29 17:24:40.0000000,HPC Other Issue: client_loop: send disconnect: Broken pipe happens every few minutes,resolved,Max Breitmeyer,mb17,Joseph Bennett,bennettj,bennettj@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Joseph, we&#39;ve received reports from other researchers who are experience something similar and we are currently investigating the issue.&nbsp;</p>  <p>A little more information might help us:</p>  <p>- First, can you give us exact times and dates for some of the disconnects you are seeing? This will help us look at the connection logs to find where it happened.</p>  <p>- Second, what kind of connection do you have? Wired or wireless? Are you on the vpn when connecting to chip?&nbsp;</p>  <p>- Third, are you experiencing it on one of the login nodes more than another?&nbsp;</p>  <p>Let me know the above and we can go from there. If you experience anymore of these disconnects please write down the exact date and time you experienced it so we can look into it further.&nbsp;</p>  <p>On Mon Sep 29 11:46:36 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Joseph Last Name:                 Bennett Email:                     bennettj@umbc.edu Campus ID:                 AZ21850  Request Type:              High Performance Cluster  Hello, since last week I have been disconnected repeatedly after logging into chip after only a few minutes of waiting for jobs to progress. I am using the standard &quot;ssh bennettj@chip.rs.umbc.edu&quot; command that has been working for months, but it is disconnecting me too frequently. Why is the Connection to chip.rs.umbc.edu closed by remote host closed so much while I&#39;m working? This is interrupting our research. Is there a different way to login now of which I am unaware? Thank you!  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3282455,72080990,Correspond,DoIT-Research-Computing,2025-10-03 13:48:18.0000000,HPC Other Issue: client_loop: send disconnect: Broken pipe happens every few minutes,resolved,Max Breitmeyer,mb17,Joseph Bennett,bennettj,bennettj@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Joseph,</p>  <p>We&#39;ve made a change to the ssh config that we are hoping will solve the issue. Could you keep an eye on it let us know if you notice a difference?&nbsp;</p>  <p>On Mon Sep 29 13:24:40 2025, OL73413 wrote:</p>  <blockquote> <div> <p>Hi Joseph, we&#39;ve received reports from other researchers who are experience something similar and we are currently investigating the issue.&nbsp;</p>  <p>A little more information might help us:</p>  <p>- First, can you give us exact times and dates for some of the disconnects you are seeing? This will help us look at the connection logs to find where it happened.</p>  <p>- Second, what kind of connection do you have? Wired or wireless? Are you on the vpn when connecting to chip?&nbsp;</p>  <p>- Third, are you experiencing it on one of the login nodes more than another?&nbsp;</p>  <p>Let me know the above and we can go from there. If you experience anymore of these disconnects please write down the exact date and time you experienced it so we can look into it further.&nbsp;</p>  <p>On Mon Sep 29 11:46:36 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Joseph Last Name:                 Bennett Email:                     bennettj@umbc.edu Campus ID:                 AZ21850  Request Type:              High Performance Cluster  Hello, since last week I have been disconnected repeatedly after logging into chip after only a few minutes of waiting for jobs to progress. I am using the standard &quot;ssh bennettj@chip.rs.umbc.edu&quot; command that has been working for months, but it is disconnecting me too frequently. Why is the Connection to chip.rs.umbc.edu closed by remote host closed so much while I&#39;m working? This is interrupting our research. Is there a different way to login now of which I am unaware? Thank you!  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3282508,71992171,Create,DoIT-Research-Computing,2025-09-29 16:33:11.0000000,HPC Other Issue: Need to install a module for WRF Simulation.,resolved,Danielle Esposito,desposi1,Omar Faruque,omarf1,omarf1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Omar Last Name:                 Faruque Email:                     omarf1@umbc.edu Campus ID:                 XC78330  Request Type:              High Performance Cluster  Hello, I need to run the WRF climate simulation model to generate datasets for my project. To run this simulation model, I need to use ""gfortran"" library, but this is not available in the CHIP cluster. It would be very helpful if you could install the library. Please check the given link if you need further information about running the WRF model.     WRF Tutorial Link: https://www2.mmm.ucar.edu/wrf/OnLineTutorial/compilation_tutorial.php#STEP1  "
3282508,71992878,Correspond,DoIT-Research-Computing,2025-09-29 16:57:26.0000000,HPC Other Issue: Need to install a module for WRF Simulation.,resolved,Danielle Esposito,desposi1,Omar Faruque,omarf1,omarf1@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Omar,</p>  <p>What happens when you attempt to use gfortran? Without loading any modules, there is a library for gfortran located in /usr/bin/gfortran, which should be on your PATH. Additionally, you can load more specific versions of gfortran through other modules. For example, if you load GCCcore with &#39;module load GCCcore&#39;, you can see that the gfortran library is also present, now located at &quot;/usr/ebuild/installs/software/GCCcore/13.3.0/bin/gfortran&quot; (you can verify yourself with &#39;which gfortran&#39; to show you the location of the gfortran library).&nbsp;</p>  <p>Let me know if this helps!</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Mon Sep 29 12:33:11 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Omar Last Name:                 Faruque Email:                     omarf1@umbc.edu Campus ID:                 XC78330  Request Type:              High Performance Cluster  Hello, I need to run the WRF climate simulation model to generate datasets for my project. To run this simulation model, I need to use &quot;gfortran&quot; library, but this is not available in the CHIP cluster. It would be very helpful if you could install the library. Please check the given link if you need further information about running the WRF model.     WRF Tutorial Link: https://www2.mmm.ucar.edu/wrf/OnLineTutorial/compilation_tutorial.php#STEP1  </pre> </blockquote> </div> "
3282508,72027996,Correspond,DoIT-Research-Computing,2025-09-30 20:10:52.0000000,HPC Other Issue: Need to install a module for WRF Simulation.,resolved,Danielle Esposito,desposi1,Omar Faruque,omarf1,omarf1@umbc.edu,Omar Faruque,omarf1@umbc.edu,"<p>Hello&nbsp;Danielle Esposito,</p>  <p>good afternoon.&nbsp;<br /> <br /> Thank you very much for providing this information. Actually, I tried to find the module using &quot;module spider&nbsp;gfortran&quot; to check if it is available or not. I will try to load the module using the way you mentioned and complete my work. If not not I will ask for further help.&nbsp;</p>  <p>Regards,</p>  <p>Omar</p>  <p>&nbsp;</p>  <p>&nbsp;</p>  <p>On Mon Sep 29 12:57:26 2025, EJ47256 wrote:</p>  <blockquote> <p>Hi Omar,</p>  <p>What happens when you attempt to use gfortran? Without loading any modules, there is a library for gfortran located in /usr/bin/gfortran, which should be on your PATH. Additionally, you can load more specific versions of gfortran through other modules. For example, if you load GCCcore with &#39;module load GCCcore&#39;, you can see that the gfortran library is also present, now located at &quot;/usr/ebuild/installs/software/GCCcore/13.3.0/bin/gfortran&quot; (you can verify yourself with &#39;which gfortran&#39; to show you the location of the gfortran library).&nbsp;</p>  <p>Let me know if this helps!</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Mon Sep 29 12:33:11 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Omar Last Name:                 Faruque Email:                     omarf1@umbc.edu Campus ID:                 XC78330  Request Type:              High Performance Cluster  Hello, I need to run the WRF climate simulation model to generate datasets for my project. To run this simulation model, I need to use &quot;gfortran&quot; library, but this is not available in the CHIP cluster. It would be very helpful if you could install the library. Please check the given link if you need further information about running the WRF model.     WRF Tutorial Link: https://www2.mmm.ucar.edu/wrf/OnLineTutorial/compilation_tutorial.php#STEP1  </pre> </blockquote> </div> </blockquote> "
3282508,72028002,Correspond,DoIT-Research-Computing,2025-09-30 20:11:03.0000000,HPC Other Issue: Need to install a module for WRF Simulation.,resolved,Danielle Esposito,desposi1,Omar Faruque,omarf1,omarf1@umbc.edu,Omar Faruque,omarf1@umbc.edu,"<p>Hello&nbsp;Danielle Esposito,</p>  <p>good afternoon.&nbsp;<br /> <br /> Thank you very much for providing this information. Actually, I tried to find the module using &quot;module spider&nbsp;gfortran&quot; to check if it is available or not. I will try to load the module using the way you mentioned and complete my work. If not not I will ask for further help.&nbsp;</p>  <p>Regards,</p>  <p>Omar</p>  <p>&nbsp;</p>  <p>&nbsp;</p>  <p>On Mon Sep 29 12:57:26 2025, EJ47256 wrote:</p>  <blockquote> <p>Hi Omar,</p>  <p>What happens when you attempt to use gfortran? Without loading any modules, there is a library for gfortran located in /usr/bin/gfortran, which should be on your PATH. Additionally, you can load more specific versions of gfortran through other modules. For example, if you load GCCcore with &#39;module load GCCcore&#39;, you can see that the gfortran library is also present, now located at &quot;/usr/ebuild/installs/software/GCCcore/13.3.0/bin/gfortran&quot; (you can verify yourself with &#39;which gfortran&#39; to show you the location of the gfortran library).&nbsp;</p>  <p>Let me know if this helps!</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Mon Sep 29 12:33:11 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Omar Last Name:                 Faruque Email:                     omarf1@umbc.edu Campus ID:                 XC78330  Request Type:              High Performance Cluster  Hello, I need to run the WRF climate simulation model to generate datasets for my project. To run this simulation model, I need to use &quot;gfortran&quot; library, but this is not available in the CHIP cluster. It would be very helpful if you could install the library. Please check the given link if you need further information about running the WRF model.     WRF Tutorial Link: https://www2.mmm.ucar.edu/wrf/OnLineTutorial/compilation_tutorial.php#STEP1  </pre> </blockquote> </div> </blockquote> "
3282986,72003419,Create,DoIT-Research-Computing,2025-09-29 20:00:57.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_anuradha,stalled,Greg Ballantine,gballan1,Anuradha Ravi,anuradha,anuradha@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<p>Hello Anuradha,<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 250GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/anuradha&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_anuradha&rdquo;.<br /> <br /> Thank you,<br /> Greg</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3282986,72187550,Correspond,DoIT-Research-Computing,2025-10-09 16:10:00.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_anuradha,stalled,Greg Ballantine,gballan1,Anuradha Ravi,anuradha,anuradha@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hello Anuradha,</p>  <p>This is a reminder email, in case you missed the first. We need a response by October 15th, or we&#39;ll be forced to go with Option 2, randomly scheduling a time.<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 250GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/anuradha&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_anuradha&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3282986,72349368,Correspond,DoIT-Research-Computing,2025-10-17 15:52:31.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_anuradha,stalled,Greg Ballantine,gballan1,Anuradha Ravi,anuradha,anuradha@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Anuradha,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 27th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3282986,72508912,Correspond,DoIT-Research-Computing,2025-10-27 17:21:50.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_anuradha,stalled,Greg Ballantine,gballan1,Anuradha Ravi,anuradha,anuradha@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello&nbsp;Anuradha,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 11:52:31 2025, IO12693 wrote:</p>  <blockquote>Anuradha</blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3282986,72517731,Correspond,DoIT-Research-Computing,2025-10-27 22:20:16.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_anuradha,stalled,Greg Ballantine,gballan1,Anuradha Ravi,anuradha,anuradha@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Anuradha,<br /> <br /> We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note:</p>  <ul> 	<li>The path has changed, and is now available under <strong>/umbc/rs/pi_anuradha</strong>.</li> 	<li>The alias used to reach the volume is now <strong>pi_anuradha_common</strong> and <strong>pi_anuradha_user</strong>.</li> 	<li>Your new volume has a quota of <strong>25TB</strong>.</li> </ul>  <p>When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good?<br /> <br /> Thank you,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Mon Oct 27 13:21:50 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Hello&nbsp;Anuradha,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 11:52:31 2025, IO12693 wrote:</p>  <blockquote>Anuradha</blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3282987,72003466,Create,DoIT-Research-Computing,2025-09-29 20:02:41.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_walker,stalled,Greg Ballantine,gballan1,Unknown," ",walker@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<p>Hello Artie,<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/walker&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_walker&rdquo;.<br /> <br /> Thank you,<br /> Greg</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3282987,72207215,Correspond,DoIT-Research-Computing,2025-10-10 15:22:50.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_walker,stalled,Greg Ballantine,gballan1,Unknown," ",walker@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hello Artie,</p>  <p>This is a reminder email, in case you missed the first. We need a response by October 15th, or we&#39;ll be forced to go with Option 2, randomly scheduling a time.<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/walker&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_walker&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3282987,72452175,Correspond,DoIT-Research-Computing,2025-10-23 14:16:12.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_walker,stalled,Greg Ballantine,gballan1,Unknown," ",walker@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Artie,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3282989,72003536,Create,DoIT-Research-Computing,2025-09-29 20:03:49.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_gangopad,stalled,Greg Ballantine,gballan1,Aryya Gangopadhyay,gangopad,gangopad@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<p>Hello Aryya,<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/gangopad&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_gangopad&rdquo;.<br /> <br /> Thank you,<br /> Greg</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3282989,72205376,Correspond,DoIT-Research-Computing,2025-10-10 14:38:25.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_gangopad,stalled,Greg Ballantine,gballan1,Aryya Gangopadhyay,gangopad,gangopad@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hello Aryya,</p>  <p>This is a reminder email, in case you missed the first. We need a response by October 15th, or we&#39;ll be forced to go with Option 2, randomly scheduling a time.<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/gangopad&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_gangopad&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3282989,72352493,Correspond,DoIT-Research-Computing,2025-10-17 17:09:09.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_gangopad,stalled,Greg Ballantine,gballan1,Aryya Gangopadhyay,gangopad,gangopad@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Aryya,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 4th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3282989,72717359,Correspond,DoIT-Research-Computing,2025-11-04 17:04:40.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_gangopad,stalled,Greg Ballantine,gballan1,Aryya Gangopadhyay,gangopad,gangopad@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Aryya,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 13:09:09 2025, IO12693 wrote:</p>  <blockquote> <p>Dear Aryya,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 4th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3282989,72725795,Correspond,DoIT-Research-Computing,2025-11-04 18:28:42.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_gangopad,stalled,Greg Ballantine,gballan1,Aryya Gangopadhyay,gangopad,gangopad@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Aryya,<br /> <br /> We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note:</p>  <ul> 	<li>The path has changed, and is now available under <strong>/umbc/rs/pi_gangopad</strong>.</li> 	<li>The alias used to reach the volume is now <strong>pi_gangopad_common</strong> and <strong>pi_gangopad_user</strong>.</li> 	<li>Your new volume has a quota of <strong>25TB</strong>.</li> </ul>  <p>When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good?<br /> <br /> Thank you,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Tue Nov 04 12:04:40 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Hello Aryya,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 13:09:09 2025, IO12693 wrote:</p>  <blockquote> <p>Dear Aryya,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 4th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3282995,72003638,Create,DoIT-Research-Computing,2025-09-29 20:05:35.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_bsol,stalled,Beamlak Bekele,bbekele1,Brian Soller,bsol,bsol@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<p>Hello Brian,<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 250GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/bsol&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_bsol&rdquo;.<br /> <br /> Thank you,<br /> Greg</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3282995,72204418,Correspond,DoIT-Research-Computing,2025-10-10 14:16:53.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_bsol,stalled,Beamlak Bekele,bbekele1,Brian Soller,bsol,bsol@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hello Brian,</p>  <p>This is a reminder email, in case you missed the first. We need a response by October 15th, or we&#39;ll be forced to go with Option 2, randomly scheduling a time.<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 250GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/bsol&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_bsol&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3282995,72350607,Correspond,DoIT-Research-Computing,2025-10-17 16:16:06.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_bsol,stalled,Beamlak Bekele,bbekele1,Brian Soller,bsol,bsol@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Brian,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 29th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3282995,72562507,Correspond,DoIT-Research-Computing,2025-10-29 17:43:53.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_bsol,stalled,Beamlak Bekele,bbekele1,Brian Soller,bsol,bsol@umbc.edu,Beamlak Bekele,bbekele1@umbc.edu,"<div> <p>Hello ,<br /> <br /> This is just a reminder that we are starting your migration today. During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.</p>  <p>On Fri Oct 17 12:16:06 2025, IO12693 wrote:</p>  <blockquote> <p>Dear Brian,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 29th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> "
3282995,72602966,Correspond,DoIT-Research-Computing,2025-10-30 16:55:51.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_bsol,stalled,Beamlak Bekele,bbekele1,Brian Soller,bsol,bsol@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Brian,<br /> <br /> It took longer than expected, but we have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note:</p>  <ul> 	<li>The path has changed, and is now available under <strong>/umbc/rs/pi_bsol</strong>.</li> 	<li>The alias used to reach the volume is now <strong>pi_bsol_common</strong> and <strong>pi_bsol_user</strong>.</li> 	<li>Your new volume has a quota of <strong>10TB</strong>.</li> </ul>  <p>When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good?<br /> <br /> I apologize if the extended length of the migration caused any issues for you or your group.<br /> <br /> Thank you,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Wed Oct 29 13:43:53 2025, PG41777 wrote:</p>  <blockquote> <div> <p>Hello ,<br /> <br /> This is just a reminder that we are starting your migration today. During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.</p>  <p>On Fri Oct 17 12:16:06 2025, IO12693 wrote:</p>  <blockquote> <p>Dear Brian,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 29th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3282997,72003677,Create,DoIT-Research-Computing,2025-09-29 20:06:52.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_bieberic,stalled,Greg Ballantine,gballan1,Charles Bieberich,bieberic,bieberic@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<p>Hello Charles,<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/bieberic&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_bieberic&rdquo;.<br /> <br /> Thank you,<br /> Greg</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3282997,72203646,Correspond,DoIT-Research-Computing,2025-10-10 14:00:26.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_bieberic,stalled,Greg Ballantine,gballan1,Charles Bieberich,bieberic,bieberic@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hello Charles,</p>  <p>This is a reminder email, in case you missed the first. We need a response by October 15th, or we&#39;ll be forced to go with Option 2, randomly scheduling a time.<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/bieberic&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_bieberic&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3282997,72350188,Correspond,DoIT-Research-Computing,2025-10-17 16:04:45.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_bieberic,stalled,Greg Ballantine,gballan1,Charles Bieberich,bieberic,bieberic@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Charles,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 28th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3282997,72527864,Correspond,DoIT-Research-Computing,2025-10-28 14:49:25.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_bieberic,stalled,Greg Ballantine,gballan1,Charles Bieberich,bieberic,bieberic@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Charles,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 12:04:45 2025, IO12693 wrote:</p>  <blockquote> <p>Dear Charles,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 28th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3282997,72539090,Correspond,DoIT-Research-Computing,2025-10-28 19:36:46.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_bieberic,stalled,Greg Ballantine,gballan1,Charles Bieberich,bieberic,bieberic@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Charles,<br /> <br /> We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note:</p>  <ul> 	<li>The path has changed, and is now available under <strong>/umbc/rs/pi_bieberic</strong>.</li> 	<li>The alias used to reach the volume is now <strong>pi_bieberic_common</strong> and <strong>pi_bieberic_user</strong>.</li> 	<li>Your new volume has a quota of <strong>10TB</strong>.</li> </ul>  <p>When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good?<br /> <br /> Thank you,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Tue Oct 28 10:49:25 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Hello Charles,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 12:04:45 2025, IO12693 wrote:</p>  <blockquote> <p>Dear Charles,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 28th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283002,72003754,Create,DoIT-Research-Computing,2025-09-29 20:08:12.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_curranfc,resolved,Beamlak Bekele,bbekele1,Chris Curran,curranfc,curranfc@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<p>Hello Chris,<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/curranfc&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_curranfc&rdquo;.<br /> <br /> Thank you,<br /> Greg</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283002,72204888,Correspond,DoIT-Research-Computing,2025-10-10 14:28:37.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_curranfc,resolved,Beamlak Bekele,bbekele1,Chris Curran,curranfc,curranfc@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hello Chris,</p>  <p>This is a reminder email, in case you missed the first. We need a response by October 15th, or we&#39;ll be forced to go with Option 2, randomly scheduling a time.<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/curranfc&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_curranfc&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3283002,72351318,Correspond,DoIT-Research-Computing,2025-10-17 16:35:08.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_curranfc,resolved,Beamlak Bekele,bbekele1,Chris Curran,curranfc,curranfc@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Chris,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 31st for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3283002,72619186,Correspond,DoIT-Research-Computing,2025-10-31 13:53:06.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_curranfc,resolved,Beamlak Bekele,bbekele1,Chris Curran,curranfc,curranfc@umbc.edu,Beamlak Bekele,bbekele1@umbc.edu,"<div> <p>Hello&nbsp;<br /> <br /> We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note:</p>  <ul> 	<li> 	<p>The path has changed, and is now available under <strong>/umbc/rs/&nbsp;pi_curranfc</strong></p> 	</li> 	<li> 	<p>The alias used to reach the volume is now <strong>pi_curranfc_common </strong>and <strong>pi_curranfc</strong><strong>_user</strong>.</p> 	</li> 	<li> 	<p>Your new volume has a quota of <strong>10TB</strong>.</p> 	</li> </ul>  <p><br /> When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good?<br /> <br /> Thank you</p>  <p>On Fri Oct 17 12:35:08 2025, IO12693 wrote:</p>  <blockquote> <p>Dear Chris,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 31st for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> "
3283005,72003817,Create,DoIT-Research-Computing,2025-09-29 20:09:07.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_cmarron,resolved,Beamlak Bekele,bbekele1,Chris Marron,cmarron,cmarron@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<p>Hello Chris,<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/cmarron&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_cmarron&rdquo;.<br /> <br /> Thank you,<br /> Greg</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283005,72204730,Correspond,DoIT-Research-Computing,2025-10-10 14:26:03.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_cmarron,resolved,Beamlak Bekele,bbekele1,Chris Marron,cmarron,cmarron@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hello Chris,</p>  <p>This is a reminder email, in case you missed the first. We need a response by October 15th, or we&#39;ll be forced to go with Option 2, randomly scheduling a time.<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/cmarron&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_cmarron&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3283005,72207004,Correspond,DoIT-Research-Computing,2025-10-10 15:17:29.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_cmarron,resolved,Beamlak Bekele,bbekele1,Chris Marron,cmarron,cmarron@umbc.edu,Chris Marron,cmarron@umbc.edu,"Good Morning,  It is fine with me if you schedule the transfer of /umbc/rs/cmarron for an arbitrary date 10/16 - 11/15.  Chris Marron  On Fri, Oct 10, 2025 at 10:26=E2=80=AFAM Elliot Gobbert via RT <UMBCHelp@rt= .umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3283005 > > > Last Update From Ticket: > > Hello Chris, > > This is a reminder email, in case you missed the first. We need a response > by > October 15th, or we'll be forced to go with Option 2, randomly scheduling=  a > time. > > As per the communication via myUMBC earlier this summer > (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the > process of > migrating data off of an older storage server to our new RRStor Ceph > storage > cluster. Your group is using 0GB of a 500GB quota on the old storage > server. > > To perform these migrations, we need to take individual storage volumes > offline > while we migrate them to the Ceph cluster. Thus we are reaching out to > schedule > a date where we can migrate your volume located at =E2=80=9C/umbc/rs/cmar= ron=E2=80=9D. > During > the migration, we will take your volume offline and will terminate any jo= bs > running on the chip compute cluster that are accessing this volume. > > Below we=E2=80=99ve listed two options for handling this data migration -=  please > let us > know which of these you=E2=80=99d prefer. > > Option 1: Schedule a group-wide downtime date during standard business > hours, > which can be done by responding to this email with your preferred date(s) > to > perform the migration. During this time, DoIT staff will work to migrate > your > volume to the Ceph storage cluster. DoIT staff will send an email alert on > this > email thread when the migration has begun and when it has completed. For > most > storage volumes, this process should take less than a business day. > Option 2: If you don=E2=80=99t respond to this email by October 15th, DoI= T staff > will > assign a day over the following month (October 16th through November 15th) > to > migrate your volume. The day chosen will be random and will occur during > business hours. You will be notified of the date chosen to perform the > migration, and will be notified when the migration begins and completes. > > Note: After this process has completed, the new storage volume will have a > new > name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its data und= er > =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, > or in your group=E2=80=99s case you will find your volume under > =E2=80=9C/umbc/rs/pi_cmarron=E2=80=9D. > > Thank you, > Elliot > >  --=20 Chris Marron cmarron@umbc.edu "
3283005,72207004,Correspond,DoIT-Research-Computing,2025-10-10 15:17:29.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_cmarron,resolved,Beamlak Bekele,bbekele1,Chris Marron,cmarron,cmarron@umbc.edu,Chris Marron,cmarron@umbc.edu,"<div dir=3D""ltr"">Good Morning,<div><br></div><div>It is fine with me if you=  schedule the transfer of /umbc/rs/cmarron for an arbitrary date 10/16 - 11= /15. =C2=A0</div><div><br></div><div>Chris Marron</div></div><br><div class= =3D""gmail_quote gmail_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr= "">On Fri, Oct 10, 2025 at 10:26=E2=80=AFAM Elliot Gobbert via RT &lt;<a hre= f=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></= div><blockquote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;bor= der-left-width:1px;border-left-style:solid;border-left-color:rgb(204,204,20= 4);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/= Display.html?id=3D3283005"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.= umbc.edu/Ticket/Display.html?id=3D3283005</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hello Chris,<br> <br> This is a reminder email, in case you missed the first. We need a response = by<br> October 15th, or we&#39;ll be forced to go with Option 2, randomly scheduli= ng a<br> time.<br> <br> As per the communication via myUMBC earlier this summer<br> (<a href=3D""https://my3.my.umbc.edu/groups/hpcf/posts/150838"" rel=3D""norefe= rrer"" target=3D""_blank"">https://my3.my.umbc.edu/groups/hpcf/posts/150838</a= >), DoIT is in the process of<br> migrating data off of an older storage server to our new RRStor Ceph storag= e<br> cluster. Your group is using 0GB of a 500GB quota on the old storage server= .<br> <br> To perform these migrations, we need to take individual storage volumes off= line<br> while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule<br> a date where we can migrate your volume located at =E2=80=9C/umbc/rs/cmarro= n=E2=80=9D. During<br> the migration, we will take your volume offline and will terminate any jobs= <br> running on the chip compute cluster that are accessing this volume.<br> <br> Below we=E2=80=99ve listed two options for handling this data migration - p= lease let us<br> know which of these you=E2=80=99d prefer.<br> <br> Option 1: Schedule a group-wide downtime date during standard business hour= s,<br> which can be done by responding to this email with your preferred date(s) t= o<br> perform the migration. During this time, DoIT staff will work to migrate yo= ur<br> volume to the Ceph storage cluster. DoIT staff will send an email alert on = this<br> email thread when the migration has begun and when it has completed. For mo= st<br> storage volumes, this process should take less than a business day.<br> Option 2: If you don=E2=80=99t respond to this email by October 15th, DoIT = staff will<br> assign a day over the following month (October 16th through November 15th) = to<br> migrate your volume. The day chosen will be random and will occur during<br> business hours. You will be notified of the date chosen to perform the<br> migration, and will be notified when the migration begins and completes.<br> <br> Note: After this process has completed, the new storage volume will have a = new<br> name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its data under=  =E2=80=9C/umbc/rs/pi_doit=E2=80=9D,<br> or in your group=E2=80=99s case you will find your volume under =E2=80=9C/u= mbc/rs/pi_cmarron=E2=80=9D.<br> <br> Thank you,<br> Elliot<br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr"">Chris Marron<div><a href=3D""mailto:cmarron@umbc.= edu"" target=3D""_blank"">cmarron@umbc.edu</a></div><div><br></div></div></div> "
3283005,72211441,Correspond,DoIT-Research-Computing,2025-10-10 17:37:41.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_cmarron,resolved,Beamlak Bekele,bbekele1,Chris Marron,cmarron,cmarron@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Chris,<br /> <br /> Sounds good, I&#39;ve put your group on our schedule for Wednesday October = 22nd. We will send you an email alert via this RT ticket when we begin the = migration process, and again once it has completed.<br /> <br /> Please let us know if you have any questions or concerns.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 10 11:17:29 2025, NN66844 wrote:</p>  <blockquote> <div>Good Morning, <div>&nbsp;</div>  <div>It is fine with me if you schedule the transfer of /umbc/rs/cmarron fo= r an arbitrary date 10/16 - 11/15. &nbsp;</div>  <div>&nbsp;</div>  <div>Chris Marron</div> </div> &nbsp;  <div> <div>On Fri, Oct 10, 2025 at 10:26=E2=80=AFAM Elliot Gobbert via RT &lt;UMB= CHelp@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 83005 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hello Chris,<br /> <br /> This is a reminder email, in case you missed the first. We need a response = by<br /> October 15th, or we&#39;ll be forced to go with Option 2, randomly scheduli= ng a<br /> time.<br /> <br /> As per the communication via myUMBC earlier this summer<br /> (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process = of<br /> migrating data off of an older storage server to our new RRStor Ceph storag= e<br /> cluster. Your group is using 0GB of a 500GB quota on the old storage server= .<br /> <br /> To perform these migrations, we need to take individual storage volumes off= line<br /> while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule<br /> a date where we can migrate your volume located at &ldquo;/umbc/rs/cmarron&= rdquo;. During<br /> the migration, we will take your volume offline and will terminate any jobs= <br /> running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - ple= ase let us<br /> know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hour= s,<br /> which can be done by responding to this email with your preferred date(s) t= o<br /> perform the migration. During this time, DoIT staff will work to migrate yo= ur<br /> volume to the Ceph storage cluster. DoIT staff will send an email alert on = this<br /> email thread when the migration has begun and when it has completed. For mo= st<br /> storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT st= aff will<br /> assign a day over the following month (October 16th through November 15th) = to<br /> migrate your volume. The day chosen will be random and will occur during<br=  /> business hours. You will be notified of the date chosen to perform the<br /> migration, and will be notified when the migration begins and completes.<br=  /> <br /> Note: After this process has completed, the new storage volume will have a = new<br /> name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ld= quo;/umbc/rs/pi_doit&rdquo;,<br /> or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/= rs/pi_cmarron&rdquo;.<br /> <br /> Thank you,<br /> Elliot<br /> &nbsp;</blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --  <div> <div>Chris Marron <div>cmarron@umbc.edu</div>  <div>&nbsp;</div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283005,72420984,Correspond,DoIT-Research-Computing,2025-10-22 13:30:55.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_cmarron,resolved,Beamlak Bekele,bbekele1,Chris Marron,cmarron,cmarron@umbc.edu,Beamlak Bekele,bbekele1@umbc.edu,"<div> <p>Hello&nbsp;<br /> <br /> This is just a reminder that we are starting your migration today. During t= his time, please ensure there are not any jobs being run in your research g= roup, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.</p>  <p>On Fri Oct 10 13:37:41 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Hello Chris,<br /> <br /> Sounds good, I&#39;ve put your group on our schedule for Wednesday October = 22nd. We will send you an email alert via this RT ticket when we begin the = migration process, and again once it has completed.<br /> <br /> Please let us know if you have any questions or concerns.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 10 11:17:29 2025, NN66844 wrote:</p>  <blockquote> <div>Good Morning, <div>&nbsp;</div>  <div>It is fine with me if you schedule the transfer of /umbc/rs/cmarron fo= r an arbitrary date 10/16 - 11/15. &nbsp;</div>  <div>&nbsp;</div>  <div>Chris Marron</div> </div> &nbsp;  <div> <div>On Fri, Oct 10, 2025 at 10:26=E2=80=AFAM Elliot Gobbert via RT &lt;UMB= CHelp@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 83005 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hello Chris,<br /> <br /> This is a reminder email, in case you missed the first. We need a response = by<br /> October 15th, or we&#39;ll be forced to go with Option 2, randomly scheduli= ng a<br /> time.<br /> <br /> As per the communication via myUMBC earlier this summer<br /> (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process = of<br /> migrating data off of an older storage server to our new RRStor Ceph storag= e<br /> cluster. Your group is using 0GB of a 500GB quota on the old storage server= .<br /> <br /> To perform these migrations, we need to take individual storage volumes off= line<br /> while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule<br /> a date where we can migrate your volume located at &ldquo;/umbc/rs/cmarron&= rdquo;. During<br /> the migration, we will take your volume offline and will terminate any jobs= <br /> running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - ple= ase let us<br /> know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hour= s,<br /> which can be done by responding to this email with your preferred date(s) t= o<br /> perform the migration. During this time, DoIT staff will work to migrate yo= ur<br /> volume to the Ceph storage cluster. DoIT staff will send an email alert on = this<br /> email thread when the migration has begun and when it has completed. For mo= st<br /> storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT st= aff will<br /> assign a day over the following month (October 16th through November 15th) = to<br /> migrate your volume. The day chosen will be random and will occur during<br=  /> business hours. You will be notified of the date chosen to perform the<br /> migration, and will be notified when the migration begins and completes.<br=  /> <br /> Note: After this process has completed, the new storage volume will have a = new<br /> name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ld= quo;/umbc/rs/pi_doit&rdquo;,<br /> or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/= rs/pi_cmarron&rdquo;.<br /> <br /> Thank you,<br /> Elliot<br /> &nbsp;</blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --  <div> <div>Chris Marron <div>cmarron@umbc.edu</div>  <div>&nbsp;</div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> "
3283005,72422708,Correspond,DoIT-Research-Computing,2025-10-22 14:01:34.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_cmarron,resolved,Beamlak Bekele,bbekele1,Chris Marron,cmarron,cmarron@umbc.edu,Beamlak Bekele,bbekele1@umbc.edu,"<div> <p>Hello</p>  <p>We have finished migrating your volume to the Ceph cluster! As far as we=  can tell everything seems to have gone smoothly. There are a few things to=  note:</p>  <p>The path has changed, and is now available under /umbc/rs/pi_cmarron.</p>  <p>The alias used to reach the volume is now pi_cmarron_common and pi_cmarr= on_user.</p>  <p>Your new volume has a quota of 25TB.</p>  <p>When you have a chance, please try running some jobs on Chip using the n= ew volume to verify everything looks good.</p>  <p>Thank you</p>  <p>On Wed Oct 22 09:30:55 2025, PG41777 wrote:</p>  <blockquote> <div> <p>Hello&nbsp;<br /> <br /> This is just a reminder that we are starting your migration today. During t= his time, please ensure there are not any jobs being run in your research g= roup, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.</p>  <p>On Fri Oct 10 13:37:41 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Hello Chris,<br /> <br /> Sounds good, I&#39;ve put your group on our schedule for Wednesday October = 22nd. We will send you an email alert via this RT ticket when we begin the = migration process, and again once it has completed.<br /> <br /> Please let us know if you have any questions or concerns.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 10 11:17:29 2025, NN66844 wrote:</p>  <blockquote> <div>Good Morning, <div>&nbsp;</div>  <div>It is fine with me if you schedule the transfer of /umbc/rs/cmarron fo= r an arbitrary date 10/16 - 11/15. &nbsp;</div>  <div>&nbsp;</div>  <div>Chris Marron</div> </div> &nbsp;  <div> <div>On Fri, Oct 10, 2025 at 10:26=E2=80=AFAM Elliot Gobbert via RT &lt;UMB= CHelp@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 83005 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hello Chris,<br /> <br /> This is a reminder email, in case you missed the first. We need a response = by<br /> October 15th, or we&#39;ll be forced to go with Option 2, randomly scheduli= ng a<br /> time.<br /> <br /> As per the communication via myUMBC earlier this summer<br /> (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process = of<br /> migrating data off of an older storage server to our new RRStor Ceph storag= e<br /> cluster. Your group is using 0GB of a 500GB quota on the old storage server= .<br /> <br /> To perform these migrations, we need to take individual storage volumes off= line<br /> while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule<br /> a date where we can migrate your volume located at &ldquo;/umbc/rs/cmarron&= rdquo;. During<br /> the migration, we will take your volume offline and will terminate any jobs= <br /> running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - ple= ase let us<br /> know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hour= s,<br /> which can be done by responding to this email with your preferred date(s) t= o<br /> perform the migration. During this time, DoIT staff will work to migrate yo= ur<br /> volume to the Ceph storage cluster. DoIT staff will send an email alert on = this<br /> email thread when the migration has begun and when it has completed. For mo= st<br /> storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT st= aff will<br /> assign a day over the following month (October 16th through November 15th) = to<br /> migrate your volume. The day chosen will be random and will occur during<br=  /> business hours. You will be notified of the date chosen to perform the<br /> migration, and will be notified when the migration begins and completes.<br=  /> <br /> Note: After this process has completed, the new storage volume will have a = new<br /> name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ld= quo;/umbc/rs/pi_doit&rdquo;,<br /> or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/= rs/pi_cmarron&rdquo;.<br /> <br /> Thank you,<br /> Elliot<br /> &nbsp;</blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --  <div> <div>Chris Marron <div>cmarron@umbc.edu</div>  <div>&nbsp;</div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> "
3283011,72003971,Create,DoIT-Research-Computing,2025-09-29 20:13:32.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ckviauro,stalled,Greg Ballantine,gballan1,Christelle Viauroux,ckviauro,ckviauro@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<p>Hello Christelle,<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/ckviauro&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_ckviauro&rdquo;.<br /> <br /> Thank you,<br /> Greg</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283011,72204601,Correspond,DoIT-Research-Computing,2025-10-10 14:22:40.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ckviauro,stalled,Greg Ballantine,gballan1,Christelle Viauroux,ckviauro,ckviauro@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hello Christelle,</p>  <p>This is a reminder email, in case you missed the first. We need a response by October 15th, or we&#39;ll be forced to go with Option 2, randomly scheduling a time.<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/ckviauro&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_ckviauro&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3283011,72350926,Correspond,DoIT-Research-Computing,2025-10-17 16:24:40.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ckviauro,stalled,Greg Ballantine,gballan1,Christelle Viauroux,ckviauro,ckviauro@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Christelle,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 30th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3283011,72353119,Correspond,DoIT-Research-Computing,2025-10-17 17:30:20.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ckviauro,stalled,Greg Ballantine,gballan1,Christelle Viauroux,ckviauro,ckviauro@umbc.edu,Christelle Viauroux,ckviauro@umbc.edu,"Hi Eliot,  I apologize for my delayed response. October 30 should work for me. What is the procedure, do you come to my office? How can I help you?  Christelle  On Fri, Oct 17, 2025 at 12:24=E2=80=AFPM Elliot Gobbert via RT <UMBCHelp@rt= .umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3283011 > > > Last Update From Ticket: > > Dear Christelle, > > As per our previous communications, since you did not schedule a date by > October 15, we will be going with Option 2, randomly assigning a date > between > October 16 and November 15 for your migration. > > We have assigned the date of October 30th for your migration. Let us know > if > there is a better day for you, and within reason, we can reschedule that > date. > > You will be notified when the migration begins and completes. > > Thank you, > > Elliot > >  --=20 Christelle Viauroux Associate Professor of Economics University of Maryland, Baltimore County / 1000 Hilltop Circle / Baltimore, MD 21250 phone 410-455-3117 / fax 410-455-1054 / email ckviauro@umbc.edu "
3283011,72353119,Correspond,DoIT-Research-Computing,2025-10-17 17:30:20.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ckviauro,stalled,Greg Ballantine,gballan1,Christelle Viauroux,ckviauro,ckviauro@umbc.edu,Christelle Viauroux,ckviauro@umbc.edu,"<div dir=3D""ltr"">Hi Eliot,<div><br></div><div>I apologize for my delayed re= sponse. October 30 should work for me. What is the procedure, do you come t= o my office? How can I help you?</div><div><br></div><div>Christelle</div><= /div><br><div class=3D""gmail_quote gmail_quote_container""><div dir=3D""ltr"" = class=3D""gmail_attr"">On Fri, Oct 17, 2025 at 12:24=E2=80=AFPM Elliot Gobber= t via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</= a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""margin:0p= x 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"">Ti= cket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D32830= 11"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display= .html?id=3D3283011</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Dear Christelle,<br> <br> As per our previous communications, since you did not schedule a date by<br> October 15, we will be going with Option 2, randomly assigning a date betwe= en<br> October 16 and November 15 for your migration.<br> <br> We have assigned the date of October 30th for your migration. Let us know i= f<br> there is a better day for you, and within reason, we can reschedule that da= te.<br> <br> You will be notified when the migration begins and completes.<br> <br> Thank you,<br> <br> Elliot<br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><font size=3D""1"">Christelle Viauroux</font><div>= <font size=3D""1"">Associate Professor of Economics</font></div><div><font si= ze=3D""1"">University of Maryland, Baltimore County /=C2=A01000 Hilltop Circl= e /=C2=A0Baltimore, MD 21250</font></div><div><font size=3D""1"">phone=C2=A0<= a value=3D""+14104552385"" style=3D""color:rgb(17,85,204)"">410-455-3117</a>=C2= =A0/ fax=C2=A0<a value=3D""+14104551095"" style=3D""color:rgb(17,85,204)"">410-= 455-1054</a>=C2=A0/ email <a href=3D""mailto:ckviauro@umbc.edu"" style=3D""col= or:rgb(17,85,204)"" target=3D""_blank"">ckviauro@umbc.edu</a></font></div></di= v></div> "
3283011,72361121,Correspond,DoIT-Research-Computing,2025-10-18 11:28:24.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ckviauro,stalled,Greg Ballantine,gballan1,Christelle Viauroux,ckviauro,ckviauro@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hi,</p>  <p>No worries, you don&#39;t need to do anything except NOT use the cluster on Oct. 30th. To transfer the files, we make the old directory read only, so just don&#39;t use the cluster while we transfer things. We will email you in this ticket when we start and finish the migration.</p>  <p>&nbsp;</p>  <p>Best,</p>  <p>Elliot</p> "
3283011,72597812,Correspond,DoIT-Research-Computing,2025-10-30 14:33:42.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ckviauro,stalled,Greg Ballantine,gballan1,Christelle Viauroux,ckviauro,ckviauro@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Christelle,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Sat Oct 18 07:28:24 2025, IO12693 wrote:</p>  <blockquote> <p>Hi,</p>  <p>No worries, you don&#39;t need to do anything except NOT use the cluster on Oct. 30th. To transfer the files, we make the old directory read only, so just don&#39;t use the cluster while we transfer things. We will email you in this ticket when we start and finish the migration.</p>  <p>&nbsp;</p>  <p>Best,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283011,72612724,Correspond,DoIT-Research-Computing,2025-10-30 23:26:51.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ckviauro,stalled,Greg Ballantine,gballan1,Christelle Viauroux,ckviauro,ckviauro@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Christelle,<br /> <br /> We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note:</p>  <ul> 	<li>The path has changed, and is now available under <strong>/umbc/rs/pi_ckviauro</strong>.  	<ul> 		<li>The alias used to reach the volume is now <strong>pi_ckviauro_common</strong> and <strong>pi_ckviauro_user</strong>.</li> 	</ul> 	</li> 	<li>Your new volume has a quota of <strong>10TB</strong>.</li> </ul>  <p>When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good?<br /> <br /> Thank you,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Thu Oct 30 10:33:42 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Hello Christelle,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Sat Oct 18 07:28:24 2025, IO12693 wrote:</p>  <blockquote> <p>Hi,</p>  <p>No worries, you don&#39;t need to do anything except NOT use the cluster on Oct. 30th. To transfer the files, we make the old directory read only, so just don&#39;t use the cluster while we transfer things. We will email you in this ticket when we start and finish the migration.</p>  <p>&nbsp;</p>  <p>Best,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283013,72004085,Create,DoIT-Research-Computing,2025-09-29 20:16:40.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_colleenb,stalled,Greg Ballantine,gballan1,Colleen Burge,colleenb,colleenb@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<p>Hello Colleen,<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/colleenb&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_colleenb&rdquo;.<br /> <br /> Thank you,<br /> Greg</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283013,72204858,Correspond,DoIT-Research-Computing,2025-10-10 14:27:57.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_colleenb,stalled,Greg Ballantine,gballan1,Colleen Burge,colleenb,colleenb@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hello Colleen,</p>  <p>This is a reminder email, in case you missed the first. We need a response by October 15th, or we&#39;ll be forced to go with Option 2, randomly scheduling a time.<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/colleenb&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_colleenb&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3283013,72351150,Correspond,DoIT-Research-Computing,2025-10-17 16:30:41.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_colleenb,stalled,Greg Ballantine,gballan1,Colleen Burge,colleenb,colleenb@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Colleen,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 30th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3283013,72597787,Correspond,DoIT-Research-Computing,2025-10-30 14:32:46.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_colleenb,stalled,Greg Ballantine,gballan1,Colleen Burge,colleenb,colleenb@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Colleen,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 12:30:41 2025, IO12693 wrote:</p>  <blockquote> <p>Dear Colleen,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 30th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283013,72612692,Correspond,DoIT-Research-Computing,2025-10-30 23:25:03.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_colleenb,stalled,Greg Ballantine,gballan1,Colleen Burge,colleenb,colleenb@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Colleen,<br /> <br /> We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note:</p>  <ul> 	<li>The path has changed, and is now available under <strong>/umbc/rs/pi_colleenb</strong>.  	<ul> 		<li>The alias used to reach the volume is now <strong>pi_colleenb_common</strong> and <strong>pi_colleenb_user</strong>.</li> 	</ul> 	</li> 	<li>Your new volume has a quota of <strong>10TB</strong>.</li> </ul>  <p>When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good?<br /> <br /> Thank you,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Thu Oct 30 10:32:46 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Hello Colleen,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 12:30:41 2025, IO12693 wrote:</p>  <blockquote> <p>Dear Colleen,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 30th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283015,72004192,Create,DoIT-Research-Computing,2025-09-29 20:20:15.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dchapm2,stalled,Greg Ballantine,gballan1,David Chapman,dchapm2,dchapm2@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<p>Hello David,<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/dchapm2&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_dchapm2&rdquo;.<br /> <br /> Thank you,<br /> Greg</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283015,72204908,Correspond,DoIT-Research-Computing,2025-10-10 14:29:23.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dchapm2,stalled,Greg Ballantine,gballan1,David Chapman,dchapm2,dchapm2@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hello David,</p>  <p>This is a reminder email, in case you missed the first. We need a response by October 15th, or we&#39;ll be forced to go with Option 2, randomly scheduling a time.<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/dchapm2&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_dchapm2&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3283015,72351740,Correspond,DoIT-Research-Computing,2025-10-17 16:48:06.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dchapm2,stalled,Greg Ballantine,gballan1,David Chapman,dchapm2,dchapm2@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear David,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 31st for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3283015,72621326,Correspond,DoIT-Research-Computing,2025-10-31 14:42:00.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dchapm2,stalled,Greg Ballantine,gballan1,David Chapman,dchapm2,dchapm2@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Good morning David,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 12:48:06 2025, IO12693 wrote:</p>  <blockquote> <p>Dear David,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 31st for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283015,72639783,Correspond,DoIT-Research-Computing,2025-10-31 18:20:42.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dchapm2,stalled,Greg Ballantine,gballan1,David Chapman,dchapm2,dchapm2@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello David,<br /> <br /> We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note:</p>  <ul> 	<li>The path has changed, and is now available under <strong>/umbc/rs/pi_dchapm2</strong>.</li> 	<li>The alias used to reach the volume is now <strong>pi_dchapm2_common</strong> and <strong>pi_dchapm2_user</strong>.</li> 	<li>Your new volume has a quota of <strong>25TB</strong>.</li> </ul>  <p>When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good?<br /> <br /> Thank you,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 31 10:42:00 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Good morning David,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 12:48:06 2025, IO12693 wrote:</p>  <blockquote> <p>Dear David,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 31st for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283018,72004302,Create,DoIT-Research-Computing,2025-09-29 20:23:12.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_phatak,stalled,Greg Ballantine,gballan1,Dhananjay Phatak,phatak,phatak@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<p>Hello Dhananjay,<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/phatak&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_phatak&rdquo;.<br /> <br /> Thank you,<br /> Greg</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283018,72206406,Correspond,DoIT-Research-Computing,2025-10-10 15:06:31.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_phatak,stalled,Greg Ballantine,gballan1,Dhananjay Phatak,phatak,phatak@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hello Dhananjay,</p>  <p>This is a reminder email, in case you missed the first. We need a response by October 15th, or we&#39;ll be forced to go with Option 2, randomly scheduling a time.<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/phatak&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_phatak&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3283018,72451113,Correspond,DoIT-Research-Computing,2025-10-23 13:53:20.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_phatak,stalled,Greg Ballantine,gballan1,Dhananjay Phatak,phatak,phatak@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Dhananjay,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3283023,72004470,Create,DoIT-Research-Computing,2025-09-29 20:25:18.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dillonm,stalled,Greg Ballantine,gballan1,Dillon Mahmoudi,dillonm,dillonm@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<p>Hello Dillon,<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 100GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/dillonm&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_dillonm&rdquo;.<br /> <br /> Thank you,<br /> Greg</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283023,72205079,Correspond,DoIT-Research-Computing,2025-10-10 14:33:28.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dillonm,stalled,Greg Ballantine,gballan1,Dillon Mahmoudi,dillonm,dillonm@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hello Dillon,</p>  <p>This is a reminder email, in case you missed the first. We need a response by October 15th, or we&#39;ll be forced to go with Option 2, randomly scheduling a time.<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 100GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/dillonm&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_dillonm&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3283023,72351843,Correspond,DoIT-Research-Computing,2025-10-17 16:52:22.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dillonm,stalled,Greg Ballantine,gballan1,Dillon Mahmoudi,dillonm,dillonm@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Dillon,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 3rd for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3283023,72683773,Correspond,DoIT-Research-Computing,2025-11-03 16:41:24.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dillonm,stalled,Greg Ballantine,gballan1,Dillon Mahmoudi,dillonm,dillonm@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Dillon,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 12:52:22 2025, IO12693 wrote:</p>  <blockquote> <p>Dear Dillon,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 3rd for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283023,72699606,Correspond,DoIT-Research-Computing,2025-11-04 01:26:21.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dillonm,stalled,Greg Ballantine,gballan1,Dillon Mahmoudi,dillonm,dillonm@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Dillon,<br /> <br /> We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note:</p>  <ul> 	<li>The path has changed, and is now available under <strong>/umbc/rs/pi_dillonm</strong>.</li> 	<li>The alias used to reach the volume is now <strong>pi_dillonm_common</strong> and <strong>pi_dillonm_user</strong>.</li> 	<li>Your new volume has a quota of <strong>10TB</strong>.</li> </ul>  <p>When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good?<br /> <br /> Thank you,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Mon Nov 03 11:41:24 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Hello Dillon,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 12:52:22 2025, IO12693 wrote:</p>  <blockquote> <p>Dear Dillon,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 3rd for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283023,72707701,Correspond,DoIT-Research-Computing,2025-11-04 15:03:43.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dillonm,stalled,Greg Ballantine,gballan1,Dillon Mahmoudi,dillonm,dillonm@umbc.edu,Dillon Mahmoudi,dillonm@umbc.edu,"Excellent, cheers!  On Mon, Nov 3, 2025 at 8:26=E2=80=AFPM Greg Ballantine via RT <UMBCHelp@rt.= umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3283023 > > > Last Update From Ticket: > > Hello Dillon, > > We have finished migrating your volume to the Ceph cluster! As far as we > can > tell everything seems to have gone smoothly. There are a few things to > note: > > > * The path has changed, and is now available under /umbc/rs/pi_dillonm. > > > * The alias used to reach the volume is now pi_dillonm_common and > >> pi_dillonm_user. > > > * Your new volume has a quota of 10TB. > > When you have a chance, could you try running some jobs on Chip using the > new > volume to verify everything looks good? > > Thank you, > Greg > > On Mon Nov 03 11:41:24 2025, OJ87090 wrote: > > > Hello Dillon, > > > This is a reminder that we will be performing your group's migration to > the > > Ceph storage cluster today. During this time, please ensure there are n= ot > > any jobs being run in your research group, otherwise these may be > > terminated. > > > We will provide an update once completed. > > > Best, > > Greg > > > On Fri Oct 17 12:52:22 2025, IO12693 wrote: > > >> Dear Dillon, > > >> As per our previous communications, since you did not schedule a date > >> by October 15, we will be going with Option 2, randomly assigning a > >> date between October 16 and November 15 for your migration. > > >> We have assigned the date of November 3rd for your migration. Let us > >> know if there is a better day for you, and within reason, we can > >> reschedule that date. > > >> You will be notified when the migration begins and completes. > > >> Thank you, > > >> Elliot > > > -- > > > Gregory BallantineSystem Administrator for Research and Enterprise > > ComputingUMBC - DoIT > > -- > > Gregory BallantineSystem Administrator for Research and Enterprise > ComputingUMBC > - DoIT > > "
3283023,72707701,Correspond,DoIT-Research-Computing,2025-11-04 15:03:43.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dillonm,stalled,Greg Ballantine,gballan1,Dillon Mahmoudi,dillonm,dillonm@umbc.edu,Dillon Mahmoudi,dillonm@umbc.edu,"<div dir=3D""ltr"">Excellent, cheers!</div><br><div class=3D""gmail_quote gmai= l_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Mon, Nov 3, 202= 5 at 8:26=E2=80=AFPM Greg Ballantine via RT &lt;<a href=3D""mailto:UMBCHelp@= rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class= =3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rg= b(204,204,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc= .edu/Ticket/Display.html?id=3D3283023"" rel=3D""noreferrer"" target=3D""_blank""= >https://rt.umbc.edu/Ticket/Display.html?id=3D3283023</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hello Dillon,<br> <br> We have finished migrating your volume to the Ceph cluster! As far as we ca= n<br> tell everything seems to have gone smoothly. There are a few things to note= :<br> <br> &gt; * The path has changed, and is now available under /umbc/rs/pi_dillonm= .<br> <br> &gt; * The alias used to reach the volume is now pi_dillonm_common and<br> &gt;&gt; pi_dillonm_user.<br> <br> &gt; * Your new volume has a quota of 10TB.<br> <br> When you have a chance, could you try running some jobs on Chip using the n= ew<br> volume to verify everything looks good?<br> <br> Thank you,<br> Greg<br> <br> On Mon Nov 03 11:41:24 2025, OJ87090 wrote:<br> <br> &gt; Hello Dillon,<br> <br> &gt; This is a reminder that we will be performing your group&#39;s migrati= on to the<br> &gt; Ceph storage cluster today. During this time, please ensure there are = not<br> &gt; any jobs being run in your research group, otherwise these may be<br> &gt; terminated.<br> <br> &gt; We will provide an update once completed.<br> <br> &gt; Best,<br> &gt; Greg<br> <br> &gt; On Fri Oct 17 12:52:22 2025, IO12693 wrote:<br> <br> &gt;&gt; Dear Dillon,<br> <br> &gt;&gt; As per our previous communications, since you did not schedule a d= ate<br> &gt;&gt; by October 15, we will be going with Option 2, randomly assigning = a<br> &gt;&gt; date between October 16 and November 15 for your migration.<br> <br> &gt;&gt; We have assigned the date of November 3rd for your migration. Let = us<br> &gt;&gt; know if there is a better day for you, and within reason, we can<b= r> &gt;&gt; reschedule that date.<br> <br> &gt;&gt; You will be notified when the migration begins and completes.<br> <br> &gt;&gt; Thank you,<br> <br> &gt;&gt; Elliot<br> <br> &gt; --<br> <br> &gt; Gregory BallantineSystem Administrator for Research and Enterprise<br> &gt; ComputingUMBC - DoIT<br> <br> --<br> <br> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br> - DoIT<br> <br> </blockquote></div> "
3283027,72004513,Create,DoIT-Research-Computing,2025-09-29 20:26:26.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dhpark,stalled,Greg Ballantine,gballan1,DoHwan Park,dhpark,dhpark@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<p>Hello DoHwan,<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/dhpark&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_dhpark&rdquo;.<br /> <br /> Thank you,<br /> Greg</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283027,72205059,Correspond,DoIT-Research-Computing,2025-10-10 14:32:43.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dhpark,stalled,Greg Ballantine,gballan1,DoHwan Park,dhpark,dhpark@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hello DoHwan,</p>  <p>This is a reminder email, in case you missed the first. We need a response by October 15th, or we&#39;ll be forced to go with Option 2, randomly scheduling a time.<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/dhpark&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_dhpark&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3283027,72217580,Correspond,DoIT-Research-Computing,2025-10-10 21:36:42.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dhpark,stalled,Greg Ballantine,gballan1,DoHwan Park,dhpark,dhpark@umbc.edu,DoHwan Park,dhpark@umbc.edu,"Hi Elliot, Thank you for reaching out to me. I prefer 10.29,30,31. Thanks, DoHwan  On Fri, Oct 10, 2025 at 11:32=E2=80=AFPM Elliot Gobbert via RT <UMBCHelp@rt= .umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3283027 > > > Last Update From Ticket: > > Hello DoHwan, > > This is a reminder email, in case you missed the first. We need a response > by > October 15th, or we'll be forced to go with Option 2, randomly scheduling=  a > time. > > As per the communication via myUMBC earlier this summer > (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the > process of > migrating data off of an older storage server to our new RRStor Ceph > storage > cluster. Your group is using 0GB of a 500GB quota on the old storage > server. > > To perform these migrations, we need to take individual storage volumes > offline > while we migrate them to the Ceph cluster. Thus we are reaching out to > schedule > a date where we can migrate your volume located at =E2=80=9C/umbc/rs/dhpa= rk=E2=80=9D. > During > the migration, we will take your volume offline and will terminate any jo= bs > running on the chip compute cluster that are accessing this volume. > > Below we=E2=80=99ve listed two options for handling this data migration -=  please > let us > know which of these you=E2=80=99d prefer. > > Option 1: Schedule a group-wide downtime date during standard business > hours, > which can be done by responding to this email with your preferred date(s) > to > perform the migration. During this time, DoIT staff will work to migrate > your > volume to the Ceph storage cluster. DoIT staff will send an email alert on > this > email thread when the migration has begun and when it has completed. For > most > storage volumes, this process should take less than a business day. > Option 2: If you don=E2=80=99t respond to this email by October 15th, DoI= T staff > will > assign a day over the following month (October 16th through November 15th) > to > migrate your volume. The day chosen will be random and will occur during > business hours. You will be notified of the date chosen to perform the > migration, and will be notified when the migration begins and completes. > > Note: After this process has completed, the new storage volume will have a > new > name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its data und= er > =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, > or in your group=E2=80=99s case you will find your volume under > =E2=80=9C/umbc/rs/pi_dhpark=E2=80=9D. > > Thank you, > Elliot > > "
3283027,72217580,Correspond,DoIT-Research-Computing,2025-10-10 21:36:42.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dhpark,stalled,Greg Ballantine,gballan1,DoHwan Park,dhpark,dhpark@umbc.edu,DoHwan Park,dhpark@umbc.edu,"<div dir=3D""auto"">Hi Elliot,</div><div dir=3D""auto"">Thank you for reaching = out to me. I prefer 10.29,30,31.=C2=A0</div><div dir=3D""auto"">Thanks,</div>= <div dir=3D""auto"">DoHwan</div><div><br><div class=3D""gmail_quote gmail_quot= e_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Fri, Oct 10, 2025 at = 11:32=E2=80=AFPM Elliot Gobbert via RT &lt;<a href=3D""mailto:UMBCHelp@rt.um= bc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""g= mail_quote"" style=3D""margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-l= eft:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html= ?id=3D3283027"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tic= ket/Display.html?id=3D3283027</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hello DoHwan,<br> <br> This is a reminder email, in case you missed the first. We need a response = by<br> October 15th, or we&#39;ll be forced to go with Option 2, randomly scheduli= ng a<br> time.<br> <br> As per the communication via myUMBC earlier this summer<br> (<a href=3D""https://my3.my.umbc.edu/groups/hpcf/posts/150838"" rel=3D""norefe= rrer"" target=3D""_blank"">https://my3.my.umbc.edu/groups/hpcf/posts/150838</a= >), DoIT is in the process of<br> migrating data off of an older storage server to our new RRStor Ceph storag= e<br> cluster. Your group is using 0GB of a 500GB quota on the old storage server= .<br> <br> To perform these migrations, we need to take individual storage volumes off= line<br> while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule<br> a date where we can migrate your volume located at =E2=80=9C/umbc/rs/dhpark= =E2=80=9D. During<br> the migration, we will take your volume offline and will terminate any jobs= <br> running on the chip compute cluster that are accessing this volume.<br> <br> Below we=E2=80=99ve listed two options for handling this data migration - p= lease let us<br> know which of these you=E2=80=99d prefer.<br> <br> Option 1: Schedule a group-wide downtime date during standard business hour= s,<br> which can be done by responding to this email with your preferred date(s) t= o<br> perform the migration. During this time, DoIT staff will work to migrate yo= ur<br> volume to the Ceph storage cluster. DoIT staff will send an email alert on = this<br> email thread when the migration has begun and when it has completed. For mo= st<br> storage volumes, this process should take less than a business day.<br> Option 2: If you don=E2=80=99t respond to this email by October 15th, DoIT = staff will<br> assign a day over the following month (October 16th through November 15th) = to<br> migrate your volume. The day chosen will be random and will occur during<br> business hours. You will be notified of the date chosen to perform the<br> migration, and will be notified when the migration begins and completes.<br> <br> Note: After this process has completed, the new storage volume will have a = new<br> name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its data under=  =E2=80=9C/umbc/rs/pi_doit=E2=80=9D,<br> or in your group=E2=80=99s case you will find your volume under =E2=80=9C/u= mbc/rs/pi_dhpark=E2=80=9D.<br> <br> Thank you,<br> Elliot<br> <br> </blockquote></div></div> "
3283027,72218159,Correspond,DoIT-Research-Computing,2025-10-11 03:05:56.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dhpark,stalled,Greg Ballantine,gballan1,DoHwan Park,dhpark,dhpark@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello&nbsp;DoHwan,<br /> <br /> Thanks for getting back to us! I put your group&#39;s migration on our sche= dule for Wednesday October 29th. We will send you an email alert via this R= T ticket when we begin the migration, and again once it has completed.<br /> <br /> Please let us know if you have any questions or concerns.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 10 17:36:42 2025, LC79377 wrote:</p>  <blockquote> <div>Hi Elliot,</div>  <div>Thank you for reaching out to me. I prefer 10.29,30,31.&nbsp;</div>  <div>Thanks,</div>  <div>DoHwan</div>  <div>&nbsp; <div> <div>On Fri, Oct 10, 2025 at 11:32=E2=80=AFPM Elliot Gobbert via RT &lt;UMB= CHelp@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 83027 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hello DoHwan,<br /> <br /> This is a reminder email, in case you missed the first. We need a response = by<br /> October 15th, or we&#39;ll be forced to go with Option 2, randomly scheduli= ng a<br /> time.<br /> <br /> As per the communication via myUMBC earlier this summer<br /> (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process = of<br /> migrating data off of an older storage server to our new RRStor Ceph storag= e<br /> cluster. Your group is using 0GB of a 500GB quota on the old storage server= .<br /> <br /> To perform these migrations, we need to take individual storage volumes off= line<br /> while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule<br /> a date where we can migrate your volume located at &ldquo;/umbc/rs/dhpark&r= dquo;. During<br /> the migration, we will take your volume offline and will terminate any jobs= <br /> running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - ple= ase let us<br /> know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hour= s,<br /> which can be done by responding to this email with your preferred date(s) t= o<br /> perform the migration. During this time, DoIT staff will work to migrate yo= ur<br /> volume to the Ceph storage cluster. DoIT staff will send an email alert on = this<br /> email thread when the migration has begun and when it has completed. For mo= st<br /> storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT st= aff will<br /> assign a day over the following month (October 16th through November 15th) = to<br /> migrate your volume. The day chosen will be random and will occur during<br=  /> business hours. You will be notified of the date chosen to perform the<br /> migration, and will be notified when the migration begins and completes.<br=  /> <br /> Note: After this process has completed, the new storage volume will have a = new<br /> name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ld= quo;/umbc/rs/pi_doit&rdquo;,<br /> or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/= rs/pi_dhpark&rdquo;.<br /> <br /> Thank you,<br /> Elliot<br /> &nbsp;</blockquote> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283030,72004577,Create,DoIT-Research-Computing,2025-09-29 20:28:44.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dli,resolved,Beamlak Bekele,bbekele1,Dong Li,dli,dli@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<p>Hello Dong,<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 250GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/dli&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_dli&rdquo;.<br /> <br /> Thank you,<br /> Greg</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283030,72056415,Correspond,DoIT-Research-Computing,2025-10-02 13:34:10.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dli,resolved,Beamlak Bekele,bbekele1,Dong Li,dli,dli@umbc.edu,Dong Li,dli@umbc.edu,"Hi Greg,  Thank you for reaching out. Any day before October 30th works for me for migrations.  Best, Dong  On Mon, Sep 29, 2025 at 4:28=E2=80=AFPM via RT <UMBCHelp@rt.umbc.edu> wrote:  > Greetings, > > This message has been automatically generated in response to the > creation of a ticket regarding: > > ------------------------------------------------------------------------- > Subject: ""Migrating Research Storage Volume to Ceph Cluster - pi_dli"" > > Message: > > Hello Dong, > > As per the communication via myUMBC earlier this summer > (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the > process of > migrating data off of an older storage server to our new RRStor Ceph > storage > cluster. Your group is using 0GB of a 250GB quota on the old storage > server. > > To perform these migrations, we need to take individual storage volumes > offline > while we migrate them to the Ceph cluster. Thus we are reaching out to > schedule > a date where we can migrate your volume located at =E2=80=9C/umbc/rs/dli= =E2=80=9D. During > the > migration, we will take your volume offline and will terminate any jobs > running > on the chip compute cluster that are accessing this volume. > > Below we=E2=80=99ve listed two options for handling this data migration -=  please > let us > know which of these you=E2=80=99d prefer. > > Option 1: Schedule a group-wide downtime date during standard business > hours, > which can be done by responding to this email with your preferred date(s) > to > perform the migration. During this time, DoIT staff will work to migrate > your > volume to the Ceph storage cluster. DoIT staff will send an email alert on > this > email thread when the migration has begun and when it has completed. For > most > storage volumes, this process should take less than a business day. > Option 2: If you don=E2=80=99t respond to this email by October 15th, DoI= T staff > will > assign a day over the following month (October 16th through November 15th) > to > migrate your volume. The day chosen will be random and will occur during > business hours. You will be notified of the date chosen to perform the > migration, and will be notified when the migration begins and completes. > > Note: After this process has completed, the new storage volume will have a > new > name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its data und= er > =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, > or in your group=E2=80=99s case you will find your volume under =E2=80=9C= /umbc/rs/pi_dli=E2=80=9D. > > Thank you, > Greg > > -- > > Gregory BallantineSystem Administrator for Research and Enterprise > ComputingUMBC > - DoIT > > > ------------------------------------------------------------------------- > > There is no need to reply to this message right now. > > Your ticket has been assigned an ID of [Research Computing #3283030] or > you can go there directly by clicking the link below. > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3283030 > > > You can login to view your open tickets at any time by visiting > http://my.umbc.edu and clicking on ""Help"" and ""Request Help"". > > Alternately you can click on http://my.umbc.edu/help > >                         Thank you > >  --=20 Dong Li, Ph.D. Assistant Professor Department of Computer Science and Electrical Engineering University of Maryland Baltimore County Homepage: https://leetton.github.io/ Schedule a meeting: Google Calendar <https://calendar.app.google/xYK55Uy8HUuaM2ic8> "
3283030,72056415,Correspond,DoIT-Research-Computing,2025-10-02 13:34:10.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dli,resolved,Beamlak Bekele,bbekele1,Dong Li,dli,dli@umbc.edu,Dong Li,dli@umbc.edu,"<div dir=3D""ltr"">Hi Greg,<div><br></div><div>Thank you for reaching out. An= y day before October 30th works for me for migrations.</div><div><br></div>= <div>Best,</div><div>Dong</div></div><br><div class=3D""gmail_quote gmail_qu= ote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Mon, Sep 29, 2025 a= t 4:28=E2=80=AFPM via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHe= lp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" st= yle=3D""margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-style:sol= id;border-left-color:rgb(204,204,204);padding-left:1ex"">Greetings,<br> <br> This message has been automatically generated in response to the<br> creation of a ticket regarding:<br> <br> -------------------------------------------------------------------------<b= r> Subject: &quot;Migrating Research Storage Volume to Ceph Cluster - pi_dli&q= uot;<br> <br> Message: <br> <br> Hello Dong,<br> <br> As per the communication via myUMBC earlier this summer<br> (<a href=3D""https://my3.my.umbc.edu/groups/hpcf/posts/150838"" rel=3D""norefe= rrer"" target=3D""_blank"">https://my3.my.umbc.edu/groups/hpcf/posts/150838</a= >), DoIT is in the process of<br> migrating data off of an older storage server to our new RRStor Ceph storag= e<br> cluster. Your group is using 0GB of a 250GB quota on the old storage server= .<br> <br> To perform these migrations, we need to take individual storage volumes off= line<br> while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule<br> a date where we can migrate your volume located at =E2=80=9C/umbc/rs/dli=E2= =80=9D. During the<br> migration, we will take your volume offline and will terminate any jobs run= ning<br> on the chip compute cluster that are accessing this volume.<br> <br> Below we=E2=80=99ve listed two options for handling this data migration - p= lease let us<br> know which of these you=E2=80=99d prefer.<br> <br> Option 1: Schedule a group-wide downtime date during standard business hour= s,<br> which can be done by responding to this email with your preferred date(s) t= o<br> perform the migration. During this time, DoIT staff will work to migrate yo= ur<br> volume to the Ceph storage cluster. DoIT staff will send an email alert on = this<br> email thread when the migration has begun and when it has completed. For mo= st<br> storage volumes, this process should take less than a business day.<br> Option 2: If you don=E2=80=99t respond to this email by October 15th, DoIT = staff will<br> assign a day over the following month (October 16th through November 15th) = to<br> migrate your volume. The day chosen will be random and will occur during<br> business hours. You will be notified of the date chosen to perform the<br> migration, and will be notified when the migration begins and completes.<br> <br> Note: After this process has completed, the new storage volume will have a = new<br> name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its data under=  =E2=80=9C/umbc/rs/pi_doit=E2=80=9D,<br> or in your group=E2=80=99s case you will find your volume under =E2=80=9C/u= mbc/rs/pi_dli=E2=80=9D.<br> <br> Thank you,<br> Greg<br> <br> --<br> <br> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br> - DoIT<br> <br> <br> -------------------------------------------------------------------------<b= r> <br> There is no need to reply to this message right now.=C2=A0 <br> <br> Your ticket has been assigned an ID of [Research Computing #3283030] or you=  can go there directly by clicking the link below.<br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D328= 3030"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3283030</a> &gt;<br> <br> You can login to view your open tickets at any time by visiting <a href=3D""= http://my.umbc.edu"" rel=3D""noreferrer"" target=3D""_blank"">http://my.umbc.edu= </a> and clicking on &quot;Help&quot; and &quot;Request Help&quot;. <br> <br> Alternately you can click on <a href=3D""http://my.umbc.edu/help"" rel=3D""nor= eferrer"" target=3D""_blank"">http://my.umbc.edu/help</a><br> <br> =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 Thank you<br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><div>Dong Li, Ph.D.<br></div><div><div>Assistant=  Professor</div><div>Department of Computer Science and Electrical Engineer= ing</div><div>University of Maryland Baltimore County</div></div><div>Homep= age:=C2=A0<a href=3D""https://leetton.github.io/"" target=3D""_blank"">https://= leetton.github.io/</a><br></div><div>Schedule a meeting: <a href=3D""https:/= /calendar.app.google/xYK55Uy8HUuaM2ic8"" target=3D""_blank"">Google Calendar</= a></div><div><br></div></div></div> "
3283030,72057993,Correspond,DoIT-Research-Computing,2025-10-02 14:17:53.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dli,resolved,Beamlak Bekele,bbekele1,Dong Li,dli,dli@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Good morning Dong,<br /> <br /> Sounds good, I&#39;ve put your group&#39;s migration on our schedule for Oc= tober 17th. We will send you a notification via this RT ticket when we begi= n the migration, and again once the migration has completed.<br /> <br /> Please let me know if you have any questions or concerns.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Thu Oct 02 09:34:10 2025, RK43819 wrote:</p>  <blockquote> <div>Hi Greg, <div>&nbsp;</div>  <div>Thank you for reaching out. Any day before October 30th works for me f= or migrations.</div>  <div>&nbsp;</div>  <div>Best,</div>  <div>Dong</div> </div> &nbsp;  <div> <div>On Mon, Sep 29, 2025 at 4:28=E2=80=AFPM via RT &lt;UMBCHelp@rt.umbc.ed= u&gt; wrote:</div>  <blockquote>Greetings,<br /> <br /> This message has been automatically generated in response to the<br /> creation of a ticket regarding:<br /> <br /> -------------------------------------------------------------------------<b= r /> Subject: &quot;Migrating Research Storage Volume to Ceph Cluster - pi_dli&q= uot;<br /> <br /> Message:<br /> <br /> Hello Dong,<br /> <br /> As per the communication via myUMBC earlier this summer<br /> (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process = of<br /> migrating data off of an older storage server to our new RRStor Ceph storag= e<br /> cluster. Your group is using 0GB of a 250GB quota on the old storage server= .<br /> <br /> To perform these migrations, we need to take individual storage volumes off= line<br /> while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule<br /> a date where we can migrate your volume located at &ldquo;/umbc/rs/dli&rdqu= o;. During the<br /> migration, we will take your volume offline and will terminate any jobs run= ning<br /> on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - ple= ase let us<br /> know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hour= s,<br /> which can be done by responding to this email with your preferred date(s) t= o<br /> perform the migration. During this time, DoIT staff will work to migrate yo= ur<br /> volume to the Ceph storage cluster. DoIT staff will send an email alert on = this<br /> email thread when the migration has begun and when it has completed. For mo= st<br /> storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT st= aff will<br /> assign a day over the following month (October 16th through November 15th) = to<br /> migrate your volume. The day chosen will be random and will occur during<br=  /> business hours. You will be notified of the date chosen to perform the<br /> migration, and will be notified when the migration begins and completes.<br=  /> <br /> Note: After this process has completed, the new storage volume will have a = new<br /> name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ld= quo;/umbc/rs/pi_doit&rdquo;,<br /> or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/= rs/pi_dli&rdquo;.<br /> <br /> Thank you,<br /> Greg<br /> <br /> --<br /> <br /> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br /> - DoIT<br /> <br /> <br /> -------------------------------------------------------------------------<b= r /> <br /> There is no need to reply to this message right now.&nbsp;<br /> <br /> Your ticket has been assigned an ID of [Research Computing #3283030] or you=  can go there directly by clicking the link below.<br /> <br /> Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3283030 &gt;<b= r /> <br /> You can login to view your open tickets at any time by visiting http://my.u= mbc.edu and clicking on &quot;Help&quot; and &quot;Request Help&quot;.<br /> <br /> Alternately you can click on http://my.umbc.edu/help<br /> <br /> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp= ; &nbsp; Thank you<br /> &nbsp;</blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --  <div> <div> <div>Dong Li, Ph.D.</div>  <div> <div>Assistant Professor</div>  <div>Department of Computer Science and Electrical Engineering</div>  <div>University of Maryland Baltimore County</div> </div>  <div>Homepage:&nbsp;https://leetton.github.io/</div>  <div>Schedule a meeting: Google Calendar</div>  <div>&nbsp;</div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283030,72345664,Correspond,DoIT-Research-Computing,2025-10-17 13:43:32.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dli,resolved,Beamlak Bekele,bbekele1,Dong Li,dli,dli@umbc.edu,Beamlak Bekele,bbekele1@umbc.edu,"<div> <p>Hello&nbsp;<br /> This is just a reminder that we are starting your migration today. During t= his time, please ensure there are not any jobs being run in your research g= roup, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> &nbsp;</p>  <p>&nbsp;</p>  <p>On Thu Oct 02 10:17:53 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Good morning Dong,<br /> <br /> Sounds good, I&#39;ve put your group&#39;s migration on our schedule for Oc= tober 17th. We will send you a notification via this RT ticket when we begi= n the migration, and again once the migration has completed.<br /> <br /> Please let me know if you have any questions or concerns.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Thu Oct 02 09:34:10 2025, RK43819 wrote:</p>  <blockquote> <div>Hi Greg, <div>&nbsp;</div>  <div>Thank you for reaching out. Any day before October 30th works for me f= or migrations.</div>  <div>&nbsp;</div>  <div>Best,</div>  <div>Dong</div> </div> &nbsp;  <div> <div>On Mon, Sep 29, 2025 at 4:28=E2=80=AFPM via RT &lt;UMBCHelp@rt.umbc.ed= u&gt; wrote:</div>  <blockquote>Greetings,<br /> <br /> This message has been automatically generated in response to the<br /> creation of a ticket regarding:<br /> <br /> -------------------------------------------------------------------------<b= r /> Subject: &quot;Migrating Research Storage Volume to Ceph Cluster - pi_dli&q= uot;<br /> <br /> Message:<br /> <br /> Hello Dong,<br /> <br /> As per the communication via myUMBC earlier this summer<br /> (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process = of<br /> migrating data off of an older storage server to our new RRStor Ceph storag= e<br /> cluster. Your group is using 0GB of a 250GB quota on the old storage server= .<br /> <br /> To perform these migrations, we need to take individual storage volumes off= line<br /> while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule<br /> a date where we can migrate your volume located at &ldquo;/umbc/rs/dli&rdqu= o;. During the<br /> migration, we will take your volume offline and will terminate any jobs run= ning<br /> on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - ple= ase let us<br /> know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hour= s,<br /> which can be done by responding to this email with your preferred date(s) t= o<br /> perform the migration. During this time, DoIT staff will work to migrate yo= ur<br /> volume to the Ceph storage cluster. DoIT staff will send an email alert on = this<br /> email thread when the migration has begun and when it has completed. For mo= st<br /> storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT st= aff will<br /> assign a day over the following month (October 16th through November 15th) = to<br /> migrate your volume. The day chosen will be random and will occur during<br=  /> business hours. You will be notified of the date chosen to perform the<br /> migration, and will be notified when the migration begins and completes.<br=  /> <br /> Note: After this process has completed, the new storage volume will have a = new<br /> name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ld= quo;/umbc/rs/pi_doit&rdquo;,<br /> or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/= rs/pi_dli&rdquo;.<br /> <br /> Thank you,<br /> Greg<br /> <br /> --<br /> <br /> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br /> - DoIT<br /> <br /> <br /> -------------------------------------------------------------------------<b= r /> <br /> There is no need to reply to this message right now.&nbsp;<br /> <br /> Your ticket has been assigned an ID of [Research Computing #3283030] or you=  can go there directly by clicking the link below.<br /> <br /> Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3283030 &gt;<b= r /> <br /> You can login to view your open tickets at any time by visiting http://my.u= mbc.edu and clicking on &quot;Help&quot; and &quot;Request Help&quot;.<br /> <br /> Alternately you can click on http://my.umbc.edu/help<br /> <br /> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp= ; &nbsp; Thank you<br /> &nbsp;</blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --  <div> <div> <div>Dong Li, Ph.D.</div>  <div> <div>Assistant Professor</div>  <div>Department of Computer Science and Electrical Engineering</div>  <div>University of Maryland Baltimore County</div> </div>  <div>Homepage:&nbsp;https://leetton.github.io/</div>  <div>Schedule a meeting: Google Calendar</div>  <div>&nbsp;</div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> "
3283030,72346415,Correspond,DoIT-Research-Computing,2025-10-17 14:14:08.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dli,resolved,Beamlak Bekele,bbekele1,Dong Li,dli,dli@umbc.edu,Beamlak Bekele,bbekele1@umbc.edu,"<div> <p>Hello&nbsp;<br /> <br /> We have finished migrating your volume to the Ceph cluster! As far as we ca= n tell everything seems to have gone smoothly. There are a few things to no= te:</p>  <ul> 	<li> 	<p>The path has changed, and is now available under /umbc/rs/<strong>pi_dl= i</strong>.</p> 	</li> 	<li> 	<p>The alias used to reach the volume is now <strong>pi_dli_common</strong= > and <strong>pi_dli_user</strong>.</p> 	</li> 	<li> 	<p>Your new volume has a quota of 25<strong>TB</strong>.</p> 	</li> </ul>  <p><br /> When you have a chance, please&nbsp; try running some jobs on Chip using th= e new volume to verify everything looks good.<br /> <br /> Thank you,</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 09:43:32 2025, PG41777 wrote:</p>  <blockquote> <div> <p>Hello&nbsp;<br /> This is just a reminder that we are starting your migration today. During t= his time, please ensure there are not any jobs being run in your research g= roup, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> &nbsp;</p>  <p>&nbsp;</p>  <p>On Thu Oct 02 10:17:53 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Good morning Dong,<br /> <br /> Sounds good, I&#39;ve put your group&#39;s migration on our schedule for Oc= tober 17th. We will send you a notification via this RT ticket when we begi= n the migration, and again once the migration has completed.<br /> <br /> Please let me know if you have any questions or concerns.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Thu Oct 02 09:34:10 2025, RK43819 wrote:</p>  <blockquote> <div>Hi Greg, <div>&nbsp;</div>  <div>Thank you for reaching out. Any day before October 30th works for me f= or migrations.</div>  <div>&nbsp;</div>  <div>Best,</div>  <div>Dong</div> </div> &nbsp;  <div> <div>On Mon, Sep 29, 2025 at 4:28=E2=80=AFPM via RT &lt;UMBCHelp@rt.umbc.ed= u&gt; wrote:</div>  <blockquote>Greetings,<br /> <br /> This message has been automatically generated in response to the<br /> creation of a ticket regarding:<br /> <br /> -------------------------------------------------------------------------<b= r /> Subject: &quot;Migrating Research Storage Volume to Ceph Cluster - pi_dli&q= uot;<br /> <br /> Message:<br /> <br /> Hello Dong,<br /> <br /> As per the communication via myUMBC earlier this summer<br /> (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process = of<br /> migrating data off of an older storage server to our new RRStor Ceph storag= e<br /> cluster. Your group is using 0GB of a 250GB quota on the old storage server= .<br /> <br /> To perform these migrations, we need to take individual storage volumes off= line<br /> while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule<br /> a date where we can migrate your volume located at &ldquo;/umbc/rs/dli&rdqu= o;. During the<br /> migration, we will take your volume offline and will terminate any jobs run= ning<br /> on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - ple= ase let us<br /> know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hour= s,<br /> which can be done by responding to this email with your preferred date(s) t= o<br /> perform the migration. During this time, DoIT staff will work to migrate yo= ur<br /> volume to the Ceph storage cluster. DoIT staff will send an email alert on = this<br /> email thread when the migration has begun and when it has completed. For mo= st<br /> storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT st= aff will<br /> assign a day over the following month (October 16th through November 15th) = to<br /> migrate your volume. The day chosen will be random and will occur during<br=  /> business hours. You will be notified of the date chosen to perform the<br /> migration, and will be notified when the migration begins and completes.<br=  /> <br /> Note: After this process has completed, the new storage volume will have a = new<br /> name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ld= quo;/umbc/rs/pi_doit&rdquo;,<br /> or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/= rs/pi_dli&rdquo;.<br /> <br /> Thank you,<br /> Greg<br /> <br /> --<br /> <br /> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br /> - DoIT<br /> <br /> <br /> -------------------------------------------------------------------------<b= r /> <br /> There is no need to reply to this message right now.&nbsp;<br /> <br /> Your ticket has been assigned an ID of [Research Computing #3283030] or you=  can go there directly by clicking the link below.<br /> <br /> Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3283030 &gt;<b= r /> <br /> You can login to view your open tickets at any time by visiting http://my.u= mbc.edu and clicking on &quot;Help&quot; and &quot;Request Help&quot;.<br /> <br /> Alternately you can click on http://my.umbc.edu/help<br /> <br /> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp= ; &nbsp; Thank you<br /> &nbsp;</blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --  <div> <div> <div>Dong Li, Ph.D.</div>  <div> <div>Assistant Professor</div>  <div>Department of Computer Science and Electrical Engineering</div>  <div>University of Maryland Baltimore County</div> </div>  <div>Homepage:&nbsp;https://leetton.github.io/</div>  <div>Schedule a meeting: Google Calendar</div>  <div>&nbsp;</div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> "
3283033,72004646,Create,DoIT-Research-Computing,2025-09-29 20:30:43.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dfrey1,stalled,Greg Ballantine,gballan1,Douglas Frey,dfrey1,dfrey1@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<p>Hello Douglas,<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/dfrey1&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_dfrey1&rdquo;.<br /> <br /> Thank you,<br /> Greg</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283033,72205004,Correspond,DoIT-Research-Computing,2025-10-10 14:32:03.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dfrey1,stalled,Greg Ballantine,gballan1,Douglas Frey,dfrey1,dfrey1@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hello Douglas,</p>  <p>This is a reminder email, in case you missed the first. We need a response by October 15th, or we&#39;ll be forced to go with Option 2, randomly scheduling a time.<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/dfrey1&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_dfrey1&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3283033,72351814,Correspond,DoIT-Research-Computing,2025-10-17 16:50:43.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dfrey1,stalled,Greg Ballantine,gballan1,Douglas Frey,dfrey1,dfrey1@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Douglas,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 31st for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3283033,72621349,Correspond,DoIT-Research-Computing,2025-10-31 14:42:33.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dfrey1,stalled,Greg Ballantine,gballan1,Douglas Frey,dfrey1,dfrey1@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Good morning Douglas,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 12:50:43 2025, IO12693 wrote:</p>  <blockquote> <p>Dear Douglas,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 31st for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283033,72639739,Correspond,DoIT-Research-Computing,2025-10-31 18:20:05.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_dfrey1,stalled,Greg Ballantine,gballan1,Douglas Frey,dfrey1,dfrey1@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Douglas,<br /> <br /> We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note:</p>  <ul> 	<li>The path has changed, and is now available under <strong>/umbc/rs/pi_dfrey1</strong>.</li> 	<li>The alias used to reach the volume is now <strong>pi_dfrey1_common</strong> and <strong>pi_dfrey1_user</strong>.</li> 	<li>Your new volume has a quota of <strong>25TB</strong>.</li> </ul>  <p>When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good?<br /> <br /> Thank you,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 31 10:42:33 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Good morning Douglas,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 12:50:43 2025, IO12693 wrote:</p>  <blockquote> <p>Dear Douglas,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 31st for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283034,72004748,Create,DoIT-Research-Computing,2025-09-29 20:34:16.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_estokan,resolved,Beamlak Bekele,bbekele1,Eric Stokan,estokan,estokan@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<p>Hello Eric,<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 250GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/estokan&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_estokan&rdquo;.<br /> <br /> Thank you,<br /> Greg</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283034,72205228,Correspond,DoIT-Research-Computing,2025-10-10 14:36:16.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_estokan,resolved,Beamlak Bekele,bbekele1,Eric Stokan,estokan,estokan@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hello Eric,</p>  <p>This is a reminder email, in case you missed the first. We need a response by October 15th, or we&#39;ll be forced to go with Option 2, randomly scheduling a time.<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 250GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/estokan&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_estokan&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3283034,72205476,Correspond,DoIT-Research-Computing,2025-10-10 14:40:46.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_estokan,resolved,Beamlak Bekele,bbekele1,Eric Stokan,estokan,estokan@umbc.edu,Eric Stokan,estokan@umbc.edu,"Thank you Elliot,  I am fine with it whenever it gets migrated, as I have yet to store data on the server.  Thanks,  Eric  On Fri, Oct 10, 2025 at 10:36=E2=80=AFAM Elliot Gobbert via RT <UMBCHelp@rt= .umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3283034 > > > Last Update From Ticket: > > Hello Eric, > > This is a reminder email, in case you missed the first. We need a response > by > October 15th, or we'll be forced to go with Option 2, randomly scheduling=  a > time. > > As per the communication via myUMBC earlier this summer > (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the > process of > migrating data off of an older storage server to our new RRStor Ceph > storage > cluster. Your group is using 0GB of a 250GB quota on the old storage > server. > > To perform these migrations, we need to take individual storage volumes > offline > while we migrate them to the Ceph cluster. Thus we are reaching out to > schedule > a date where we can migrate your volume located at =E2=80=9C/umbc/rs/esto= kan=E2=80=9D. > During > the migration, we will take your volume offline and will terminate any jo= bs > running on the chip compute cluster that are accessing this volume. > > Below we=E2=80=99ve listed two options for handling this data migration -=  please > let us > know which of these you=E2=80=99d prefer. > > Option 1: Schedule a group-wide downtime date during standard business > hours, > which can be done by responding to this email with your preferred date(s) > to > perform the migration. During this time, DoIT staff will work to migrate > your > volume to the Ceph storage cluster. DoIT staff will send an email alert on > this > email thread when the migration has begun and when it has completed. For > most > storage volumes, this process should take less than a business day. > Option 2: If you don=E2=80=99t respond to this email by October 15th, DoI= T staff > will > assign a day over the following month (October 16th through November 15th) > to > migrate your volume. The day chosen will be random and will occur during > business hours. You will be notified of the date chosen to perform the > migration, and will be notified when the migration begins and completes. > > Note: After this process has completed, the new storage volume will have a > new > name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its data und= er > =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, > or in your group=E2=80=99s case you will find your volume under > =E2=80=9C/umbc/rs/pi_estokan=E2=80=9D. > > Thank you, > Elliot > >  --=20 *Eric J. Stokan, Ph.D.* Associate Professor | Department of Political Science Director | Center for Social Science Scholarship <https://socialscience.umbc.edu/> USM Wilson H. Elkins Professor (2025-2026) Affiliate Faculty | School of Public Policy University of Maryland, Baltimore County (UMBC) Co-Director | MGMT Lab <https://mgmt.lab.indiana.edu/> | Indiana University Bloomington Editorial Board | Urban Affairs Review <https://journals.sagepub.com/editorial-board/UAR> and State and Local Government Review <https://journals.sagepub.com/editorial-board/SLG> 586-202-7540 estokan@umbc.edu www.ericjstokan.com Pronouns: He/Him/His "
3283034,72205476,Correspond,DoIT-Research-Computing,2025-10-10 14:40:46.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_estokan,resolved,Beamlak Bekele,bbekele1,Eric Stokan,estokan,estokan@umbc.edu,Eric Stokan,estokan@umbc.edu,"<div dir=3D""ltr""><div class=3D""gmail_default"" style=3D""font-family:garamond= ,times new roman,serif;font-size:large"">Thank you Elliot,</div><div class= =3D""gmail_default"" style=3D""font-family:garamond,times new roman,serif;font= -size:large""><br>I am fine with it whenever it gets migrated, as I have yet=  to store data on the server.</div><div class=3D""gmail_default"" style=3D""fo= nt-family:garamond,times new roman,serif;font-size:large""><br>Thanks,</div>= <div class=3D""gmail_default"" style=3D""font-family:garamond,times new roman,= serif;font-size:large""><br>Eric</div></div><br><div class=3D""gmail_quote gm= ail_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Fri, Oct 10, = 2025 at 10:36=E2=80=AFAM Elliot Gobbert via RT &lt;<a href=3D""mailto:UMBCHe= lp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote cl= ass=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid=  rgb(204,204,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.u= mbc.edu/Ticket/Display.html?id=3D3283034"" rel=3D""noreferrer"" target=3D""_bla= nk"">https://rt.umbc.edu/Ticket/Display.html?id=3D3283034</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hello Eric,<br> <br> This is a reminder email, in case you missed the first. We need a response = by<br> October 15th, or we&#39;ll be forced to go with Option 2, randomly scheduli= ng a<br> time.<br> <br> As per the communication via myUMBC earlier this summer<br> (<a href=3D""https://my3.my.umbc.edu/groups/hpcf/posts/150838"" rel=3D""norefe= rrer"" target=3D""_blank"">https://my3.my.umbc.edu/groups/hpcf/posts/150838</a= >), DoIT is in the process of<br> migrating data off of an older storage server to our new RRStor Ceph storag= e<br> cluster. Your group is using 0GB of a 250GB quota on the old storage server= .<br> <br> To perform these migrations, we need to take individual storage volumes off= line<br> while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule<br> a date where we can migrate your volume located at =E2=80=9C/umbc/rs/estoka= n=E2=80=9D. During<br> the migration, we will take your volume offline and will terminate any jobs= <br> running on the chip compute cluster that are accessing this volume.<br> <br> Below we=E2=80=99ve listed two options for handling this data migration - p= lease let us<br> know which of these you=E2=80=99d prefer.<br> <br> Option 1: Schedule a group-wide downtime date during standard business hour= s,<br> which can be done by responding to this email with your preferred date(s) t= o<br> perform the migration. During this time, DoIT staff will work to migrate yo= ur<br> volume to the Ceph storage cluster. DoIT staff will send an email alert on = this<br> email thread when the migration has begun and when it has completed. For mo= st<br> storage volumes, this process should take less than a business day.<br> Option 2: If you don=E2=80=99t respond to this email by October 15th, DoIT = staff will<br> assign a day over the following month (October 16th through November 15th) = to<br> migrate your volume. The day chosen will be random and will occur during<br> business hours. You will be notified of the date chosen to perform the<br> migration, and will be notified when the migration begins and completes.<br> <br> Note: After this process has completed, the new storage volume will have a = new<br> name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its data under=  =E2=80=9C/umbc/rs/pi_doit=E2=80=9D,<br> or in your group=E2=80=99s case you will find your volume under =E2=80=9C/u= mbc/rs/pi_estokan=E2=80=9D.<br> <br> Thank you,<br> Elliot<br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><b>Eric J. Stokan, Ph.D.</b><div><div style=3D""c= olor:rgb(34,34,34)"">Associate Professor | Department of Political Science</= div><div style=3D""color:rgb(34,34,34)"">Director |=C2=A0<a href=3D""https://s= ocialscience.umbc.edu/"" style=3D""color:rgb(17,85,204)"" target=3D""_blank""><f= ont color=3D""#000000"">Center for Social Science Scholarship</font></a></div= ><div style=3D""color:rgb(34,34,34)""><div>USM Wilson H. Elkins Professor (20= 25-2026)=C2=A0</div></div><div style=3D""color:rgb(34,34,34)"">Affiliate Facu= lty | School of Public Policy</div><div style=3D""color:rgb(34,34,34)"">Unive= rsity of Maryland, Baltimore County (UMBC)</div><div style=3D""color:rgb(34,= 34,34)""><a href=3D""https://mgmt.lab.indiana.edu/"" style=3D""color:rgb(17,85,= 204)"" target=3D""_blank""><font color=3D""#000000"">Co-Director</font><font col= or=3D""#0000ff"">=C2=A0</font><font color=3D""#000000"">|</font><font color=3D""= #0000ff"">=C2=A0</font><font color=3D""#000000"">MGMT Lab</font></a><font colo= r=3D""#0000ff"">=C2=A0</font><font color=3D""#000000"">|=C2=A0</font><font colo= r=3D""#000000"">Indiana University Bloomington</font></div><div style=3D""colo= r:rgb(34,34,34)""><font color=3D""#222222"">Editorial Board |=C2=A0</font><a h= ref=3D""https://journals.sagepub.com/editorial-board/UAR"" style=3D""color:rgb= (17,85,204)"" target=3D""_blank""><font color=3D""#000000"">Urban Affairs Review= </font></a><font color=3D""#222222"">=C2=A0and </font><a href=3D""https://jour= nals.sagepub.com/editorial-board/SLG"" target=3D""_blank""><font color=3D""#000= 000"">State and Local Government Review</font></a></div></div><div>586-202-7= 540<br></div><div><a href=3D""mailto:estokan@umbc.edu"" target=3D""_blank"">est= okan@umbc.edu</a></div><div><a href=3D""http://www.ericjstokan.com"" target= =3D""_blank"">www.ericjstokan.com</a></div><div>Pronouns: He/Him/His</div><di= v><br></div><div><img width=3D""200"" height=3D""70"" src=3D""https://ci3.google= usercontent.com/mail-sig/AIorK4y-XuOxCGAC-WnSnUm_GF28y5ykmqOJTt2gjpGxi8ZEHh= MMHMH7OmnZUxo0JdG4De_Tb_SS1GYvrt-S""><br></div><div><br></div></div></div> "
3283034,72211451,Correspond,DoIT-Research-Computing,2025-10-10 17:38:02.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_estokan,resolved,Beamlak Bekele,bbekele1,Eric Stokan,estokan,estokan@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Eric,<br /> <br /> Sounds good, I&#39;ve put your group on our schedule for Wednesday October = 22nd. We will send you an email alert via this RT ticket when we begin the = migration process, and again once it has completed.<br /> <br /> Please let us know if you have any questions or concerns.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 10 10:40:46 2025, RT46171 wrote:</p>  <blockquote> <div> <div>Thank you Elliot,</div>  <div><br /> I am fine with it whenever it gets migrated, as I have yet to store data on=  the server.</div>  <div><br /> Thanks,</div>  <div><br /> Eric</div> </div> &nbsp;  <div> <div>On Fri, Oct 10, 2025 at 10:36=E2=80=AFAM Elliot Gobbert via RT &lt;UMB= CHelp@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 83034 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hello Eric,<br /> <br /> This is a reminder email, in case you missed the first. We need a response = by<br /> October 15th, or we&#39;ll be forced to go with Option 2, randomly scheduli= ng a<br /> time.<br /> <br /> As per the communication via myUMBC earlier this summer<br /> (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process = of<br /> migrating data off of an older storage server to our new RRStor Ceph storag= e<br /> cluster. Your group is using 0GB of a 250GB quota on the old storage server= .<br /> <br /> To perform these migrations, we need to take individual storage volumes off= line<br /> while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule<br /> a date where we can migrate your volume located at &ldquo;/umbc/rs/estokan&= rdquo;. During<br /> the migration, we will take your volume offline and will terminate any jobs= <br /> running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - ple= ase let us<br /> know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hour= s,<br /> which can be done by responding to this email with your preferred date(s) t= o<br /> perform the migration. During this time, DoIT staff will work to migrate yo= ur<br /> volume to the Ceph storage cluster. DoIT staff will send an email alert on = this<br /> email thread when the migration has begun and when it has completed. For mo= st<br /> storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT st= aff will<br /> assign a day over the following month (October 16th through November 15th) = to<br /> migrate your volume. The day chosen will be random and will occur during<br=  /> business hours. You will be notified of the date chosen to perform the<br /> migration, and will be notified when the migration begins and completes.<br=  /> <br /> Note: After this process has completed, the new storage volume will have a = new<br /> name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ld= quo;/umbc/rs/pi_doit&rdquo;,<br /> or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/= rs/pi_estokan&rdquo;.<br /> <br /> Thank you,<br /> Elliot<br /> &nbsp;</blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --  <div> <div><strong>Eric J. Stokan, Ph.D.</strong>  <div> <div>Associate Professor | Department of Political Science</div>  <div>Director |&nbsp;<span style=3D""color:#000000"">Center for Social Scienc= e Scholarship</span></div>  <div> <div>USM Wilson H. Elkins Professor (2025-2026)&nbsp;</div> </div>  <div>Affiliate Faculty | School of Public Policy</div>  <div>University of Maryland, Baltimore County (UMBC)</div>  <div><span style=3D""color:#000000"">Co-Director</span><span style=3D""color:#= 0000ff"">&nbsp;</span><span style=3D""color:#000000"">|</span><span style=3D""c= olor:#0000ff"">&nbsp;</span><span style=3D""color:#000000"">MGMT Lab</span><sp= an style=3D""color:#0000ff"">&nbsp;</span><span style=3D""color:#000000"">|&nbs= p;</span><span style=3D""color:#000000"">Indiana University Bloomington</span= ></div>  <div><span style=3D""color:#222222"">Editorial Board |&nbsp;</span><span styl= e=3D""color:#000000"">Urban Affairs Review</span><span style=3D""color:#222222= "">&nbsp;and </span><span style=3D""color:#000000"">State and Local Government=  Review</span></div> </div>  <div>586-202-7540</div>  <div>estokan@umbc.edu</div>  <div>www.ericjstokan.com</div>  <div>Pronouns: He/Him/His</div>  <div>&nbsp;</div>  <div>&nbsp;</div>  <div>&nbsp;</div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283034,72211792,Correspond,DoIT-Research-Computing,2025-10-10 17:48:10.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_estokan,resolved,Beamlak Bekele,bbekele1,Eric Stokan,estokan,estokan@umbc.edu,Eric Stokan,estokan@umbc.edu,"Great, thank you!  Eric  On Fri, Oct 10, 2025 at 1:38=E2=80=AFPM Greg Ballantine via RT <UMBCHelp@rt= .umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3283034 > > > Last Update From Ticket: > > Hello Eric, > > Sounds good, I've put your group on our schedule for Wednesday October > 22nd. We > will send you an email alert via this RT ticket when we begin the migrati= on > process, and again once it has completed. > > Please let us know if you have any questions or concerns. > > Best, > Greg > > On Fri Oct 10 10:40:46 2025, RT46171 wrote: > > > Thank you Elliot, > > I am fine with it whenever it gets migrated, as I have yet to store data > on > > the server. > > Thanks, > > Eric On Fri, Oct 10, 2025 at 10:36 AM Elliot Gobbert via RT > > <UMBCHelp@rt.umbc.edu> wrote: > > >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3283034 > > > >> Last Update From Ticket: > > >> Hello Eric, > > >> This is a reminder email, in case you missed the first. We need a > >> response by > >> October 15th, or we'll be forced to go with Option 2, randomly > >> scheduling a > >> time. > > >> As per the communication via myUMBC earlier this summer > >> (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the > >> process of > >> migrating data off of an older storage server to our new RRStor Ceph > >> storage > >> cluster. Your group is using 0GB of a 250GB quota on the old storage > >> server. > > >> To perform these migrations, we need to take individual storage volumes > >> offline > >> while we migrate them to the Ceph cluster. Thus we are reaching out to > >> schedule > >> a date where we can migrate your volume located at =E2=80=9C/umbc/rs/e= stokan=E2=80=9D. > >> During > >> the migration, we will take your volume offline and will terminate any > >> jobs > >> running on the chip compute cluster that are accessing this volume. > > >> Below we=E2=80=99ve listed two options for handling this data migratio= n - > >> please let us > >> know which of these you=E2=80=99d prefer. > > >> Option 1: Schedule a group-wide downtime date during standard business > >> hours, > >> which can be done by responding to this email with your preferred > >> date(s) to > >> perform the migration. During this time, DoIT staff will work to > >> migrate your > >> volume to the Ceph storage cluster. DoIT staff will send an email alert > >> on this > >> email thread when the migration has begun and when it has completed. > >> For most > >> storage volumes, this process should take less than a business day. > >> Option 2: If you don=E2=80=99t respond to this email by October 15th, = DoIT > >> staff will > >> assign a day over the following month (October 16th through November > >> 15th) to > >> migrate your volume. The day chosen will be random and will occur > >> during > >> business hours. You will be notified of the date chosen to perform the > >> migration, and will be notified when the migration begins and > >> completes. > > >> Note: After this process has completed, the new storage volume will > >> have a new > >> name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its data = under > >> =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, > >> or in your group=E2=80=99s case you will find your volume under > >> =E2=80=9C/umbc/rs/pi_estokan=E2=80=9D. > > >> Thank you, > >> Elliot > > > -- Eric J. Stokan, Ph.D.Associate Professor | Department of Political > > ScienceDirector | Center for Social Science ScholarshipUSM Wilson H. > Elkins > > Professor (2025-2026) Affiliate Faculty | School of Public > PolicyUniversity > > of Maryland, Baltimore County (UMBC)Co-Director | MGMT Lab | Indiana > > University BloomingtonEditorial Board | Urban Affairs Review and State > and > > Local Government > Review586-202-7540estokan@umbc.eduwww.ericjstokan.comPronouns: > > He/Him/His > > -- > > Gregory BallantineSystem Administrator for Research and Enterprise > ComputingUMBC > - DoIT > >  --=20 *Eric J. Stokan, Ph.D.* Associate Professor | Department of Political Science Director | Center for Social Science Scholarship <https://socialscience.umbc.edu/> USM Wilson H. Elkins Professor (2025-2026) Affiliate Faculty | School of Public Policy University of Maryland, Baltimore County (UMBC) Co-Director | MGMT Lab <https://mgmt.lab.indiana.edu/> | Indiana University Bloomington Editorial Board | Urban Affairs Review <https://journals.sagepub.com/editorial-board/UAR> and State and Local Government Review <https://journals.sagepub.com/editorial-board/SLG> 586-202-7540 estokan@umbc.edu www.ericjstokan.com Pronouns: He/Him/His "
3283034,72211792,Correspond,DoIT-Research-Computing,2025-10-10 17:48:10.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_estokan,resolved,Beamlak Bekele,bbekele1,Eric Stokan,estokan,estokan@umbc.edu,Eric Stokan,estokan@umbc.edu,"<div dir=3D""ltr""><div class=3D""gmail_default"" style=3D""font-family:garamond= ,times new roman,serif;font-size:large"">Great, thank you!</div><div class= =3D""gmail_default"" style=3D""font-family:garamond,times new roman,serif;font= -size:large""><br>Eric</div></div><br><div class=3D""gmail_quote gmail_quote_= container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Fri, Oct 10, 2025 at 1:= 38=E2=80=AFPM Greg Ballantine via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc= .edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gma= il_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,2= 04,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ti= cket/Display.html?id=3D3283034"" rel=3D""noreferrer"" target=3D""_blank"">https:= //rt.umbc.edu/Ticket/Display.html?id=3D3283034</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hello Eric,<br> <br> Sounds good, I&#39;ve put your group on our schedule for Wednesday October = 22nd. We<br> will send you an email alert via this RT ticket when we begin the migration= <br> process, and again once it has completed.<br> <br> Please let us know if you have any questions or concerns.<br> <br> Best,<br> Greg<br> <br> On Fri Oct 10 10:40:46 2025, RT46171 wrote:<br> <br> &gt; Thank you Elliot,<br> &gt; I am fine with it whenever it gets migrated, as I have yet to store da= ta on<br> &gt; the server.<br> &gt; Thanks,<br> &gt; Eric On Fri, Oct 10, 2025 at 10:36 AM Elliot Gobbert via RT<br> &gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp= @rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html= ?id=3D3283034"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tic= ket/Display.html?id=3D3283034</a> &gt;<br> <br> &gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt; Hello Eric,<br> <br> &gt;&gt; This is a reminder email, in case you missed the first. We need a<= br> &gt;&gt; response by<br> &gt;&gt; October 15th, or we&#39;ll be forced to go with Option 2, randomly= <br> &gt;&gt; scheduling a<br> &gt;&gt; time.<br> <br> &gt;&gt; As per the communication via myUMBC earlier this summer<br> &gt;&gt; (<a href=3D""https://my3.my.umbc.edu/groups/hpcf/posts/150838"" rel= =3D""noreferrer"" target=3D""_blank"">https://my3.my.umbc.edu/groups/hpcf/posts= /150838</a>), DoIT is in the<br> &gt;&gt; process of<br> &gt;&gt; migrating data off of an older storage server to our new RRStor Ce= ph<br> &gt;&gt; storage<br> &gt;&gt; cluster. Your group is using 0GB of a 250GB quota on the old stora= ge<br> &gt;&gt; server.<br> <br> &gt;&gt; To perform these migrations, we need to take individual storage vo= lumes<br> &gt;&gt; offline<br> &gt;&gt; while we migrate them to the Ceph cluster. Thus we are reaching ou= t to<br> &gt;&gt; schedule<br> &gt;&gt; a date where we can migrate your volume located at =E2=80=9C/umbc/= rs/estokan=E2=80=9D.<br> &gt;&gt; During<br> &gt;&gt; the migration, we will take your volume offline and will terminate=  any<br> &gt;&gt; jobs<br> &gt;&gt; running on the chip compute cluster that are accessing this volume= .<br> <br> &gt;&gt; Below we=E2=80=99ve listed two options for handling this data migr= ation -<br> &gt;&gt; please let us<br> &gt;&gt; know which of these you=E2=80=99d prefer.<br> <br> &gt;&gt; Option 1: Schedule a group-wide downtime date during standard busi= ness<br> &gt;&gt; hours,<br> &gt;&gt; which can be done by responding to this email with your preferred<= br> &gt;&gt; date(s) to<br> &gt;&gt; perform the migration. During this time, DoIT staff will work to<b= r> &gt;&gt; migrate your<br> &gt;&gt; volume to the Ceph storage cluster. DoIT staff will send an email = alert<br> &gt;&gt; on this<br> &gt;&gt; email thread when the migration has begun and when it has complete= d.<br> &gt;&gt; For most<br> &gt;&gt; storage volumes, this process should take less than a business day= .<br> &gt;&gt; Option 2: If you don=E2=80=99t respond to this email by October 15= th, DoIT<br> &gt;&gt; staff will<br> &gt;&gt; assign a day over the following month (October 16th through Novemb= er<br> &gt;&gt; 15th) to<br> &gt;&gt; migrate your volume. The day chosen will be random and will occur<= br> &gt;&gt; during<br> &gt;&gt; business hours. You will be notified of the date chosen to perform=  the<br> &gt;&gt; migration, and will be notified when the migration begins and<br> &gt;&gt; completes.<br> <br> &gt;&gt; Note: After this process has completed, the new storage volume wil= l<br> &gt;&gt; have a new<br> &gt;&gt; name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its d= ata under<br> &gt;&gt; =E2=80=9C/umbc/rs/pi_doit=E2=80=9D,<br> &gt;&gt; or in your group=E2=80=99s case you will find your volume under<br> &gt;&gt; =E2=80=9C/umbc/rs/pi_estokan=E2=80=9D.<br> <br> &gt;&gt; Thank you,<br> &gt;&gt; Elliot<br> <br> &gt; -- Eric J. Stokan, Ph.D.Associate Professor | Department of Political<= br> &gt; ScienceDirector | Center for Social Science ScholarshipUSM Wilson H. E= lkins<br> &gt; Professor (2025-2026) Affiliate Faculty | School of Public PolicyUnive= rsity<br> &gt; of Maryland, Baltimore County (UMBC)Co-Director | MGMT Lab | Indiana<b= r> &gt; University BloomingtonEditorial Board | Urban Affairs Review and State=  and<br> &gt; Local Government Review586-202-7540estokan@umbc.eduwww.ericjstokan.com= Pronouns:<br> &gt; He/Him/His<br> <br> --<br> <br> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br> - DoIT<br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><b>Eric J. Stokan, Ph.D.</b><div><div style=3D""c= olor:rgb(34,34,34)"">Associate Professor | Department of Political Science</= div><div style=3D""color:rgb(34,34,34)"">Director |=C2=A0<a href=3D""https://s= ocialscience.umbc.edu/"" style=3D""color:rgb(17,85,204)"" target=3D""_blank""><f= ont color=3D""#000000"">Center for Social Science Scholarship</font></a></div= ><div style=3D""color:rgb(34,34,34)""><div>USM Wilson H. Elkins Professor (20= 25-2026)=C2=A0</div></div><div style=3D""color:rgb(34,34,34)"">Affiliate Facu= lty | School of Public Policy</div><div style=3D""color:rgb(34,34,34)"">Unive= rsity of Maryland, Baltimore County (UMBC)</div><div style=3D""color:rgb(34,= 34,34)""><a href=3D""https://mgmt.lab.indiana.edu/"" style=3D""color:rgb(17,85,= 204)"" target=3D""_blank""><font color=3D""#000000"">Co-Director</font><font col= or=3D""#0000ff"">=C2=A0</font><font color=3D""#000000"">|</font><font color=3D""= #0000ff"">=C2=A0</font><font color=3D""#000000"">MGMT Lab</font></a><font colo= r=3D""#0000ff"">=C2=A0</font><font color=3D""#000000"">|=C2=A0</font><font colo= r=3D""#000000"">Indiana University Bloomington</font></div><div style=3D""colo= r:rgb(34,34,34)""><font color=3D""#222222"">Editorial Board |=C2=A0</font><a h= ref=3D""https://journals.sagepub.com/editorial-board/UAR"" style=3D""color:rgb= (17,85,204)"" target=3D""_blank""><font color=3D""#000000"">Urban Affairs Review= </font></a><font color=3D""#222222"">=C2=A0and </font><a href=3D""https://jour= nals.sagepub.com/editorial-board/SLG"" target=3D""_blank""><font color=3D""#000= 000"">State and Local Government Review</font></a></div></div><div>586-202-7= 540<br></div><div><a href=3D""mailto:estokan@umbc.edu"" target=3D""_blank"">est= okan@umbc.edu</a></div><div><a href=3D""http://www.ericjstokan.com"" target= =3D""_blank"">www.ericjstokan.com</a></div><div>Pronouns: He/Him/His</div><di= v><br></div><div><img width=3D""200"" height=3D""70"" src=3D""https://ci3.google= usercontent.com/mail-sig/AIorK4y-XuOxCGAC-WnSnUm_GF28y5ykmqOJTt2gjpGxi8ZEHh= MMHMH7OmnZUxo0JdG4De_Tb_SS1GYvrt-S""><br></div><div><br></div></div></div> "
3283034,72420936,Correspond,DoIT-Research-Computing,2025-10-22 13:29:50.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_estokan,resolved,Beamlak Bekele,bbekele1,Eric Stokan,estokan,estokan@umbc.edu,Beamlak Bekele,bbekele1@umbc.edu,"<div> <p>Hello&nbsp;<br /> This is just a reminder that we are starting your migration today. During t= his time, please ensure there are not any jobs being run in your research g= roup, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.</p>  <p>On Fri Oct 10 13:48:10 2025, RT46171 wrote:</p>  <blockquote> <div> <div>Great, thank you!</div>  <div><br /> Eric</div> </div> &nbsp;  <div> <div>On Fri, Oct 10, 2025 at 1:38=E2=80=AFPM Greg Ballantine via RT &lt;UMB= CHelp@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 83034 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hello Eric,<br /> <br /> Sounds good, I&#39;ve put your group on our schedule for Wednesday October = 22nd. We<br /> will send you an email alert via this RT ticket when we begin the migration= <br /> process, and again once it has completed.<br /> <br /> Please let us know if you have any questions or concerns.<br /> <br /> Best,<br /> Greg<br /> <br /> On Fri Oct 10 10:40:46 2025, RT46171 wrote:<br /> <br /> &gt; Thank you Elliot,<br /> &gt; I am fine with it whenever it gets migrated, as I have yet to store da= ta on<br /> &gt; the server.<br /> &gt; Thanks,<br /> &gt; Eric On Fri, Oct 10, 2025 at 10:36 AM Elliot Gobbert via RT<br /> &gt; &lt;UMBCHelp@rt.umbc.edu&gt; wrote:<br /> <br /> &gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32830= 34 &gt;<br /> <br /> &gt;&gt; Last Update From Ticket:<br /> <br /> &gt;&gt; Hello Eric,<br /> <br /> &gt;&gt; This is a reminder email, in case you missed the first. We need a<= br /> &gt;&gt; response by<br /> &gt;&gt; October 15th, or we&#39;ll be forced to go with Option 2, randomly= <br /> &gt;&gt; scheduling a<br /> &gt;&gt; time.<br /> <br /> &gt;&gt; As per the communication via myUMBC earlier this summer<br /> &gt;&gt; (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the= <br /> &gt;&gt; process of<br /> &gt;&gt; migrating data off of an older storage server to our new RRStor Ce= ph<br /> &gt;&gt; storage<br /> &gt;&gt; cluster. Your group is using 0GB of a 250GB quota on the old stora= ge<br /> &gt;&gt; server.<br /> <br /> &gt;&gt; To perform these migrations, we need to take individual storage vo= lumes<br /> &gt;&gt; offline<br /> &gt;&gt; while we migrate them to the Ceph cluster. Thus we are reaching ou= t to<br /> &gt;&gt; schedule<br /> &gt;&gt; a date where we can migrate your volume located at &ldquo;/umbc/rs= /estokan&rdquo;.<br /> &gt;&gt; During<br /> &gt;&gt; the migration, we will take your volume offline and will terminate=  any<br /> &gt;&gt; jobs<br /> &gt;&gt; running on the chip compute cluster that are accessing this volume= .<br /> <br /> &gt;&gt; Below we&rsquo;ve listed two options for handling this data migrat= ion -<br /> &gt;&gt; please let us<br /> &gt;&gt; know which of these you&rsquo;d prefer.<br /> <br /> &gt;&gt; Option 1: Schedule a group-wide downtime date during standard busi= ness<br /> &gt;&gt; hours,<br /> &gt;&gt; which can be done by responding to this email with your preferred<= br /> &gt;&gt; date(s) to<br /> &gt;&gt; perform the migration. During this time, DoIT staff will work to<b= r /> &gt;&gt; migrate your<br /> &gt;&gt; volume to the Ceph storage cluster. DoIT staff will send an email = alert<br /> &gt;&gt; on this<br /> &gt;&gt; email thread when the migration has begun and when it has complete= d.<br /> &gt;&gt; For most<br /> &gt;&gt; storage volumes, this process should take less than a business day= .<br /> &gt;&gt; Option 2: If you don&rsquo;t respond to this email by October 15th= , DoIT<br /> &gt;&gt; staff will<br /> &gt;&gt; assign a day over the following month (October 16th through Novemb= er<br /> &gt;&gt; 15th) to<br /> &gt;&gt; migrate your volume. The day chosen will be random and will occur<= br /> &gt;&gt; during<br /> &gt;&gt; business hours. You will be notified of the date chosen to perform=  the<br /> &gt;&gt; migration, and will be notified when the migration begins and<br /> &gt;&gt; completes.<br /> <br /> &gt;&gt; Note: After this process has completed, the new storage volume wil= l<br /> &gt;&gt; have a new<br /> &gt;&gt; name. For example, group &ldquo;pi_doit&rdquo; will find its data = under<br /> &gt;&gt; &ldquo;/umbc/rs/pi_doit&rdquo;,<br /> &gt;&gt; or in your group&rsquo;s case you will find your volume under<br /> &gt;&gt; &ldquo;/umbc/rs/pi_estokan&rdquo;.<br /> <br /> &gt;&gt; Thank you,<br /> &gt;&gt; Elliot<br /> <br /> &gt; -- Eric J. Stokan, Ph.D.Associate Professor | Department of Political<= br /> &gt; ScienceDirector | Center for Social Science ScholarshipUSM Wilson H. E= lkins<br /> &gt; Professor (2025-2026) Affiliate Faculty | School of Public PolicyUnive= rsity<br /> &gt; of Maryland, Baltimore County (UMBC)Co-Director | MGMT Lab | Indiana<b= r /> &gt; University BloomingtonEditorial Board | Urban Affairs Review and State=  and<br /> &gt; Local Government Review586-202-7540estokan@umbc.eduwww.ericjstokan.com= Pronouns:<br /> &gt; He/Him/His<br /> <br /> --<br /> <br /> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br /> - DoIT<br /> &nbsp;</blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --  <div> <div><strong>Eric J. Stokan, Ph.D.</strong>  <div> <div>Associate Professor | Department of Political Science</div>  <div>Director |&nbsp;<span style=3D""color:#000000"">Center for Social Scienc= e Scholarship</span></div>  <div> <div>USM Wilson H. Elkins Professor (2025-2026)&nbsp;</div> </div>  <div>Affiliate Faculty | School of Public Policy</div>  <div>University of Maryland, Baltimore County (UMBC)</div>  <div><span style=3D""color:#000000"">Co-Director</span><span style=3D""color:#= 0000ff"">&nbsp;</span><span style=3D""color:#000000"">|</span><span style=3D""c= olor:#0000ff"">&nbsp;</span><span style=3D""color:#000000"">MGMT Lab</span><sp= an style=3D""color:#0000ff"">&nbsp;</span><span style=3D""color:#000000"">|&nbs= p;</span><span style=3D""color:#000000"">Indiana University Bloomington</span= ></div>  <div><span style=3D""color:#222222"">Editorial Board |&nbsp;</span><span styl= e=3D""color:#000000"">Urban Affairs Review</span><span style=3D""color:#222222= "">&nbsp;and </span><span style=3D""color:#000000"">State and Local Government=  Review</span></div> </div>  <div>586-202-7540</div>  <div>estokan@umbc.edu</div>  <div>www.ericjstokan.com</div>  <div>Pronouns: He/Him/His</div>  <div>&nbsp;</div>  <div>&nbsp;</div>  <div>&nbsp;</div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> "
3283034,72434819,Correspond,DoIT-Research-Computing,2025-10-22 18:02:33.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_estokan,resolved,Beamlak Bekele,bbekele1,Eric Stokan,estokan,estokan@umbc.edu,Beamlak Bekele,bbekele1@umbc.edu,"<div> <p>Hello</p>  <p>We have finished migrating your volume to the Ceph cluster! As far as we=  can tell everything seems to have gone smoothly. There are a few things to=  note:</p>  <p>The path has changed, and is now available under /umbc/rs/pi_estokan</p>  <p>The alias used to reach the volume is now pi_estokan_common and pi_estok= an _user.</p>  <p>Your new volume has a quota of <strong>10TB</strong>.</p>  <p>When you have a chance, please try running some jobs on Chip using the n= ew volume to verify everything looks good.</p>  <p>Thank you</p>  <p>&nbsp;</p>  <p>On Wed Oct 22 09:29:50 2025, PG41777 wrote:</p>  <blockquote> <div> <p>Hello&nbsp;<br /> This is just a reminder that we are starting your migration today. During t= his time, please ensure there are not any jobs being run in your research g= roup, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.</p>  <p>On Fri Oct 10 13:48:10 2025, RT46171 wrote:</p>  <blockquote> <div> <div>Great, thank you!</div>  <div><br /> Eric</div> </div> &nbsp;  <div> <div>On Fri, Oct 10, 2025 at 1:38=E2=80=AFPM Greg Ballantine via RT &lt;UMB= CHelp@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 83034 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hello Eric,<br /> <br /> Sounds good, I&#39;ve put your group on our schedule for Wednesday October = 22nd. We<br /> will send you an email alert via this RT ticket when we begin the migration= <br /> process, and again once it has completed.<br /> <br /> Please let us know if you have any questions or concerns.<br /> <br /> Best,<br /> Greg<br /> <br /> On Fri Oct 10 10:40:46 2025, RT46171 wrote:<br /> <br /> &gt; Thank you Elliot,<br /> &gt; I am fine with it whenever it gets migrated, as I have yet to store da= ta on<br /> &gt; the server.<br /> &gt; Thanks,<br /> &gt; Eric On Fri, Oct 10, 2025 at 10:36 AM Elliot Gobbert via RT<br /> &gt; &lt;UMBCHelp@rt.umbc.edu&gt; wrote:<br /> <br /> &gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32830= 34 &gt;<br /> <br /> &gt;&gt; Last Update From Ticket:<br /> <br /> &gt;&gt; Hello Eric,<br /> <br /> &gt;&gt; This is a reminder email, in case you missed the first. We need a<= br /> &gt;&gt; response by<br /> &gt;&gt; October 15th, or we&#39;ll be forced to go with Option 2, randomly= <br /> &gt;&gt; scheduling a<br /> &gt;&gt; time.<br /> <br /> &gt;&gt; As per the communication via myUMBC earlier this summer<br /> &gt;&gt; (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the= <br /> &gt;&gt; process of<br /> &gt;&gt; migrating data off of an older storage server to our new RRStor Ce= ph<br /> &gt;&gt; storage<br /> &gt;&gt; cluster. Your group is using 0GB of a 250GB quota on the old stora= ge<br /> &gt;&gt; server.<br /> <br /> &gt;&gt; To perform these migrations, we need to take individual storage vo= lumes<br /> &gt;&gt; offline<br /> &gt;&gt; while we migrate them to the Ceph cluster. Thus we are reaching ou= t to<br /> &gt;&gt; schedule<br /> &gt;&gt; a date where we can migrate your volume located at &ldquo;/umbc/rs= /estokan&rdquo;.<br /> &gt;&gt; During<br /> &gt;&gt; the migration, we will take your volume offline and will terminate=  any<br /> &gt;&gt; jobs<br /> &gt;&gt; running on the chip compute cluster that are accessing this volume= .<br /> <br /> &gt;&gt; Below we&rsquo;ve listed two options for handling this data migrat= ion -<br /> &gt;&gt; please let us<br /> &gt;&gt; know which of these you&rsquo;d prefer.<br /> <br /> &gt;&gt; Option 1: Schedule a group-wide downtime date during standard busi= ness<br /> &gt;&gt; hours,<br /> &gt;&gt; which can be done by responding to this email with your preferred<= br /> &gt;&gt; date(s) to<br /> &gt;&gt; perform the migration. During this time, DoIT staff will work to<b= r /> &gt;&gt; migrate your<br /> &gt;&gt; volume to the Ceph storage cluster. DoIT staff will send an email = alert<br /> &gt;&gt; on this<br /> &gt;&gt; email thread when the migration has begun and when it has complete= d.<br /> &gt;&gt; For most<br /> &gt;&gt; storage volumes, this process should take less than a business day= .<br /> &gt;&gt; Option 2: If you don&rsquo;t respond to this email by October 15th= , DoIT<br /> &gt;&gt; staff will<br /> &gt;&gt; assign a day over the following month (October 16th through Novemb= er<br /> &gt;&gt; 15th) to<br /> &gt;&gt; migrate your volume. The day chosen will be random and will occur<= br /> &gt;&gt; during<br /> &gt;&gt; business hours. You will be notified of the date chosen to perform=  the<br /> &gt;&gt; migration, and will be notified when the migration begins and<br /> &gt;&gt; completes.<br /> <br /> &gt;&gt; Note: After this process has completed, the new storage volume wil= l<br /> &gt;&gt; have a new<br /> &gt;&gt; name. For example, group &ldquo;pi_doit&rdquo; will find its data = under<br /> &gt;&gt; &ldquo;/umbc/rs/pi_doit&rdquo;,<br /> &gt;&gt; or in your group&rsquo;s case you will find your volume under<br /> &gt;&gt; &ldquo;/umbc/rs/pi_estokan&rdquo;.<br /> <br /> &gt;&gt; Thank you,<br /> &gt;&gt; Elliot<br /> <br /> &gt; -- Eric J. Stokan, Ph.D.Associate Professor | Department of Political<= br /> &gt; ScienceDirector | Center for Social Science ScholarshipUSM Wilson H. E= lkins<br /> &gt; Professor (2025-2026) Affiliate Faculty | School of Public PolicyUnive= rsity<br /> &gt; of Maryland, Baltimore County (UMBC)Co-Director | MGMT Lab | Indiana<b= r /> &gt; University BloomingtonEditorial Board | Urban Affairs Review and State=  and<br /> &gt; Local Government Review586-202-7540estokan@umbc.eduwww.ericjstokan.com= Pronouns:<br /> &gt; He/Him/His<br /> <br /> --<br /> <br /> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br /> - DoIT<br /> &nbsp;</blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --  <div> <div><strong>Eric J. Stokan, Ph.D.</strong>  <div> <div>Associate Professor | Department of Political Science</div>  <div>Director |&nbsp;<span style=3D""color:#000000"">Center for Social Scienc= e Scholarship</span></div>  <div> <div>USM Wilson H. Elkins Professor (2025-2026)&nbsp;</div> </div>  <div>Affiliate Faculty | School of Public Policy</div>  <div>University of Maryland, Baltimore County (UMBC)</div>  <div><span style=3D""color:#000000"">Co-Director</span><span style=3D""color:#= 0000ff"">&nbsp;</span><span style=3D""color:#000000"">|</span><span style=3D""c= olor:#0000ff"">&nbsp;</span><span style=3D""color:#000000"">MGMT Lab</span><sp= an style=3D""color:#0000ff"">&nbsp;</span><span style=3D""color:#000000"">|&nbs= p;</span><span style=3D""color:#000000"">Indiana University Bloomington</span= ></div>  <div><span style=3D""color:#222222"">Editorial Board |&nbsp;</span><span styl= e=3D""color:#000000"">Urban Affairs Review</span><span style=3D""color:#222222= "">&nbsp;and </span><span style=3D""color:#000000"">State and Local Government=  Review</span></div> </div>  <div>586-202-7540</div>  <div>estokan@umbc.edu</div>  <div>www.ericjstokan.com</div>  <div>Pronouns: He/Him/His</div>  <div>&nbsp;</div>  <div>&nbsp;</div>  <div>&nbsp;</div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> "
3283037,72004805,Create,DoIT-Research-Computing,2025-09-29 20:35:29.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_egreen,resolved,Greg Ballantine,gballan1,Erin Green,egreen,egreen@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<p>Hello Erin,<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 250GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/egreen&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_egreen&rdquo;.<br /> <br /> Thank you,<br /> Greg</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283037,72026136,Correspond,DoIT-Research-Computing,2025-09-30 19:20:57.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_egreen,resolved,Greg Ballantine,gballan1,Erin Green,egreen,egreen@umbc.edu,Erin Green,egreen@umbc.edu,"Hi Greg, We don't currently have any data stored yet, so you are welcome to do any migration at any time-- it won't impact us at the moment. Thanks, Erin  On Mon, Sep 29, 2025 at 4:35=E2=80=AFPM via RT <UMBCHelp@rt.umbc.edu> wrote:  > Greetings, > > This message has been automatically generated in response to the > creation of a ticket regarding: > > ------------------------------------------------------------------------- > Subject: ""Migrating Research Storage Volume to Ceph Cluster - pi_egreen"" > > Message: > > Hello Erin, > > As per the communication via myUMBC earlier this summer > (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the > process of > migrating data off of an older storage server to our new RRStor Ceph > storage > cluster. Your group is using 0GB of a 250GB quota on the old storage > server. > > To perform these migrations, we need to take individual storage volumes > offline > while we migrate them to the Ceph cluster. Thus we are reaching out to > schedule > a date where we can migrate your volume located at =E2=80=9C/umbc/rs/egre= en=E2=80=9D. > During > the migration, we will take your volume offline and will terminate any jo= bs > running on the chip compute cluster that are accessing this volume. > > Below we=E2=80=99ve listed two options for handling this data migration -=  please > let us > know which of these you=E2=80=99d prefer. > > Option 1: Schedule a group-wide downtime date during standard business > hours, > which can be done by responding to this email with your preferred date(s) > to > perform the migration. During this time, DoIT staff will work to migrate > your > volume to the Ceph storage cluster. DoIT staff will send an email alert on > this > email thread when the migration has begun and when it has completed. For > most > storage volumes, this process should take less than a business day. > Option 2: If you don=E2=80=99t respond to this email by October 15th, DoI= T staff > will > assign a day over the following month (October 16th through November 15th) > to > migrate your volume. The day chosen will be random and will occur during > business hours. You will be notified of the date chosen to perform the > migration, and will be notified when the migration begins and completes. > > Note: After this process has completed, the new storage volume will have a > new > name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its data und= er > =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, > or in your group=E2=80=99s case you will find your volume under > =E2=80=9C/umbc/rs/pi_egreen=E2=80=9D. > > Thank you, > Greg > > -- > > Gregory BallantineSystem Administrator for Research and Enterprise > ComputingUMBC > - DoIT > > > ------------------------------------------------------------------------- > > There is no need to reply to this message right now. > > Your ticket has been assigned an ID of [Research Computing #3283037] or > you can go there directly by clicking the link below. > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3283037 > > > You can login to view your open tickets at any time by visiting > http://my.umbc.edu and clicking on ""Help"" and ""Request Help"". > > Alternately you can click on http://my.umbc.edu/help > >                         Thank you > > "
3283037,72026136,Correspond,DoIT-Research-Computing,2025-09-30 19:20:57.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_egreen,resolved,Greg Ballantine,gballan1,Erin Green,egreen,egreen@umbc.edu,Erin Green,egreen@umbc.edu,"<div dir=3D""ltr"">Hi Greg,=C2=A0<div>We don&#39;t currently have any data st= ored yet, so you are welcome to do any migration at any time-- it won&#39;t=  impact us at the moment.=C2=A0</div><div>Thanks,=C2=A0</div><div>Erin</div= ></div><br><div class=3D""gmail_quote gmail_quote_container""><div dir=3D""ltr= "" class=3D""gmail_attr"">On Mon, Sep 29, 2025 at 4:35=E2=80=AFPM via RT &lt;<= a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<= br></div><blockquote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8e= x;border-left:1px solid rgb(204,204,204);padding-left:1ex"">Greetings,<br> <br> This message has been automatically generated in response to the<br> creation of a ticket regarding:<br> <br> -------------------------------------------------------------------------<b= r> Subject: &quot;Migrating Research Storage Volume to Ceph Cluster - pi_egree= n&quot;<br> <br> Message: <br> <br> Hello Erin,<br> <br> As per the communication via myUMBC earlier this summer<br> (<a href=3D""https://my3.my.umbc.edu/groups/hpcf/posts/150838"" rel=3D""norefe= rrer"" target=3D""_blank"">https://my3.my.umbc.edu/groups/hpcf/posts/150838</a= >), DoIT is in the process of<br> migrating data off of an older storage server to our new RRStor Ceph storag= e<br> cluster. Your group is using 0GB of a 250GB quota on the old storage server= .<br> <br> To perform these migrations, we need to take individual storage volumes off= line<br> while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule<br> a date where we can migrate your volume located at =E2=80=9C/umbc/rs/egreen= =E2=80=9D. During<br> the migration, we will take your volume offline and will terminate any jobs= <br> running on the chip compute cluster that are accessing this volume.<br> <br> Below we=E2=80=99ve listed two options for handling this data migration - p= lease let us<br> know which of these you=E2=80=99d prefer.<br> <br> Option 1: Schedule a group-wide downtime date during standard business hour= s,<br> which can be done by responding to this email with your preferred date(s) t= o<br> perform the migration. During this time, DoIT staff will work to migrate yo= ur<br> volume to the Ceph storage cluster. DoIT staff will send an email alert on = this<br> email thread when the migration has begun and when it has completed. For mo= st<br> storage volumes, this process should take less than a business day.<br> Option 2: If you don=E2=80=99t respond to this email by October 15th, DoIT = staff will<br> assign a day over the following month (October 16th through November 15th) = to<br> migrate your volume. The day chosen will be random and will occur during<br> business hours. You will be notified of the date chosen to perform the<br> migration, and will be notified when the migration begins and completes.<br> <br> Note: After this process has completed, the new storage volume will have a = new<br> name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its data under=  =E2=80=9C/umbc/rs/pi_doit=E2=80=9D,<br> or in your group=E2=80=99s case you will find your volume under =E2=80=9C/u= mbc/rs/pi_egreen=E2=80=9D.<br> <br> Thank you,<br> Greg<br> <br> --<br> <br> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br> - DoIT<br> <br> <br> -------------------------------------------------------------------------<b= r> <br> There is no need to reply to this message right now.=C2=A0 <br> <br> Your ticket has been assigned an ID of [Research Computing #3283037] or you=  can go there directly by clicking the link below.<br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D328= 3037"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3283037</a> &gt;<br> <br> You can login to view your open tickets at any time by visiting <a href=3D""= http://my.umbc.edu"" rel=3D""noreferrer"" target=3D""_blank"">http://my.umbc.edu= </a> and clicking on &quot;Help&quot; and &quot;Request Help&quot;. <br> <br> Alternately you can click on <a href=3D""http://my.umbc.edu/help"" rel=3D""nor= eferrer"" target=3D""_blank"">http://my.umbc.edu/help</a><br> <br> =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 Thank you<br> <br> </blockquote></div> "
3283037,72037440,Correspond,DoIT-Research-Computing,2025-10-01 14:38:55.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_egreen,resolved,Greg Ballantine,gballan1,Erin Green,egreen,egreen@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Good morning Erin,<br /> <br /> Understood. We will send you an email later today with the details for your=  storage volume after the migration.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Tue Sep 30 15:20:57 2025, DA65012 wrote:</p>  <blockquote> <div>Hi Greg,&nbsp; <div>We don&#39;t currently have any data stored yet, so you are welcome to=  do any migration at any time-- it won&#39;t impact us at the moment.&nbsp;= </div>  <div>Thanks,&nbsp;</div>  <div>Erin</div> </div> &nbsp;  <div> <div>On Mon, Sep 29, 2025 at 4:35=E2=80=AFPM via RT &lt;UMBCHelp@rt.umbc.ed= u&gt; wrote:</div>  <blockquote>Greetings,<br /> <br /> This message has been automatically generated in response to the<br /> creation of a ticket regarding:<br /> <br /> -------------------------------------------------------------------------<b= r /> Subject: &quot;Migrating Research Storage Volume to Ceph Cluster - pi_egree= n&quot;<br /> <br /> Message:<br /> <br /> Hello Erin,<br /> <br /> As per the communication via myUMBC earlier this summer<br /> (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process = of<br /> migrating data off of an older storage server to our new RRStor Ceph storag= e<br /> cluster. Your group is using 0GB of a 250GB quota on the old storage server= .<br /> <br /> To perform these migrations, we need to take individual storage volumes off= line<br /> while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule<br /> a date where we can migrate your volume located at &ldquo;/umbc/rs/egreen&r= dquo;. During<br /> the migration, we will take your volume offline and will terminate any jobs= <br /> running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - ple= ase let us<br /> know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hour= s,<br /> which can be done by responding to this email with your preferred date(s) t= o<br /> perform the migration. During this time, DoIT staff will work to migrate yo= ur<br /> volume to the Ceph storage cluster. DoIT staff will send an email alert on = this<br /> email thread when the migration has begun and when it has completed. For mo= st<br /> storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT st= aff will<br /> assign a day over the following month (October 16th through November 15th) = to<br /> migrate your volume. The day chosen will be random and will occur during<br=  /> business hours. You will be notified of the date chosen to perform the<br /> migration, and will be notified when the migration begins and completes.<br=  /> <br /> Note: After this process has completed, the new storage volume will have a = new<br /> name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ld= quo;/umbc/rs/pi_doit&rdquo;,<br /> or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/= rs/pi_egreen&rdquo;.<br /> <br /> Thank you,<br /> Greg<br /> <br /> --<br /> <br /> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br /> - DoIT<br /> <br /> <br /> -------------------------------------------------------------------------<b= r /> <br /> There is no need to reply to this message right now.&nbsp;<br /> <br /> Your ticket has been assigned an ID of [Research Computing #3283037] or you=  can go there directly by clicking the link below.<br /> <br /> Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3283037 &gt;<b= r /> <br /> You can login to view your open tickets at any time by visiting http://my.u= mbc.edu and clicking on &quot;Help&quot; and &quot;Request Help&quot;.<br /> <br /> Alternately you can click on http://my.umbc.edu/help<br /> <br /> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp= ; &nbsp; Thank you<br /> &nbsp;</blockquote> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283037,72046066,Correspond,DoIT-Research-Computing,2025-10-01 18:36:19.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_egreen,resolved,Greg Ballantine,gballan1,Erin Green,egreen,egreen@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Erin,<br /> <br /> We have finished migrating your volume to the Ceph cluster! As far as we ca= n tell everything seems to have gone smoothly. There are a few things to no= te:</p>  <ul> 	<li>The path has changed, and is now available under <strong>/umbc/rs/pi_e= green</strong>.</li> 	<li>The alias used to reach the volume is now <strong>pi_egreen_common</st= rong> and <strong>pi_egreen_user</strong>.</li> 	<li>Your new volume has a quota of <strong>10TB</strong>.</li> </ul>  <p>We know you&#39;re not actively using it right now, but please don&#39;t=  hesitate to reach out if you hit any snags once you start working with the=  new volume.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Wed Oct 01 10:38:55 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Good morning Erin,<br /> <br /> Understood. We will send you an email later today with the details for your=  storage volume after the migration.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Tue Sep 30 15:20:57 2025, DA65012 wrote:</p>  <blockquote> <div>Hi Greg,&nbsp; <div>We don&#39;t currently have any data stored yet, so you are welcome to=  do any migration at any time-- it won&#39;t impact us at the moment.&nbsp;= </div>  <div>Thanks,&nbsp;</div>  <div>Erin</div> </div> &nbsp;  <div> <div>On Mon, Sep 29, 2025 at 4:35=E2=80=AFPM via RT &lt;UMBCHelp@rt.umbc.ed= u&gt; wrote:</div>  <blockquote>Greetings,<br /> <br /> This message has been automatically generated in response to the<br /> creation of a ticket regarding:<br /> <br /> -------------------------------------------------------------------------<b= r /> Subject: &quot;Migrating Research Storage Volume to Ceph Cluster - pi_egree= n&quot;<br /> <br /> Message:<br /> <br /> Hello Erin,<br /> <br /> As per the communication via myUMBC earlier this summer<br /> (https://my3.my.umbc.edu/groups/hpcf/posts/150838), DoIT is in the process = of<br /> migrating data off of an older storage server to our new RRStor Ceph storag= e<br /> cluster. Your group is using 0GB of a 250GB quota on the old storage server= .<br /> <br /> To perform these migrations, we need to take individual storage volumes off= line<br /> while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule<br /> a date where we can migrate your volume located at &ldquo;/umbc/rs/egreen&r= dquo;. During<br /> the migration, we will take your volume offline and will terminate any jobs= <br /> running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - ple= ase let us<br /> know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hour= s,<br /> which can be done by responding to this email with your preferred date(s) t= o<br /> perform the migration. During this time, DoIT staff will work to migrate yo= ur<br /> volume to the Ceph storage cluster. DoIT staff will send an email alert on = this<br /> email thread when the migration has begun and when it has completed. For mo= st<br /> storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT st= aff will<br /> assign a day over the following month (October 16th through November 15th) = to<br /> migrate your volume. The day chosen will be random and will occur during<br=  /> business hours. You will be notified of the date chosen to perform the<br /> migration, and will be notified when the migration begins and completes.<br=  /> <br /> Note: After this process has completed, the new storage volume will have a = new<br /> name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ld= quo;/umbc/rs/pi_doit&rdquo;,<br /> or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/= rs/pi_egreen&rdquo;.<br /> <br /> Thank you,<br /> Greg<br /> <br /> --<br /> <br /> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br /> - DoIT<br /> <br /> <br /> -------------------------------------------------------------------------<b= r /> <br /> There is no need to reply to this message right now.&nbsp;<br /> <br /> Your ticket has been assigned an ID of [Research Computing #3283037] or you=  can go there directly by clicking the link below.<br /> <br /> Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3283037 &gt;<b= r /> <br /> You can login to view your open tickets at any time by visiting http://my.u= mbc.edu and clicking on &quot;Help&quot; and &quot;Request Help&quot;.<br /> <br /> Alternately you can click on http://my.umbc.edu/help<br /> <br /> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp= ; &nbsp; Thank you<br /> &nbsp;</blockquote> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283044,72005075,Create,DoIT-Research-Computing,2025-09-29 20:42:21.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ece,stalled,Greg Ballantine,gballan1,Erle Ellis,ece,ece@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<p>Hello&nbsp;Erle,<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838 ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/ece&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_ece&rdquo;.<br /> <br /> Thank you,<br /> Greg</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283044,72205169,Correspond,DoIT-Research-Computing,2025-10-10 14:35:10.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ece,stalled,Greg Ballantine,gballan1,Erle Ellis,ece,ece@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hello&nbsp;Erle,</p>  <p>This is a reminder email, in case you missed the first. We need a response by October 15th, or we&#39;ll be forced to go with Option 2, randomly scheduling a time.<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838 ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/ece&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_ece&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3283044,72352145,Correspond,DoIT-Research-Computing,2025-10-17 17:02:36.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ece,stalled,Greg Ballantine,gballan1,Erle Ellis,ece,ece@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Erle,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 4th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3283044,72717793,Correspond,DoIT-Research-Computing,2025-11-04 17:05:33.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ece,stalled,Greg Ballantine,gballan1,Erle Ellis,ece,ece@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Erle,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 13:02:36 2025, IO12693 wrote:</p>  <blockquote> <p>Dear Erle,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 4th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283044,72734810,Correspond,DoIT-Research-Computing,2025-11-04 21:27:21.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ece,stalled,Greg Ballantine,gballan1,Erle Ellis,ece,ece@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Erle,<br /> <br /> We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note:</p>  <ul> 	<li>The path has changed, and is now available under <strong>/umbc/rs/pi_ece</strong>.</li> 	<li>The alias used to reach the volume is now <strong>pi_ece_common</strong> and <strong>pi_ece_user</strong>.</li> 	<li>Your new volume has a quota of <strong>10TB</strong>.</li> </ul>  <p>When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good?<br /> <br /> Thank you,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Tue Nov 04 12:05:33 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Hello Erle,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 13:02:36 2025, IO12693 wrote:</p>  <blockquote> <p>Dear Erle,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 4th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283046,72005106,Create,DoIT-Research-Computing,2025-09-29 20:43:26.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_choa,stalled,Greg Ballantine,gballan1,Fow-sen Choa,choa,choa@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<p>Hello Fow-sen,<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838 ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 100GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/choa&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_choa&rdquo;.<br /> <br /> Thank you,<br /> Greg</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283046,72204561,Correspond,DoIT-Research-Computing,2025-10-10 14:21:15.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_choa,stalled,Greg Ballantine,gballan1,Fow-sen Choa,choa,choa@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hello Fow-sen,</p>  <p>This is a reminder email, in case you missed the first. We need a response by October 15th, or we&#39;ll be forced to go with Option 2, randomly scheduling a time.<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838 ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 100GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/choa&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_choa&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3283046,72350869,Correspond,DoIT-Research-Computing,2025-10-17 16:22:56.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_choa,stalled,Greg Ballantine,gballan1,Fow-sen Choa,choa,choa@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Fow-sen,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 29th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3283046,72549380,Correspond,DoIT-Research-Computing,2025-10-29 13:48:02.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_choa,stalled,Greg Ballantine,gballan1,Fow-sen Choa,choa,choa@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Fow-sen,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 12:22:56 2025, IO12693 wrote:</p>  <blockquote> <p>Dear Fow-sen,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 29th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283046,72552093,Correspond,DoIT-Research-Computing,2025-10-29 14:50:44.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_choa,stalled,Greg Ballantine,gballan1,Fow-sen Choa,choa,choa@umbc.edu,Fow-sen Choa,choa@umbc.edu,"Thanks!   On Wed, Oct 29, 2025 at 9:48=E2=80=AFAM Greg Ballantine via RT <UMBCHelp@rt= .umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3283046 > > > Last Update From Ticket: > > Hello Fow-sen, > > This is a reminder that we will be performing your group's migration to t= he > Ceph storage cluster today. During this time, please ensure there are not > any > jobs being run in your research group, otherwise these may be terminated. > > We will provide an update once completed. > > Best, > Greg > > On Fri Oct 17 12:22:56 2025, IO12693 wrote: > > > Dear Fow-sen, > > > As per our previous communications, since you did not schedule a date by > > October 15, we will be going with Option 2, randomly assigning a date > > between October 16 and November 15 for your migration. > > > We have assigned the date of October 29th for your migration. Let us kn= ow > > if there is a better day for you, and within reason, we can reschedule > that > > date. > > > You will be notified when the migration begins and completes. > > > Thank you, > > > Elliot > > -- > > Gregory BallantineSystem Administrator for Research and Enterprise > ComputingUMBC > - DoIT > > "
3283046,72552093,Correspond,DoIT-Research-Computing,2025-10-29 14:50:44.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_choa,stalled,Greg Ballantine,gballan1,Fow-sen Choa,choa,choa@umbc.edu,Fow-sen Choa,choa@umbc.edu,"<div dir=3D""ltr"">Thanks!<div><br></div></div><br><div class=3D""gmail_quote = gmail_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Wed, Oct 29= , 2025 at 9:48=E2=80=AFAM Greg Ballantine via RT &lt;<a href=3D""mailto:UMBC= Help@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote = class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px sol= id rgb(204,204,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt= .umbc.edu/Ticket/Display.html?id=3D3283046"" rel=3D""noreferrer"" target=3D""_b= lank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3283046</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hello Fow-sen,<br> <br> This is a reminder that we will be performing your group&#39;s migration to=  the<br> Ceph storage cluster today. During this time, please ensure there are not a= ny<br> jobs being run in your research group, otherwise these may be terminated.<b= r> <br> We will provide an update once completed.<br> <br> Best,<br> Greg<br> <br> On Fri Oct 17 12:22:56 2025, IO12693 wrote:<br> <br> &gt; Dear Fow-sen,<br> <br> &gt; As per our previous communications, since you did not schedule a date = by<br> &gt; October 15, we will be going with Option 2, randomly assigning a date<= br> &gt; between October 16 and November 15 for your migration.<br> <br> &gt; We have assigned the date of October 29th for your migration. Let us k= now<br> &gt; if there is a better day for you, and within reason, we can reschedule=  that<br> &gt; date.<br> <br> &gt; You will be notified when the migration begins and completes.<br> <br> &gt; Thank you,<br> <br> &gt; Elliot<br> <br> --<br> <br> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br> - DoIT<br> <br> </blockquote></div> "
3283046,72589150,Correspond,DoIT-Research-Computing,2025-10-29 21:28:36.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_choa,stalled,Greg Ballantine,gballan1,Fow-sen Choa,choa,choa@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello again Fow-sen,<br /> <br /> We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note:</p>  <ul> 	<li>The path has changed, and is now available under <strong>/umbc/rs/pi_choa</strong>.</li> 	<li>The alias used to reach the volume is now <strong>pi_choa_common</strong> and <strong>pi_choa_user</strong>.</li> 	<li>Your new volume has a quota of <strong>25TB</strong>.</li> </ul>  <p>When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good?<br /> <br /> Thank you,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Wed Oct 29 09:48:02 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Hello Fow-sen,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 12:22:56 2025, IO12693 wrote:</p>  <blockquote> <p>Dear Fow-sen,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 29th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283048,72005136,Create,DoIT-Research-Computing,2025-09-29 20:44:33.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_carter,resolved,Beamlak Bekele,bbekele1,Gary Carter,carter,carter@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<p>Hello Gary,<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838 ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/carter&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_carter&rdquo;.<br /> <br /> Thank you,<br /> Greg</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283048,72204476,Correspond,DoIT-Research-Computing,2025-10-10 14:18:50.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_carter,resolved,Beamlak Bekele,bbekele1,Gary Carter,carter,carter@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hello Gary,</p>  <p>This is a reminder email, in case you missed the first. We need a response by October 15th, or we&#39;ll be forced to go with Option 2, randomly scheduling a time.<br /> <br /> As per the communication via myUMBC earlier this summer (https://my3.my.umbc.edu/groups/hpcf/posts/150838 ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0GB of a 500GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/carter&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_carter&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3283048,72205370,Correspond,DoIT-Research-Computing,2025-10-10 14:38:11.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_carter,resolved,Beamlak Bekele,bbekele1,Gary Carter,carter,carter@umbc.edu,Gary Carter,carter@umbc.edu,"Option 2 works for me  Sent from my iPhone  > On Oct 10, 2025, at 10:18=E2=80=AFAM, Elliot Gobbert via RT <UMBCHelp@rt.= umbc.edu> wrote: >=20 > =EF=BB=BFTicket <URL: https://www.google.com/url?q=3Dhttps://rt.umbc.edu/= Ticket/Display.html?id%3D3283048&source=3Dgmail-imap&ust=3D1760710734000000= &usg=3DAOvVaw3jzM8Zoj_BcneIzzeR3ovB > >=20 > Last Update From Ticket: >=20 > Hello Gary, >=20 > This is a reminder email, in case you missed the first. We need a respons= e by > October 15th, or we'll be forced to go with Option 2, randomly scheduling=  a > time. >=20 > As per the communication via myUMBC earlier this summer > (https://www.google.com/url?q=3Dhttps://my3.my.umbc.edu/groups/hpcf/posts= /150838&source=3Dgmail-imap&ust=3D1760710734000000&usg=3DAOvVaw0QiOQLv2tFpj= uhaz6PtrX2 ), DoIT is in the process of > migrating data off of an older storage server to our new RRStor Ceph stor= age > cluster. Your group is using 0GB of a 500GB quota on the old storage serv= er. >=20 > To perform these migrations, we need to take individual storage volumes o= ffline > while we migrate them to the Ceph cluster. Thus we are reaching out to sc= hedule > a date where we can migrate your volume located at =E2=80=9C/umbc/rs/cart= er=E2=80=9D. During > the migration, we will take your volume offline and will terminate any jo= bs > running on the chip compute cluster that are accessing this volume. >=20 > Below we=E2=80=99ve listed two options for handling this data migration -=  please let us > know which of these you=E2=80=99d prefer. >=20 > Option 1: Schedule a group-wide downtime date during standard business ho= urs, > which can be done by responding to this email with your preferred date(s)=  to > perform the migration. During this time, DoIT staff will work to migrate = your > volume to the Ceph storage cluster. DoIT staff will send an email alert o= n this > email thread when the migration has begun and when it has completed. For = most > storage volumes, this process should take less than a business day. > Option 2: If you don=E2=80=99t respond to this email by October 15th, DoI= T staff will > assign a day over the following month (October 16th through November 15th= ) to > migrate your volume. The day chosen will be random and will occur during > business hours. You will be notified of the date chosen to perform the > migration, and will be notified when the migration begins and completes. >=20 > Note: After this process has completed, the new storage volume will have = a new > name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its data und= er =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, > or in your group=E2=80=99s case you will find your volume under =E2=80=9C= /umbc/rs/pi_carter=E2=80=9D. >=20 > Thank you, > Elliot >=20  "
3283048,72211695,Correspond,DoIT-Research-Computing,2025-10-10 17:45:17.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_carter,resolved,Beamlak Bekele,bbekele1,Gary Carter,carter,carter@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Gary,<br /> <br /> Sounds good, I&#39;ve put your group on our schedule for Tuesday October 21= st. We will send you an email alert via this RT ticket when we begin the mi= gration process, and again once it has completed.<br /> <br /> Please let us know if you have any questions or concerns.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 10 10:38:11 2025, KP59325 wrote:</p>  <blockquote> <pre> Option 2 works for me  Sent from my iPhone  &gt; On Oct 10, 2025, at 10:18=E2=80=AFAM, Elliot Gobbert via RT &lt;UMBCHe= lp@rt.umbc.edu&gt; wrote: &gt;=20 &gt; =EF=BB=BFTicket &lt;URL: https://www.google.com/url?q=3Dhttps://rt.umb= c.edu/Ticket/Display.html?id%3D3283048&amp;source=3Dgmail-imap&amp;ust=3D17= 60710734000000&amp;usg=3DAOvVaw3jzM8Zoj_BcneIzzeR3ovB &gt; &gt;=20 &gt; Last Update From Ticket: &gt;=20 &gt; Hello Gary, &gt;=20 &gt; This is a reminder email, in case you missed the first. We need a resp= onse by &gt; October 15th, or we&#39;ll be forced to go with Option 2, randomly sch= eduling a &gt; time. &gt;=20 &gt; As per the communication via myUMBC earlier this summer &gt; (https://www.google.com/url?q=3Dhttps://my3.my.umbc.edu/groups/hpcf/po= sts/150838&amp;source=3Dgmail-imap&amp;ust=3D1760710734000000&amp;usg=3DAOv= Vaw0QiOQLv2tFpjuhaz6PtrX2 ), DoIT is in the process of &gt; migrating data off of an older storage server to our new RRStor Ceph s= torage &gt; cluster. Your group is using 0GB of a 500GB quota on the old storage s= erver. &gt;=20 &gt; To perform these migrations, we need to take individual storage volume= s offline &gt; while we migrate them to the Ceph cluster. Thus we are reaching out to=  schedule &gt; a date where we can migrate your volume located at &ldquo;/umbc/rs/car= ter&rdquo;. During &gt; the migration, we will take your volume offline and will terminate any=  jobs &gt; running on the chip compute cluster that are accessing this volume. &gt;=20 &gt; Below we&rsquo;ve listed two options for handling this data migration = - please let us &gt; know which of these you&rsquo;d prefer. &gt;=20 &gt; Option 1: Schedule a group-wide downtime date during standard business=  hours, &gt; which can be done by responding to this email with your preferred date= (s) to &gt; perform the migration. During this time, DoIT staff will work to migra= te your &gt; volume to the Ceph storage cluster. DoIT staff will send an email aler= t on this &gt; email thread when the migration has begun and when it has completed. F= or most &gt; storage volumes, this process should take less than a business day. &gt; Option 2: If you don&rsquo;t respond to this email by October 15th, Do= IT staff will &gt; assign a day over the following month (October 16th through November 1= 5th) to &gt; migrate your volume. The day chosen will be random and will occur duri= ng &gt; business hours. You will be notified of the date chosen to perform the &gt; migration, and will be notified when the migration begins and complete= s. &gt;=20 &gt; Note: After this process has completed, the new storage volume will ha= ve a new &gt; name. For example, group &ldquo;pi_doit&rdquo; will find its data unde= r &ldquo;/umbc/rs/pi_doit&rdquo;, &gt; or in your group&rsquo;s case you will find your volume under &ldquo;/= umbc/rs/pi_carter&rdquo;. &gt;=20 &gt; Thank you, &gt; Elliot &gt;=20  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3283048,72390562,Correspond,DoIT-Research-Computing,2025-10-21 13:40:27.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_carter,resolved,Beamlak Bekele,bbekele1,Gary Carter,carter,carter@umbc.edu,Beamlak Bekele,bbekele1@umbc.edu,"<div> <p>Hello ,<br /> <br /> This is just a reminder that we are starting your migration today. During t= his time, please ensure there are not any jobs being run in your research g= roup, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> On Fri Oct 10 13:45:17 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Hello Gary,<br /> <br /> Sounds good, I&#39;ve put your group on our schedule for Tuesday October 21= st. We will send you an email alert via this RT ticket when we begin the mi= gration process, and again once it has completed.<br /> <br /> Please let us know if you have any questions or concerns.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 10 10:38:11 2025, KP59325 wrote:</p>  <blockquote> <pre> Option 2 works for me  Sent from my iPhone  &gt; On Oct 10, 2025, at 10:18=E2=80=AFAM, Elliot Gobbert via RT &lt;UMBCHe= lp@rt.umbc.edu&gt; wrote: &gt;=20 &gt; =EF=BB=BFTicket &lt;URL: https://www.google.com/url?q=3Dhttps://rt.umb= c.edu/Ticket/Display.html?id%3D3283048&amp;source=3Dgmail-imap&amp;ust=3D17= 60710734000000&amp;usg=3DAOvVaw3jzM8Zoj_BcneIzzeR3ovB &gt; &gt;=20 &gt; Last Update From Ticket: &gt;=20 &gt; Hello Gary, &gt;=20 &gt; This is a reminder email, in case you missed the first. We need a resp= onse by &gt; October 15th, or we&#39;ll be forced to go with Option 2, randomly sch= eduling a &gt; time. &gt;=20 &gt; As per the communication via myUMBC earlier this summer &gt; (https://www.google.com/url?q=3Dhttps://my3.my.umbc.edu/groups/hpcf/po= sts/150838&amp;source=3Dgmail-imap&amp;ust=3D1760710734000000&amp;usg=3DAOv= Vaw0QiOQLv2tFpjuhaz6PtrX2 ), DoIT is in the process of &gt; migrating data off of an older storage server to our new RRStor Ceph s= torage &gt; cluster. Your group is using 0GB of a 500GB quota on the old storage s= erver. &gt;=20 &gt; To perform these migrations, we need to take individual storage volume= s offline &gt; while we migrate them to the Ceph cluster. Thus we are reaching out to=  schedule &gt; a date where we can migrate your volume located at &ldquo;/umbc/rs/car= ter&rdquo;. During &gt; the migration, we will take your volume offline and will terminate any=  jobs &gt; running on the chip compute cluster that are accessing this volume. &gt;=20 &gt; Below we&rsquo;ve listed two options for handling this data migration = - please let us &gt; know which of these you&rsquo;d prefer. &gt;=20 &gt; Option 1: Schedule a group-wide downtime date during standard business=  hours, &gt; which can be done by responding to this email with your preferred date= (s) to &gt; perform the migration. During this time, DoIT staff will work to migra= te your &gt; volume to the Ceph storage cluster. DoIT staff will send an email aler= t on this &gt; email thread when the migration has begun and when it has completed. F= or most &gt; storage volumes, this process should take less than a business day. &gt; Option 2: If you don&rsquo;t respond to this email by October 15th, Do= IT staff will &gt; assign a day over the following month (October 16th through November 1= 5th) to &gt; migrate your volume. The day chosen will be random and will occur duri= ng &gt; business hours. You will be notified of the date chosen to perform the &gt; migration, and will be notified when the migration begins and complete= s. &gt;=20 &gt; Note: After this process has completed, the new storage volume will ha= ve a new &gt; name. For example, group &ldquo;pi_doit&rdquo; will find its data unde= r &ldquo;/umbc/rs/pi_doit&rdquo;, &gt; or in your group&rsquo;s case you will find your volume under &ldquo;/= umbc/rs/pi_carter&rdquo;. &gt;=20 &gt; Thank you, &gt; Elliot &gt;=20  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> "
3283048,72392280,Correspond,DoIT-Research-Computing,2025-10-21 14:09:34.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_carter,resolved,Beamlak Bekele,bbekele1,Gary Carter,carter,carter@umbc.edu,Beamlak Bekele,bbekele1@umbc.edu,"<div> <p>Hello&nbsp;<br /> <br /> We have finished migrating your volume to the Ceph cluster! As far as we ca= n tell everything seems to have gone smoothly. There are a few things to no= te:</p>  <ul> 	<li> 	<p>The path has changed, and is now available under /umbc/rs/<strong>pi_ca= rter</strong>.</p> 	</li> 	<li> 	<p>The alias used to reach the volume is now <strong>pi_carter_common</str= ong> and <strong>pi_carter_user</strong>.</p> 	</li> 	<li> 	<p>Your new volume has a quota of <strong>25TB</strong>.</p> 	</li> </ul>  <p><br /> When you have a chance, please&nbsp; try running some jobs on Chip using th= e new volume to verify everything looks good.<br /> <br /> Thank you,</p>  <p>On Tue Oct 21 09:40:27 2025, PG41777 wrote:</p>  <blockquote> <div> <p>Hello ,<br /> <br /> This is just a reminder that we are starting your migration today. During t= his time, please ensure there are not any jobs being run in your research g= roup, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> On Fri Oct 10 13:45:17 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Hello Gary,<br /> <br /> Sounds good, I&#39;ve put your group on our schedule for Tuesday October 21= st. We will send you an email alert via this RT ticket when we begin the mi= gration process, and again once it has completed.<br /> <br /> Please let us know if you have any questions or concerns.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 10 10:38:11 2025, KP59325 wrote:</p>  <blockquote> <pre> Option 2 works for me  Sent from my iPhone  &gt; On Oct 10, 2025, at 10:18=E2=80=AFAM, Elliot Gobbert via RT &lt;UMBCHe= lp@rt.umbc.edu&gt; wrote: &gt;=20 &gt; =EF=BB=BFTicket &lt;URL: https://www.google.com/url?q=3Dhttps://rt.umb= c.edu/Ticket/Display.html?id%3D3283048&amp;source=3Dgmail-imap&amp;ust=3D17= 60710734000000&amp;usg=3DAOvVaw3jzM8Zoj_BcneIzzeR3ovB &gt; &gt;=20 &gt; Last Update From Ticket: &gt;=20 &gt; Hello Gary, &gt;=20 &gt; This is a reminder email, in case you missed the first. We need a resp= onse by &gt; October 15th, or we&#39;ll be forced to go with Option 2, randomly sch= eduling a &gt; time. &gt;=20 &gt; As per the communication via myUMBC earlier this summer &gt; (https://www.google.com/url?q=3Dhttps://my3.my.umbc.edu/groups/hpcf/po= sts/150838&amp;source=3Dgmail-imap&amp;ust=3D1760710734000000&amp;usg=3DAOv= Vaw0QiOQLv2tFpjuhaz6PtrX2 ), DoIT is in the process of &gt; migrating data off of an older storage server to our new RRStor Ceph s= torage &gt; cluster. Your group is using 0GB of a 500GB quota on the old storage s= erver. &gt;=20 &gt; To perform these migrations, we need to take individual storage volume= s offline &gt; while we migrate them to the Ceph cluster. Thus we are reaching out to=  schedule &gt; a date where we can migrate your volume located at &ldquo;/umbc/rs/car= ter&rdquo;. During &gt; the migration, we will take your volume offline and will terminate any=  jobs &gt; running on the chip compute cluster that are accessing this volume. &gt;=20 &gt; Below we&rsquo;ve listed two options for handling this data migration = - please let us &gt; know which of these you&rsquo;d prefer. &gt;=20 &gt; Option 1: Schedule a group-wide downtime date during standard business=  hours, &gt; which can be done by responding to this email with your preferred date= (s) to &gt; perform the migration. During this time, DoIT staff will work to migra= te your &gt; volume to the Ceph storage cluster. DoIT staff will send an email aler= t on this &gt; email thread when the migration has begun and when it has completed. F= or most &gt; storage volumes, this process should take less than a business day. &gt; Option 2: If you don&rsquo;t respond to this email by October 15th, Do= IT staff will &gt; assign a day over the following month (October 16th through November 1= 5th) to &gt; migrate your volume. The day chosen will be random and will occur duri= ng &gt; business hours. You will be notified of the date chosen to perform the &gt; migration, and will be notified when the migration begins and complete= s. &gt;=20 &gt; Note: After this process has completed, the new storage volume will ha= ve a new &gt; name. For example, group &ldquo;pi_doit&rdquo; will find its data unde= r &ldquo;/umbc/rs/pi_doit&rdquo;, &gt; or in your group&rsquo;s case you will find your volume under &ldquo;/= umbc/rs/pi_carter&rdquo;. &gt;=20 &gt; Thank you, &gt; Elliot &gt;=20  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> "
3283387,72016770,Create,DoIT-Research-Computing,2025-09-30 15:07:13.0000000,HPC User Account: pravi1 in pi_jianwu,resolved,Beamlak Bekele,bbekele1,Pavan Raj Ravi,pravi1,pravi1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Pavan Raj Last Name:                 Ravi Email:                     pravi1@umbc.edu Campus ID:                 MJ24933  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    jianwu@umbc.edu Existing Group:       pi_jianwu Project Title:        NSF career award Project Abstract:     A fundamental problem in climate science is climate c= ausality analysis that studies the cause-effect relationship among climate = variables, such as temperature and humidity. By studying how the climate sy= stem works from a causality perspective, the findings could be used for man= y research areas including climate variability, climate dynamics, climate s= imulation, and extreme climate prediction. Nowadays, climate causality stud= y faces many computing challenges, such as processing very large and high-d= imensional datasets, and the complexity of modern computing resources. To t= ackle these challenges, this project targets novel causality discovery algo= rithms and related scalable computing techniques. The project is expected t= o greatly aid Earth System scientists and climate scientists to explore new=  hypotheses and use cases related to climate causality. The project include= s an integrated program of research, education and outreach to help better = understand and evaluate climate simulation, fostering workforce development=  for a multidisciplinary research community on =E2=80=9CData + Computing + = Climate Science=E2=80=9D, and raising interest in both IT technology and cl= imate studies among K-12 students, and various underrepresented groups. The=  project thus serves the national interest, as stated in NSF=E2=80=99s miss= ion, by promoting the progress of science and advancing national prosperity=  and welfare.  The goal of this CAREER project is to study efficient and reproducible caus= ality analytics for large-scale climate data, so that climate scientists ca= n easily test their causal hypotheses, reproduce existing studies and compa= re different causality analytics results. To handle the increasing dimensio= nality and resolution of spatiotemporal climate datasets, the project will = study incremental causality discovery algorithms for large-scale climate da= tasets and parallel causality discovery for spatiotemporal climate data. To=  address the variety of both causal discovery algorithms and climate simula= tion/observation datasets, the project will study how to effectively measur= e climate causality results from different causality algorithms and differe= nt climate datasets, and integrate causality results through ensemble techn= iques. To cope with difficulties in conducting and reproducing causality an= alytics with large-scale climate datasets, the project will study cloud com= puting for big data climate analytics pipeline construction and execution o= ptimization. The project will be evaluated from two perspectives. From the = computing perspective, the research will be evaluated in terms of algorithm=  computation complexity, algorithm accuracy and algorithm scalability. From=  the climate perspective, the applicability of the research will be evaluat= ed by collaborating with climate scientists in their specific research prog= rams.  Can you please add me to Dr.Wangs group.  "
3283387,72017588,Correspond,DoIT-Research-Computing,2025-09-30 15:30:11.0000000,HPC User Account: pravi1 in pi_jianwu,resolved,Beamlak Bekele,bbekele1,Pavan Raj Ravi,pravi1,pravi1@umbc.edu,Jianwu Wang,jianwu@umbc.edu,"Approve it. Thanks!  On Tue, Sep 30, 2025 at 11:07=E2=80=AFAM RT API via RT <UMBCHelp@rt.umbc.ed= u> wrote:  > This e-mail is a notification that a UMBC user: Pavan Raj Ravi < > pravi1@umbc.edu> has requested an account within UMBC's HPC environment > in your group <pi_jianwu>. As the PI, we request that you acknowledge and > approve this account creation by replying to this message. Alternatively > you can go to this link and review the ticket and indicate your decision > here: > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3283387 > > > Once we have your approval, we will create the account and you and the new > user will receive another e-mail notifying you that the account has been > created. If you have any other questions or concerns please contact us. > > - UMBC DoIT Research Computing Support Staff >   --=20 Best wishes Sincerely yours  Jianwu Wang, Ph.D. (he/him) jianwu@umbc.edu Website: https://bdal.umbc.edu/people/jianwu/ <https://userpages.umbc.edu/~jianwu/> WebEx: https://umbc.webex.com/meet/jianwu  Professor of Data Science, Department of Information Systems <http://informationsystems.umbc.edu/> Director, Big Data Analytics Lab <https://bdal.umbc.edu/> Director, Center for Scalable Data and Computational Science (ScaleS) <https://scales.umbc.edu/> Co-Director, NSF REU Site on Online Big Data Analytics <https://bigdatareu.umbc.edu/> Co-Lead, NSF HDR institute for Data and Model Revolution in the Polar Regions (iHARP) <https://iharp.umbc.edu/> University of Maryland, Baltimore County 410-455-3883 ITE 423 "
3283387,72017588,Correspond,DoIT-Research-Computing,2025-09-30 15:30:11.0000000,HPC User Account: pravi1 in pi_jianwu,resolved,Beamlak Bekele,bbekele1,Pavan Raj Ravi,pravi1,pravi1@umbc.edu,Jianwu Wang,jianwu@umbc.edu,"<div dir=3D""ltr"">Approve it. Thanks!</div><br><div class=3D""gmail_quote gma= il_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Tue, Sep 30, 2= 025 at 11:07=E2=80=AFAM RT API via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umb= c.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gm= ail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,= 204,204);padding-left:1ex"">This e-mail is a notification that a UMBC user: = Pavan Raj Ravi &lt;<a href=3D""mailto:pravi1@umbc.edu"" target=3D""_blank"">pra= vi1@umbc.edu</a>&gt; has requested an account within UMBC&#39;s HPC environ= ment in your group &lt;pi_jianwu&gt;. As the PI, we request that you acknow= ledge and approve this account creation by replying to this message. Altern= atively you can go to this link and review the ticket and indicate your dec= ision here:<br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D328= 3387"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3283387</a> &gt;<br> <br> Once we have your approval, we will create the account and you and the new = user will receive another e-mail notifying you that the account has been cr= eated. If you have any other questions or concerns please contact us.<br> <br> - UMBC DoIT Research Computing Support Staff<br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><div style=3D""color:rgb(0,0,0)"">Best wishes<br>S= incerely yours<br><br>Jianwu Wang, Ph.D. (he/him)<br><a href=3D""mailto:jian= wu@umbc.edu"" style=3D""color:rgb(17,85,204)"" target=3D""_blank"">jianwu@umbc.e= du</a><br>Website:=C2=A0<a href=3D""https://userpages.umbc.edu/~jianwu/"" sty= le=3D""color:rgb(17,85,204)"" target=3D""_blank"">https://bdal.umbc.edu/people/= jianwu/</a><br>WebEx:=C2=A0<a href=3D""https://umbc.webex.com/meet/jianwu"" s= tyle=3D""color:rgb(17,85,204)"" target=3D""_blank"">https://umbc.webex.com/meet= /jianwu</a><br><br>Professor of Data Science,=C2=A0<a href=3D""http://inform= ationsystems.umbc.edu/"" style=3D""color:rgb(17,85,204)"" target=3D""_blank"">De= partment of Information Systems</a></div><div style=3D""color:rgb(0,0,0)"">Di= rector,=C2=A0<a href=3D""https://bdal.umbc.edu/"" style=3D""color:rgb(17,85,20= 4)"" target=3D""_blank"">Big Data Analytics Lab</a></div><div style=3D""color:r= gb(0,0,0)"">Director,=C2=A0<a href=3D""https://scales.umbc.edu/"" target=3D""_b= lank"">Center for Scalable Data and Computational Science (ScaleS)</a></div>= <div style=3D""color:rgb(0,0,0)"">Co-Director,=C2=A0<a href=3D""https://bigdat= areu.umbc.edu/"" target=3D""_blank"">NSF REU Site on Online Big Data=C2=A0Anal= ytics</a></div><div style=3D""color:rgb(0,0,0)"">Co-Lead,=C2=A0<a href=3D""htt= ps://iharp.umbc.edu/"" style=3D""color:rgb(17,85,204)"" target=3D""_blank"">NSF = HDR institute for Data=C2=A0and Model Revolution in the=C2=A0Polar Regions = (iHARP)</a><br>University of Maryland, Baltimore=C2=A0County<br>410-455-388= 3<br>ITE 423=C2=A0</div></div></div> "
3283387,72018279,Correspond,DoIT-Research-Computing,2025-09-30 15:55:36.0000000,HPC User Account: pravi1 in pi_jianwu,resolved,Beamlak Bekele,bbekele1,Pavan Raj Ravi,pravi1,pravi1@umbc.edu,Beamlak Bekele,bbekele1@umbc.edu,"<div> <p>Hi Pavan,<br /> Your account (pravi1) has been added to the pi_jianwu group on chip.rs.umbc= .edu .</p>  <p>You can find a short tutorial on how to use chip here: https://umbc.atla= ssian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip</p>  <p>Please read through the documentation found at hpcf.umbc.edu &gt; User S= upport.<br /> All available modules can be viewed using the command &#39;module avail&#39= ;.<br /> Please submit additional questions or issues as separate tickets via the fo= llowing link.<br /> (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/)</p>  <p>&nbsp;</p>  <p>On Tue Sep 30 11:30:11 2025, JK06455 wrote:</p>  <blockquote> <div>Approve it. Thanks!</div> &nbsp;  <div> <div>On Tue, Sep 30, 2025 at 11:07=E2=80=AFAM RT API via RT &lt;UMBCHelp@rt= .umbc.edu&gt; wrote:</div>  <blockquote>This e-mail is a notification that a UMBC user: Pavan Raj Ravi = &lt;pravi1@umbc.edu&gt; has requested an account within UMBC&#39;s HPC envi= ronment in your group &lt;pi_jianwu&gt;. As the PI, we request that you ack= nowledge and approve this account creation by replying to this message. Alt= ernatively you can go to this link and review the ticket and indicate your = decision here:<br /> <br /> Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3283387 &gt;<b= r /> <br /> Once we have your approval, we will create the account and you and the new = user will receive another e-mail notifying you that the account has been cr= eated. If you have any other questions or concerns please contact us.<br /> <br /> - UMBC DoIT Research Computing Support Staff</blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --  <div> <div> <div>Best wishes<br /> Sincerely yours<br /> <br /> Jianwu Wang, Ph.D. (he/him)<br /> jianwu@umbc.edu<br /> Website:&nbsp;https://bdal.umbc.edu/people/jianwu/<br /> WebEx:&nbsp;https://umbc.webex.com/meet/jianwu<br /> <br /> Professor of Data Science,&nbsp;Department of Information Systems</div>  <div>Director,&nbsp;Big Data Analytics Lab</div>  <div>Director,&nbsp;Center for Scalable Data and Computational Science (Sca= leS)</div>  <div>Co-Director,&nbsp;NSF REU Site on Online Big Data&nbsp;Analytics</div>  <div>Co-Lead,&nbsp;NSF HDR institute for Data&nbsp;and Model Revolution in = the&nbsp;Polar Regions (iHARP)<br /> University of Maryland, Baltimore&nbsp;County<br /> 410-455-3883<br /> ITE 423&nbsp;</div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> "
3283454,72018944,Create,DoIT-Research-Computing,2025-09-30 16:09:33.0000000,HPC User Account: yliao1 in Student Group,resolved,Elliot Gobbert,elliotg2,Yiming Liao,yliao1,yliao1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Yiming Last Name:                 Liao Email:                     yliao1@umbc.edu Campus ID:                 AU07819  Request Type:              High Performance Cluster  Create/Modify account in Student group  Hello, I am a PhD student in Computer Science, and my advisor is Dr. Keke Chen. I am here requesting to get an account on chip server to run experiments. Thank you.  "
3283454,72021378,Correspond,DoIT-Research-Computing,2025-09-30 17:15:05.0000000,HPC User Account: yliao1 in Student Group,resolved,Elliot Gobbert,elliotg2,Yiming Liao,yliao1,yliao1@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hello there,</p>  <p>Here is how to request an account on chip:</p>  <p>https://umbc.atlassian.net/wiki/spaces/faq/pages/1327431728/How+to+request+a+user+group+account+on+chip</p>  <p>&nbsp;</p>  <p>Let me know if that helps,</p>  <p>Elliot Gobbert</p> "
3283590,72022578,Create,DoIT-Research-Computing,2025-09-30 17:50:36.0000000,HPC User Account: vs76219 in pi_leizhang,resolved,Beamlak Bekele,bbekele1,Naiyue Liang,vs76219,vs76219@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Naiyue Last Name:                 Liang Email:                     vs76219@umbc.edu Campus ID:                 VS76219  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    leizhang@umbc.edu Existing Group:       pi_leizhang Project Title:        Password Leak Detection with Machine Learning Project Abstract:     Training Machine Learning Models to Detect Password Leaks  Training Machine Learning Models to Detect Password Leaks.  "
3283590,72024160,Correspond,DoIT-Research-Computing,2025-09-30 18:29:01.0000000,HPC User Account: vs76219 in pi_leizhang,resolved,Beamlak Bekele,bbekele1,Naiyue Liang,vs76219,vs76219@umbc.edu,Lei Zhang,leizhang@umbc.edu,"<html><head><meta http-equiv=3D""content-type"" content=3D""text/html; charset= =3Dutf-8""></head><body style=3D""overflow-wrap: break-word; -webkit-nbsp-mod= e: space; line-break: after-white-space;"">Yes, Naiyue is a PhD student work= ing in my group. Thanks.<br id=3D""lineBreakAtBeginningOfMessage""><div> <meta charset=3D""UTF-8""><div dir=3D""auto"" style=3D""caret-color: rgb(0, 0, 0= ); color: rgb(0, 0, 0); letter-spacing: normal; text-align: start; text-ind= ent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; -we= bkit-text-stroke-width: 0px; text-decoration: none; overflow-wrap: break-wo= rd; -webkit-nbsp-mode: space; line-break: after-white-space;""><div dir=3D""a= uto"" style=3D""caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0); letter-spacin= g: normal; text-align: start; text-indent: 0px; text-transform: none; white= -space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-dec= oration: none; overflow-wrap: break-word; -webkit-nbsp-mode: space; line-br= eak: after-white-space;""><div dir=3D""auto"" style=3D""caret-color: rgb(0, 0, = 0); color: rgb(0, 0, 0); letter-spacing: normal; text-align: start; text-in= dent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; -w= ebkit-text-stroke-width: 0px; text-decoration: none; overflow-wrap: break-w= ord; -webkit-nbsp-mode: space; line-break: after-white-space;""><div dir=3D""= auto"" style=3D""caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0); letter-spaci= ng: normal; text-align: start; text-indent: 0px; text-transform: none; whit= e-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-de= coration: none; overflow-wrap: break-word; -webkit-nbsp-mode: space; line-b= reak: after-white-space;""><div style=3D""caret-color: rgb(0, 0, 0); color: r= gb(0, 0, 0); font-family: Helvetica; font-size: 12px; font-style: normal; f= ont-variant-caps: normal; font-weight: 400; letter-spacing: normal; text-al= ign: start; text-indent: 0px; text-transform: none; white-space: normal; wo= rd-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration: none;""><b= r class=3D""Apple-interchange-newline"">Best regards,</div><div style=3D""care= t-color: rgb(0, 0, 0); color: rgb(0, 0, 0); font-family: Helvetica; font-si= ze: 12px; font-style: normal; font-variant-caps: normal; font-weight: 400; = letter-spacing: normal; text-align: start; text-indent: 0px; text-transform= : none; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: = 0px; text-decoration: none;"">Lei</div><div style=3D""caret-color: rgb(0, 0, = 0); color: rgb(0, 0, 0); font-family: Helvetica; font-size: 12px; font-styl= e: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: nor= mal; text-align: start; text-indent: 0px; text-transform: none; white-space= : normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoratio= n: none;""><br></div><div style=3D""caret-color: rgb(0, 0, 0); color: rgb(0, = 0, 0); font-family: Helvetica; font-size: 12px; font-style: normal; font-va= riant-caps: normal; font-weight: 400; letter-spacing: normal; text-align: s= tart; text-indent: 0px; text-transform: none; white-space: normal; word-spa= cing: 0px; -webkit-text-stroke-width: 0px; text-decoration: none;"">=E2=80= =94=E2=80=94=E2=80=94</div><div style=3D""caret-color: rgb(0, 0, 0); color: = rgb(0, 0, 0); font-family: Helvetica; font-size: 12px; font-style: normal; = font-variant-caps: normal; font-weight: 400; letter-spacing: normal; text-a= lign: start; text-indent: 0px; text-transform: none; white-space: normal; w= ord-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration: none;"">L= ei Zhang, Ph.D.</div><div style=3D""caret-color: rgb(0, 0, 0); color: rgb(0,=  0, 0); font-family: Helvetica; font-size: 12px; font-style: normal; font-v= ariant-caps: normal; font-weight: 400; letter-spacing: normal; text-align: = start; text-indent: 0px; text-transform: none; white-space: normal; word-sp= acing: 0px; -webkit-text-stroke-width: 0px; text-decoration: none;"">Assista= nt Professor&nbsp;<br>Department of Information Systems<br>University of Ma= ryland, Baltimore County</div><div style=3D""caret-color: rgb(0, 0, 0); colo= r: rgb(0, 0, 0); font-family: Helvetica; font-size: 12px; font-style: norma= l; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; tex= t-align: start; text-indent: 0px; text-transform: none; white-space: normal= ; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration: none;= "">Lab website:&nbsp;<a href=3D""https://est.umbc.edu"">https://est.umbc.edu</= a></div><div style=3D""caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0); font-= family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: = normal; font-weight: 400; letter-spacing: normal; text-align: start; text-i= ndent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; -= webkit-text-stroke-width: 0px; text-decoration: none;"">Schedule a meeting w= ith me:&nbsp;<a href=3D""https://app.reclaim.ai/m/lei-umbc"">https://app.recl= aim.ai/m/lei-umbc</a></div><div style=3D""caret-color: rgb(0, 0, 0); color: = rgb(0, 0, 0); font-family: Helvetica; font-size: 12px; font-style: normal; = font-variant-caps: normal; font-weight: 400; letter-spacing: normal; text-a= lign: start; text-indent: 0px; text-transform: none; white-space: normal; w= ord-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration: none;"">O= ffice: ITE 444</div><div style=3D""caret-color: rgb(0, 0, 0); color: rgb(0, = 0, 0); font-family: Helvetica; font-size: 12px; font-style: normal; font-va= riant-caps: normal; font-weight: 400; letter-spacing: normal; text-align: s= tart; text-indent: 0px; text-transform: none; white-space: normal; word-spa= cing: 0px; -webkit-text-stroke-width: 0px; text-decoration: none;"">&nbsp;</= div></div></div></div></div> </div> <div><br><blockquote type=3D""cite""><div>On Sep 30, 2025, at 1:50=E2=80=AFPM= , RT API via RT &lt;UMBCHelp@rt.umbc.edu&gt; wrote:</div><br class=3D""Apple= -interchange-newline""><div><div>Ticket &lt;URL: https://www.google.com/url?= q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3283590&amp;source=3Dgmail= -imap&amp;ust=3D1759859442000000&amp;usg=3DAOvVaw2-v6vJyRuF4e2vJ8ihgueB &gt= ;<br><br>Last Update From Ticket:<br><br>First Name: &nbsp;&nbsp;&nbsp;&nbs= p;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Naiyue<= br>Last Name: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&= nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Liang<br>Email: &nbsp;&nbsp;&nbsp;&nbsp;= &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nb= sp;&nbsp;&nbsp;&nbsp;vs76219@umbc.edu<br>Campus ID: &nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;VS= 76219<br><br>Request Type: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;High Performance Cluster<br><br>Create/Modify=  account in existing PI group<br>Existing PI Email: &nbsp;&nbsp;&nbsp;leizh= ang@umbc.edu<br>Existing Group: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pi_leiz= hang<br>Project Title: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Password L= eak Detection with Machine Learning<br>Project Abstract: &nbsp;&nbsp;&nbsp;= &nbsp;Training Machine Learning Models to Detect Password Leaks<br><br>Trai= ning Machine Learning Models to Detect Password Leaks.<br><br><br></div></d= iv></blockquote></div><br></body></html>= "
3283590,72024160,Correspond,DoIT-Research-Computing,2025-09-30 18:29:01.0000000,HPC User Account: vs76219 in pi_leizhang,resolved,Beamlak Bekele,bbekele1,Naiyue Liang,vs76219,vs76219@umbc.edu,Lei Zhang,leizhang@umbc.edu,"Yes, Naiyue is a PhD student working in my group. Thanks.  Best regards, Lei  =E2=80=94=E2=80=94=E2=80=94 Lei Zhang, Ph.D. Assistant Professor=20 Department of Information Systems University of Maryland, Baltimore County Lab website: https://est.umbc.edu <https://est.umbc.edu/> Schedule a meeting with me: https://app.reclaim.ai/m/lei-umbc Office: ITE 444 =20  > On Sep 30, 2025, at 1:50=E2=80=AFPM, RT API via RT <UMBCHelp@rt.umbc.edu>=  wrote: >=20 > Ticket <URL: https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Di= splay.html?id%3D3283590&source=3Dgmail-imap&ust=3D1759859442000000&usg=3DAO= vVaw2-v6vJyRuF4e2vJ8ihgueB > >=20 > Last Update From Ticket: >=20 > First Name:                Naiyue > Last Name:                 Liang > Email:                     vs76219@umbc.edu > Campus ID:                 VS76219 >=20 > Request Type:              High Performance Cluster >=20 > Create/Modify account in existing PI group > Existing PI Email:    leizhang@umbc.edu > Existing Group:       pi_leizhang > Project Title:        Password Leak Detection with Machine Learning > Project Abstract:     Training Machine Learning Models to Detect Password=  Leaks >=20 > Training Machine Learning Models to Detect Password Leaks. >=20 >=20  "
3283590,72035681,Correspond,DoIT-Research-Computing,2025-10-01 13:55:24.0000000,HPC User Account: vs76219 in pi_leizhang,resolved,Beamlak Bekele,bbekele1,Naiyue Liang,vs76219,vs76219@umbc.edu,Beamlak Bekele,bbekele1@umbc.edu,"<div> <p>Hi Naiyue,</p>  <p>Your account (vs76219) has been created on chip.rs.umbc.edu.<br /> Your primary group is pi_leizhang.<br /> Your home directory has 500M of storage.<br /> You can find a short tutorial on how to use chip here: https://umbc.atlassi= an.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip<br /> Please read through the documentation found at hpcf.umbc.edu &gt; User Supp= ort.<br /> All available modules can be viewed using the command &#39;module avail&#39= ;.<br /> Please submit additional questions or issues as separate tickets via the fo= llowing link.<br /> (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/)<br /> &nbsp;</p>  <p>&nbsp;</p>  <p>On Tue Sep 30 14:29:01 2025, EA14775 wrote:</p>  <blockquote>Yes, Naiyue is a PhD student working in my group. Thanks. <div> <div> <div> <div> <div> <div><br /> Best regards,</div>  <div>Lei</div>  <div>&nbsp;</div>  <div>&mdash;&mdash;&mdash;</div>  <div>Lei Zhang, Ph.D.</div>  <div>Assistant Professor&nbsp;<br /> Department of Information Systems<br /> University of Maryland, Baltimore County</div>  <div>Lab website:&nbsp;https://est.umbc.edu</div>  <div>Schedule a meeting with me:&nbsp;https://app.reclaim.ai/m/lei-umbc</di= v>  <div>Office: ITE 444</div>  <div>&nbsp;</div> </div> </div> </div> </div> </div>  <div>&nbsp; <blockquote> <div>On Sep 30, 2025, at 1:50=E2=80=AFPM, RT API via RT &lt;UMBCHelp@rt.umb= c.edu&gt; wrote:</div> &nbsp;  <div> <div>Ticket &lt;URL: https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Tic= ket/Display.html?id%3D3283590&amp;source=3Dgmail-imap&amp;ust=3D17598594420= 00000&amp;usg=3DAOvVaw2-v6vJyRuF4e2vJ8ihgueB &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> First Name: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nb= sp;&nbsp;&nbsp;&nbsp;&nbsp;Naiyue<br /> Last Name: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbs= p;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Liang<br /> Email: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;vs76219@umbc.edu<br /> Campus ID: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbs= p;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;VS76219<br /> <br /> Request Type: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&= nbsp;&nbsp;&nbsp;High Performance Cluster<br /> <br /> Create/Modify account in existing PI group<br /> Existing PI Email: &nbsp;&nbsp;&nbsp;leizhang@umbc.edu<br /> Existing Group: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pi_leizhang<br /> Project Title: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Password Leak Dete= ction with Machine Learning<br /> Project Abstract: &nbsp;&nbsp;&nbsp;&nbsp;Training Machine Learning Models = to Detect Password Leaks<br /> <br /> Training Machine Learning Models to Detect Password Leaks.<br /> <br /> &nbsp;</div> </div> </blockquote> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> "
3283593,72022642,Create,DoIT-Research-Computing,2025-09-30 17:52:23.0000000,HPC Other Issue: recent publication,resolved,Beamlak Bekele,bbekele1,Sergio De souza-machad,sergio,sergio@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Sergio Last Name:                 De souza-machad Email:                     sergio@umbc.edu Campus ID:                 VR64161  Request Type:              High Performance Cluster  Hi Roy,  Geophysical Trends Inferred From 20 Years of AIRS Infrared Global Observations S. DeSouza-Machado, L. Larrabee Strow, R. J. Kramer First published: 11 August 2025 https://doi.org/10.1029/2025JD043501  See the link for bib info  Sergio  "
3283593,72025803,Correspond,DoIT-Research-Computing,2025-09-30 19:09:58.0000000,HPC Other Issue: recent publication,resolved,Beamlak Bekele,bbekele1,Sergio De souza-machad,sergio,sergio@umbc.edu,Beamlak Bekele,bbekele1@umbc.edu,"<div> <p>Hello&nbsp;</p>  <p>I have added this publication to our website: https://hpcf.umbc.edu/publications/</p>  <p>On Tue Sep 30 13:52:23 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Sergio Last Name:                 De souza-machad Email:                     sergio@umbc.edu Campus ID:                 VR64161  Request Type:              High Performance Cluster  Hi Roy,  Geophysical Trends Inferred From 20 Years of AIRS Infrared Global Observations S. DeSouza-Machado, L. Larrabee Strow, R. J. Kramer First published: 11 August 2025 https://doi.org/10.1029/2025JD043501  See the link for bib info  Sergio  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> "
3283767,72028953,Create,DoIT-Research-Computing,2025-09-30 20:50:49.0000000,HPC Slurm/Software Issue: Deleting .julia directories,resolved,Beamlak Bekele,bbekele1,Eric Lowe,elowe2,elowe2@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Eric Last Name:                 Lowe Email:                     elowe2@umbc.edu Campus ID:                 WU00932  Request Type:              High Performance Cluster   Ok, so this goes back to an issue I was at office hours last Friday with Beamlak. We thought we resolved it, and it is kind of resolved, but I still have an issue deleting what I think are essentially corrupted directories. So, I have been dealing with issues downloading packages in Julia on the cluster, and I narrowed down the issue to the specific package ""DataFrames"", which is actually quite a simple package, all it does is allows you to read CSV and other text files. The workaround that I have to do is that I just wrote my code in such a way that does not require the usage of DataFrames which is fine, but I still have a few directories in my research storage that house DataFrames package that I need to be deleted.  Specifically, under /umbc/rs/pi_jkestner/users/elowe2 I have 2 directories, "".julia"" and "".julia_new"" both of which need to be deleted as they house the package DataFrame which prevents me from really running anything. I also have the directory "".julia_test"" which I am currently using and does work, so it shouldn't be deleted. I also have another .julia directory under /umbc/rs/pi_jkestner/common that needs to be deleted as well. I have tried running ""rm -rf .julia"" for hours on end and it hangs. Even going into the directory itself, following the path, ""/umbc/rs/pi_jkestner/users/elowe2/.julia/packages"" and then doing ""rm -rf DataFrames"" will hang as well.  What ended up working on Friday, which was doing ""chmod -R u+wrx DataFrames"" hangs as well now too. I have narrowed the issue all the way down to a specific file in DataFrames, it would be in the path ""/umbc/rs/pi_jkestner/users/elowe2/.julia/packages/DataFrames/C5AEe/src"" which is the source code for this specific package. The file in ""subdataframe"" in the source directory is always the issue, which is weird because looking on github, the subdataframe.jl file, with which this is located, looks perfectly fine to me. Regardless, I would still like there to be a way to delete these excess directories in my research storage. Thanks.  "
3283767,72140063,Correspond,DoIT-Research-Computing,2025-10-07 16:00:19.0000000,HPC Slurm/Software Issue: Deleting .julia directories,resolved,Beamlak Bekele,bbekele1,Eric Lowe,elowe2,elowe2@umbc.edu,Beamlak Bekele,bbekele1@umbc.edu,"<div> <p>Hi Eric&nbsp;</p>  <p>The solution is still to run&nbsp;<code>chmod -R u+wrx DataFrames&nbsp;</code>before attempting to remove the directory. The process is hanging because there are many files and directories to modify, which takes time. While the command is running, if your session is idle for too long, you may get logged out before it completes. To avoid this, I recommend using a <code>tmux</code> session to run the command so it can finish without interruption. You can learn more about <code>tmux</code> here (https://www.geeksforgeeks.org/linux-unix/tmux-in-linux/). Once the permissions have been updated, you can remove the directory.</p>  <p>I have already removed the <code>/umbc/rs/pi_jkestner/users/elowe2/.julia/packages/DataFrames&nbsp;</code>directory for you after changing the permissions. Please reply to the ticket if you still have questions.&nbsp;</p>  <p>&nbsp;</p>  <p>&nbsp;</p>  <p>On Tue Sep 30 16:50:49 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Eric Last Name:                 Lowe Email:                     elowe2@umbc.edu Campus ID:                 WU00932  Request Type:              High Performance Cluster   Ok, so this goes back to an issue I was at office hours last Friday with Beamlak. We thought we resolved it, and it is kind of resolved, but I still have an issue deleting what I think are essentially corrupted directories. So, I have been dealing with issues downloading packages in Julia on the cluster, and I narrowed down the issue to the specific package &quot;DataFrames&quot;, which is actually quite a simple package, all it does is allows you to read CSV and other text files. The workaround that I have to do is that I just wrote my code in such a way that does not require the usage of DataFrames which is fine, but I still have a few directories in my research storage that house DataFrames package that I need to be deleted.  Specifically, under /umbc/rs/pi_jkestner/users/elowe2 I have 2 directories, &quot;.julia&quot; and &quot;.julia_new&quot; both of which need to be deleted as they house the package DataFrame which prevents me from really running anything. I also have the directory &quot;.julia_test&quot; which I am currently using and does work, so it shouldn&#39;t be deleted. I also have another .julia directory under /umbc/rs/pi_jkestner/common that needs to be deleted as well. I have tried running &quot;rm -rf .julia&quot; for hours on end and it hangs. Even going into the directory itself, following the path, &quot;/umbc/rs/pi_jkestner/users/elowe2/.julia/packages&quot; and then doing &quot;rm -rf DataFrames&quot; will hang as well.  What ended up working on Friday, which was doing &quot;chmod -R u+wrx DataFrames&quot; hangs as well now too. I have narrowed the issue all the way down to a specific file in DataFrames, it would be in the path &quot;/umbc/rs/pi_jkestner/users/elowe2/.julia/packages/DataFrames/C5AEe/src&quot; which is the source code for this specific package. The file in &quot;subdataframe&quot; in the source directory is always the issue, which is weird because looking on github, the subdataframe.jl file, with which this is located, looks perfectly fine to me. Regardless, I would still like there to be a way to delete these excess directories in my research storage. Thanks.  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> "
3284012,72036097,Create,DoIT-Research-Computing,2025-10-01 14:03:57.0000000,HPC Slurm/Software Issue: sbatch job is not running,resolved,Tartela Tabassum,tartelt1,Zahid Hassan Tushar,ztushar1,ztushar1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Zahid Hassan<br /> Last Name:                 Tushar<br /> Email:                     ztushar1@umbc.edu<br /> Campus ID:                 TS41250<br /> <br /> Request Type:              High Performance Cluster<br /> <br /> <br /> Hi, I have scheduled jobs but they seem not to run. I have attached the bash file that I used for scheduling the job. Since it was not running, I cancelled them. [job id 101100, 100991,100993]. I activated the conda environment, and then scheduled the job like below. <br />  <br /> ->conda activate co <br /> ->sbatch ES2_train_cam.sh <br />  <br /> What am I doing wrong? Please let me know. Thanks<br /> <br /> Attachment 1: <a href=""https://umbc.box.com/s/sca5g8f346t3ya5utnxvkarkzv15ee0a"" target=""_blank"">ES2_train_cam.sh</a><br /> "
3284012,72036460,Comment,DoIT-Research-Computing,2025-10-01 14:11:07.0000000,HPC Slurm/Software Issue: sbatch job is not running,resolved,Tartela Tabassum,tartelt1,Zahid Hassan Tushar,ztushar1,ztushar1@umbc.edu,Roy Prouty,proutyr1@umbc.edu,"<div> <p>User running on chip-gpu<br /> <br /> Seems like jobs did run:<br /> ```<br /> [root@chip-mgt1 /cm/shared]$ sacct -M chip-gpu -Xj 101100,100991,100993 -o &quot;elapsedraw&quot;<br /> ElapsedRaw&nbsp;<br /> ----------&nbsp;<br /> &nbsp; &nbsp; &nbsp;61162&nbsp;<br /> &nbsp; &nbsp; &nbsp;61159&nbsp;<br /> &nbsp; &nbsp; &nbsp; 1650&nbsp;<br /> [root@chip-mgt1 /cm/shared]$<br /> ```<br /> <br /> Here is .sh file inline:<br /> ```<br /> &nbsp;</p>  <pre> <code>#!/bin/bash #SBATCH --cluster chip-gpu #SBATCH --job-name BCAM #SBATCH --gres=gpu:1 #SBATCH --constraint=&#39;RTX_8000|RTX_6000|RTX_2080&#39; #SBATCH --output=chip_files/ES2cam.out #SBATCH --error=chip_files/ES2cam.err #SBATCH --time=72:00:00 #SBATCH --mem=10G  export CUDA_LAUNCH_BLOCKING=1 # module load Anaconda3 # eval &quot;$(conda shell.bash hook)&quot; conda init conda activate /umbc/rs/psanjay/users/ztushar1/conda_envs/co  echo &quot;--- Checking GPUs with nvidia-smi ---&quot; nvidia-smi  echo &quot;--- Checking GPU memory explicitly ---&quot; nvidia-smi --query-gpu=gpu_name,memory.total --format=csv,noheader  python ES2_main.py \ --model_name cam9s \ --batch_size 128 --lr 0.0001 --scheduler None \ --func MSE \ --num_epochs 2 --patience 1 \ --mode cot ```</code> </pre>  <p>&nbsp;</p>  <p>On Wed Oct 01 10:03:57 2025, ZZ99999 wrote:</p>  <blockquote>First Name: Zahid Hassan<br /> Last Name: Tushar<br /> Email: ztushar1@umbc.edu<br /> Campus ID: TS41250<br /> <br /> Request Type: High Performance Cluster<br /> <br /> <br /> Hi, I have scheduled jobs but they seem not to run. I have attached the bash file that I used for scheduling the job. Since it was not running, I cancelled them. [job id 101100, 100991,100993]. I activated the conda environment, and then scheduled the job like below.<br /> <br /> -&gt;conda activate co<br /> -&gt;sbatch ES2_train_cam.sh<br /> <br /> What am I doing wrong? Please let me know. Thanks<br /> <br /> Attachment 1: ES2_train_cam.sh</blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Roy Prouty<br /> DoIT Research Computing Team</p> "
3284012,72037028,Correspond,DoIT-Research-Computing,2025-10-01 14:27:45.0000000,HPC Slurm/Software Issue: sbatch job is not running,resolved,Tartela Tabassum,tartelt1,Zahid Hassan Tushar,ztushar1,ztushar1@umbc.edu,Tartela Tabassum,tartelt1@umbc.edu,"<p>Hello,</p>  <p>It seems like, from the cluster logs, your jobs <strong>did run,</strong>&nbsp;If it looked like nothing was happening on your side, the issue may be with what&rsquo;s happening <em>inside</em> the job (e.g., conda not activating or Python exiting early).</p>  <p>Could you share the <code>.err</code> file output and a screenshot of your <code>squeue</code> when jobs are queued/running? That will help me check what happened during execution.</p>  <p>Thank you.</p>  <p>Best,</p>  <p>Tartela</p>  <p>&nbsp;</p>  <p>&nbsp;</p>  <p>On Wed Oct 01 10:03:57 2025, ZZ99999 wrote:</p>  <blockquote>First Name: Zahid Hassan<br /> Last Name: Tushar<br /> Email: ztushar1@umbc.edu<br /> Campus ID: TS41250<br /> <br /> Request Type: High Performance Cluster<br /> <br /> <br /> Hi, I have scheduled jobs but they seem not to run. I have attached the bash file that I used for scheduling the job. Since it was not running, I cancelled them. [job id 101100, 100991,100993]. I activated the conda environment, and then scheduled the job like below.<br /> <br /> -&gt;conda activate co<br /> -&gt;sbatch ES2_train_cam.sh<br /> <br /> What am I doing wrong? Please let me know. Thanks<br /> <br /> Attachment 1: ES2_train_cam.sh</blockquote> "
3284012,72038282,Correspond,DoIT-Research-Computing,2025-10-01 15:04:45.0000000,HPC Slurm/Software Issue: sbatch job is not running,resolved,Tartela Tabassum,tartelt1,Zahid Hassan Tushar,ztushar1,ztushar1@umbc.edu,Zahid Hassan Tushar,ztushar1@umbc.edu,"Hi Tartela,  Thank you for  your quick response. I have print statements inside the python script. The python script did not run, therefore nothing was printed. I have attached the .err and .out file for your reference.  Thank you.  Best Zahid   On Wed, Oct 1, 2025 at 10:27=E2=80=AFAM Tartela Tabassum via RT < UMBCHelp@rt.umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3284012 > > > Last Update From Ticket: > > Hello, > > It seems like, from the cluster logs, your jobs did run, If it looked like > nothing was happening on your side, the issue may be with what=E2=80=99s = happening > inside the job (e.g., conda not activating or Python exiting early). > > Could you share the .err file output and a screenshot of your squeue when > jobs > are queued/running? That will help me check what happened during executio= n. > > Thank you. > > Best, > > Tartela > > On Wed Oct 01 10:03:57 2025, ZZ99999 wrote: > > > First Name: Zahid Hassan > > Last Name: Tushar > > Email: ztushar1@umbc.edu > > Campus ID: TS41250 > > > Request Type: High Performance Cluster > > > > Hi, I have scheduled jobs but they seem not to run. I have attached the > > bash file that I used for scheduling the job. Since it was not running,=  I > > cancelled them. [job id 101100, 100991,100993]. I activated the conda > > environment, and then scheduled the job like below. > > > ->conda activate co > > ->sbatch ES2_train_cam.sh > > > What am I doing wrong? Please let me know. Thanks > > > Attachment 1: ES2_train_cam.sh > > "
3284012,72038282,Correspond,DoIT-Research-Computing,2025-10-01 15:04:45.0000000,HPC Slurm/Software Issue: sbatch job is not running,resolved,Tartela Tabassum,tartelt1,Zahid Hassan Tushar,ztushar1,ztushar1@umbc.edu,Zahid Hassan Tushar,ztushar1@umbc.edu,"<div dir=3D""ltr""><div>Hi Tartela,</div><div><br></div><div>Thank you for=C2= =A0 your quick=C2=A0response. I have print statements inside the python scr= ipt. The python script did not run, therefore nothing was printed. I have a= ttached the .err and .out file for your reference.</div><div><br></div><div= >Thank you.=C2=A0</div><div><br></div><div><div dir=3D""ltr"" class=3D""gmail_= signature"" data-smartmail=3D""gmail_signature""><div dir=3D""ltr"">Best<div>Zah= id</div></div></div></div><br></div><br><div class=3D""gmail_quote gmail_quo= te_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Wed, Oct 1, 2025 at = 10:27=E2=80=AFAM Tartela Tabassum via RT &lt;<a href=3D""mailto:UMBCHelp@rt.= umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D= ""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(2= 04,204,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.ed= u/Ticket/Display.html?id=3D3284012"" rel=3D""noreferrer"" target=3D""_blank"">ht= tps://rt.umbc.edu/Ticket/Display.html?id=3D3284012</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hello,<br> <br> It seems like, from the cluster logs, your jobs did run, If it looked like<= br> nothing was happening on your side, the issue may be with what=E2=80=99s ha= ppening<br> inside the job (e.g., conda not activating or Python exiting early).<br> <br> Could you share the .err file output and a screenshot of your squeue when j= obs<br> are queued/running? That will help me check what happened during execution.= <br> <br> Thank you.<br> <br> Best,<br> <br> Tartela<br> <br> On Wed Oct 01 10:03:57 2025, ZZ99999 wrote:<br> <br> &gt; First Name: Zahid Hassan<br> &gt; Last Name: Tushar<br> &gt; Email: <a href=3D""mailto:ztushar1@umbc.edu"" target=3D""_blank"">ztushar1= @umbc.edu</a><br> &gt; Campus ID: TS41250<br> <br> &gt; Request Type: High Performance Cluster<br> <br> <br> &gt; Hi, I have scheduled jobs but they seem not to run. I have attached th= e<br> &gt; bash file that I used for scheduling the job. Since it was not running= , I<br> &gt; cancelled them. [job id 101100, 100991,100993]. I activated the conda<= br> &gt; environment, and then scheduled the job like below.<br> <br> &gt; -&gt;conda activate co<br> &gt; -&gt;sbatch ES2_train_cam.sh<br> <br> &gt; What am I doing wrong? Please let me know. Thanks<br> <br> &gt; Attachment 1: ES2_train_cam.sh<br> <br> </blockquote></div> "
3284012,72040820,Correspond,DoIT-Research-Computing,2025-10-01 16:06:41.0000000,HPC Slurm/Software Issue: sbatch job is not running,resolved,Tartela Tabassum,tartelt1,Zahid Hassan Tushar,ztushar1,ztushar1@umbc.edu,Tartela Tabassum,tartelt1@umbc.edu,"<p><span style=3D""color:#000000"">Hello,</span></p>  <p><span style=3D""color:#000000"">The issue seems to be with the way the con= da environment is being initialized inside your job script. You don&rsquo;t=  need to use <code>conda init</code>, and the <code>module load Anaconda3</= code> plus <code>eval &quot;$(conda shell.bash hook)&quot;</code> lines can=  remain but be commented out.</span></p>  <p><span style=3D""color:#000000"">Update your <code>ES2_train_cam.sh</code> = script so that only the following line is used for environment activation:<= /span></p>  <p><span style=3D""color:#000000""><code>conda activate /umbc/rs/psanjay/user= s/ztushar1/conda_envs/co</code></span></p>  <p><span style=3D""color:#000000""><code>If that doesn&#39;t resolve the issu= e, please feel free to schedule an office hour with us.</code>&nbsp;</span>= Here:&nbsp;Office Hours &ndash; High Performance Computing Facility &ndash;=  UMBC</p>  <p>Best,</p>  <p>Tartela</p>  <p>&nbsp;</p>  <p>On Wed Oct 01 11:04:45 2025, TS41250 wrote:</p>  <blockquote> <div> <div>Hi Tartela,</div>  <div>&nbsp;</div>  <div>Thank you for&nbsp; your quick&nbsp;response. I have print statements = inside the python script. The python script did not run, therefore nothing = was printed. I have attached the .err and .out file for your reference.</di= v>  <div>&nbsp;</div>  <div>Thank you.&nbsp;</div>  <div>&nbsp;</div>  <div> <div> <div>Best <div>Zahid</div> </div> </div> </div> </div> &nbsp;  <div> <div>On Wed, Oct 1, 2025 at 10:27=E2=80=AFAM Tartela Tabassum via RT &lt;UM= BCHelp@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 84012 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hello,<br /> <br /> It seems like, from the cluster logs, your jobs did run, If it looked like<= br /> nothing was happening on your side, the issue may be with what&rsquo;s happ= ening<br /> inside the job (e.g., conda not activating or Python exiting early).<br /> <br /> Could you share the .err file output and a screenshot of your squeue when j= obs<br /> are queued/running? That will help me check what happened during execution.= <br /> <br /> Thank you.<br /> <br /> Best,<br /> <br /> Tartela<br /> <br /> On Wed Oct 01 10:03:57 2025, ZZ99999 wrote:<br /> <br /> &gt; First Name: Zahid Hassan<br /> &gt; Last Name: Tushar<br /> &gt; Email: ztushar1@umbc.edu<br /> &gt; Campus ID: TS41250<br /> <br /> &gt; Request Type: High Performance Cluster<br /> <br /> <br /> &gt; Hi, I have scheduled jobs but they seem not to run. I have attached th= e<br /> &gt; bash file that I used for scheduling the job. Since it was not running= , I<br /> &gt; cancelled them. [job id 101100, 100991,100993]. I activated the conda<= br /> &gt; environment, and then scheduled the job like below.<br /> <br /> &gt; -&gt;conda activate co<br /> &gt; -&gt;sbatch ES2_train_cam.sh<br /> <br /> &gt; What am I doing wrong? Please let me know. Thanks<br /> <br /> &gt; Attachment 1: ES2_train_cam.sh<br /> &nbsp;</blockquote> </div> </blockquote> "
3284012,72080835,Correspond,DoIT-Research-Computing,2025-10-03 13:43:14.0000000,HPC Slurm/Software Issue: sbatch job is not running,resolved,Tartela Tabassum,tartelt1,Zahid Hassan Tushar,ztushar1,ztushar1@umbc.edu,Tartela Tabassum,tartelt1@umbc.edu,"<p>Hello,</p>  <p>I wanted to follow up to make sure everything is working as expected now. If so, I&rsquo;ll close out this ticket, but you can always reach back out if you need more help.</p>  <p>&nbsp;</p>  <p>&nbsp;</p>  <p>Best,</p>  <p>Tartela</p> "
3284012,72080904,Correspond,DoIT-Research-Computing,2025-10-03 13:44:54.0000000,HPC Slurm/Software Issue: sbatch job is not running,resolved,Tartela Tabassum,tartelt1,Zahid Hassan Tushar,ztushar1,ztushar1@umbc.edu,Zahid Hassan Tushar,ztushar1@umbc.edu,"Yes, it is working. Thank you.  Best Zahid  On Fri, Oct 3, 2025, 9:43=E2=80=AFAM Tartela Tabassum via RT <UMBCHelp@rt.u= mbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3284012 > > > Last Update From Ticket: > > Hello, > > I wanted to follow up to make sure everything is working as expected now. > If > so, I=E2=80=99ll close out this ticket, but you can always reach back out=  if you > need > more help. > > Best, > > Tartela > > "
3284012,72080904,Correspond,DoIT-Research-Computing,2025-10-03 13:44:54.0000000,HPC Slurm/Software Issue: sbatch job is not running,resolved,Tartela Tabassum,tartelt1,Zahid Hassan Tushar,ztushar1,ztushar1@umbc.edu,Zahid Hassan Tushar,ztushar1@umbc.edu,"<div dir=3D""auto"">Yes, it is working. Thank you.<br><br><div data-smartmail= =3D""gmail_signature""><div dir=3D""ltr"">Best<div>Zahid</div></div></div></div= ><br><div class=3D""gmail_quote gmail_quote_container""><div dir=3D""ltr"" clas= s=3D""gmail_attr"">On Fri, Oct 3, 2025, 9:43=E2=80=AFAM Tartela Tabassum via = RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt;=  wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""margin:0 0 0 .8= ex;border-left:1px #ccc solid;padding-left:1ex"">Ticket &lt;URL: <a href=3D""= https://rt.umbc.edu/Ticket/Display.html?id=3D3284012"" rel=3D""noreferrer nor= eferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D328= 4012</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hello,<br> <br> I wanted to follow up to make sure everything is working as expected now. I= f<br> so, I=E2=80=99ll close out this ticket, but you can always reach back out i= f you need<br> more help.<br> <br> Best,<br> <br> Tartela<br> <br> </blockquote></div> "
3284272,72044360,Create,DoIT-Research-Computing,2025-10-01 17:55:13.0000000,HPC User Account: samratb1 in Student Group,resolved,Roy Prouty,proutyr1,Samrat Bojanola,samratb1,samratb1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Samrat Last Name:                 Bojanola Email:                     samratb1@umbc.edu Campus ID:                 NP42296  Request Type:              High Performance Cluster  Create/Modify account in Student group  We are part of the Center for Research in Emergent Manufacturing (CREM) at = UMBC. Our group conducts research in digital manufacturing, Industry 4.0, o= perational technology (OT) cybersecurity, and AI/ML for engineering applica= tions.  We are requesting an HPCF GPU account to support experiments with large vis= ion=E2=80=93language models (e.g., InternVL, Qwen2-VL, LLaVA) image analysi= s and parameter extraction. These projects require access to multi-GPU node= s with sufficient VRAM, ideally on the Ada cluster (rtx_8000 nodes with 48 = GB GPUs) or the gpu2018 V100 partition, so that we can run inference and pe= rform LoRA fine-tuning on models in the 7B=E2=80=9326B parameter range.  This work directly supports CREM=E2=80=99s mission to advance digital engin= eering and cybersecurity training and research at UMBC, and contributes to = collaborations with DoD and industry partners. We will follow HPCF policies=  and are happy to coordinate resource usage schedules as needed.  We are requesting access to GPU resources to support model development and = training experiments that require high-memory multi-GPU nodes (e.g., Ada RT= X 8000 or gpu2018 V100).  We will comply with all HPCF usage policies and are happy to coordinate sch= eduling to make efficient use of the resources.  "
3284272,72057576,Correspond,DoIT-Research-Computing,2025-10-02 14:05:44.0000000,HPC User Account: samratb1 in Student Group,resolved,Roy Prouty,proutyr1,Samrat Bojanola,samratb1,samratb1@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hello there,</p>  <p>Of course, you can request an account, but to answer your questions:</p>  <p>We are no longer on the Ada cluster; we&#39;ve moved to the Chip cluster. Looking at the GPUs you want to use, the Chip cluster should be able to handle that well, but for more exact information on our cluster specifications, look here:</p>  <p>https://umbc.atlassian.net/wiki/spaces/faq/pages/1289486353/Cluster+Specifications</p>  <p>Given that you are part of CREM, I would recommend having another faculty member there sponsor your group on the cluster (i.e&nbsp;Nilanjan Banerjee, Director). This would make creating a group on the cluster much easier. Besides that, I will direct you to this page on how exactly to request a user/group account on the cluster:</p>  <p>https://umbc.atlassian.net/wiki/spaces/faq/pages/1327431728/How+to+request+a+user+group+account+on+chip</p>  <p>&nbsp;</p>  <p>Hope that helps,</p>  <p>Elliot Gobbert</p>  <p>&nbsp;</p> "
3284272,72061131,Correspond,DoIT-Research-Computing,2025-10-02 15:53:01.0000000,HPC User Account: samratb1 in Student Group,resolved,Roy Prouty,proutyr1,Samrat Bojanola,samratb1,samratb1@umbc.edu,Nilanjan Banerjee,nilanb@umbc.edu,"<div dir=3D""ltr""><div>Hi Elliot:</div><div><br></div><div>Yes Samrat is wor= king at CREM and thanks for getting him access.</div><div><br></div><div>I = was also wondering if there is any premium access (more storage, access to = bigger GPUs in the cluster) that CREM could get. Of course, the center is w= illing to pay for that access?</div><div><br></div><div>-Nilanjan</div><div= ><br></div></div><br><div class=3D""gmail_quote gmail_quote_container""><div = dir=3D""ltr"" class=3D""gmail_attr"">On Thu, Oct 2, 2025 at 10:05=E2=80=AFAM El= liot Gobbert via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt= .umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" style= =3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding= -left:1ex"">If you agree your issue is resolved, please give us feedback on = your experience by completing a brief satisfaction survey: <br> <br> <a href=3D""https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&amp;= customeremail=3Dsamratb1%40umbc.edu&amp;groupid=3DEIS&amp;ticketid=3D328427= 2&amp;ticketowner=3Delliotg2%40umbc.edu&amp;ticketsubject=3DHPC%20User%20Ac= count%3A%20samratb1%20in%20Student%20Group"" rel=3D""noreferrer"" target=3D""_b= lank"">https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&amp;custo= meremail=3Dsamratb1%40umbc.edu&amp;groupid=3DEIS&amp;ticketid=3D3284272&amp= ;ticketowner=3Delliotg2%40umbc.edu&amp;ticketsubject=3DHPC%20User%20Account= %3A%20samratb1%20in%20Student%20Group</a><br> <br> If you believe your issue has not been resolved, please respond to this mes= sage, which will reopen your ticket. Note: A full record of your request ca= n be found at:=C2=A0 <br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D328= 4272"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3284272</a> &gt;<br> <br> Thank You<br> <br> _________________________________________<br> <br> R e s o l u t i o n:<br> =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D= =C2=A0 <br> <br> Hello there,<br> <br> Of course, you can request an account, but to answer your questions:<br> <br> We are no longer on the Ada cluster; we&#39;ve moved to the Chip cluster. L= ooking<br> at the GPUs you want to use, the Chip cluster should be able to handle that= <br> well, but for more exact information on our cluster specifications, look he= re:<br> <br> <a href=3D""https://umbc.atlassian.net/wiki/spaces/faq/pages/1289486353/Clus= ter+Specifications"" rel=3D""noreferrer"" target=3D""_blank"">https://umbc.atlas= sian.net/wiki/spaces/faq/pages/1289486353/Cluster+Specifications</a><br> <br> Given that you are part of CREM, I would recommend having another faculty<b= r> member there sponsor your group on the cluster (i.e Nilanjan Banerjee,<br> Director). This would make creating a group on the cluster much easier. Bes= ides<br> that, I will direct you to this page on how exactly to request a user/group= <br> account on the cluster:<br> <br> <a href=3D""https://umbc.atlassian.net/wiki/spaces/faq/pages/1327431728/How+= to+request+a+user+group+account+on+chip"" rel=3D""noreferrer"" target=3D""_blan= k"">https://umbc.atlassian.net/wiki/spaces/faq/pages/1327431728/How+to+reque= st+a+user+group+account+on+chip</a><br> <br> Hope that helps,<br> <br> Elliot Gobbert<br> <br> <br> <br> ______________________________________<br> <br> Original Request:<br> <br> Requestors: Samrat Bojanola<br> <br> First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Samrat<b= r> Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0Boj= anola<br> Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=  =C2=A0<a href=3D""mailto:samratb1@umbc.edu"" target=3D""_blank"">samratb1@umbc= .edu</a><br> Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0NP4= 2296<br> <br> Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Performa= nce Cluster<br> <br> Create/Modify account in Student group<br> <br> We are part of the Center for Research in Emergent Manufacturing (CREM) at = UMBC. Our group conducts research in digital manufacturing, Industry 4.0, o= perational technology (OT) cybersecurity, and AI/ML for engineering applica= tions.<br> <br> We are requesting an HPCF GPU account to support experiments with large vis= ion=E2=80=93language models (e.g., InternVL, Qwen2-VL, LLaVA) image analysi= s and parameter extraction. These projects require access to multi-GPU node= s with sufficient VRAM, ideally on the Ada cluster (rtx_8000 nodes with 48 = GB GPUs) or the gpu2018 V100 partition, so that we can run inference and pe= rform LoRA fine-tuning on models in the 7B=E2=80=9326B parameter range.<br> <br> This work directly supports CREM=E2=80=99s mission to advance digital engin= eering and cybersecurity training and research at UMBC, and contributes to = collaborations with DoD and industry partners. We will follow HPCF policies=  and are happy to coordinate resource usage schedules as needed.<br> <br> We are requesting access to GPU resources to support model development and = training experiments that require high-memory multi-GPU nodes (e.g., Ada RT= X 8000 or gpu2018 V100).<br> <br> We will comply with all HPCF usage policies and are happy to coordinate sch= eduling to make efficient use of the resources.<br> <br> <br> </blockquote></div><div><br clear=3D""all""></div><br><span class=3D""gmail_si= gnature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_signature""><d= iv dir=3D""ltr""><div><div><div><div>Nilanjan Banerjee, Ph.D.<br></div>Profes= sor, Computer Science and Electrical Engineering,</div></div><div>Associate=  Director for Cybersecurity in Manufacturing</div><div>Director, Center for=  Research in Emergent Manufacturing<br></div>University of Maryland, Baltim= ore County<br></div>WWW: <a href=3D""http://www.csee.umbc.edu/~nilanb/"" targ= et=3D""_blank"">http://www.csee.umbc.edu/~nilanb/</a><br><div><img width=3D""2= 00"" height=3D""46"" src=3D""https://ci3.googleusercontent.com/mail-sig/AIorK4y= 3HTi6xEEnlZ9MozYXdykP1SXkVd3fMyNuO5FlrGjveqv1T8eU0HrEeqKP2vZGDteL01GX05GyUP= ZO""><br></div></div></div> "
3284272,72061131,Correspond,DoIT-Research-Computing,2025-10-02 15:53:01.0000000,HPC User Account: samratb1 in Student Group,resolved,Roy Prouty,proutyr1,Samrat Bojanola,samratb1,samratb1@umbc.edu,Nilanjan Banerjee,nilanb@umbc.edu,"Hi Elliot:  Yes Samrat is working at CREM and thanks for getting him access.  I was also wondering if there is any premium access (more storage, access to bigger GPUs in the cluster) that CREM could get. Of course, the center is willing to pay for that access?  -Nilanjan   On Thu, Oct 2, 2025 at 10:05=E2=80=AFAM Elliot Gobbert via RT <UMBCHelp@rt.= umbc.edu> wrote:  > If you agree your issue is resolved, please give us feedback on your > experience by completing a brief satisfaction survey: > > > https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&customeremail= =3Dsamratb1%40umbc.edu&groupid=3DEIS&ticketid=3D3284272&ticketowner=3Dellio= tg2%40umbc.edu&ticketsubject=3DHPC%20User%20Account%3A%20samratb1%20in%20St= udent%20Group > > If you believe your issue has not been resolved, please respond to this > message, which will reopen your ticket. Note: A full record of your reque= st > can be found at: > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3284272 > > > Thank You > > _________________________________________ > > R e s o l u t i o n: > =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D = =3D > > Hello there, > > Of course, you can request an account, but to answer your questions: > > We are no longer on the Ada cluster; we've moved to the Chip cluster. > Looking > at the GPUs you want to use, the Chip cluster should be able to handle th= at > well, but for more exact information on our cluster specifications, look > here: > > > https://umbc.atlassian.net/wiki/spaces/faq/pages/1289486353/Cluster+Speci= fications > > Given that you are part of CREM, I would recommend having another faculty > member there sponsor your group on the cluster (i.e Nilanjan Banerjee, > Director). This would make creating a group on the cluster much easier. > Besides > that, I will direct you to this page on how exactly to request a user/gro= up > account on the cluster: > > > https://umbc.atlassian.net/wiki/spaces/faq/pages/1327431728/How+to+reques= t+a+user+group+account+on+chip > > Hope that helps, > > Elliot Gobbert > > > > ______________________________________ > > Original Request: > > Requestors: Samrat Bojanola > > First Name:                Samrat > Last Name:                 Bojanola > Email:                     samratb1@umbc.edu > Campus ID:                 NP42296 > > Request Type:              High Performance Cluster > > Create/Modify account in Student group > > We are part of the Center for Research in Emergent Manufacturing (CREM) at > UMBC. Our group conducts research in digital manufacturing, Industry 4.0, > operational technology (OT) cybersecurity, and AI/ML for engineering > applications. > > We are requesting an HPCF GPU account to support experiments with large > vision=E2=80=93language models (e.g., InternVL, Qwen2-VL, LLaVA) image an= alysis and > parameter extraction. These projects require access to multi-GPU nodes wi= th > sufficient VRAM, ideally on the Ada cluster (rtx_8000 nodes with 48 GB > GPUs) or the gpu2018 V100 partition, so that we can run inference and > perform LoRA fine-tuning on models in the 7B=E2=80=9326B parameter range. > > This work directly supports CREM=E2=80=99s mission to advance digital eng= ineering > and cybersecurity training and research at UMBC, and contributes to > collaborations with DoD and industry partners. We will follow HPCF polici= es > and are happy to coordinate resource usage schedules as needed. > > We are requesting access to GPU resources to support model development and > training experiments that require high-memory multi-GPU nodes (e.g., Ada > RTX 8000 or gpu2018 V100). > > We will comply with all HPCF usage policies and are happy to coordinate > scheduling to make efficient use of the resources. > > >  --=20 Nilanjan Banerjee, Ph.D. Professor, Computer Science and Electrical Engineering, Associate Director for Cybersecurity in Manufacturing Director, Center for Research in Emergent Manufacturing University of Maryland, Baltimore County WWW: http://www.csee.umbc.edu/~nilanb/ "
3284272,72085635,Correspond,DoIT-Research-Computing,2025-10-03 16:05:31.0000000,HPC User Account: samratb1 in Student Group,resolved,Roy Prouty,proutyr1,Samrat Bojanola,samratb1,samratb1@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>The user account has been created:</p>  <p>Hi Samrat,</p>  <p>Your account (samratb1) has been created on chip.rs.umbc.edu.<br /> Your primary group is student.<br /> Your home directory has 500M of storage.<br /> You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip<br /> Please read through the documentation found at hpcf.umbc.edu &gt; User Support.<br /> All available modules can be viewed using the command &#39;module avail&#39;.<br /> Please submit additional questions or issues as separate tickets via the following link.<br /> (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/)<br /> <br /> Hi Samrat,<br /> Your user has been added to pi_nilanb as a secondary group.<br /> Your home directory has additional symbolic links to your group storage space.<br /> Please read through the documentation found at hpcf.umbc.edu.<br /> Please submit additional questions or issues as separate tickets via the following link.<br /> (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/)<br /> <br /> <br /> As you can see, I added&nbsp; Samrat to both the &quot;student&quot; group (That was the group mentioned in the original request), and also added him to pi_nilanb. Let me know if that was an error, and I can remove him from any unnecessary groups. For the &quot;premium access&quot; question, I&#39;ll forward you to Roy.</p>  <p>&nbsp;</p>  <p>Best,</p>  <p>Elliot Gobbert</p> "
3284322,72045592,Create,DoIT-Research-Computing,2025-10-01 18:24:20.0000000,HPC User Account: yliao1 in pi_kekechen,resolved,Elliot Gobbert,elliotg2,Yiming Liao,yliao1,yliao1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Yiming Last Name:                 Liao Email:                     yliao1@umbc.edu Campus ID:                 AU07819  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    kekechen@umbc.edu Existing Group:       pi_kekechen Project Title:        Medical LLM Hallucination Benchmark Project Abstract:     As large language models (LLMs) are increasingly appl= ied in clinical decision support and documentation tasks, their reliability=  becomes critical=E2=80=94particularly in avoiding medical hallucinations t= hat can mislead clinicians and harm patient care. This research aims to sys= tematically benchmark hallucination behavior in high-capacity LLMs using Eh= rNoteQA, a medically grounded evaluation dataset constructed from real-worl= d discharge summaries in MIMIC-IV. EhrNoteQA includes both open-ended and m= ultiple-choice clinical questions aligned with patient cases, allowing stru= ctured measurement of factual errors, reasoning inconsistencies, and alignm= ent failures. We investigate how hallucination patterns emerge when models = interpret real patient data, with outputs evaluated against clinical guidel= ines and human expert judgment. The use of 70B-parameter models is motivate= d by their increasingly prominent role in medical applications and the grow= ing assumption that scale leads to safer, more capable reasoning. However, = few studies have critically examined whether larger models reduce hallucina= tions or simply mask them with more fluent output. By stress-testing these = models on complex, high-stakes clinical QA tasks, our benchmark provides es= sential insights into the limits of LLM reliability and establishes a found= ation for safer, evidence-aligned AI deployment in healthcare.  Hi, I am a PhD student in Computer Science, and I am working on a new resea= rch project requiring running LLM models scaled up to 70B parameters (30 GB=  GRAM) with my advisor Dr. Keke Chen. Can you create a new account? User: Yiming Liao: yliao1@umbc.edu  "
3284322,72045784,Correspond,DoIT-Research-Computing,2025-10-01 18:29:06.0000000,HPC User Account: yliao1 in pi_kekechen,resolved,Elliot Gobbert,elliotg2,Yiming Liao,yliao1,yliao1@umbc.edu,Keke Chen,kekechen@umbc.edu,"I approve this request. Thanks.  Keke On Oct 1, 2025 at 2:24=E2=80=AFPM -0400, RT API via RT <UMBCHelp@rt.umbc.ed= u>, wrote: > This e-mail is a notification that a UMBC user: Yiming Liao <yliao1@umbc.= edu> has requested an account within UMBC's HPC environment in your group <= pi_kekechen>. As the PI, we request that you acknowledge and approve this a= ccount creation by replying to this message. Alternatively you can go to th= is link and review the ticket and indicate your decision here: > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3284322 > > > Once we have your approval, we will create the account and you and the ne= w user will receive another e-mail notifying you that the account has been = created. If you have any other questions or concerns please contact us. > > - UMBC DoIT Research Computing Support Staff "
3284322,72045784,Correspond,DoIT-Research-Computing,2025-10-01 18:29:06.0000000,HPC User Account: yliao1 in pi_kekechen,resolved,Elliot Gobbert,elliotg2,Yiming Liao,yliao1,yliao1@umbc.edu,Keke Chen,kekechen@umbc.edu,"<html xmlns=3D""http://www.w3.org/1999/xhtml""> <head> <title></title> </head> <body> <div name=3D""messageBodySection""> <div dir=3D""auto"">I approve this request. Thanks.<br /> <br /> Keke</div> </div> <div name=3D""messageReplySection"">On Oct 1, 2025 at 2:24=E2=80=AFPM -0400, = RT API via RT &lt;UMBCHelp@rt.umbc.edu&gt;, wrote:<br /> <blockquote type=3D""cite"" style=3D""border-left-color: grey; border-left-wid= th: thin; border-left-style: solid; margin: 5px 5px;padding-left: 10px;"">Th= is e-mail is a notification that a UMBC user: Yiming Liao &lt;yliao1@umbc.e= du&gt; has requested an account within UMBC's HPC environment in your group=  &lt;pi_kekechen&gt;. As the PI, we request that you acknowledge and approv= e this account creation by replying to this message. Alternatively you can = go to this link and review the ticket and indicate your decision here:<br /> <br /> Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3284322 &gt;<b= r /> <br /> Once we have your approval, we will create the account and you and the new = user will receive another e-mail notifying you that the account has been cr= eated. If you have any other questions or concerns please contact us.<br /> <br /> - UMBC DoIT Research Computing Support Staff<br /></blockquote> </div> </body> </html> "
3284322,72056251,Correspond,DoIT-Research-Computing,2025-10-02 13:28:43.0000000,HPC User Account: yliao1 in pi_kekechen,resolved,Elliot Gobbert,elliotg2,Yiming Liao,yliao1,yliao1@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Your account has been created:&nbsp;</p>  <p>&nbsp;</p>  <p>Hi Yiming,</p>  <p>Your account (yliao1) has been created on chip.rs.umbc.edu.<br /> Your primary group is pi_kekechen.<br /> Your home directory has 500M of storage.<br /> You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip<br /> Please read through the documentation found at hpcf.umbc.edu &gt; User Support.<br /> All available modules can be viewed using the command &#39;module avail&#39;.<br /> Please submit additional questions or issues as separate tickets via the following link.<br /> (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/)</p>  <p>&nbsp;</p>  <p>Let me know if you have any more questions,</p>  <p>Elliot Gobbert</p> "
3284639,72053522,Create,DoIT-Research-Computing,2025-10-02 11:49:47.0000000,HPC Other Issue: slurm mem error,resolved,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Sergio Last Name:                 De souza-machad Email:                     sergio@umbc.edu Campus ID:                 VR64161  Request Type:              High Performance Cluster  For some reason the last couple days I have been getting the following error on a script that (I think) has worked for ages on eg taki, now modified for chip  srun: fatal: SLURM_MEM_PER_CPU, SLURM_MEM_PER_GPU, and SLURM_MEM_PER_NODE are mutually exclusive.  And googling, I finally found a solution on   https://harvardmed.atlassian.net/wiki/spaces/O2/pages/1586793613/Troubleshooting+Slurm+Jobs  which says at the command line to first do unset SLURM_MEM_PER_CPU SLURM_MEM_PER_GPU SLURM_MEM_PER_NODE  Can you look at the following to see if there is a double call to srun or something?   /home/sergio/MATLABCODE/RTPMAKE/CLUST_RTPMAKE/CLUSTMAKE_ERA5/sergio_matlab_chip.sbatch   Thanks  Sergio   "
3284639,72054162,Comment,DoIT-Research-Computing,2025-10-02 12:27:27.0000000,HPC Other Issue: slurm mem error,resolved,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Line 45: `#SBATCH --mem-per-cpu=10000`</p>  <p>Line 55: `#SBATCH --mem-per-cpu=30000`</p>  <p>Can&#39;t have more one than one mem&nbsp; be called in the file.&nbsp;</p>  <p>On Thu Oct 02 07:49:47 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Sergio Last Name:                 De souza-machad Email:                     sergio@umbc.edu Campus ID:                 VR64161  Request Type:              High Performance Cluster  For some reason the last couple days I have been getting the following error on a script that (I think) has worked for ages on eg taki, now modified for chip  srun: fatal: SLURM_MEM_PER_CPU, SLURM_MEM_PER_GPU, and SLURM_MEM_PER_NODE are mutually exclusive.  And googling, I finally found a solution on   https://harvardmed.atlassian.net/wiki/spaces/O2/pages/1586793613/Troubleshooting+Slurm+Jobs  which says at the command line to first do unset SLURM_MEM_PER_CPU SLURM_MEM_PER_GPU SLURM_MEM_PER_NODE  Can you look at the following to see if there is a double call to srun or something?   /home/sergio/MATLABCODE/RTPMAKE/CLUST_RTPMAKE/CLUSTMAKE_ERA5/sergio_matlab_chip.sbatch   Thanks  Sergio   </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3284639,72054372,Comment,DoIT-Research-Computing,2025-10-02 12:33:58.0000000,HPC Other Issue: slurm mem error,resolved,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Ignore</p>  <p>On Thu Oct 02 08:27:27 2025, OL73413 wrote:</p>  <blockquote> <div> <p>Line 45: `#SBATCH --mem-per-cpu=10000`</p>  <p>Line 55: `#SBATCH --mem-per-cpu=30000`</p>  <p>Can&#39;t have more one than one mem&nbsp; be called in the file.&nbsp;</p>  <p>On Thu Oct 02 07:49:47 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Sergio Last Name:                 De souza-machad Email:                     sergio@umbc.edu Campus ID:                 VR64161  Request Type:              High Performance Cluster  For some reason the last couple days I have been getting the following error on a script that (I think) has worked for ages on eg taki, now modified for chip  srun: fatal: SLURM_MEM_PER_CPU, SLURM_MEM_PER_GPU, and SLURM_MEM_PER_NODE are mutually exclusive.  And googling, I finally found a solution on   https://harvardmed.atlassian.net/wiki/spaces/O2/pages/1586793613/Troubleshooting+Slurm+Jobs  which says at the command line to first do unset SLURM_MEM_PER_CPU SLURM_MEM_PER_GPU SLURM_MEM_PER_NODE  Can you look at the following to see if there is a double call to srun or something?   /home/sergio/MATLABCODE/RTPMAKE/CLUST_RTPMAKE/CLUSTMAKE_ERA5/sergio_matlab_chip.sbatch   Thanks  Sergio   </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3284639,72054793,Correspond,DoIT-Research-Computing,2025-10-02 12:45:20.0000000,HPC Other Issue: slurm mem error,resolved,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Sergio,</p>  <p>I added a .err and a .out file location to your sbatch to help with debugging a little bit. First, I must recommend cleaning up all the stray SBATCH directives. There are quite a few that will conflict with each other if you&#39;re not careful (see line 45 and 55 in your sbatch file). Second, I ran the sbatch file myself and didn&#39;t get the same error you did. Instead, I got</p>  <p>```</p>  <p>[sergio@chip-login1 CLUSTMAKE_ERA5]$ cat CLUST_MAKE_ERA_RTP-435429.err<br /> Unrecognized function or variable<br /> &#39;clustbatch_make_eracloudrtp_sergio_sarta_filelist&#39;.</p>  <p>```</p>  <p>Which seems to be missing some sort of command. When you run this file are you doing it from the login node? Do you normally have any modules loaded?&nbsp;</p>  <p>On Thu Oct 02 07:49:47 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Sergio Last Name:                 De souza-machad Email:                     sergio@umbc.edu Campus ID:                 VR64161  Request Type:              High Performance Cluster  For some reason the last couple days I have been getting the following error on a script that (I think) has worked for ages on eg taki, now modified for chip  srun: fatal: SLURM_MEM_PER_CPU, SLURM_MEM_PER_GPU, and SLURM_MEM_PER_NODE are mutually exclusive.  And googling, I finally found a solution on   https://harvardmed.atlassian.net/wiki/spaces/O2/pages/1586793613/Troubleshooting+Slurm+Jobs  which says at the command line to first do unset SLURM_MEM_PER_CPU SLURM_MEM_PER_GPU SLURM_MEM_PER_NODE  Can you look at the following to see if there is a double call to srun or something?   /home/sergio/MATLABCODE/RTPMAKE/CLUST_RTPMAKE/CLUSTMAKE_ERA5/sergio_matlab_chip.sbatch   Thanks  Sergio   </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3284639,72056196,Correspond,DoIT-Research-Computing,2025-10-02 13:27:50.0000000,HPC Other Issue: slurm mem error,resolved,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"Hi Max,  Try it with argument 10 or with argument 4 eg  sbatch  --array=3D241-276  sergio_matlab_chip.sbatch 10  The problem is intermittent ie does not happen each time I submit a job. Looking backwards at history,I believe it happened last night with job 435223  -Sergio  On Thu, Oct 2, 2025 at 8:45=E2=80=AFAM Max Breitmeyer via RT <UMBCHelp@rt.u= mbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3284639 > > > Last Update From Ticket: > > Hi Sergio, > > I added a .err and a .out file location to your sbatch to help with > debugging a > little bit. First, I must recommend cleaning up all the stray SBATCH > directives. There are quite a few that will conflict with each other if > you're > not careful (see line 45 and 55 in your sbatch file). Second, I ran the > sbatch > file myself and didn't get the same error you did. Instead, I got > > ``` > > [sergio@chip-login1 CLUSTMAKE_ERA5]$ cat CLUST_MAKE_ERA_RTP-435429.err > Unrecognized function or variable > 'clustbatch_make_eracloudrtp_sergio_sarta_filelist'. > > ``` > > Which seems to be missing some sort of command. When you run this file are > you > doing it from the login node? Do you normally have any modules loaded? > > On Thu Oct 02 07:49:47 2025, ZZ99999 wrote: > > > First Name:                Sergio > > Last Name:                 De souza-machad > > Email:                     sergio@umbc.edu > > Campus ID:                 VR64161 > > > > Request Type:              High Performance Cluster > > > > For some reason the last couple days I have been getting the following > error on a script that (I think) has worked for ages on eg taki, now > modified for chip > > > > srun: fatal: SLURM_MEM_PER_CPU, SLURM_MEM_PER_GPU, and > SLURM_MEM_PER_NODE are mutually exclusive. > > > > And googling, I finally found a solution on > >> > https://harvardmed.atlassian.net/wiki/spaces/O2/pages/1586793613/Troubles= hooting+Slurm+Jobs > > > > which says at the command line to first do > > unset SLURM_MEM_PER_CPU SLURM_MEM_PER_GPU SLURM_MEM_PER_NODE > > > > Can you look at the following to see if there is a double call to srun > or something? > >> > /home/sergio/MATLABCODE/RTPMAKE/CLUST_RTPMAKE/CLUSTMAKE_ERA5/sergio_matla= b_chip.sbatch > > > > > Thanks > > > > Sergio > > > > > -- > > Best, > Max Breitmeyer > DOIT HPC System Administrator > >  --=20 ---------------------------------------------------------------------------= ------------------------------------------ Sergio DeSouza-Machado sergio@umbc.edu Research Assoc. Professor,                                              (W) 410-455-1944 JCET/Dept of Physics (F) 410-455-1072 UMBC, Baltimore MD 21250 "
3284639,72056196,Correspond,DoIT-Research-Computing,2025-10-02 13:27:50.0000000,HPC Other Issue: slurm mem error,resolved,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"<div dir=3D""ltr"">Hi Max,<br><br>Try it with argument 10 or with argument 4 = eg<br><br>sbatch =C2=A0--array=3D241-276 =C2=A0sergio_matlab_chip.sbatch 10= <br><br>The problem is intermittent=C2=A0ie does not happen each time I sub= mit a job. Looking backwards at history,I believe it happened last night wi= th job 435223<div><br></div><div>-Sergio</div></div><br><div class=3D""gmail= _quote gmail_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Thu,=  Oct 2, 2025 at 8:45=E2=80=AFAM Max Breitmeyer via RT &lt;<a href=3D""mailto= :UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockq= uote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1p= x solid rgb(204,204,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https= ://rt.umbc.edu/Ticket/Display.html?id=3D3284639"" rel=3D""noreferrer"" target= =3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3284639</a> &gt;<b= r> <br> Last Update From Ticket:<br> <br> Hi Sergio,<br> <br> I added a .err and a .out file location to your sbatch to help with debuggi= ng a<br> little bit. First, I must recommend cleaning up all the stray SBATCH<br> directives. There are quite a few that will conflict with each other if you= &#39;re<br> not careful (see line 45 and 55 in your sbatch file). Second, I ran the sba= tch<br> file myself and didn&#39;t get the same error you did. Instead, I got<br> <br> ```<br> <br> [sergio@chip-login1 CLUSTMAKE_ERA5]$ cat CLUST_MAKE_ERA_RTP-435429.err<br> Unrecognized function or variable<br> &#39;clustbatch_make_eracloudrtp_sergio_sarta_filelist&#39;.<br> <br> ```<br> <br> Which seems to be missing some sort of command. When you run this file are = you<br> doing it from the login node? Do you normally have any modules loaded?<br> <br> On Thu Oct 02 07:49:47 2025, ZZ99999 wrote:<br> <br> &gt; First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Ser= gio<br> &gt; Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0De souza-machad<br> &gt; Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 =C2=A0<a href=3D""mailto:sergio@umbc.edu"" target=3D""_blank"">sergio@um= bc.edu</a><br> &gt; Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0VR64161<br> &gt; <br> &gt; Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Per= formance Cluster<br> &gt; <br> &gt; For some reason the last couple days I have been getting the following=  error on a script that (I think) has worked for ages on eg taki, now modif= ied for chip<br> &gt; <br> &gt; srun: fatal: SLURM_MEM_PER_CPU, SLURM_MEM_PER_GPU, and SLURM_MEM_PER_N= ODE are mutually exclusive.<br> &gt; <br> &gt; And googling, I finally found a solution on<br> &gt;&gt; <a href=3D""https://harvardmed.atlassian.net/wiki/spaces/O2/pages/1= 586793613/Troubleshooting+Slurm+Jobs"" rel=3D""noreferrer"" target=3D""_blank"">= https://harvardmed.atlassian.net/wiki/spaces/O2/pages/1586793613/Troublesho= oting+Slurm+Jobs</a><br> &gt; <br> &gt; which says at the command line to first do<br> &gt; unset SLURM_MEM_PER_CPU SLURM_MEM_PER_GPU SLURM_MEM_PER_NODE<br> &gt; <br> &gt; Can you look at the following to see if there is a double call to srun=  or something?<br> &gt;&gt; /home/sergio/MATLABCODE/RTPMAKE/CLUST_RTPMAKE/CLUSTMAKE_ERA5/sergi= o_matlab_chip.sbatch <br> &gt; <br> &gt; Thanks<br> &gt; <br> &gt; Sergio<br> &gt; <br> <br> <br> --<br> <br> Best,<br> Max Breitmeyer<br> DOIT HPC System Administrator<br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><div><div dir=3D""ltr""><div><div dir=3D""ltr""><div= >--------------------------------------------------------------------------= -------------------------------------------<br>Sergio DeSouza-Machado=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 <a href=3D""mailto:sergio@umbc.edu"" target= =3D""_blank"">sergio@umbc.edu</a><br>Research Assoc. Professor,=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 (W) 410-455-1944<br>JCET/Dept of Physics= =C2=A0=C2=A0 =C2=A0=C2=A0 =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0 (F) 410-455-1072<br></div><div>UMBC, Baltimore MD 21250<br>= </div></div></div></div></div></div></div> "
3284639,72139518,Correspond,DoIT-Research-Computing,2025-10-07 15:45:04.0000000,HPC Other Issue: slurm mem error,resolved,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Sergio</p>  <p>I&#39;ve tried this a few times and am not able to replicate the issue y= ou&#39;re having. My first thought was that something was getting set in yo= ur files and then not being unset in future runs on the same terminal, but = I&#39;ve tried a few times and am unable to replicate. My runs I did today = can be found in the output of&nbsp;CLUST_MAKE_ERA_RTP-442284.out and&nbsp;C= LUST_MAKE_ERA_RTP-442245.out. Without being able to replicate the issue the= re&#39;s not much else I can do to debug, as I&#39;ve looked through your f= ile and didn&#39;t see anything that stood out to me. I would advise removi= ng as many of the commented SBATCH directives as possible as it could be re= ading them in funny unexpected ways, but as a policy, we don&#39;t make cha= nges to people&#39;s files without their presence so they can confirm the c= hanges that are being made. If you see this pop up again, feel free to reop= en this ticket, but for now I&#39;ll going to close it.&nbsp;</p>  <p>&nbsp;</p>  <p>On Thu Oct 02 09:27:50 2025, VR64161 wrote:</p>  <blockquote> <div>Hi Max,<br /> <br /> Try it with argument 10 or with argument 4 eg<br /> <br /> sbatch &nbsp;--array=3D241-276 &nbsp;sergio_matlab_chip.sbatch 10<br /> <br /> The problem is intermittent&nbsp;ie does not happen each time I submit a jo= b. Looking backwards at history,I believe it happened last night with job 4= 35223 <div>&nbsp;</div>  <div>-Sergio</div> </div> &nbsp;  <div> <div>On Thu, Oct 2, 2025 at 8:45=E2=80=AFAM Max Breitmeyer via RT &lt;UMBCH= elp@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 84639 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hi Sergio,<br /> <br /> I added a .err and a .out file location to your sbatch to help with debuggi= ng a<br /> little bit. First, I must recommend cleaning up all the stray SBATCH<br /> directives. There are quite a few that will conflict with each other if you= &#39;re<br /> not careful (see line 45 and 55 in your sbatch file). Second, I ran the sba= tch<br /> file myself and didn&#39;t get the same error you did. Instead, I got<br /> <br /> ```<br /> <br /> [sergio@chip-login1 CLUSTMAKE_ERA5]$ cat CLUST_MAKE_ERA_RTP-435429.err<br /> Unrecognized function or variable<br /> &#39;clustbatch_make_eracloudrtp_sergio_sarta_filelist&#39;.<br /> <br /> ```<br /> <br /> Which seems to be missing some sort of command. When you run this file are = you<br /> doing it from the login node? Do you normally have any modules loaded?<br /> <br /> On Thu Oct 02 07:49:47 2025, ZZ99999 wrote:<br /> <br /> &gt; First Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ser= gio<br /> &gt; Last Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p;De souza-machad<br /> &gt; Email:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &= nbsp; &nbsp;sergio@umbc.edu<br /> &gt; Campus ID:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p;VR64161<br /> &gt;<br /> &gt; Request Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; High Per= formance Cluster<br /> &gt;<br /> &gt; For some reason the last couple days I have been getting the following=  error on a script that (I think) has worked for ages on eg taki, now modif= ied for chip<br /> &gt;<br /> &gt; srun: fatal: SLURM_MEM_PER_CPU, SLURM_MEM_PER_GPU, and SLURM_MEM_PER_N= ODE are mutually exclusive.<br /> &gt;<br /> &gt; And googling, I finally found a solution on<br /> &gt;&gt; https://harvardmed.atlassian.net/wiki/spaces/O2/pages/1586793613/T= roubleshooting+Slurm+Jobs<br /> &gt;<br /> &gt; which says at the command line to first do<br /> &gt; unset SLURM_MEM_PER_CPU SLURM_MEM_PER_GPU SLURM_MEM_PER_NODE<br /> &gt;<br /> &gt; Can you look at the following to see if there is a double call to srun=  or something?<br /> &gt;&gt; /home/sergio/MATLABCODE/RTPMAKE/CLUST_RTPMAKE/CLUSTMAKE_ERA5/sergi= o_matlab_chip.sbatch<br /> &gt;<br /> &gt; Thanks<br /> &gt;<br /> &gt; Sergio<br /> &gt;<br /> <br /> <br /> --<br /> <br /> Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator<br /> &nbsp;</blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --  <div> <div> <div> <div> <div> <div> <div>----------------------------------------------------------------------= -----------------------------------------------<br /> Sergio DeSouza-Machado&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sergio@umbc.e= du<br /> Research Assoc. Professor,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&= nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbs= p;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&= nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (W)=  410-455-1944<br /> JCET/Dept of Physics&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (F) 410-455-1072</div>  <div>UMBC, Baltimore MD 21250</div> </div> </div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3284639,72139665,Correspond,DoIT-Research-Computing,2025-10-07 15:49:45.0000000,HPC Other Issue: slurm mem error,resolved,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"Hi Max  OK I'll keep an eye out. The job asked for 240 processors, which had issues when I started them out but then mafically worked fine after doing the   unset SLURM_MEM_PER_CPU SLURM_MEM_PER_GPU SLURM_MEM_PER_NODE Last two days I started jobs which asked for 64 cpus, all ran fine! SO like you, I have no idea why this happened.  Cheers  Sergio  On Tue, Oct 7, 2025 at 11:45=E2=80=AFAM Max Breitmeyer via RT <UMBCHelp@rt.= umbc.edu> wrote:  > If you agree your issue is resolved, please give us feedback on your > experience by completing a brief satisfaction survey: > > > https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&customeremail= =3Dsergio%40umbc.edu&groupid=3DEIS&ticketid=3D3284639&ticketowner=3Dmb17%40= umbc.edu&ticketsubject=3DHPC%20Other%20Issue%3A%20slurm%20mem%20error > > If you believe your issue has not been resolved, please respond to this > message, which will reopen your ticket. Note: A full record of your reque= st > can be found at: > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3284639 > > > Thank You > > _________________________________________ > > R e s o l u t i o n: > =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D = =3D > > Hi Sergio > > I've tried this a few times and am not able to replicate the issue you're > having. My first thought was that something was getting set in your files > and > then not being unset in future runs on the same terminal, but I've tried a > few > times and am unable to replicate. My runs I did today can be found in the > output of CLUST_MAKE_ERA_RTP-442284.out and CLUST_MAKE_ERA_RTP-442245.out. > Without being able to replicate the issue there's not much else I can do = to > debug, as I've looked through your file and didn't see anything that stood > out > to me. I would advise removing as many of the commented SBATCH directives > as > possible as it could be reading them in funny unexpected ways, but as a > policy, > we don't make changes to people's files without their presence so they can > confirm the changes that are being made. If you see this pop up again, fe= el > free to reopen this ticket, but for now I'll going to close it. > > On Thu Oct 02 09:27:50 2025, VR64161 wrote: > > > Hi Max, > > > Try it with argument 10 or with argument 4 eg > > > sbatch --array=3D241-276 sergio_matlab_chip.sbatch 10 > > > The problem is intermittent ie does not happen each time I submit a job. > > Looking backwards at history,I believe it happened last night with job > > 435223 -Sergio On Thu, Oct 2, 2025 at 8:45 AM Max Breitmeyer via RT > > <UMBCHelp@rt.umbc.edu> wrote: > > >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3284639 > > > >> Last Update From Ticket: > > >> Hi Sergio, > > >> I added a .err and a .out file location to your sbatch to help with > >> debugging a > >> little bit. First, I must recommend cleaning up all the stray SBATCH > >> directives. There are quite a few that will conflict with each other if > >> you're > >> not careful (see line 45 and 55 in your sbatch file). Second, I ran the > >> sbatch > >> file myself and didn't get the same error you did. Instead, I got > > >> ``` > > >> [sergio@chip-login1 CLUSTMAKE_ERA5]$ cat CLUST_MAKE_ERA_RTP-435429.err > >> Unrecognized function or variable > >> 'clustbatch_make_eracloudrtp_sergio_sarta_filelist'. > > >> ``` > > >> Which seems to be missing some sort of command. When you run this file > >> are you > >> doing it from the login node? Do you normally have any modules loaded? > > >> On Thu Oct 02 07:49:47 2025, ZZ99999 wrote: > > >> > First Name: Sergio > >> > Last Name: De souza-machad > >> > Email: sergio@umbc.edu > >> > Campus ID: VR64161 > >> > > >> > Request Type: High Performance Cluster > >> > > >> > For some reason the last couple days I have been getting the > >> following error on a script that (I think) has worked for ages on eg > >> taki, now modified for chip > >> > > >> > srun: fatal: SLURM_MEM_PER_CPU, SLURM_MEM_PER_GPU, and > >> SLURM_MEM_PER_NODE are mutually exclusive. > >> > > >> > And googling, I finally found a solution on > >> >> > >> > https://harvardmed.atlassian.net/wiki/spaces/O2/pages/1586793613/Troubles= hooting+Slurm+Jobs > >> > > >> > which says at the command line to first do > >> > unset SLURM_MEM_PER_CPU SLURM_MEM_PER_GPU SLURM_MEM_PER_NODE > >> > > >> > Can you look at the following to see if there is a double call to > >> srun or something? > >> >> > >> > /home/sergio/MATLABCODE/RTPMAKE/CLUST_RTPMAKE/CLUSTMAKE_ERA5/sergio_matla= b_chip.sbatch > >> > > >> > Thanks > >> > > >> > Sergio > >> > > > > >> -- > > >> Best, > >> Max Breitmeyer > >> DOIT HPC System Administrator > > > -- > > > -------------------------------------------------------------------------= -------------------------------------------- > > Sergio DeSouza-Machado sergio@umbc.edu > > Research Assoc. Professor, (W) 410-455-1944 > > JCET/Dept of Physics (F) 410-455-1072UMBC, Baltimore MD 21250 > > -- > > Best, > Max Breitmeyer > DOIT HPC System Administrator > > > > ______________________________________ > > Original Request: > > Requestors: Sergio De souza-machad > > First Name:                Sergio > Last Name:                 De souza-machad > Email:                     sergio@umbc.edu > Campus ID:                 VR64161 > > Request Type:              High Performance Cluster > > For some reason the last couple days I have been getting the following > error on a script that (I think) has worked for ages on eg taki, now > modified for chip > > srun: fatal: SLURM_MEM_PER_CPU, SLURM_MEM_PER_GPU, and SLURM_MEM_PER_NODE > are mutually exclusive. > > And googling, I finally found a solution on > > https://harvardmed.atlassian.net/wiki/spaces/O2/pages/1586793613/Troubles= hooting+Slurm+Jobs > > which says at the command line to first do > unset SLURM_MEM_PER_CPU SLURM_MEM_PER_GPU SLURM_MEM_PER_NODE > > Can you look at the following to see if there is a double call to srun or > something? > > /home/sergio/MATLABCODE/RTPMAKE/CLUST_RTPMAKE/CLUSTMAKE_ERA5/sergio_matla= b_chip.sbatch > > > Thanks > > Sergio > > > >  --=20 ---------------------------------------------------------------------------= ------------------------------------------ Sergio DeSouza-Machado sergio@umbc.edu Research Assoc. Professor,                                              (W) 410-455-1944 JCET/Dept of Physics (F) 410-455-1072 UMBC, Baltimore MD 21250 "
3284639,72139665,Correspond,DoIT-Research-Computing,2025-10-07 15:49:45.0000000,HPC Other Issue: slurm mem error,resolved,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"<div dir=3D""ltr"">Hi Max<div><br></div><div>OK I&#39;ll keep an eye out. The=  job asked for 240 processors, which had issues when I started them out but=  then mafically worked fine after doing the <br>=C2=A0 unset SLURM_MEM_PER_= CPU SLURM_MEM_PER_GPU SLURM_MEM_PER_NODE<br>Last two days I started jobs wh= ich asked for 64 cpus, all ran fine! SO like you, I have no idea why this h= appened.</div><div><br></div><div>Cheers</div><div><br></div><div>Sergio</d= iv></div><br><div class=3D""gmail_quote gmail_quote_container""><div dir=3D""l= tr"" class=3D""gmail_attr"">On Tue, Oct 7, 2025 at 11:45=E2=80=AFAM Max Breitm= eyer via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.ed= u</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""margin= :0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex""= >If you agree your issue is resolved, please give us feedback on your exper= ience by completing a brief satisfaction survey: <br> <br> <a href=3D""https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&amp;= customeremail=3Dsergio%40umbc.edu&amp;groupid=3DEIS&amp;ticketid=3D3284639&= amp;ticketowner=3Dmb17%40umbc.edu&amp;ticketsubject=3DHPC%20Other%20Issue%3= A%20slurm%20mem%20error"" rel=3D""noreferrer"" target=3D""_blank"">https://umbc.= us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&amp;customeremail=3Dsergio%4= 0umbc.edu&amp;groupid=3DEIS&amp;ticketid=3D3284639&amp;ticketowner=3Dmb17%4= 0umbc.edu&amp;ticketsubject=3DHPC%20Other%20Issue%3A%20slurm%20mem%20error<= /a><br> <br> If you believe your issue has not been resolved, please respond to this mes= sage, which will reopen your ticket. Note: A full record of your request ca= n be found at:=C2=A0 <br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D328= 4639"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3284639</a> &gt;<br> <br> Thank You<br> <br> _________________________________________<br> <br> R e s o l u t i o n:<br> =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D= =C2=A0 <br> <br> Hi Sergio<br> <br> I&#39;ve tried this a few times and am not able to replicate the issue you&= #39;re<br> having. My first thought was that something was getting set in your files a= nd<br> then not being unset in future runs on the same terminal, but I&#39;ve trie= d a few<br> times and am unable to replicate. My runs I did today can be found in the<b= r> output of CLUST_MAKE_ERA_RTP-442284.out and CLUST_MAKE_ERA_RTP-442245.out.<= br> Without being able to replicate the issue there&#39;s not much else I can d= o to<br> debug, as I&#39;ve looked through your file and didn&#39;t see anything tha= t stood out<br> to me. I would advise removing as many of the commented SBATCH directives a= s<br> possible as it could be reading them in funny unexpected ways, but as a pol= icy,<br> we don&#39;t make changes to people&#39;s files without their presence so t= hey can<br> confirm the changes that are being made. If you see this pop up again, feel= <br> free to reopen this ticket, but for now I&#39;ll going to close it.<br> <br> On Thu Oct 02 09:27:50 2025, VR64161 wrote:<br> <br> &gt; Hi Max,<br> <br> &gt; Try it with argument 10 or with argument 4 eg<br> <br> &gt; sbatch --array=3D241-276 sergio_matlab_chip.sbatch 10<br> <br> &gt; The problem is intermittent ie does not happen each time I submit a jo= b.<br> &gt; Looking backwards at history,I believe it happened last night with job= <br> &gt; 435223 -Sergio On Thu, Oct 2, 2025 at 8:45 AM Max Breitmeyer via RT<br> &gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp= @rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html= ?id=3D3284639"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tic= ket/Display.html?id=3D3284639</a> &gt;<br> <br> &gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt; Hi Sergio,<br> <br> &gt;&gt; I added a .err and a .out file location to your sbatch to help wit= h<br> &gt;&gt; debugging a<br> &gt;&gt; little bit. First, I must recommend cleaning up all the stray SBAT= CH<br> &gt;&gt; directives. There are quite a few that will conflict with each oth= er if<br> &gt;&gt; you&#39;re<br> &gt;&gt; not careful (see line 45 and 55 in your sbatch file). Second, I ra= n the<br> &gt;&gt; sbatch<br> &gt;&gt; file myself and didn&#39;t get the same error you did. Instead, I = got<br> <br> &gt;&gt; ```<br> <br> &gt;&gt; [sergio@chip-login1 CLUSTMAKE_ERA5]$ cat CLUST_MAKE_ERA_RTP-435429= .err<br> &gt;&gt; Unrecognized function or variable<br> &gt;&gt; &#39;clustbatch_make_eracloudrtp_sergio_sarta_filelist&#39;.<br> <br> &gt;&gt; ```<br> <br> &gt;&gt; Which seems to be missing some sort of command. When you run this = file<br> &gt;&gt; are you<br> &gt;&gt; doing it from the login node? Do you normally have any modules loa= ded?<br> <br> &gt;&gt; On Thu Oct 02 07:49:47 2025, ZZ99999 wrote:<br> <br> &gt;&gt; &gt; First Name: Sergio<br> &gt;&gt; &gt; Last Name: De souza-machad<br> &gt;&gt; &gt; Email: <a href=3D""mailto:sergio@umbc.edu"" target=3D""_blank"">s= ergio@umbc.edu</a><br> &gt;&gt; &gt; Campus ID: VR64161<br> &gt;&gt; &gt;<br> &gt;&gt; &gt; Request Type: High Performance Cluster<br> &gt;&gt; &gt;<br> &gt;&gt; &gt; For some reason the last couple days I have been getting the<= br> &gt;&gt; following error on a script that (I think) has worked for ages on = eg<br> &gt;&gt; taki, now modified for chip<br> &gt;&gt; &gt;<br> &gt;&gt; &gt; srun: fatal: SLURM_MEM_PER_CPU, SLURM_MEM_PER_GPU, and<br> &gt;&gt; SLURM_MEM_PER_NODE are mutually exclusive.<br> &gt;&gt; &gt;<br> &gt;&gt; &gt; And googling, I finally found a solution on<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; <a href=3D""https://harvardmed.atlassian.net/wiki/spaces/O2/pages/1= 586793613/Troubleshooting+Slurm+Jobs"" rel=3D""noreferrer"" target=3D""_blank"">= https://harvardmed.atlassian.net/wiki/spaces/O2/pages/1586793613/Troublesho= oting+Slurm+Jobs</a><br> &gt;&gt; &gt;<br> &gt;&gt; &gt; which says at the command line to first do<br> &gt;&gt; &gt; unset SLURM_MEM_PER_CPU SLURM_MEM_PER_GPU SLURM_MEM_PER_NODE<= br> &gt;&gt; &gt;<br> &gt;&gt; &gt; Can you look at the following to see if there is a double cal= l to<br> &gt;&gt; srun or something?<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; /home/sergio/MATLABCODE/RTPMAKE/CLUST_RTPMAKE/CLUSTMAKE_ERA5/sergi= o_matlab_chip.sbatch<br> &gt;&gt; &gt;<br> &gt;&gt; &gt; Thanks<br> &gt;&gt; &gt;<br> &gt;&gt; &gt; Sergio<br> &gt;&gt; &gt;<br> <br> <br> &gt;&gt; --<br> <br> &gt;&gt; Best,<br> &gt;&gt; Max Breitmeyer<br> &gt;&gt; DOIT HPC System Administrator<br> <br> &gt; --<br> &gt; ----------------------------------------------------------------------= -----------------------------------------------<br> &gt; Sergio DeSouza-Machado <a href=3D""mailto:sergio@umbc.edu"" target=3D""_b= lank"">sergio@umbc.edu</a><br> &gt; Research Assoc. Professor, (W) 410-455-1944<br> &gt; JCET/Dept of Physics (F) 410-455-1072UMBC, Baltimore MD 21250<br> <br> --<br> <br> Best,<br> Max Breitmeyer<br> DOIT HPC System Administrator<br> <br> <br> <br> ______________________________________<br> <br> Original Request:<br> <br> Requestors: Sergio De souza-machad<br> <br> First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Sergio<b= r> Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0De = souza-machad<br> Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=  =C2=A0<a href=3D""mailto:sergio@umbc.edu"" target=3D""_blank"">sergio@umbc.edu= </a><br> Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0VR6= 4161<br> <br> Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Performa= nce Cluster<br> <br> For some reason the last couple days I have been getting the following erro= r on a script that (I think) has worked for ages on eg taki, now modified f= or chip<br> <br> srun: fatal: SLURM_MEM_PER_CPU, SLURM_MEM_PER_GPU, and SLURM_MEM_PER_NODE a= re mutually exclusive.<br> <br> And googling, I finally found a solution on<br> =C2=A0 <a href=3D""https://harvardmed.atlassian.net/wiki/spaces/O2/pages/158= 6793613/Troubleshooting+Slurm+Jobs"" rel=3D""noreferrer"" target=3D""_blank"">ht= tps://harvardmed.atlassian.net/wiki/spaces/O2/pages/1586793613/Troubleshoot= ing+Slurm+Jobs</a><br> <br> which says at the command line to first do<br> unset SLURM_MEM_PER_CPU SLURM_MEM_PER_GPU SLURM_MEM_PER_NODE<br> <br> Can you look at the following to see if there is a double call to srun or s= omething?<br> =C2=A0 /home/sergio/MATLABCODE/RTPMAKE/CLUST_RTPMAKE/CLUSTMAKE_ERA5/sergio_= matlab_chip.sbatch <br> <br> Thanks<br> <br> Sergio<br> <br> <br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><div><div dir=3D""ltr""><div><div dir=3D""ltr""><div= >--------------------------------------------------------------------------= -------------------------------------------<br>Sergio DeSouza-Machado=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 <a href=3D""mailto:sergio@umbc.edu"" target= =3D""_blank"">sergio@umbc.edu</a><br>Research Assoc. Professor,=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 (W) 410-455-1944<br>JCET/Dept of Physics= =C2=A0=C2=A0 =C2=A0=C2=A0 =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0 (F) 410-455-1072<br></div><div>UMBC, Baltimore MD 21250<br>= </div></div></div></div></div></div></div> "
3284658,72054571,Create,DoIT-Research-Computing,2025-10-02 12:39:49.0000000,Assistance using UMBC HPCF Cluster to run LS Dyna simulations,resolved,Max Breitmeyer,mb17,Riley Digennaro,av23132,av23132@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Riley<br /> Last Name:                 Di Gennaro<br /> Email:                     av23132@umbc.edu<br /> Campus ID:                 AV23132<br /> <br /> Cc:                        oajimat1@umbc.edu, stephas1@umbc.edu, pvonlock@umbc.edu<br /> <br /> Hello,  <br />  <br /> I'm a student currently taking ENME 444 Capstone and I am using Ansys and LS Dyna to run simulations of a car crash. The simulations are too large for the student version of the software and also too large to run locally on my laptop.  <br />  <br /> My professor recommended reaching out to see how I can set up LS Dyna to run on the UMBC cluster.<br /> <br /> Attachment 1: <a href=""https://umbc.box.com/s/18vn0klbbihn06gxabjygymmc63dq4l2"" target=""_blank"">Ansys remote.png</a><br /> "
3284658,72056033,Comment,DoIT-Research-Computing,2025-10-02 13:22:37.0000000,Assistance using UMBC HPCF Cluster to run LS Dyna simulations,resolved,Max Breitmeyer,mb17,Riley Digennaro,av23132,av23132@umbc.edu,Roy Prouty,proutyr1@umbc.edu,"<div> <p>Check to see if we can install Ansys on the cluster. We obv have it as student software. Maybe not available as broader license. Then check to see if the license would allow us to install it on the cluster. Note: If it&#39;s strictly a GUI app, we should be clear in any response about lags off campus.<br /> <br /> With success there: See if there is a prof associated with the capstone (likely Jamie Gurganus), they can sponsor a class group. We don&#39;t have the form setup for that yet, so we&#39;ll just have to track things elsewhere. Work with me on that tracking system so I can instigate a clean-up if this all works out.</p>  <p>&nbsp;</p>  <p>On Thu Oct 02 08:39:49 2025, ZZ99999 wrote:</p>  <blockquote>First Name: Riley<br /> Last Name: Di Gennaro<br /> Email: av23132@umbc.edu<br /> Campus ID: AV23132<br /> <br /> Cc: oajimat1@umbc.edu, stephas1@umbc.edu, pvonlock@umbc.edu<br /> <br /> Hello,<br /> <br /> I&#39;m a student currently taking ENME 444 Capstone and I am using Ansys and LS Dyna to run simulations of a car crash. The simulations are too large for the student version of the software and also too large to run locally on my laptop.<br /> <br /> My professor recommended reaching out to see how I can set up LS Dyna to run on the UMBC cluster.<br /> <br /> Attachment 1: Ansys remote.png</blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Roy Prouty<br /> DoIT Research Computing Team</p> "
3284658,72056034,CommentEmailRecord,DoIT-Research-Computing,2025-10-02 13:22:38.0000000,Assistance using UMBC HPCF Cluster to run LS Dyna simulations,resolved,Max Breitmeyer,mb17,Riley Digennaro,av23132,av23132@umbc.edu,The RT System itself,NULL,"Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3284658 >  Comment just added.    Check to see if we can install Ansys on the cluster. We obv have it as student software. Maybe not available as broader license. Then check to see if the license would allow us to install it on the cluster. Note: If it's strictly a GUI app, we should be clear in any response about lags off campus.  With success there: See if there is a prof associated with the capstone (likely Jamie Gurganus), they can sponsor a class group. We don't have the form setup for that yet, so we'll just have to track things elsewhere. Work with me on that tracking system so I can instigate a clean-up if this all works out.  On Thu Oct 02 08:39:49 2025, ZZ99999 wrote:  > First Name: Riley > Last Name: Di Gennaro > Email: av23132@umbc.edu > Campus ID: AV23132  > Cc: oajimat1@umbc.edu, stephas1@umbc.edu, pvonlock@umbc.edu  > Hello,  > I'm a student currently taking ENME 444 Capstone and I am using Ansys and > LS Dyna to run simulations of a car crash. The simulations are too large > for the student version of the software and also too large to run locally > on my laptop.  > My professor recommended reaching out to see how I can set up LS Dyna to > run on the UMBC cluster.  > Attachment 1: Ansys remote.png  --  Roy Prouty DoIT Research Computing Team  "
3284658,72139047,Correspond,DoIT-Research-Computing,2025-10-07 15:33:22.0000000,Assistance using UMBC HPCF Cluster to run LS Dyna simulations,resolved,Max Breitmeyer,mb17,Riley Digennaro,av23132,av23132@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Sorry for the late response to this. We&#39;re looking at making this available on our shared resource, but am waiting to hear back about our licenses. In the meantime it might be possible to install these programs in your user directory. What I would like for your professor to do is set up a new group as a class, so if possible have them go here: https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research%20Computing and request a new group under a PI, and in the notes write that this is for the&nbsp;ENME 444 class. Once that&#39;s done we can get you on the cluster, and if we still don&#39;t know anything about the license, we&#39;ll work to try to set up the software for you as an individual.&nbsp;</p>  <p>On Thu Oct 02 08:39:49 2025, ZZ99999 wrote:</p>  <blockquote>First Name: Riley<br /> Last Name: Di Gennaro<br /> Email: av23132@umbc.edu<br /> Campus ID: AV23132<br /> <br /> Cc: oajimat1@umbc.edu, stephas1@umbc.edu, pvonlock@umbc.edu<br /> <br /> Hello,<br /> <br /> I&#39;m a student currently taking ENME 444 Capstone and I am using Ansys and LS Dyna to run simulations of a car crash. The simulations are too large for the student version of the software and also too large to run locally on my laptop.<br /> <br /> My professor recommended reaching out to see how I can set up LS Dyna to run on the UMBC cluster.<br /> <br /> Attachment 1: Ansys remote.png</blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3284658,72142044,Correspond,DoIT-Research-Computing,2025-10-07 17:04:16.0000000,Assistance using UMBC HPCF Cluster to run LS Dyna simulations,resolved,Max Breitmeyer,mb17,Riley Digennaro,av23132,av23132@umbc.edu,Paris von Lockette,pvonlock@umbc.edu,"This is awesome. I will setup this up after lunch.  Thank you immensely Max!  On Tue, Oct 7, 2025 at 11:33=E2=80=AFAM Max Breitmeyer via RT <UMBCHelp@rt.= umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3284658 > > > Last Update From Ticket: > > Sorry for the late response to this. We're looking at making this > available on > our shared resource, but am waiting to hear back about our licenses. In t= he > meantime it might be possible to install these programs in your user > directory. > What I would like for your professor to do is set up a new group as a > class, so > if possible have them go here: > > https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=3DRe= search%20Computing > and request a new group under a PI, and in the notes write that this is > for the > ENME 444 class. Once that's done we can get you on the cluster, and if we > still > don't know anything about the license, we'll work to try to set up the > software > for you as an individual. > > On Thu Oct 02 08:39:49 2025, ZZ99999 wrote: > > > First Name: Riley > > Last Name: Di Gennaro > > Email: av23132@umbc.edu > > Campus ID: AV23132 > > > Cc: oajimat1@umbc.edu, stephas1@umbc.edu, pvonlock@umbc.edu > > > Hello, > > > I'm a student currently taking ENME 444 Capstone and I am using Ansys a= nd > > LS Dyna to run simulations of a car crash. The simulations are too large > > for the student version of the software and also too large to run local= ly > > on my laptop. > > > My professor recommended reaching out to see how I can set up LS Dyna to > > run on the UMBC cluster. > > > Attachment 1: Ansys remote.png > > -- > > Best, > Max Breitmeyer > DOIT HPC System Administrator > > > "
3284658,72142044,Correspond,DoIT-Research-Computing,2025-10-07 17:04:16.0000000,Assistance using UMBC HPCF Cluster to run LS Dyna simulations,resolved,Max Breitmeyer,mb17,Riley Digennaro,av23132,av23132@umbc.edu,Paris von Lockette,pvonlock@umbc.edu,"<div dir=3D""auto"">This is awesome. I will setup this up after lunch.</div><= div dir=3D""auto""><br></div><div dir=3D""auto"">Thank you immensely Max!</div>= <div><br><div class=3D""gmail_quote gmail_quote_container""><div dir=3D""ltr"" = class=3D""gmail_attr"">On Tue, Oct 7, 2025 at 11:33=E2=80=AFAM Max Breitmeyer=  via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a= >&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""margin:0 0=  0 .8ex;border-left:1px #ccc solid;padding-left:1ex"">Ticket &lt;URL: <a hre= f=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D3284658"" rel=3D""noreferre= r"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3284658</= a> &gt;<br> <br> Last Update From Ticket:<br> <br> Sorry for the late response to this. We&#39;re looking at making this avail= able on<br> our shared resource, but am waiting to hear back about our licenses. In the= <br> meantime it might be possible to install these programs in your user direct= ory.<br> What I would like for your professor to do is set up a new group as a class= , so<br> if possible have them go here:<br> <a href=3D""https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?= auto=3DResearch%20Computing"" rel=3D""noreferrer"" target=3D""_blank"">https://r= tforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=3DResearch%20Co= mputing</a><br> and request a new group under a PI, and in the notes write that this is for=  the<br> ENME 444 class. Once that&#39;s done we can get you on the cluster, and if = we still<br> don&#39;t know anything about the license, we&#39;ll work to try to set up = the software<br> for you as an individual.<br> <br> On Thu Oct 02 08:39:49 2025, ZZ99999 wrote:<br> <br> &gt; First Name: Riley<br> &gt; Last Name: Di Gennaro<br> &gt; Email: <a href=3D""mailto:av23132@umbc.edu"" target=3D""_blank"">av23132@u= mbc.edu</a><br> &gt; Campus ID: AV23132<br> <br> &gt; Cc: <a href=3D""mailto:oajimat1@umbc.edu"" target=3D""_blank"">oajimat1@um= bc.edu</a>, <a href=3D""mailto:stephas1@umbc.edu"" target=3D""_blank"">stephas1= @umbc.edu</a>, <a href=3D""mailto:pvonlock@umbc.edu"" target=3D""_blank"">pvonl= ock@umbc.edu</a><br> <br> &gt; Hello,<br> <br> &gt; I&#39;m a student currently taking ENME 444 Capstone and I am using An= sys and<br> &gt; LS Dyna to run simulations of a car crash. The simulations are too lar= ge<br> &gt; for the student version of the software and also too large to run loca= lly<br> &gt; on my laptop.<br> <br> &gt; My professor recommended reaching out to see how I can set up LS Dyna = to<br> &gt; run on the UMBC cluster.<br> <br> &gt; Attachment 1: Ansys remote.png<br> <br> --<br> <br> Best,<br> Max Breitmeyer<br> DOIT HPC System Administrator<br> <br> <br> </blockquote></div></div> "
3284658,72186323,Correspond,DoIT-Research-Computing,2025-10-09 15:40:07.0000000,Assistance using UMBC HPCF Cluster to run LS Dyna simulations,resolved,Max Breitmeyer,mb17,Riley Digennaro,av23132,av23132@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi all,</p>  <p>Checking in on this. Were you able to submit a group request?&nbsp;</p>  <p>On Tue Oct 07 13:04:16 2025, FA15338 wrote:</p>  <blockquote> <div>This is awesome. I will setup this up after lunch.</div>  <div>&nbsp;</div>  <div>Thank you immensely Max!</div>  <div>&nbsp; <div> <div>On Tue, Oct 7, 2025 at 11:33=E2=80=AFAM Max Breitmeyer via RT &lt;UMBC= Help@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 84658 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Sorry for the late response to this. We&#39;re looking at making this avail= able on<br /> our shared resource, but am waiting to hear back about our licenses. In the= <br /> meantime it might be possible to install these programs in your user direct= ory.<br /> What I would like for your professor to do is set up a new group as a class= , so<br /> if possible have them go here:<br /> https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=3DRese= arch%20Computing<br /> and request a new group under a PI, and in the notes write that this is for=  the<br /> ENME 444 class. Once that&#39;s done we can get you on the cluster, and if = we still<br /> don&#39;t know anything about the license, we&#39;ll work to try to set up = the software<br /> for you as an individual.<br /> <br /> On Thu Oct 02 08:39:49 2025, ZZ99999 wrote:<br /> <br /> &gt; First Name: Riley<br /> &gt; Last Name: Di Gennaro<br /> &gt; Email: av23132@umbc.edu<br /> &gt; Campus ID: AV23132<br /> <br /> &gt; Cc: oajimat1@umbc.edu, stephas1@umbc.edu, pvonlock@umbc.edu<br /> <br /> &gt; Hello,<br /> <br /> &gt; I&#39;m a student currently taking ENME 444 Capstone and I am using An= sys and<br /> &gt; LS Dyna to run simulations of a car crash. The simulations are too lar= ge<br /> &gt; for the student version of the software and also too large to run loca= lly<br /> &gt; on my laptop.<br /> <br /> &gt; My professor recommended reaching out to see how I can set up LS Dyna = to<br /> &gt; run on the UMBC cluster.<br /> <br /> &gt; Attachment 1: Ansys remote.png<br /> <br /> --<br /> <br /> Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator<br /> <br /> &nbsp;</blockquote> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3284658,72189082,Correspond,DoIT-Research-Computing,2025-10-09 16:55:44.0000000,Assistance using UMBC HPCF Cluster to run LS Dyna simulations,resolved,Max Breitmeyer,mb17,Riley Digennaro,av23132,av23132@umbc.edu,Paris von Lockette,pvonlock@umbc.edu,"Just did. One group for class, one for my research lab.    On Thu, Oct 9, 2025 at 11:40=E2=80=AFAM Max Breitmeyer via RT <UMBCHelp@rt.= umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3284658 > > > Last Update From Ticket: > > Hi all, > > Checking in on this. Were you able to submit a group request? > > On Tue Oct 07 13:04:16 2025, FA15338 wrote: > > > This is awesome. I will setup this up after lunch. Thank you immensely > Max! > > On Tue, Oct 7, 2025 at 11:33 AM Max Breitmeyer via RT > > <UMBCHelp@rt.umbc.edu> wrote: > > >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3284658 > > > >> Last Update From Ticket: > > >> Sorry for the late response to this. We're looking at making this > >> available on > >> our shared resource, but am waiting to hear back about our licenses. In > >> the > >> meantime it might be possible to install these programs in your user > >> directory. > >> What I would like for your professor to do is set up a new group as a > >> class, so > >> if possible have them go here: > >> > https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=3DRe= search%20Computing > >> and request a new group under a PI, and in the notes write that this is > >> for the > >> ENME 444 class. Once that's done we can get you on the cluster, and if > >> we still > >> don't know anything about the license, we'll work to try to set up the > >> software > >> for you as an individual. > > >> On Thu Oct 02 08:39:49 2025, ZZ99999 wrote: > > >> > First Name: Riley > >> > Last Name: Di Gennaro > >> > Email: av23132@umbc.edu > >> > Campus ID: AV23132 > > >> > Cc: oajimat1@umbc.edu, stephas1@umbc.edu, pvonlock@umbc.edu > > >> > Hello, > > >> > I'm a student currently taking ENME 444 Capstone and I am using Ansys > >> and > >> > LS Dyna to run simulations of a car crash. The simulations are too > >> large > >> > for the student version of the software and also too large to run > >> locally > >> > on my laptop. > > >> > My professor recommended reaching out to see how I can set up LS Dyna > >> to > >> > run on the UMBC cluster. > > >> > Attachment 1: Ansys remote.png > > >> -- > > >> Best, > >> Max Breitmeyer > >> DOIT HPC System Administrator > > -- > > Best, > Max Breitmeyer > DOIT HPC System Administrator > > > "
3284658,72189082,Correspond,DoIT-Research-Computing,2025-10-09 16:55:44.0000000,Assistance using UMBC HPCF Cluster to run LS Dyna simulations,resolved,Max Breitmeyer,mb17,Riley Digennaro,av23132,av23132@umbc.edu,Paris von Lockette,pvonlock@umbc.edu,"<div dir=3D""auto"">Just did. One group for class, one for my research lab.</= div><div dir=3D""auto""><br></div><div dir=3D""auto""><br></div><div><br><div c= lass=3D""gmail_quote gmail_quote_container""><div dir=3D""ltr"" class=3D""gmail_= attr"">On Thu, Oct 9, 2025 at 11:40=E2=80=AFAM Max Breitmeyer via RT &lt;<a = href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br= ></div><blockquote class=3D""gmail_quote"" style=3D""margin:0 0 0 .8ex;border-= left:1px #ccc solid;padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt= .umbc.edu/Ticket/Display.html?id=3D3284658"" rel=3D""noreferrer"" target=3D""_b= lank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3284658</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi all,<br> <br> Checking in on this. Were you able to submit a group request?<br> <br> On Tue Oct 07 13:04:16 2025, FA15338 wrote:<br> <br> &gt; This is awesome. I will setup this up after lunch. Thank you immensely=  Max!<br> &gt; On Tue, Oct 7, 2025 at 11:33 AM Max Breitmeyer via RT<br> &gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp= @rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html= ?id=3D3284658"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tic= ket/Display.html?id=3D3284658</a> &gt;<br> <br> &gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt; Sorry for the late response to this. We&#39;re looking at making t= his<br> &gt;&gt; available on<br> &gt;&gt; our shared resource, but am waiting to hear back about our license= s. In<br> &gt;&gt; the<br> &gt;&gt; meantime it might be possible to install these programs in your us= er<br> &gt;&gt; directory.<br> &gt;&gt; What I would like for your professor to do is set up a new group a= s a<br> &gt;&gt; class, so<br> &gt;&gt; if possible have them go here:<br> &gt;&gt; <a href=3D""https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-sup= port.php?auto=3DResearch%20Computing"" rel=3D""noreferrer"" target=3D""_blank"">= https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=3DRese= arch%20Computing</a><br> &gt;&gt; and request a new group under a PI, and in the notes write that th= is is<br> &gt;&gt; for the<br> &gt;&gt; ENME 444 class. Once that&#39;s done we can get you on the cluster= , and if<br> &gt;&gt; we still<br> &gt;&gt; don&#39;t know anything about the license, we&#39;ll work to try t= o set up the<br> &gt;&gt; software<br> &gt;&gt; for you as an individual.<br> <br> &gt;&gt; On Thu Oct 02 08:39:49 2025, ZZ99999 wrote:<br> <br> &gt;&gt; &gt; First Name: Riley<br> &gt;&gt; &gt; Last Name: Di Gennaro<br> &gt;&gt; &gt; Email: <a href=3D""mailto:av23132@umbc.edu"" target=3D""_blank"">= av23132@umbc.edu</a><br> &gt;&gt; &gt; Campus ID: AV23132<br> <br> &gt;&gt; &gt; Cc: <a href=3D""mailto:oajimat1@umbc.edu"" target=3D""_blank"">oa= jimat1@umbc.edu</a>, <a href=3D""mailto:stephas1@umbc.edu"" target=3D""_blank""= >stephas1@umbc.edu</a>, <a href=3D""mailto:pvonlock@umbc.edu"" target=3D""_bla= nk"">pvonlock@umbc.edu</a><br> <br> &gt;&gt; &gt; Hello,<br> <br> &gt;&gt; &gt; I&#39;m a student currently taking ENME 444 Capstone and I am=  using Ansys<br> &gt;&gt; and<br> &gt;&gt; &gt; LS Dyna to run simulations of a car crash. The simulations ar= e too<br> &gt;&gt; large<br> &gt;&gt; &gt; for the student version of the software and also too large to=  run<br> &gt;&gt; locally<br> &gt;&gt; &gt; on my laptop.<br> <br> &gt;&gt; &gt; My professor recommended reaching out to see how I can set up=  LS Dyna<br> &gt;&gt; to<br> &gt;&gt; &gt; run on the UMBC cluster.<br> <br> &gt;&gt; &gt; Attachment 1: Ansys remote.png<br> <br> &gt;&gt; --<br> <br> &gt;&gt; Best,<br> &gt;&gt; Max Breitmeyer<br> &gt;&gt; DOIT HPC System Administrator<br> <br> --<br> <br> Best,<br> Max Breitmeyer<br> DOIT HPC System Administrator<br> <br> <br> </blockquote></div></div> "
3284658,72320184,Correspond,DoIT-Research-Computing,2025-10-16 12:23:42.0000000,Assistance using UMBC HPCF Cluster to run LS Dyna simulations,resolved,Max Breitmeyer,mb17,Riley Digennaro,av23132,av23132@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Paris,</p>  <p>I saw the groups were created for the class and the lab. I finally heard=  back from our licensing team, and it seems like we are unable to install t= he program that Riley asked for on a system wide scale available for all us= ers. That being said, it&#39;s probably possible to install the program for=  individual users.&nbsp;</p>  <p>Riley, have you been added to the class yet? If not I can add you now. I= f you have, have you taken a shot at installing the program yourself? If yo= u have any issues you can schedule an office hours with our team here:&nbsp= ;https://hpcf.umbc.edu/help/office-hours/</p>  <p>I&#39;ll leave this open for a few days in case there is any questions a= bout it.</p>  <p>On Thu Oct 09 12:55:44 2025, FA15338 wrote:</p>  <blockquote> <div>Just did. One group for class, one for my research lab.</div>  <div>&nbsp;</div>  <div>&nbsp;</div>  <div>&nbsp; <div> <div>On Thu, Oct 9, 2025 at 11:40=E2=80=AFAM Max Breitmeyer via RT &lt;UMBC= Help@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 84658 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hi all,<br /> <br /> Checking in on this. Were you able to submit a group request?<br /> <br /> On Tue Oct 07 13:04:16 2025, FA15338 wrote:<br /> <br /> &gt; This is awesome. I will setup this up after lunch. Thank you immensely=  Max!<br /> &gt; On Tue, Oct 7, 2025 at 11:33 AM Max Breitmeyer via RT<br /> &gt; &lt;UMBCHelp@rt.umbc.edu&gt; wrote:<br /> <br /> &gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32846= 58 &gt;<br /> <br /> &gt;&gt; Last Update From Ticket:<br /> <br /> &gt;&gt; Sorry for the late response to this. We&#39;re looking at making t= his<br /> &gt;&gt; available on<br /> &gt;&gt; our shared resource, but am waiting to hear back about our license= s. In<br /> &gt;&gt; the<br /> &gt;&gt; meantime it might be possible to install these programs in your us= er<br /> &gt;&gt; directory.<br /> &gt;&gt; What I would like for your professor to do is set up a new group a= s a<br /> &gt;&gt; class, so<br /> &gt;&gt; if possible have them go here:<br /> &gt;&gt; https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?au= to=3DResearch%20Computing<br /> &gt;&gt; and request a new group under a PI, and in the notes write that th= is is<br /> &gt;&gt; for the<br /> &gt;&gt; ENME 444 class. Once that&#39;s done we can get you on the cluster= , and if<br /> &gt;&gt; we still<br /> &gt;&gt; don&#39;t know anything about the license, we&#39;ll work to try t= o set up the<br /> &gt;&gt; software<br /> &gt;&gt; for you as an individual.<br /> <br /> &gt;&gt; On Thu Oct 02 08:39:49 2025, ZZ99999 wrote:<br /> <br /> &gt;&gt; &gt; First Name: Riley<br /> &gt;&gt; &gt; Last Name: Di Gennaro<br /> &gt;&gt; &gt; Email: av23132@umbc.edu<br /> &gt;&gt; &gt; Campus ID: AV23132<br /> <br /> &gt;&gt; &gt; Cc: oajimat1@umbc.edu, stephas1@umbc.edu, pvonlock@umbc.edu<b= r /> <br /> &gt;&gt; &gt; Hello,<br /> <br /> &gt;&gt; &gt; I&#39;m a student currently taking ENME 444 Capstone and I am=  using Ansys<br /> &gt;&gt; and<br /> &gt;&gt; &gt; LS Dyna to run simulations of a car crash. The simulations ar= e too<br /> &gt;&gt; large<br /> &gt;&gt; &gt; for the student version of the software and also too large to=  run<br /> &gt;&gt; locally<br /> &gt;&gt; &gt; on my laptop.<br /> <br /> &gt;&gt; &gt; My professor recommended reaching out to see how I can set up=  LS Dyna<br /> &gt;&gt; to<br /> &gt;&gt; &gt; run on the UMBC cluster.<br /> <br /> &gt;&gt; &gt; Attachment 1: Ansys remote.png<br /> <br /> &gt;&gt; --<br /> <br /> &gt;&gt; Best,<br /> &gt;&gt; Max Breitmeyer<br /> &gt;&gt; DOIT HPC System Administrator<br /> <br /> --<br /> <br /> Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator<br /> <br /> &nbsp;</blockquote> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3285275,72070767,Create,DoIT-Research-Computing,2025-10-02 18:51:56.0000000,HPC User Account: ankgoel in pi_ankgoel,resolved,Elliot Gobbert,elliotg2,Ankit Goel,ankgoel,ankgoel@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Ankit Last Name:                 Goel Email:                     ankgoel@umbc.edu Campus ID:                 KZ22937  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    ankgoel@umbc.edu Existing Group:       pi_ankgoel Project Title:        Data-driven Pressure Recovery Regulation in Diffusers Project Abstract:     This project utilizes numerical simulations based on compressible Reynolds-Averaged Navier-Stokes (RANS) and Large Eddy Simulation (LES) to investigate the mean and temporal features resulting from mass injection into an S-shaped diffuser with varying injection speeds and pulse frequencies. The results will be compared with experiments to confirm the accuracy of the numerical study. Experiments have indicated that there is an optimal injection frequency between 100 Hz and 300 Hz with a mass flow rate of 1 percent of the free stream that results in optimal pressure recovery. This study aims to determine the frequency using an adaptive control framework based on retrospective cost optimization for the better design of the inlet duct, thereby maximizing pressure recovery.  Two new students in my group will be working on this project.   Olvin Moran - omoran1@umbc.edu Juan Augusto Paredes Salazar - japarede@umbc.edu  Please make an account for them.   Thanks Ankit Goel  "
3285275,72071573,Correspond,DoIT-Research-Computing,2025-10-02 19:13:43.0000000,HPC User Account: ankgoel in pi_ankgoel,resolved,Elliot Gobbert,elliotg2,Ankit Goel,ankgoel,ankgoel@umbc.edu,Ankit Goel,ankgoel@umbc.edu,"<!DOCTYPE html> <html>   <head>     <meta http-equiv=3D""Content-Type"" content=3D""text/html; charset=3DUTF-8= "">   </head>   <body>     <p>Approved.=C2=A0</p>     <div class=3D""moz-signature"">       <p><br>       </p>       <p>------------------------------------------------------------------= <a           moz-do-not-send=3D""true"" href=3D""https://ankgoel.umbc.edu/""><b><b= r>             Ankit Goel</b></a><br>         Assistant Professor<a href=3D""https://me.umbc.edu""><br>           Mechanical Engineering Department</a><a           href=3D""https://umbc.edu""><br>           University of Maryland, Baltimore County</a><a           href=3D""https://goo.gl/maps/ts7ZgnhmNV47M4kz6""><br>           ENGR 216, 1000 Hilltop Circle, Baltimore, MD 21250</a><br>         | <a href=3D""mailto:ankgoel@umbc.edu"">Email</a> |(410) 455-1176 |         <a href=3D""https://calendar.google.com/calendar/embed?src=3Dankgoel%40umbc.edu= &amp;ctz=3DAmerica%2FNew_York"">Calendar</a>         | <br> ------------------------------------------------------------------ </p>     </div>     <div class=3D""moz-cite-prefix"">On 10/2/2025 2:51 PM, RT API via RT       wrote:<br>     </div>     <blockquote type=3D""cite"" cite=3D""mid:rt-5.0.5-16286-1759431116-213.3285275-7560-0@rt.umbc.edu"">       <pre wrap=3D"""" class=3D""moz-quote-pre"">This e-mail is a notification = that a UMBC user: Ankit Goel <a class=3D""moz-txt-link-rfc2396E"" href=3D""mai= lto:ankgoel@umbc.edu"">&lt;ankgoel@umbc.edu&gt;</a> has requested an account=  within UMBC's HPC environment in your group &lt;pi_ankgoel&gt;. As the PI,=  we request that you acknowledge and approve this account creation by reply= ing to this message. Alternatively you can go to this link and review the t= icket and indicate your decision here:  Ticket &lt;URL: <a class=3D""moz-txt-link-freetext"" href=3D""https://rt.umbc.= edu/Ticket/Display.html?id=3D3285275"">https://rt.umbc.edu/Ticket/Display.ht= ml?id=3D3285275</a> &gt;  Once we have your approval, we will create the account and you and the new = user will receive another e-mail notifying you that the account has been cr= eated. If you have any other questions or concerns please contact us.  - UMBC DoIT Research Computing Support Staff </pre>     </blockquote>   </body> </html> "
3285275,72071573,Correspond,DoIT-Research-Computing,2025-10-02 19:13:43.0000000,HPC User Account: ankgoel in pi_ankgoel,resolved,Elliot Gobbert,elliotg2,Ankit Goel,ankgoel,ankgoel@umbc.edu,Ankit Goel,ankgoel@umbc.edu,"Approved.   ------------------------------------------------------------------* Ankit Goel* <https://ankgoel.umbc.edu/> Assistant Professor Mechanical Engineering Department <https://me.umbc.edu> University of Maryland, Baltimore County <https://umbc.edu> ENGR 216, 1000 Hilltop Circle, Baltimore, MD 21250  <https://goo.gl/maps/ts7ZgnhmNV47M4kz6> | Email <mailto:ankgoel@umbc.edu> |(410) 455-1176 | Calendar  <https://calendar.google.com/calendar/embed?src=ankgoel%40umbc.edu&ctz=America%2FNew_York>  | ------------------------------------------------------------------  On 10/2/2025 2:51 PM, RT API via RT wrote: > This e-mail is a notification that a UMBC user: Ankit Goel<ankgoel@umbc.edu> has requested an account within UMBC's HPC environment in your group <pi_ankgoel>. As the PI, we request that you acknowledge and approve this account creation by replying to this message. Alternatively you can go to this link and review the ticket and indicate your decision here: > > Ticket <URL:https://rt.umbc.edu/Ticket/Display.html?id=3285275 > > > Once we have your approval, we will create the account and you and the new user will receive another e-mail notifying you that the account has been created. If you have any other questions or concerns please contact us. > > - UMBC DoIT Research Computing Support Staff"
3285275,72085033,Correspond,DoIT-Research-Computing,2025-10-03 15:45:44.0000000,HPC User Account: ankgoel in pi_ankgoel,resolved,Elliot Gobbert,elliotg2,Ankit Goel,ankgoel,ankgoel@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>The accounts have been created:</p>  <p>Hi Olvin,</p>  <p>Your account (omoran1) has been created on chip.rs.umbc.edu.<br /> Your primary group is pi_ankgoel.<br /> Your home directory has 500M of storage.<br /> You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip<br /> Please read through the documentation found at hpcf.umbc.edu &gt; User Support.<br /> All available modules can be viewed using the command &#39;module avail&#39;.<br /> Please submit additional questions or issues as separate tickets via the following link.<br /> (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/)<br /> <br /> &nbsp;</p>  <p>Hi Juan,</p>  <p>Your account (japarede) has been created on chip.rs.umbc.edu.<br /> Your primary group is pi_ankgoel.<br /> Your home directory has 500M of storage.<br /> You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip<br /> Please read through the documentation found at hpcf.umbc.edu &gt; User Support.<br /> All available modules can be viewed using the command &#39;module avail&#39;.<br /> Please submit additional questions or issues as separate tickets via the following link.<br /> (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/)<br /> <br /> Let me know if you have any more questions!</p>  <p>Elliot Gobbert</p> "
3285334,72072866,Create,DoIT-Research-Computing,2025-10-02 19:42:24.0000000,HPC User Account: amian1 in Dr. Kann,resolved,Beamlak Bekele,bbekele1,Ahmad Mian,amian1,amian1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Ahmad Last Name:                 Mian Email:                     amian1@umbc.edu Campus ID:                 EN59370  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    mkann@umbc.edu Existing Group:       Dr. Kann Project Title:        Species Translator Project Abstract:     Title: Analyzing the Impact of Wildfires on Human Lung Cancer Through Molecular Homolog Identification in Drosophila  Exposure to wildfires has been linked to lung deficiencies, including lung cancer, due to air pollutants inducing DNA mutations. To better understand the genetic pressures wildfires cause toward lung cancer, we utilize a study of protein expressions in drosophila flies exposed to controlled smoke conditions. These controlled conditions in drosophila provide grounds for studying orthologous relationships caused by genetic pressures. By observing putative orthologs in drosophila in comparison with non-smoking lung cancer patients, we will identify functional driver genes for lung cancer. Specifically, this comparative study analyzes functional genes at three stages: (1) Identifying orthologs of the mutated genes in the Drosophila flies within the Human genome, (2) comparing the mutation positions to identify mutational homologs, and (3) examining the interologs to determine if protein-protein interactions are conserved between the two species. Understanding how protein-protein interactions are altered from wildfire-induced mutations can aid in discovering therapeutic targets and establishing more robust lung cancer treatments. Additionally,  this comparative model can be scaled to incorporate analysis of other model organisms and provide insights into different diseases.   N/A  "
3285334,72075723,Correspond,DoIT-Research-Computing,2025-10-02 21:05:48.0000000,HPC User Account: amian1 in Dr. Kann,resolved,Beamlak Bekele,bbekele1,Ahmad Mian,amian1,amian1@umbc.edu,Maricel Kann,mkann@umbc.edu,"Please approve the account for Ahmad M. Sent from my iPhone  > On Oct 2, 2025, at 3:42=E2=80=AFPM, RT API via RT <UMBCHelp@rt.umbc.edu> = wrote: >=20 > =EF=BB=BFThis e-mail is a notification that a UMBC user: Ahmad Mian <amia= n1@umbc.edu> has requested an account within UMBC's HPC environment in your=  group <Dr. Kann>. As the PI, we request that you acknowledge and approve t= his account creation by replying to this message. Alternatively you can go = to this link and review the ticket and indicate your decision here: >=20 > Ticket <URL: https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Di= splay.html?id%3D3285334&source=3Dgmail-imap&ust=3D1760038947000000&usg=3DAO= vVaw2vZg0pA0MUl6dPGfOFq_x1 > >=20 > Once we have your approval, we will create the account and you and the ne= w user will receive another e-mail notifying you that the account has been = created. If you have any other questions or concerns please contact us. >=20 > - UMBC DoIT Research Computing Support Staff  "
3285334,72140321,Correspond,DoIT-Research-Computing,2025-10-07 16:09:58.0000000,HPC User Account: amian1 in Dr. Kann,resolved,Beamlak Bekele,bbekele1,Ahmad Mian,amian1,amian1@umbc.edu,Beamlak Bekele,bbekele1@umbc.edu,"<div> <p>Hi Ahmad,</p>  <p>Your account (amian1) has been created on chip.rs.umbc.edu.<br /> Your primary group is pi_mkann.<br /> Your home directory has 500M of storage.<br /> You can find a short tutorial on how to use chip here: https://umbc.atlassi= an.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip<br /> Please read through the documentation found at hpcf.umbc.edu &gt; User Supp= ort.<br /> All available modules can be viewed using the command &#39;module avail&#39= ;.<br /> Please submit additional questions or issues as separate tickets via the fo= llowing link.<br /> (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/)</p>  <p>&nbsp;</p>  <p>On Thu Oct 02 17:05:48 2025, RS61560 wrote:</p>  <blockquote> <pre> Please approve the account for Ahmad M. Sent from my iPhone  &gt; On Oct 2, 2025, at 3:42=E2=80=AFPM, RT API via RT &lt;UMBCHelp@rt.umbc= .edu&gt; wrote: &gt;=20 &gt; =EF=BB=BFThis e-mail is a notification that a UMBC user: Ahmad Mian &l= t;amian1@umbc.edu&gt; has requested an account within UMBC&#39;s HPC enviro= nment in your group &lt;Dr. Kann&gt;. As the PI, we request that you acknow= ledge and approve this account creation by replying to this message. Altern= atively you can go to this link and review the ticket and indicate your dec= ision here: &gt;=20 &gt; Ticket &lt;URL: https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Tic= ket/Display.html?id%3D3285334&amp;source=3Dgmail-imap&amp;ust=3D17600389470= 00000&amp;usg=3DAOvVaw2vZg0pA0MUl6dPGfOFq_x1 &gt; &gt;=20 &gt; Once we have your approval, we will create the account and you and the=  new user will receive another e-mail notifying you that the account has be= en created. If you have any other questions or concerns please contact us. &gt;=20 &gt; - UMBC DoIT Research Computing Support Staff  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> "
3285426,72075493,Create,DoIT-Research-Computing,2025-10-02 20:49:15.0000000,HPC Other Issue: Resource Usage Concern on Shared Compute Node,resolved,Tartela Tabassum,tartelt1,Chaoqian Yuan,c285,c285@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Chaoqian<br /> Last Name:                 Yuan<br /> Email:                     c285@umbc.edu<br /> Campus ID:                 NO26412<br /> <br /> Request Type:              High Performance Cluster<br /> <br /> Hello, <br />  <br /> I would like to bring to your attention that one of the users (ID:pengy1, see attached) appears to be occupying a significant portion of the compute resources on our shared node. This high usage may impact the efficiency and workflow of other users. <br />  <br /> Could you please remind this user to be mindful of resource usage and try to avoid monopolizing the node, so that everyone can work more smoothly? <br />  <br /> Thank you for your attention and support. <br />  <br /> Best regards, <br /> Chaoqian Yuan<br /> <br /> Attachment 1: <a href=""https://umbc.box.com/s/k34l0cji0x2hlr1c111241pg4in5cf66"" target=""_blank"">Screenshot 2025-10-02 at 4.44.27PM.png</a><br /> "
3285426,72083004,Correspond,DoIT-Research-Computing,2025-10-03 14:50:43.0000000,HPC Other Issue: Resource Usage Concern on Shared Compute Node,resolved,Tartela Tabassum,tartelt1,Chaoqian Yuan,c285,c285@umbc.edu,Tartela Tabassum,tartelt1@umbc.edu,"<div> <p>Hello,</p>  <p>I understand the concern about shared node performance. However, as long as jobs are submitted within the set limits, they are considered valid and within policy. If we see repeated issues that affect the whole system, we can look at changing the limits. For now, the user is working within their allowed usage.</p>  <p>&nbsp;</p>  <p>Best,</p>  <p>Tartela</p>  <p>&nbsp;</p>  <p>On Thu Oct 02 16:49:15 2025, ZZ99999 wrote:</p>  <blockquote><br /> I would like to bring to your attention that one of the users (ID:pengy1, see attached) appears to be occupying a significant portion of the compute resources on our shared node. This high usage may impact the efficiency and workflow of other users.<br /> <br /> Could you please remind this user to be mindful of resource usage and try to avoid monopolizing the node, so that everyone can work more smoothly?</blockquote> </div> "
3285426,72087802,Correspond,DoIT-Research-Computing,2025-10-03 17:25:32.0000000,HPC Other Issue: Resource Usage Concern on Shared Compute Node,resolved,Tartela Tabassum,tartelt1,Chaoqian Yuan,c285,c285@umbc.edu,Chaoqian Yuan,c285@umbc.edu,"<div> <p>Thank you for your reply. That makes sense.</p>  <p>On Fri Oct 03 10:50:43 2025, HK41259 wrote:</p>  <blockquote> <div> <p>Hello,</p>  <p>I understand the concern about shared node performance. However, as long as jobs are submitted within the set limits, they are considered valid and within policy. If we see repeated issues that affect the whole system, we can look at changing the limits. For now, the user is working within their allowed usage.</p>  <p>&nbsp;</p>  <p>Best,</p>  <p>Tartela</p>  <p>&nbsp;</p>  <p>On Thu Oct 02 16:49:15 2025, ZZ99999 wrote:</p>  <blockquote><br /> I would like to bring to your attention that one of the users (ID:pengy1, see attached) appears to be occupying a significant portion of the compute resources on our shared node. This high usage may impact the efficiency and workflow of other users.<br /> <br /> Could you please remind this user to be mindful of resource usage and try to avoid monopolizing the node, so that everyone can work more smoothly?</blockquote> </div> </blockquote> </div> "
3285512,72076979,Create,DoIT-Research-Computing,2025-10-03 01:31:51.0000000,HPC User Account: james36 in pi_stmiller,resolved,Elliot Gobbert,elliotg2,James Williams,james36,james36@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                James Last Name:                 Williams Email:                     james36@umbc.edu Campus ID:                 BN46293  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    stmiller@umbc.edu Existing Group:       pi_stmiller Project Title:        Analysis of rlsA gene function in Volvox carteri Project Abstract:     The Volvox carteri somatic regenerator (regA) gene encodes a putative transcription factor that is expressed only in somatic cells and is essential for maintenance of the somatic cell fate. rlsA is one of the closest paralogs of regA and its spatial and temporal expression patterns are very similar to that of regA, suggesting that this gene might also function in somatic cell development. To test this idea, we used CRISPR to generate an early frameshift mutation in rlsA that eliminates rlsA function. We are conducting RNA- seq analysis of the mutant to investigate the possibility of genetic compensation, and to better understand the role of rlsA in regulating the expression of other genes.  N/A  "
3285512,72084748,Correspond,DoIT-Research-Computing,2025-10-03 15:39:17.0000000,HPC User Account: james36 in pi_stmiller,resolved,Elliot Gobbert,elliotg2,James Williams,james36,james36@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hello,</p>  <p>I&#39;d be happy to create your account, but first, we need permission from the PI, so CC&#39;ing the PI on this email. A simple &quot;I approve&quot; will do.</p>  <p>&nbsp;</p>  <p>Thanks,</p>  <p>Elliot Gobbert</p> "
3285512,72088284,Correspond,DoIT-Research-Computing,2025-10-03 17:41:32.0000000,HPC User Account: james36 in pi_stmiller,resolved,Elliot Gobbert,elliotg2,James Williams,james36,james36@umbc.edu,Stephen Miller,stmiller@umbc.edu,"Hi Elliot,  If this request came from James Williams, I approve. Thanks,  Steve  On Fri, Oct 3, 2025 at 11:39=E2=80=AFAM Elliot Gobbert via RT <UMBCHelp@rt.= umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3285512 > > > Last Update From Ticket: > > Hello, > > I'd be happy to create your account, but first, we need permission from > the PI, > so CC'ing the PI on this email. A simple ""I approve"" will do. > > Thanks, > > Elliot Gobbert > > > "
3285512,72088284,Correspond,DoIT-Research-Computing,2025-10-03 17:41:32.0000000,HPC User Account: james36 in pi_stmiller,resolved,Elliot Gobbert,elliotg2,James Williams,james36,james36@umbc.edu,Stephen Miller,stmiller@umbc.edu,"<div dir=3D""ltr"">Hi Elliot,<div><br></div><div>If this request came from Ja= mes Williams, I approve. Thanks,</div><div><br></div><div>Steve</div></div>= <br><div class=3D""gmail_quote gmail_quote_container""><div dir=3D""ltr"" class= =3D""gmail_attr"">On Fri, Oct 3, 2025 at 11:39=E2=80=AFAM Elliot Gobbert via = RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt;=  wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""margin:0px 0px = 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"">Ticket &= lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D3285512"" re= l=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?= id=3D3285512</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hello,<br> <br> I&#39;d be happy to create your account, but first, we need permission from=  the PI,<br> so CC&#39;ing the PI on this email. A simple &quot;I approve&quot; will do.= <br> <br> Thanks,<br> <br> Elliot Gobbert<br> <br> <br> </blockquote></div> "
3285512,72090698,Correspond,DoIT-Research-Computing,2025-10-03 19:05:03.0000000,HPC User Account: james36 in pi_stmiller,resolved,Elliot Gobbert,elliotg2,James Williams,james36,james36@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>No problem, here is the information on the new account:</p>  <p>&nbsp;</p>  <p>Hi James,</p>  <p>Your account (james36) has been created on chip.rs.umbc.edu.<br /> Your primary group is pi_stmiller.<br /> Your home directory has 500M of storage.<br /> You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip<br /> Please read through the documentation found at hpcf.umbc.edu &gt; User Support.<br /> All available modules can be viewed using the command &#39;module avail&#39;.<br /> Please submit additional questions or issues as separate tickets via the following link.<br /> (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/)</p>  <p>&nbsp;</p>  <p>Best,</p>  <p>Elliot Gobbert</p> "
3286322,72099476,Create,DoIT-Research-Computing,2025-10-05 19:49:38.0000000,HPC Other Issue: Can I request for a cpu and a gpu using sbatch?,resolved,Danielle Esposito,desposi1,Samrat Bojanola,samratb1,samratb1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Samrat Last Name:                 Bojanola Email:                     samratb1@umbc.edu Campus ID:                 NP42296  Request Type:              High Performance Cluster  I have taken look at the docs to find out you can us sbatch with --cluster=chip-gpu but what if my training would need both gpu and cpu?   "
3286322,72107853,Correspond,DoIT-Research-Computing,2025-10-06 14:39:34.0000000,HPC Other Issue: Can I request for a cpu and a gpu using sbatch?,resolved,Danielle Esposito,desposi1,Samrat Bojanola,samratb1,samratb1@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Samrat,</p>  <p>All nodes on chip have CPUs, as they are required to function. However, nodes on chip-gpu also have GPUs available. So yes, when using chip-gpu you can request both GPUs and CPU cores. You would request this the same way as chip-cpu.&nbsp;</p>  <p>For more information about the hardware specification, check out this page:&nbsp;https://umbc.atlassian.net/wiki/spaces/faq/pages/1289486353/Cluster+Specifications#CPU-and-GPU-Specifications</p>  <p>For more information on requesting CPU cores, check out this page:&nbsp;https://umbc.atlassian.net/wiki/spaces/faq/pages/1335951387/Basic+Slurm+Commands#Jobs%2C-Tasks%2C-CPU-cores%2C-and-Nodes</p>  <p>Let me know if you have any additional questions or clarification.&nbsp;</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Sun Oct 05 15:49:38 2025, ZZ99999 wrote: <blockquote>Samrat</blockquote> </div> "
3286362,72099934,Create,DoIT-Research-Computing,2025-10-06 00:20:51.0000000,HPC Other Issue: c24-01 tmp area full,resolved,Danielle Esposito,desposi1,Matthias Gobbert,gobbert,gobbert@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Matthias Last Name:                 Gobbert Email:                     gobbert@umbc.edu Campus ID:                 AX68683  Request Type:              High Performance Cluster  Hi,  This is surely a funky error, but if trying to compile on c24-01 in an interactive session, for Intel MPI, it says  [gobbert@c24-01 ver1.0solution]$ mpiicc -O3 trap.c -o trap icx: error #10295: error generating temporary file name, check disk space and permissions  A student also reported this to me. The /tmpfs area or something like seems to be full.  Matthias  "
3286362,72106827,Correspond,DoIT-Research-Computing,2025-10-06 14:21:21.0000000,HPC Other Issue: c24-01 tmp area full,resolved,Danielle Esposito,desposi1,Matthias Gobbert,gobbert,gobbert@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Matthias,&nbsp;</p>  <p>Thank you for letting us know. The issue has been resolved.&nbsp;</p>  <p>Additionally, if this occurs in the future, you could attempt to change the location that the compiler uses to temporarily store files. I believe this would be achieved by setting the $TMPDIR environment variable. For example, you could try to run...</p>  <p>TMPDIR=/scratch/$JOB_ID/&nbsp;mpiicc -O3 trap.c -o trap</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Sun Oct 05 20:20:51 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Matthias Last Name:                 Gobbert Email:                     gobbert@umbc.edu Campus ID:                 AX68683  Request Type:              High Performance Cluster  Hi,  This is surely a funky error, but if trying to compile on c24-01 in an interactive session, for Intel MPI, it says  [gobbert@c24-01 ver1.0solution]$ mpiicc -O3 trap.c -o trap icx: error #10295: error generating temporary file name, check disk space and permissions  A student also reported this to me. The /tmpfs area or something like seems to be full.  Matthias  </pre> </blockquote> </div> "
3286362,72123731,Correspond,DoIT-Research-Computing,2025-10-06 20:24:14.0000000,HPC Other Issue: c24-01 tmp area full,resolved,Danielle Esposito,desposi1,Matthias Gobbert,gobbert,gobbert@umbc.edu,Matthias Gobbert,gobbert@umbc.edu,"Thanks, interesting suggestion. I solved the problem by going to a different node.  Matthias K. Gobbert, Ph.D., Professor of Mathematics Department of Mathematics and Statistics Center for Interdisciplinary Research and Consulting (circ.umbc.edu) UMBC High Performance Computing Facility (hpcf.umbc.edu) REU Site: Online Interdisciplinary Big Data Analytics (BigDataREU.umbc.edu <http://bigdatareu.umbc.edu>) University of Maryland, Baltimore County 1000 Hilltop Circle, Baltimore, MD 21250 http://www.umbc.edu/~gobbert   On Mon, Oct 6, 2025 at 10:21=E2=80=AFAM Danielle Esposito via RT < UMBCHelp@rt.umbc.edu> wrote:  > If you agree your issue is resolved, please give us feedback on your > experience by completing a brief satisfaction survey: > > > https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&customeremail= =3Dgobbert%40umbc.edu&groupid=3DEIS&ticketid=3D3286362&ticketowner=3Ddespos= i1%40umbc.edu&ticketsubject=3DHPC%20Other%20Issue%3A%20c24-01%20tmp%20area%= 20full > > If you believe your issue has not been resolved, please respond to this > message, which will reopen your ticket. Note: A full record of your reque= st > can be found at: > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3286362 > > > Thank You > > _________________________________________ > > R e s o l u t i o n: > =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D = =3D > > Hi Matthias, > > Thank you for letting us know. The issue has been resolved. > > Additionally, if this occurs in the future, you could attempt to change t= he > location that the compiler uses to temporarily store files. I believe this > would be achieved by setting the $TMPDIR environment variable. For > example, you > could try to run... > > TMPDIR=3D/scratch/$JOB_ID/ mpiicc -O3 trap.c -o trap > > -- > > Kind regards, > Danielle Esposito (she/her/hers) > DoIT Unix Infra Student Worker > > On Sun Oct 05 20:20:51 2025, ZZ99999 wrote: > > > First Name:                Matthias > > Last Name:                 Gobbert > > Email:                     gobbert@umbc.edu > > Campus ID:                 AX68683 > > > > Request Type:              High Performance Cluster > > > > Hi, > > > > This is surely a funky error, but if trying to compile on c24-01 in an > interactive session, for Intel MPI, it says > > > > [gobbert@c24-01 ver1.0solution]$ mpiicc -O3 trap.c -o trap > > icx: error #10295: error generating temporary file name, check disk > space and permissions > > > > A student also reported this to me. The /tmpfs area or something like > seems to be full. > > > > Matthias > > > > > ______________________________________ > > Original Request: > > Requestors: Matthias Gobbert > > First Name:                Matthias > Last Name:                 Gobbert > Email:                     gobbert@umbc.edu > Campus ID:                 AX68683 > > Request Type:              High Performance Cluster > > Hi, > > This is surely a funky error, but if trying to compile on c24-01 in an > interactive session, for Intel MPI, it says > > [gobbert@c24-01 ver1.0solution]$ mpiicc -O3 trap.c -o trap > icx: error #10295: error generating temporary file name, check disk space > and permissions > > A student also reported this to me. The /tmpfs area or something like > seems to be full. > > Matthias > > > "
3286362,72123731,Correspond,DoIT-Research-Computing,2025-10-06 20:24:14.0000000,HPC Other Issue: c24-01 tmp area full,resolved,Danielle Esposito,desposi1,Matthias Gobbert,gobbert,gobbert@umbc.edu,Matthias Gobbert,gobbert@umbc.edu,"<div dir=3D""ltr""><div>Thanks, interesting suggestion.</div><div>I solved th= e problem by going to a different node.</div><div><div dir=3D""ltr"" class=3D= ""gmail_signature"" data-smartmail=3D""gmail_signature""><div dir=3D""ltr""><div>= <br></div><div>Matthias K. Gobbert, Ph.D., Professor of Mathematics</div><d= iv>Department of Mathematics and Statistics</div><div>Center for Interdisci= plinary Research and Consulting (<a href=3D""http://circ.umbc.edu"" target=3D= ""_blank"">circ.umbc.edu</a>)</div><div>UMBC High Performance Computing Facil= ity (<a href=3D""http://hpcf.umbc.edu"" target=3D""_blank"">hpcf.umbc.edu</a>)<= /div><div>REU Site: Online Interdisciplinary Big Data Analytics (<a href=3D= ""http://bigdatareu.umbc.edu"" target=3D""_blank"">BigDataREU.umbc.edu</a>)</di= v><div>University of Maryland, Baltimore County</div><div>1000 Hilltop Circ= le, Baltimore, MD 21250</div><div><a href=3D""http://www.umbc.edu/~gobbert"" = target=3D""_blank"">http://www.umbc.edu/~gobbert</a></div></div></div></div><= br></div><br><div class=3D""gmail_quote gmail_quote_container""><div dir=3D""l= tr"" class=3D""gmail_attr"">On Mon, Oct 6, 2025 at 10:21=E2=80=AFAM Danielle E= sposito via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc= .edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""mar= gin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1= ex"">If you agree your issue is resolved, please give us feedback on your ex= perience by completing a brief satisfaction survey: <br> <br> <a href=3D""https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&amp;= customeremail=3Dgobbert%40umbc.edu&amp;groupid=3DEIS&amp;ticketid=3D3286362= &amp;ticketowner=3Ddesposi1%40umbc.edu&amp;ticketsubject=3DHPC%20Other%20Is= sue%3A%20c24-01%20tmp%20area%20full"" rel=3D""noreferrer"" target=3D""_blank"">h= ttps://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&amp;customeremai= l=3Dgobbert%40umbc.edu&amp;groupid=3DEIS&amp;ticketid=3D3286362&amp;ticketo= wner=3Ddesposi1%40umbc.edu&amp;ticketsubject=3DHPC%20Other%20Issue%3A%20c24= -01%20tmp%20area%20full</a><br> <br> If you believe your issue has not been resolved, please respond to this mes= sage, which will reopen your ticket. Note: A full record of your request ca= n be found at:=C2=A0 <br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D328= 6362"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3286362</a> &gt;<br> <br> Thank You<br> <br> _________________________________________<br> <br> R e s o l u t i o n:<br> =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D= =C2=A0 <br> <br> Hi Matthias,<br> <br> Thank you for letting us know. The issue has been resolved.<br> <br> Additionally, if this occurs in the future, you could attempt to change the= <br> location that the compiler uses to temporarily store files. I believe this<= br> would be achieved by setting the $TMPDIR environment variable. For example,=  you<br> could try to run...<br> <br> TMPDIR=3D/scratch/$JOB_ID/ mpiicc -O3 trap.c -o trap<br> <br> --<br> <br> Kind regards,<br> Danielle Esposito (she/her/hers)<br> DoIT Unix Infra Student Worker<br> <br> On Sun Oct 05 20:20:51 2025, ZZ99999 wrote:<br> <br> &gt; First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Mat= thias<br> &gt; Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0Gobbert<br> &gt; Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 =C2=A0<a href=3D""mailto:gobbert@umbc.edu"" target=3D""_blank"">gobbert@= umbc.edu</a><br> &gt; Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0AX68683<br> &gt; <br> &gt; Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Per= formance Cluster<br> &gt; <br> &gt; Hi,<br> &gt; <br> &gt; This is surely a funky error, but if trying to compile on c24-01 in an=  interactive session, for Intel MPI, it says<br> &gt; <br> &gt; [gobbert@c24-01 ver1.0solution]$ mpiicc -O3 trap.c -o trap<br> &gt; icx: error #10295: error generating temporary file name, check disk sp= ace and permissions<br> &gt; <br> &gt; A student also reported this to me. The /tmpfs area or something like = seems to be full.<br> &gt; <br> &gt; Matthias<br> <br> <br> <br> <br> ______________________________________<br> <br> Original Request:<br> <br> Requestors: Matthias Gobbert<br> <br> First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Matthias= <br> Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0Gob= bert<br> Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=  =C2=A0<a href=3D""mailto:gobbert@umbc.edu"" target=3D""_blank"">gobbert@umbc.e= du</a><br> Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0AX6= 8683<br> <br> Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Performa= nce Cluster<br> <br> Hi,<br> <br> This is surely a funky error, but if trying to compile on c24-01 in an inte= ractive session, for Intel MPI, it says<br> <br> [gobbert@c24-01 ver1.0solution]$ mpiicc -O3 trap.c -o trap<br> icx: error #10295: error generating temporary file name, check disk space a= nd permissions<br> <br> A student also reported this to me. The /tmpfs area or something like seems=  to be full.<br> <br> Matthias<br> <br> <br> </blockquote></div> "
3286765,72113961,Create,DoIT-Research-Computing,2025-10-06 16:49:34.0000000,HPC Other Issue: Can I get the H100,resolved,Danielle Esposito,desposi1,Samrat Bojanola,samratb1,samratb1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Samrat Last Name:                 Bojanola Email:                     samratb1@umbc.edu Campus ID:                 NP42296  Request Type:              High Performance Cluster  I=E2=80=99ve submitted a SLURM job requesting 2 H100 GPUs, but it=E2=80=99s=  currently pending. I=E2=80=99d like to request access to those resources. = I=E2=80=99m working with the UMBC-CREM Center.  CLUSTER: chip-gpu              JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIS= T(REASON)             101669       gpu gemma_fi samratb1 PD       0:00      1 (Priori= ty)  "
3286765,72120417,Correspond,DoIT-Research-Computing,2025-10-06 19:11:20.0000000,HPC Other Issue: Can I get the H100,resolved,Danielle Esposito,desposi1,Samrat Bojanola,samratb1,samratb1@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Samrat,&nbsp;</p>  <p>Chip is a shared resource. Unless your group has contributed the H100 nodes, access to the node is shared between all of the clusters users. Normally, your job would run after their job has completed. I suggest waiting until the currently running jobs complete, then your job will run. Or, if your job does not actually&nbsp;<strong>require</strong>&nbsp;two H100s, you could try to run it on other GPU hardware, such as L40S&#39;s or RTX_8000s. There is a greater amount of these nodes available, which would reduce the time it takes for your job to run. Let me know if you have any additional questions. Have a nice day!</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Mon Oct 06 12:49:34 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Samrat Last Name:                 Bojanola Email:                     samratb1@umbc.edu Campus ID:                 NP42296  Request Type:              High Performance Cluster  I&rsquo;ve submitted a SLURM job requesting 2 H100 GPUs, but it&rsquo;s currently pending. I&rsquo;d like to request access to those resources. I&rsquo;m working with the UMBC-CREM Center.  CLUSTER: chip-gpu              JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)             101669       gpu gemma_fi samratb1 PD       0:00      1 (Priority)  </pre> </blockquote> </div> "
3286853,72116250,Create,DoIT-Research-Computing,2025-10-06 17:41:02.0000000,HPC Other Issue: need authorization in the common directories,resolved,Danielle Esposito,desposi1,Maricel Kann,mkann,mkann@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Maricel Last Name:                 Kann Email:                     mkann@umbc.edu Campus ID:                 RS61560  Request Type:              High Performance Cluster  Hello, I have recently had taki files transfer to chip, could you helps setting up the new common directory so all users in my lab have access and can edit, etc. The way it is now we don't have permissions set up for that. Thanks, Maricel.  "
3286853,72117286,Correspond,DoIT-Research-Computing,2025-10-06 17:57:35.0000000,HPC Other Issue: need authorization in the common directories,resolved,Danielle Esposito,desposi1,Maricel Kann,mkann,mkann@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Maricel,</p>  <p>I resolved the permission issues with pi_mkann/common. Let me know if you continue to experience errors when accessing that directory. Have a nice day!</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Mon Oct 06 13:41:02 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Maricel Last Name:                 Kann Email:                     mkann@umbc.edu Campus ID:                 RS61560  Request Type:              High Performance Cluster  Hello, I have recently had taki files transfer to chip, could you helps setting up the new common directory so all users in my lab have access and can edit, etc. The way it is now we don&#39;t have permissions set up for that. Thanks, Maricel.  </pre> </blockquote> </div> "
3286853,72197579,Correspond,DoIT-Research-Computing,2025-10-09 23:23:10.0000000,HPC Other Issue: need authorization in the common directories,resolved,Danielle Esposito,desposi1,Maricel Kann,mkann,mkann@umbc.edu,Maricel Kann,mkann@umbc.edu,"My student still gets access denied when she tried maybe you can see what t= he problem is, M.=20 Sent from my iPhone  > On Oct 6, 2025, at 1:57=E2=80=AFPM, Danielle Esposito via RT <UMBCHelp@rt= .umbc.edu> wrote: >=20 > =EF=BB=BFIf you agree your issue is resolved, please give us feedback on = your experience by completing a brief satisfaction survey: >=20 > https://www.google.com/url?q=3Dhttps://umbc.us2.qualtrics.com/SE/?SID%3DS= V_etfDUq3MTISF6Ly%26customeremail%3Dmkann%2540umbc.edu%26groupid%3DEIS%26ti= cketid%3D3286853%26ticketowner%3Ddesposi1%2540umbc.edu%26ticketsubject%3DHP= C%2520Other%2520Issue%253A%2520need%2520authorization%2520in%2520the%2520co= mmon%2520directories%2520&source=3Dgmail-imap&ust=3D1760378268000000&usg=3D= AOvVaw1QgVEjotkXyFKr3goPtkKa >=20 > If you believe your issue has not been resolved, please respond to this m= essage, which will reopen your ticket. Note: A full record of your request = can be found at:=20=20 >=20 > Ticket <URL: https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Di= splay.html?id%3D3286853&source=3Dgmail-imap&ust=3D1760378268000000&usg=3DAO= vVaw2RlaZA2EV63oZYiZeqAI1G > >=20 > Thank You >=20 > _________________________________________ >=20 > R e s o l u t i o n: > =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D = =3D=20=20 >=20 > Hi Maricel, >=20 > I resolved the permission issues with pi_mkann/common. Let me know if you > continue to experience errors when accessing that directory. Have a nice = day! >=20 > -- >=20 > Kind regards, > Danielle Esposito (she/her/hers) > DoIT Unix Infra Student Worker >=20 >> On Mon Oct 06 13:41:02 2025, ZZ99999 wrote: >>=20 >> First Name:                Maricel >> Last Name:                 Kann >> Email:                     mkann@umbc.edu >> Campus ID:                 RS61560 >>=20 >> Request Type:              High Performance Cluster >>=20 >> Hello, >> I have recently had taki files transfer to chip, could you helps setting=  up the new common directory so all users in my lab have access and can edi= t, etc. The way it is now we don't have permissions set up for that. >> Thanks, >> Maricel. >=20 >=20 >=20 >=20 > ______________________________________ >=20 > Original Request: >=20 > Requestors: Maricel Kann >=20 > First Name:                Maricel > Last Name:                 Kann > Email:                     mkann@umbc.edu > Campus ID:                 RS61560 >=20 > Request Type:              High Performance Cluster >=20 > Hello, > I have recently had taki files transfer to chip, could you helps setting = up the new common directory so all users in my lab have access and can edit= , etc. The way it is now we don't have permissions set up for that. > Thanks, > Maricel. >=20 >=20  "
3286853,72203020,Correspond,DoIT-Research-Computing,2025-10-10 13:41:08.0000000,HPC Other Issue: need authorization in the common directories,resolved,Danielle Esposito,desposi1,Maricel Kann,mkann,mkann@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Maricel,</p>  <p>I have verified again that the permissions for the common directory are = correct-- So I do not understand why your students are unable to access the=  common directory. Please see below where I tested the permissions for the = common directory using a student account. Could either you or the student w= ho submitted an additional ticket provide some more information with exactl= y what you are attempting to do? Thanks</p>  <p>&nbsp;</p>  <p>[ptembei1@chip-login2 ~]$ pi_mkann_common<br /> [ptembei1@chip-login2 common]$ pwd<br /> /umbc/rs/pi_mkann/common<br /> [ptembei1@chip-login2 common]$ touch test<br /> [ptembei1@chip-login2 common]$ ls<br /> dbraw&nbsp; downloaded_data&nbsp; Projects&nbsp; test<br /> [ptembei1@chip-login2 common]$</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Thu Oct 09 19:23:10 2025, RS61560 wrote: <blockquote> <pre> My student still gets access denied when she tried maybe you can see what t= he problem is, M.=20 Sent from my iPhone  &gt; On Oct 6, 2025, at 1:57=E2=80=AFPM, Danielle Esposito via RT &lt;UMBCH= elp@rt.umbc.edu&gt; wrote: &gt;=20 &gt; =EF=BB=BFIf you agree your issue is resolved, please give us feedback = on your experience by completing a brief satisfaction survey: &gt;=20 &gt; https://www.google.com/url?q=3Dhttps://umbc.us2.qualtrics.com/SE/?SID%= 3DSV_etfDUq3MTISF6Ly%26customeremail%3Dmkann%2540umbc.edu%26groupid%3DEIS%2= 6ticketid%3D3286853%26ticketowner%3Ddesposi1%2540umbc.edu%26ticketsubject%3= DHPC%2520Other%2520Issue%253A%2520need%2520authorization%2520in%2520the%252= 0common%2520directories%2520&amp;source=3Dgmail-imap&amp;ust=3D176037826800= 0000&amp;usg=3DAOvVaw1QgVEjotkXyFKr3goPtkKa &gt;=20 &gt; If you believe your issue has not been resolved, please respond to thi= s message, which will reopen your ticket. Note: A full record of your reque= st can be found at:=20=20 &gt;=20 &gt; Ticket &lt;URL: https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Tic= ket/Display.html?id%3D3286853&amp;source=3Dgmail-imap&amp;ust=3D17603782680= 00000&amp;usg=3DAOvVaw2RlaZA2EV63oZYiZeqAI1G &gt; &gt;=20 &gt; Thank You &gt;=20 &gt; _________________________________________ &gt;=20 &gt; R e s o l u t i o n: &gt; =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D = =3D =3D=20=20 &gt;=20 &gt; Hi Maricel, &gt;=20 &gt; I resolved the permission issues with pi_mkann/common. Let me know if = you &gt; continue to experience errors when accessing that directory. Have a ni= ce day! &gt;=20 &gt; -- &gt;=20 &gt; Kind regards, &gt; Danielle Esposito (she/her/hers) &gt; DoIT Unix Infra Student Worker &gt;=20 &gt;&gt; On Mon Oct 06 13:41:02 2025, ZZ99999 wrote: &gt;&gt;=20 &gt;&gt; First Name:                Maricel &gt;&gt; Last Name:                 Kann &gt;&gt; Email:                     mkann@umbc.edu &gt;&gt; Campus ID:                 RS61560 &gt;&gt;=20 &gt;&gt; Request Type:              High Performance Cluster &gt;&gt;=20 &gt;&gt; Hello, &gt;&gt; I have recently had taki files transfer to chip, could you helps s= etting up the new common directory so all users in my lab have access and c= an edit, etc. The way it is now we don&#39;t have permissions set up for th= at. &gt;&gt; Thanks, &gt;&gt; Maricel. &gt;=20 &gt;=20 &gt;=20 &gt;=20 &gt; ______________________________________ &gt;=20 &gt; Original Request: &gt;=20 &gt; Requestors: Maricel Kann &gt;=20 &gt; First Name:                Maricel &gt; Last Name:                 Kann &gt; Email:                     mkann@umbc.edu &gt; Campus ID:                 RS61560 &gt;=20 &gt; Request Type:              High Performance Cluster &gt;=20 &gt; Hello, &gt; I have recently had taki files transfer to chip, could you helps setti= ng up the new common directory so all users in my lab have access and can e= dit, etc. The way it is now we don&#39;t have permissions set up for that. &gt; Thanks, &gt; Maricel. &gt;=20 &gt;=20  </pre> </blockquote> </div> "
3286853,72203839,Correspond,DoIT-Research-Computing,2025-10-10 14:04:01.0000000,HPC Other Issue: need authorization in the common directories,resolved,Danielle Esposito,desposi1,Maricel Kann,mkann,mkann@umbc.edu,Maricel Kann,mkann@umbc.edu,"Thank you for your help I will talk with Petra to see if I understand the i= ssue, Maricel.=20 Sent from my iPhone  > On Oct 10, 2025, at 9:41=E2=80=AFAM, Danielle Esposito via RT <UMBCHelp@r= t.umbc.edu> wrote: >=20 > =EF=BB=BFTicket <URL: https://www.google.com/url?q=3Dhttps://rt.umbc.edu/= Ticket/Display.html?id%3D3286853&source=3Dgmail-imap&ust=3D1760708474000000= &usg=3DAOvVaw2VPcBHDZt2n9EBkFUYnz4r > >=20 > Last Update From Ticket: >=20 > Hi Maricel, >=20 > I have verified again that the permissions for the common directory are > correct-- So I do not understand why your students are unable to access t= he > common directory. Please see below where I tested the permissions for the > common directory using a student account. Could either you or the student=  who > submitted an additional ticket provide some more information with exactly=  what > you are attempting to do? Thanks >=20 > [ptembei1@chip-login2 ~]$ pi_mkann_common > [ptembei1@chip-login2 common]$ pwd > /umbc/rs/pi_mkann/common > [ptembei1@chip-login2 common]$ touch test > [ptembei1@chip-login2 common]$ ls > dbraw downloaded_data Projects test > [ptembei1@chip-login2 common]$ >=20 > -- >=20 > Kind regards, > Danielle Esposito (she/her/hers) > DoIT Unix Infra Student Worker >=20 >> On Thu Oct 09 19:23:10 2025, RS61560 wrote: >>=20 >> My student still gets access denied when she tried maybe you can see wha= t the problem is, >> M. >> Sent from my iPhone >>=20 >>>> On Oct 6, 2025, at 1:57=E2=80=AFPM, Danielle Esposito via RT <UMBCHelp= @rt.umbc.edu> wrote: >>>=20 >>> =EF=BB=BFIf you agree your issue is resolved, please give us feedback o= n your experience by completing a brief satisfaction survey: >>>=20 >>> https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://u= mbc.us2.qualtrics.com/SE/?SID%253DSV_etfDUq3MTISF6Ly%2526customeremail%253D= mkann%252540umbc.edu%2526groupid%253DEIS%2526ticketid%253D3286853%2526ticke= towner%253Ddesposi1%252540umbc.edu%2526ticketsubject%253DHPC%252520Other%25= 2520Issue%25253A%252520need%252520authorization%252520in%252520the%252520co= mmon%252520directories%252520%26source%3Dgmail-imap%26ust%3D176037826800000= 0%26usg%3DAOvVaw1QgVEjotkXyFKr3goPtkKa&source=3Dgmail-imap&ust=3D1760708474= 000000&usg=3DAOvVaw3PynOVglbs4-cJYVyjt0UP >>>=20 >>> If you believe your issue has not been resolved, please respond to this=  message, which will reopen your ticket. Note: A full record of your reques= t can be found at:=20=20 >>>=20 >>> Ticket <URL: https://www.google.com/url?q=3Dhttps://www.google.com/url?= q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3286853%26source%3Dgmail= -imap%26ust%3D1760378268000000%26usg%3DAOvVaw2RlaZA2EV63oZYiZeqAI1G&source= =3Dgmail-imap&ust=3D1760708474000000&usg=3DAOvVaw3j50qzk76vxglkehEoebBa > >>>=20 >>> Thank You >>>=20 >>> _________________________________________ >>>=20 >>> R e s o l u t i o n: >>> =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D=  =3D=20=20 >>>=20 >>> Hi Maricel, >>>=20 >>> I resolved the permission issues with pi_mkann/common. Let me know if y= ou >>> continue to experience errors when accessing that directory. Have a nic= e day! >>>=20 >>> -- >>>=20 >>> Kind regards, >>> Danielle Esposito (she/her/hers) >>> DoIT Unix Infra Student Worker >>>=20 >>>> On Mon Oct 06 13:41:02 2025, ZZ99999 wrote: >>>>=20 >>>> First Name:                Maricel >>>> Last Name:                 Kann >>>> Email:                     mkann@umbc.edu >>>> Campus ID:                 RS61560 >>>>=20 >>>> Request Type:              High Performance Cluster >>>>=20 >>>> Hello, >>>> I have recently had taki files transfer to chip, could you helps setti= ng up the new common directory so all users in my lab have access and can e= dit, etc. The way it is now we don't have permissions set up for that. >>>> Thanks, >>>> Maricel. >>>=20 >>>=20 >>>=20 >>>=20 >>> ______________________________________ >>>=20 >>> Original Request: >>>=20 >>> Requestors: Maricel Kann >>>=20 >>> First Name:                Maricel >>> Last Name:                 Kann >>> Email:                     mkann@umbc.edu >>> Campus ID:                 RS61560 >>>=20 >>> Request Type:              High Performance Cluster >>>=20 >>> Hello, >>> I have recently had taki files transfer to chip, could you helps settin= g up the new common directory so all users in my lab have access and can ed= it, etc. The way it is now we don't have permissions set up for that. >>> Thanks, >>> Maricel. >>>=20 >>>=20 >=20 >=20 >=20  "
3286853,72210005,Correspond,DoIT-Research-Computing,2025-10-10 16:40:56.0000000,HPC Other Issue: need authorization in the common directories,resolved,Danielle Esposito,desposi1,Maricel Kann,mkann,mkann@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Maricel,</p>  <p>The issue should now be resolved. If you continue to have issues with th= is, feel free to let us know. Have a nice day!</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Fri Oct 10 10:04:01 2025, RS61560 wrote: <blockquote> <pre> Thank you for your help I will talk with Petra to see if I understand the i= ssue, Maricel.=20 Sent from my iPhone  &gt; On Oct 10, 2025, at 9:41=E2=80=AFAM, Danielle Esposito via RT &lt;UMBC= Help@rt.umbc.edu&gt; wrote: &gt;=20 &gt; =EF=BB=BFTicket &lt;URL: https://www.google.com/url?q=3Dhttps://rt.umb= c.edu/Ticket/Display.html?id%3D3286853&amp;source=3Dgmail-imap&amp;ust=3D17= 60708474000000&amp;usg=3DAOvVaw2VPcBHDZt2n9EBkFUYnz4r &gt; &gt;=20 &gt; Last Update From Ticket: &gt;=20 &gt; Hi Maricel, &gt;=20 &gt; I have verified again that the permissions for the common directory are &gt; correct-- So I do not understand why your students are unable to acces= s the &gt; common directory. Please see below where I tested the permissions for = the &gt; common directory using a student account. Could either you or the stud= ent who &gt; submitted an additional ticket provide some more information with exac= tly what &gt; you are attempting to do? Thanks &gt;=20 &gt; [ptembei1@chip-login2 ~]$ pi_mkann_common &gt; [ptembei1@chip-login2 common]$ pwd &gt; /umbc/rs/pi_mkann/common &gt; [ptembei1@chip-login2 common]$ touch test &gt; [ptembei1@chip-login2 common]$ ls &gt; dbraw downloaded_data Projects test &gt; [ptembei1@chip-login2 common]$ &gt;=20 &gt; -- &gt;=20 &gt; Kind regards, &gt; Danielle Esposito (she/her/hers) &gt; DoIT Unix Infra Student Worker &gt;=20 &gt;&gt; On Thu Oct 09 19:23:10 2025, RS61560 wrote: &gt;&gt;=20 &gt;&gt; My student still gets access denied when she tried maybe you can s= ee what the problem is, &gt;&gt; M. &gt;&gt; Sent from my iPhone &gt;&gt;=20 &gt;&gt;&gt;&gt; On Oct 6, 2025, at 1:57=E2=80=AFPM, Danielle Esposito via = RT &lt;UMBCHelp@rt.umbc.edu&gt; wrote: &gt;&gt;&gt;=20 &gt;&gt;&gt; =EF=BB=BFIf you agree your issue is resolved, please give us f= eedback on your experience by completing a brief satisfaction survey: &gt;&gt;&gt;=20 &gt;&gt;&gt; https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3D= https://umbc.us2.qualtrics.com/SE/?SID%253DSV_etfDUq3MTISF6Ly%2526customere= mail%253Dmkann%252540umbc.edu%2526groupid%253DEIS%2526ticketid%253D3286853%= 2526ticketowner%253Ddesposi1%252540umbc.edu%2526ticketsubject%253DHPC%25252= 0Other%252520Issue%25253A%252520need%252520authorization%252520in%252520the= %252520common%252520directories%252520%26source%3Dgmail-imap%26ust%3D176037= 8268000000%26usg%3DAOvVaw1QgVEjotkXyFKr3goPtkKa&amp;source=3Dgmail-imap&amp= ;ust=3D1760708474000000&amp;usg=3DAOvVaw3PynOVglbs4-cJYVyjt0UP &gt;&gt;&gt;=20 &gt;&gt;&gt; If you believe your issue has not been resolved, please respon= d to this message, which will reopen your ticket. Note: A full record of yo= ur request can be found at:=20=20 &gt;&gt;&gt;=20 &gt;&gt;&gt; Ticket &lt;URL: https://www.google.com/url?q=3Dhttps://www.goo= gle.com/url?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3286853%26so= urce%3Dgmail-imap%26ust%3D1760378268000000%26usg%3DAOvVaw2RlaZA2EV63oZYiZeq= AI1G&amp;source=3Dgmail-imap&amp;ust=3D1760708474000000&amp;usg=3DAOvVaw3j5= 0qzk76vxglkehEoebBa &gt; &gt;&gt;&gt;=20 &gt;&gt;&gt; Thank You &gt;&gt;&gt;=20 &gt;&gt;&gt; _________________________________________ &gt;&gt;&gt;=20 &gt;&gt;&gt; R e s o l u t i o n: &gt;&gt;&gt; =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D = =3D =3D =3D =3D=20=20 &gt;&gt;&gt;=20 &gt;&gt;&gt; Hi Maricel, &gt;&gt;&gt;=20 &gt;&gt;&gt; I resolved the permission issues with pi_mkann/common. Let me = know if you &gt;&gt;&gt; continue to experience errors when accessing that directory. H= ave a nice day! &gt;&gt;&gt;=20 &gt;&gt;&gt; -- &gt;&gt;&gt;=20 &gt;&gt;&gt; Kind regards, &gt;&gt;&gt; Danielle Esposito (she/her/hers) &gt;&gt;&gt; DoIT Unix Infra Student Worker &gt;&gt;&gt;=20 &gt;&gt;&gt;&gt; On Mon Oct 06 13:41:02 2025, ZZ99999 wrote: &gt;&gt;&gt;&gt;=20 &gt;&gt;&gt;&gt; First Name:                Maricel &gt;&gt;&gt;&gt; Last Name:                 Kann &gt;&gt;&gt;&gt; Email:                     mkann@umbc.edu &gt;&gt;&gt;&gt; Campus ID:                 RS61560 &gt;&gt;&gt;&gt;=20 &gt;&gt;&gt;&gt; Request Type:              High Performance Cluster &gt;&gt;&gt;&gt;=20 &gt;&gt;&gt;&gt; Hello, &gt;&gt;&gt;&gt; I have recently had taki files transfer to chip, could you=  helps setting up the new common directory so all users in my lab have acce= ss and can edit, etc. The way it is now we don&#39;t have permissions set u= p for that. &gt;&gt;&gt;&gt; Thanks, &gt;&gt;&gt;&gt; Maricel. &gt;&gt;&gt;=20 &gt;&gt;&gt;=20 &gt;&gt;&gt;=20 &gt;&gt;&gt;=20 &gt;&gt;&gt; ______________________________________ &gt;&gt;&gt;=20 &gt;&gt;&gt; Original Request: &gt;&gt;&gt;=20 &gt;&gt;&gt; Requestors: Maricel Kann &gt;&gt;&gt;=20 &gt;&gt;&gt; First Name:                Maricel &gt;&gt;&gt; Last Name:                 Kann &gt;&gt;&gt; Email:                     mkann@umbc.edu &gt;&gt;&gt; Campus ID:                 RS61560 &gt;&gt;&gt;=20 &gt;&gt;&gt; Request Type:              High Performance Cluster &gt;&gt;&gt;=20 &gt;&gt;&gt; Hello, &gt;&gt;&gt; I have recently had taki files transfer to chip, could you hel= ps setting up the new common directory so all users in my lab have access a= nd can edit, etc. The way it is now we don&#39;t have permissions set up fo= r that. &gt;&gt;&gt; Thanks, &gt;&gt;&gt; Maricel. &gt;&gt;&gt;=20 &gt;&gt;&gt;=20 &gt;=20 &gt;=20 &gt;=20  </pre> </blockquote> </div> "
3286959,72119915,Create,DoIT-Research-Computing,2025-10-06 19:01:48.0000000,HPC User Account: mrippin1 in Student Group,resolved,Elliot Gobbert,elliotg2,Madeline Rippin,mrippin1,mrippin1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Madeline Last Name:                 Rippin Email:                     mrippin1@umbc.edu Campus ID:                 SV92175  Request Type:              High Performance Cluster  Create/Modify account in Student group  I am doing my thesis with Dr. Tim Oates and require access to the HPC clusters to run benchmarks on MLLMs  "
3286959,72129727,Correspond,DoIT-Research-Computing,2025-10-07 13:27:01.0000000,HPC User Account: mrippin1 in Student Group,resolved,Elliot Gobbert,elliotg2,Madeline Rippin,mrippin1,mrippin1@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hello there,</p>  <p>If you&#39;re under Dr. Oates, if he has an HPC account, he could easily sponsor your account, making things much easier on the cluster. Feel free to join the student group if Dr. Oates requested you to do that, however.</p>  <p>Anyways, here is the official link (A guide on how to create an HPC account):</p>  <p>https://umbc.atlassian.net/wiki/spaces/faq/pages/1327431728/How+to+request+a+user+group+account+on+chip</p>  <p>Remember that if Dr. Oates is sponsoring your account, we would need written permission from him. That&#39;s as simple as cc&#39;ing him to the ticket creation, and having him say &quot;I approve&quot; or something similar in a reply.</p>  <p>&nbsp;</p>  <p>Best,</p>  <p>Elliot Gobbert</p> "
3287359,72132310,Create,DoIT-Research-Computing,2025-10-07 14:15:30.0000000,HPC Slurm/Software Issue: Module Not Available,resolved,Danielle Esposito,desposi1,Abdullah Al Imran,wv96094,wv96094@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Abdullah Al Last Name:                 Imran Email:                     wv96094@umbc.edu Campus ID:                 WV96094  Request Type:              High Performance Cluster   Hello,   I need MPICH, CGNS, PETSC modules to run my cases on hpcf cluster. But these modules are not available there to load! Could you please load the latest version of these three modules to the cluster and let me know when it's ready?  Thanks, Imran  "
3287359,72143883,Comment,DoIT-Research-Computing,2025-10-07 17:50:28.0000000,HPC Slurm/Software Issue: Module Not Available,resolved,Danielle Esposito,desposi1,Abdullah Al Imran,wv96094,wv96094@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>installed mpich&nbsp;</p>  <p>MPICH-4.2.2-GCC-13.3.0.eb<br /> MPICH-4.2.1-GCC-12.3.0.eb</p>  <p>working on installing petsc</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Tue Oct 07 10:15:30 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Abdullah Al Last Name:                 Imran Email:                     wv96094@umbc.edu Campus ID:                 WV96094  Request Type:              High Performance Cluster   Hello,   I need MPICH, CGNS, PETSC modules to run my cases on hpcf cluster. But these modules are not available there to load! Could you please load the latest version of these three modules to the cluster and let me know when it&#39;s ready?  Thanks, Imran  </pre> </blockquote> </div> "
3287359,72185625,Correspond,DoIT-Research-Computing,2025-10-09 15:20:57.0000000,HPC Slurm/Software Issue: Module Not Available,resolved,Danielle Esposito,desposi1,Abdullah Al Imran,wv96094,wv96094@umbc.edu,Abdullah Al Imran,wv96094@umbc.edu,"Hello,  Is there any update on the ticket?  Thanks, Imran   On Tue, Oct 7, 2025 at 10:15=E2=80=AFAM via RT <UMBCHelp@rt.umbc.edu> wrote:  > Greetings, > > This message has been automatically generated in response to the > creation of a ticket regarding: > > ------------------------------------------------------------------------- > Subject: ""HPC Slurm/Software Issue: Module Not Available"" > > Message: > > First Name:                Abdullah Al > Last Name:                 Imran > Email:                     wv96094@umbc.edu > Campus ID:                 WV96094 > > Request Type:              High Performance Cluster > > > Hello, > > I need MPICH, CGNS, PETSC modules to run my cases on hpcf cluster. But > these modules are not available there to load! Could you please load the > latest version of these three modules to the cluster and let me know when > it's ready? > > Thanks, > Imran > > > ------------------------------------------------------------------------- > > There is no need to reply to this message right now. > > Your ticket has been assigned an ID of [Research Computing #3287359] or > you can go there directly by clicking the link below. > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287359 > > > You can login to view your open tickets at any time by visiting > http://my.umbc.edu and clicking on ""Help"" and ""Request Help"". > > Alternately you can click on http://my.umbc.edu/help > >                         Thank you > > "
3287359,72185625,Correspond,DoIT-Research-Computing,2025-10-09 15:20:57.0000000,HPC Slurm/Software Issue: Module Not Available,resolved,Danielle Esposito,desposi1,Abdullah Al Imran,wv96094,wv96094@umbc.edu,Abdullah Al Imran,wv96094@umbc.edu,"<div dir=3D""ltr""><div>Hello,</div><div><br></div><div>Is there any update o= n the ticket?</div><div><br></div><div><div dir=3D""ltr"" class=3D""gmail_sign= ature"" data-smartmail=3D""gmail_signature""><div dir=3D""ltr""><div style=3D""co= lor:rgb(34,34,34)"">Thanks,</div><div style=3D""color:rgb(34,34,34)"">Imran</d= iv></div></div></div><br></div><br><div class=3D""gmail_quote gmail_quote_co= ntainer""><div dir=3D""ltr"" class=3D""gmail_attr"">On Tue, Oct 7, 2025 at 10:15= =E2=80=AFAM via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.= umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D= ""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-le= ft:1ex"">Greetings,<br> <br> This message has been automatically generated in response to the<br> creation of a ticket regarding:<br> <br> -------------------------------------------------------------------------<b= r> Subject: &quot;HPC Slurm/Software Issue: Module Not Available&quot;<br> <br> Message: <br> <br> First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Abdullah=  Al<br> Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0Imr= an<br> Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=  =C2=A0<a href=3D""mailto:wv96094@umbc.edu"" target=3D""_blank"">wv96094@umbc.e= du</a><br> Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0WV9= 6094<br> <br> Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Performa= nce Cluster<br> <br> <br> Hello, <br> <br> I need MPICH, CGNS, PETSC modules to run my cases on hpcf cluster. But thes= e modules are not available there to load! Could you please load the latest=  version of these three modules to the cluster and let me know when it&#39;= s ready?<br> <br> Thanks,<br> Imran<br> <br> <br> -------------------------------------------------------------------------<b= r> <br> There is no need to reply to this message right now.=C2=A0 <br> <br> Your ticket has been assigned an ID of [Research Computing #3287359] or you=  can go there directly by clicking the link below.<br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D328= 7359"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3287359</a> &gt;<br> <br> You can login to view your open tickets at any time by visiting <a href=3D""= http://my.umbc.edu"" rel=3D""noreferrer"" target=3D""_blank"">http://my.umbc.edu= </a> and clicking on &quot;Help&quot; and &quot;Request Help&quot;. <br> <br> Alternately you can click on <a href=3D""http://my.umbc.edu/help"" rel=3D""nor= eferrer"" target=3D""_blank"">http://my.umbc.edu/help</a><br> <br> =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 Thank you<br> <br> </blockquote></div> "
3287359,72185854,Correspond,DoIT-Research-Computing,2025-10-09 15:27:51.0000000,HPC Slurm/Software Issue: Module Not Available,resolved,Danielle Esposito,desposi1,Abdullah Al Imran,wv96094,wv96094@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi&nbsp;Imran,</p>  <p>Yes, I was just about to update this ticket. I installed MPICH versions = 4.2.2 and 4.2.1, along with PETSc 3.20.3. For CGNS, I recommend that you co= mpile it from source. The instructions for compiling CGNS from source are l= ocated on their github page:&nbsp;https://github.com/CGNS/CGNS. You should = just need to load HDF5 and Cmake, then configure the install directory to b= e the path to your volumes, and you should be all set. If you run into any = issues or have further questions, let me know. Have a nice day!</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Thu Oct 09 11:20:57 2025, WV96094 wrote: <blockquote> <div> <div>Hello,</div>  <div>&nbsp;</div>  <div>Is there any update on the ticket?</div>  <div>&nbsp;</div>  <div> <div> <div> <div>Thanks,</div>  <div>Imran</div> </div> </div> </div> </div> &nbsp;  <div> <div>On Tue, Oct 7, 2025 at 10:15=E2=80=AFAM via RT &lt;UMBCHelp@rt.umbc.ed= u&gt; wrote:</div>  <blockquote>Greetings,<br /> <br /> This message has been automatically generated in response to the<br /> creation of a ticket regarding:<br /> <br /> -------------------------------------------------------------------------<b= r /> Subject: &quot;HPC Slurm/Software Issue: Module Not Available&quot;<br /> <br /> Message:<br /> <br /> First Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Abdullah=  Al<br /> Last Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Imr= an<br /> Email:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;=  &nbsp;wv96094@umbc.edu<br /> Campus ID:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;WV9= 6094<br /> <br /> Request Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; High Performa= nce Cluster<br /> <br /> <br /> Hello,<br /> <br /> I need MPICH, CGNS, PETSC modules to run my cases on hpcf cluster. But thes= e modules are not available there to load! Could you please load the latest=  version of these three modules to the cluster and let me know when it&#39;= s ready?<br /> <br /> Thanks,<br /> Imran<br /> <br /> <br /> -------------------------------------------------------------------------<b= r /> <br /> There is no need to reply to this message right now.&nbsp;<br /> <br /> Your ticket has been assigned an ID of [Research Computing #3287359] or you=  can go there directly by clicking the link below.<br /> <br /> Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287359 &gt;<b= r /> <br /> You can login to view your open tickets at any time by visiting http://my.u= mbc.edu and clicking on &quot;Help&quot; and &quot;Request Help&quot;.<br /> <br /> Alternately you can click on http://my.umbc.edu/help<br /> <br /> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp= ; &nbsp; Thank you<br /> &nbsp;</blockquote> </div> </blockquote> </div> "
3287359,72186847,Correspond,DoIT-Research-Computing,2025-10-09 15:52:15.0000000,HPC Slurm/Software Issue: Module Not Available,resolved,Danielle Esposito,desposi1,Abdullah Al Imran,wv96094,wv96094@umbc.edu,Abdullah Al Imran,wv96094@umbc.edu,"Hi Danielle,  Thank you very much for the update. Couldn't you please load the CGNS also? Because I could be wrong as I am not that much familiar with this chip system yet. My assumption is, if I install it by myself, then I have to do it again and again whenever I enter into the system, and run some cases. Please correct me if I am wrong! And I also need your help to make a bash file so that I don't have to install all of the modules that I need to run my cases every time separately. I need lots of modules to run every single test case of mine.  Thanks, Imran   On Thu, Oct 9, 2025 at 11:27=E2=80=AFAM Danielle Esposito via RT < UMBCHelp@rt.umbc.edu> wrote:  > If you agree your issue is resolved, please give us feedback on your > experience by completing a brief satisfaction survey: > > > https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&customeremail= =3Dwv96094%40umbc.edu&groupid=3DEIS&ticketid=3D3287359&ticketowner=3Ddespos= i1%40umbc.edu&ticketsubject=3DHPC%20Slurm%2FSoftware%20Issue%3A%20Module%20= Not%20Available > > If you believe your issue has not been resolved, please respond to this > message, which will reopen your ticket. Note: A full record of your reque= st > can be found at: > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287359 > > > Thank You > > _________________________________________ > > R e s o l u t i o n: > =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D = =3D > > Hi Imran, > > Yes, I was just about to update this ticket. I installed MPICH versions > 4.2.2 > and 4.2.1, along with PETSc 3.20.3. For CGNS, I recommend that you compile > it > from source. The instructions for compiling CGNS from source are located = on > their github page: https://github.com/CGNS/CGNS. You should just need to > load > HDF5 and Cmake, then configure the install directory to be the path to yo= ur > volumes, and you should be all set. If you run into any issues or have > further > questions, let me know. Have a nice day! > > -- > > Kind regards, > Danielle Esposito (she/her/hers) > DoIT Unix Infra Student Worker > > On Thu Oct 09 11:20:57 2025, WV96094 wrote: > > > Hello, Is there any update on the ticket? Thanks,Imran On Tue, Oct 7, > 2025 > > at 10:15 AM via RT <UMBCHelp@rt.umbc.edu> wrote: > > >> Greetings, > > >> This message has been automatically generated in response to the > >> creation of a ticket regarding: > > >> > ------------------------------------------------------------------------- > >> Subject: ""HPC Slurm/Software Issue: Module Not Available"" > > >> Message: > > >> First Name: Abdullah Al > >> Last Name: Imran > >> Email: wv96094@umbc.edu > >> Campus ID: WV96094 > > >> Request Type: High Performance Cluster > > > >> Hello, > > >> I need MPICH, CGNS, PETSC modules to run my cases on hpcf cluster. But > >> these modules are not available there to load! Could you please load > >> the latest version of these three modules to the cluster and let me > >> know when it's ready? > > >> Thanks, > >> Imran > > > >> > ------------------------------------------------------------------------- > > >> There is no need to reply to this message right now. > > >> Your ticket has been assigned an ID of [Research Computing #3287359] or > >> you can go there directly by clicking the link below. > > >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287359 > > > >> You can login to view your open tickets at any time by visiting > >> http://my.umbc.edu and clicking on ""Help"" and ""Request Help"". > > >> Alternately you can click on http://my.umbc.edu/help > > >> Thank you > > > > ______________________________________ > > Original Request: > > Requestors: Abdullah Al Imran > > First Name:                Abdullah Al > Last Name:                 Imran > Email:                     wv96094@umbc.edu > Campus ID:                 WV96094 > > Request Type:              High Performance Cluster > > > Hello, > > I need MPICH, CGNS, PETSC modules to run my cases on hpcf cluster. But > these modules are not available there to load! Could you please load the > latest version of these three modules to the cluster and let me know when > it's ready? > > Thanks, > Imran > > > "
3287359,72186847,Correspond,DoIT-Research-Computing,2025-10-09 15:52:15.0000000,HPC Slurm/Software Issue: Module Not Available,resolved,Danielle Esposito,desposi1,Abdullah Al Imran,wv96094,wv96094@umbc.edu,Abdullah Al Imran,wv96094@umbc.edu,"<div dir=3D""ltr""><div>Hi Danielle,</div><div><br></div><div>Thank you very = much for the update. Couldn&#39;t you please load the CGNS also? Because I = could be wrong as I am not that much familiar with this chip system yet. My=  assumption is, if I install it by myself, then I have to do it again and a= gain whenever I enter=C2=A0into the system, and run some cases. Please corr= ect me=C2=A0if I am wrong! And I also need your help to make a bash file so=  that I don&#39;t have to install all of the modules that I need to run my = cases every time separately. I need lots of modules to run every single tes= t case of mine.</div><div><br></div><div><div dir=3D""ltr"" class=3D""gmail_si= gnature"" data-smartmail=3D""gmail_signature""><div dir=3D""ltr""><div style=3D""= color:rgb(34,34,34)"">Thanks,</div><div style=3D""color:rgb(34,34,34)"">Imran<= /div></div></div></div><br></div><br><div class=3D""gmail_quote gmail_quote_= container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Thu, Oct 9, 2025 at 11:= 27=E2=80=AFAM Danielle Esposito via RT &lt;<a href=3D""mailto:UMBCHelp@rt.um= bc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""g= mail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204= ,204,204);padding-left:1ex"">If you agree your issue is resolved, please giv= e us feedback on your experience by completing a brief satisfaction survey:=  <br> <br> <a href=3D""https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&amp;= customeremail=3Dwv96094%40umbc.edu&amp;groupid=3DEIS&amp;ticketid=3D3287359= &amp;ticketowner=3Ddesposi1%40umbc.edu&amp;ticketsubject=3DHPC%20Slurm%2FSo= ftware%20Issue%3A%20Module%20Not%20Available"" rel=3D""noreferrer"" target=3D""= _blank"">https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&amp;cus= tomeremail=3Dwv96094%40umbc.edu&amp;groupid=3DEIS&amp;ticketid=3D3287359&am= p;ticketowner=3Ddesposi1%40umbc.edu&amp;ticketsubject=3DHPC%20Slurm%2FSoftw= are%20Issue%3A%20Module%20Not%20Available</a><br> <br> If you believe your issue has not been resolved, please respond to this mes= sage, which will reopen your ticket. Note: A full record of your request ca= n be found at:=C2=A0 <br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D328= 7359"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3287359</a> &gt;<br> <br> Thank You<br> <br> _________________________________________<br> <br> R e s o l u t i o n:<br> =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D= =C2=A0 <br> <br> Hi Imran,<br> <br> Yes, I was just about to update this ticket. I installed MPICH versions 4.2= .2<br> and 4.2.1, along with PETSc 3.20.3. For CGNS, I recommend that you compile = it<br> from source. The instructions for compiling CGNS from source are located on= <br> their github page: <a href=3D""https://github.com/CGNS/CGNS"" rel=3D""noreferr= er"" target=3D""_blank"">https://github.com/CGNS/CGNS</a>. You should just nee= d to load<br> HDF5 and Cmake, then configure the install directory to be the path to your= <br> volumes, and you should be all set. If you run into any issues or have furt= her<br> questions, let me know. Have a nice day!<br> <br> --<br> <br> Kind regards,<br> Danielle Esposito (she/her/hers)<br> DoIT Unix Infra Student Worker<br> <br> On Thu Oct 09 11:20:57 2025, WV96094 wrote:<br> <br> &gt; Hello, Is there any update on the ticket? Thanks,Imran On Tue, Oct 7, = 2025<br> &gt; at 10:15 AM via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target= =3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; Greetings,<br> <br> &gt;&gt; This message has been automatically generated in response to the<b= r> &gt;&gt; creation of a ticket regarding:<br> <br> &gt;&gt; ------------------------------------------------------------------= -------<br> &gt;&gt; Subject: &quot;HPC Slurm/Software Issue: Module Not Available&quot= ;<br> <br> &gt;&gt; Message:<br> <br> &gt;&gt; First Name: Abdullah Al<br> &gt;&gt; Last Name: Imran<br> &gt;&gt; Email: <a href=3D""mailto:wv96094@umbc.edu"" target=3D""_blank"">wv960= 94@umbc.edu</a><br> &gt;&gt; Campus ID: WV96094<br> <br> &gt;&gt; Request Type: High Performance Cluster<br> <br> <br> &gt;&gt; Hello,<br> <br> &gt;&gt; I need MPICH, CGNS, PETSC modules to run my cases on hpcf cluster.=  But<br> &gt;&gt; these modules are not available there to load! Could you please lo= ad<br> &gt;&gt; the latest version of these three modules to the cluster and let m= e<br> &gt;&gt; know when it&#39;s ready?<br> <br> &gt;&gt; Thanks,<br> &gt;&gt; Imran<br> <br> <br> &gt;&gt; ------------------------------------------------------------------= -------<br> <br> &gt;&gt; There is no need to reply to this message right now.<br> <br> &gt;&gt; Your ticket has been assigned an ID of [Research Computing #328735= 9] or<br> &gt;&gt; you can go there directly by clicking the link below.<br> <br> &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html= ?id=3D3287359"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tic= ket/Display.html?id=3D3287359</a> &gt;<br> <br> &gt;&gt; You can login to view your open tickets at any time by visiting<br> &gt;&gt; <a href=3D""http://my.umbc.edu"" rel=3D""noreferrer"" target=3D""_blank= "">http://my.umbc.edu</a> and clicking on &quot;Help&quot; and &quot;Request=  Help&quot;.<br> <br> &gt;&gt; Alternately you can click on <a href=3D""http://my.umbc.edu/help"" r= el=3D""noreferrer"" target=3D""_blank"">http://my.umbc.edu/help</a><br> <br> &gt;&gt; Thank you<br> <br> <br> <br> ______________________________________<br> <br> Original Request:<br> <br> Requestors: Abdullah Al Imran<br> <br> First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Abdullah=  Al<br> Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0Imr= an<br> Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=  =C2=A0<a href=3D""mailto:wv96094@umbc.edu"" target=3D""_blank"">wv96094@umbc.e= du</a><br> Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0WV9= 6094<br> <br> Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Performa= nce Cluster<br> <br> <br> Hello, <br> <br> I need MPICH, CGNS, PETSC modules to run my cases on hpcf cluster. But thes= e modules are not available there to load! Could you please load the latest=  version of these three modules to the cluster and let me know when it&#39;= s ready?<br> <br> Thanks,<br> Imran<br> <br> <br> </blockquote></div> "
3287359,72187025,Comment,DoIT-Research-Computing,2025-10-09 15:56:28.0000000,HPC Slurm/Software Issue: Module Not Available,resolved,Danielle Esposito,desposi1,Abdullah Al Imran,wv96094,wv96094@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Clarify what she means by install? Does she mean load??&nbsp;</p>  <p>On Thu Oct 09 11:52:15 2025, WV96094 wrote:</p>  <blockquote> <div> <div>Hi Danielle,</div>  <div>&nbsp;</div>  <div>Thank you very much for the update. Couldn&#39;t you please load the C= GNS also? Because I could be wrong as I am not that much familiar with this=  chip system yet. My assumption is, if I install it by myself, then I have = to do it again and again whenever I enter&nbsp;into the system, and run som= e cases. Please correct me&nbsp;if I am wrong! And I also need your help to=  make a bash file so that I don&#39;t have to install all of the modules th= at I need to run my cases every time separately. I need lots of modules to = run every single test case of mine.</div>  <div>&nbsp;</div>  <div> <div> <div> <div>Thanks,</div>  <div>Imran</div> </div> </div> </div> </div> &nbsp;  <div> <div>On Thu, Oct 9, 2025 at 11:27=E2=80=AFAM Danielle Esposito via RT &lt;U= MBCHelp@rt.umbc.edu&gt; wrote:</div>  <blockquote>If you agree your issue is resolved, please give us feedback on=  your experience by completing a brief satisfaction survey:<br /> <br /> https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&amp;customerema= il=3Dwv96094%40umbc.edu&amp;groupid=3DEIS&amp;ticketid=3D3287359&amp;ticket= owner=3Ddesposi1%40umbc.edu&amp;ticketsubject=3DHPC%20Slurm%2FSoftware%20Is= sue%3A%20Module%20Not%20Available<br /> <br /> If you believe your issue has not been resolved, please respond to this mes= sage, which will reopen your ticket. Note: A full record of your request ca= n be found at:&nbsp;<br /> <br /> Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287359 &gt;<b= r /> <br /> Thank You<br /> <br /> _________________________________________<br /> <br /> R e s o l u t i o n:<br /> =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D= &nbsp;<br /> <br /> Hi Imran,<br /> <br /> Yes, I was just about to update this ticket. I installed MPICH versions 4.2= .2<br /> and 4.2.1, along with PETSc 3.20.3. For CGNS, I recommend that you compile = it<br /> from source. The instructions for compiling CGNS from source are located on= <br /> their github page: https://github.com/CGNS/CGNS. You should just need to lo= ad<br /> HDF5 and Cmake, then configure the install directory to be the path to your= <br /> volumes, and you should be all set. If you run into any issues or have furt= her<br /> questions, let me know. Have a nice day!<br /> <br /> --<br /> <br /> Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker<br /> <br /> On Thu Oct 09 11:20:57 2025, WV96094 wrote:<br /> <br /> &gt; Hello, Is there any update on the ticket? Thanks,Imran On Tue, Oct 7, = 2025<br /> &gt; at 10:15 AM via RT &lt;UMBCHelp@rt.umbc.edu&gt; wrote:<br /> <br /> &gt;&gt; Greetings,<br /> <br /> &gt;&gt; This message has been automatically generated in response to the<b= r /> &gt;&gt; creation of a ticket regarding:<br /> <br /> &gt;&gt; ------------------------------------------------------------------= -------<br /> &gt;&gt; Subject: &quot;HPC Slurm/Software Issue: Module Not Available&quot= ;<br /> <br /> &gt;&gt; Message:<br /> <br /> &gt;&gt; First Name: Abdullah Al<br /> &gt;&gt; Last Name: Imran<br /> &gt;&gt; Email: wv96094@umbc.edu<br /> &gt;&gt; Campus ID: WV96094<br /> <br /> &gt;&gt; Request Type: High Performance Cluster<br /> <br /> <br /> &gt;&gt; Hello,<br /> <br /> &gt;&gt; I need MPICH, CGNS, PETSC modules to run my cases on hpcf cluster.=  But<br /> &gt;&gt; these modules are not available there to load! Could you please lo= ad<br /> &gt;&gt; the latest version of these three modules to the cluster and let m= e<br /> &gt;&gt; know when it&#39;s ready?<br /> <br /> &gt;&gt; Thanks,<br /> &gt;&gt; Imran<br /> <br /> <br /> &gt;&gt; ------------------------------------------------------------------= -------<br /> <br /> &gt;&gt; There is no need to reply to this message right now.<br /> <br /> &gt;&gt; Your ticket has been assigned an ID of [Research Computing #328735= 9] or<br /> &gt;&gt; you can go there directly by clicking the link below.<br /> <br /> &gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32873= 59 &gt;<br /> <br /> &gt;&gt; You can login to view your open tickets at any time by visiting<br=  /> &gt;&gt; http://my.umbc.edu and clicking on &quot;Help&quot; and &quot;Requ= est Help&quot;.<br /> <br /> &gt;&gt; Alternately you can click on http://my.umbc.edu/help<br /> <br /> &gt;&gt; Thank you<br /> <br /> <br /> <br /> ______________________________________<br /> <br /> Original Request:<br /> <br /> Requestors: Abdullah Al Imran<br /> <br /> First Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Abdullah=  Al<br /> Last Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Imr= an<br /> Email:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;=  &nbsp;wv96094@umbc.edu<br /> Campus ID:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;WV9= 6094<br /> <br /> Request Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; High Performa= nce Cluster<br /> <br /> <br /> Hello,<br /> <br /> I need MPICH, CGNS, PETSC modules to run my cases on hpcf cluster. But thes= e modules are not available there to load! Could you please load the latest=  version of these three modules to the cluster and let me know when it&#39;= s ready?<br /> <br /> Thanks,<br /> Imran<br /> <br /> &nbsp;</blockquote> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3287359,72187026,CommentEmailRecord,DoIT-Research-Computing,2025-10-09 15:56:29.0000000,HPC Slurm/Software Issue: Module Not Available,resolved,Danielle Esposito,desposi1,Abdullah Al Imran,wv96094,wv96094@umbc.edu,The RT System itself,NULL,"Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3287359 >  Comment just added.    Clarify what she means by install? Does she mean load??  On Thu Oct 09 11:52:15 2025, WV96094 wrote:  > Hi Danielle, Thank you very much for the update. Couldn't you please load > the CGNS also? Because I could be wrong as I am not that much familiar with > this chip system yet. My assumption is, if I install it by myself, then I > have to do it again and again whenever I enter into the system, and run > some cases. Please correct me if I am wrong! And I also need your help to > make a bash file so that I don't have to install all of the modules that I > need to run my cases every time separately. I need lots of modules to run > every single test case of mine. Thanks,Imran On Thu, Oct 9, 2025 at 11:27 > AM Danielle Esposito via RT <UMBCHelp@rt.umbc.edu> wrote:  >> If you agree your issue is resolved, please give us feedback on your >> experience by completing a brief satisfaction survey:  >> https://umbc.us2.qualtrics.com/SE/?SID=SV_etfDUq3MTISF6Ly&customeremail=wv96094%40umbc.edu&groupid=EIS&ticketid=3287359&ticketowner=desposi1%40umbc.edu&ticketsubject=HPC%20Slurm%2FSoftware%20Issue%3A%20Module%20Not%20Available  >> If you believe your issue has not been resolved, please respond to this >> message, which will reopen your ticket. Note: A full record of your >> request can be found at:  >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3287359 >  >> Thank You  >> _________________________________________  >> R e s o l u t i o n: >> = = = = = = = = = = = = = = = = = = =  >> Hi Imran,  >> Yes, I was just about to update this ticket. I installed MPICH versions >> 4.2.2 >> and 4.2.1, along with PETSc 3.20.3. For CGNS, I recommend that you >> compile it >> from source. The instructions for compiling CGNS from source are >> located on >> their github page: https://github.com/CGNS/CGNS. You should just need >> to load >> HDF5 and Cmake, then configure the install directory to be the path to >> your >> volumes, and you should be all set. If you run into any issues or have >> further >> questions, let me know. Have a nice day!  >> --  >> Kind regards, >> Danielle Esposito (she/her/hers) >> DoIT Unix Infra Student Worker  >> On Thu Oct 09 11:20:57 2025, WV96094 wrote:  >> > Hello, Is there any update on the ticket? Thanks,Imran On Tue, Oct 7, >> 2025 >> > at 10:15 AM via RT <UMBCHelp@rt.umbc.edu> wrote:  >> >> Greetings,  >> >> This message has been automatically generated in response to the >> >> creation of a ticket regarding:  >> >> >> ------------------------------------------------------------------------- >> >> Subject: ""HPC Slurm/Software Issue: Module Not Available""  >> >> Message:  >> >> First Name: Abdullah Al >> >> Last Name: Imran >> >> Email: wv96094@umbc.edu >> >> Campus ID: WV96094  >> >> Request Type: High Performance Cluster   >> >> Hello,  >> >> I need MPICH, CGNS, PETSC modules to run my cases on hpcf cluster. >> But >> >> these modules are not available there to load! Could you please load >> >> the latest version of these three modules to the cluster and let me >> >> know when it's ready?  >> >> Thanks, >> >> Imran   >> >> >> -------------------------------------------------------------------------  >> >> There is no need to reply to this message right now.  >> >> Your ticket has been assigned an ID of [Research Computing #3287359] >> or >> >> you can go there directly by clicking the link below.  >> >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3287359 >  >> >> You can login to view your open tickets at any time by visiting >> >> http://my.umbc.edu and clicking on ""Help"" and ""Request Help"".  >> >> Alternately you can click on http://my.umbc.edu/help  >> >> Thank you    >> ______________________________________  >> Original Request:  >> Requestors: Abdullah Al Imran  >> First Name: Abdullah Al >> Last Name: Imran >> Email: wv96094@umbc.edu >> Campus ID: WV96094  >> Request Type: High Performance Cluster   >> Hello,  >> I need MPICH, CGNS, PETSC modules to run my cases on hpcf cluster. But >> these modules are not available there to load! Could you please load >> the latest version of these three modules to the cluster and let me >> know when it's ready?  >> Thanks, >> Imran  --  Best, Max Breitmeyer DOIT HPC System Administrator  "
3287359,72187414,Correspond,DoIT-Research-Computing,2025-10-09 16:05:07.0000000,HPC Slurm/Software Issue: Module Not Available,resolved,Danielle Esposito,desposi1,Abdullah Al Imran,wv96094,wv96094@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Imran,</p>  <p>You do not need to reinstall the software every time you log in/out. Aft= er you compile the software, it is installed to whichever directory of your=  choosing. From there, all you need to do is add the path to your install d= irectory to your PATH environment variable (or just run the binaries from i= nside the install directory).&nbsp;</p>  <p>The software is not readily supported by the module system, easybuild. T= here are some ways to possibly working around this, but for most use cases,=  to ensure compatibility it is recommended to just compile from source. I c= an take a crack at installing it as a custom module though.</p>  <p>Also, could you elaborate on what you mean by &quot;a bash file so that = I don&#39;t have to install all of the modules&quot;? All the modules have = already been installed, and just need to be loaded using &#39;module load $= MODULE_NAME&#39;.&nbsp;</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Thu Oct 09 11:52:15 2025, WV96094 wrote: <blockquote> <div> <div>Hi Danielle,</div>  <div>&nbsp;</div>  <div>Thank you very much for the update. Couldn&#39;t you please load the C= GNS also? Because I could be wrong as I am not that much familiar with this=  chip system yet. My assumption is, if I install it by myself, then I have = to do it again and again whenever I enter&nbsp;into the system, and run som= e cases. Please correct me&nbsp;if I am wrong! And I also need your help to=  make a bash file so that I don&#39;t have to install all of the modules th= at I need to run my cases every time separately. I need lots of modules to = run every single test case of mine.</div>  <div>&nbsp;</div>  <div> <div> <div> <div>Thanks,</div>  <div>Imran</div> </div> </div> </div> </div> &nbsp;  <div> <div>On Thu, Oct 9, 2025 at 11:27=E2=80=AFAM Danielle Esposito via RT &lt;U= MBCHelp@rt.umbc.edu&gt; wrote:</div>  <blockquote>If you agree your issue is resolved, please give us feedback on=  your experience by completing a brief satisfaction survey:<br /> <br /> https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&amp;customerema= il=3Dwv96094%40umbc.edu&amp;groupid=3DEIS&amp;ticketid=3D3287359&amp;ticket= owner=3Ddesposi1%40umbc.edu&amp;ticketsubject=3DHPC%20Slurm%2FSoftware%20Is= sue%3A%20Module%20Not%20Available<br /> <br /> If you believe your issue has not been resolved, please respond to this mes= sage, which will reopen your ticket. Note: A full record of your request ca= n be found at:&nbsp;<br /> <br /> Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287359 &gt;<b= r /> <br /> Thank You<br /> <br /> _________________________________________<br /> <br /> R e s o l u t i o n:<br /> =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D= &nbsp;<br /> <br /> Hi Imran,<br /> <br /> Yes, I was just about to update this ticket. I installed MPICH versions 4.2= .2<br /> and 4.2.1, along with PETSc 3.20.3. For CGNS, I recommend that you compile = it<br /> from source. The instructions for compiling CGNS from source are located on= <br /> their github page: https://github.com/CGNS/CGNS. You should just need to lo= ad<br /> HDF5 and Cmake, then configure the install directory to be the path to your= <br /> volumes, and you should be all set. If you run into any issues or have furt= her<br /> questions, let me know. Have a nice day!<br /> <br /> --<br /> <br /> Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker<br /> <br /> On Thu Oct 09 11:20:57 2025, WV96094 wrote:<br /> <br /> &gt; Hello, Is there any update on the ticket? Thanks,Imran On Tue, Oct 7, = 2025<br /> &gt; at 10:15 AM via RT &lt;UMBCHelp@rt.umbc.edu&gt; wrote:<br /> <br /> &gt;&gt; Greetings,<br /> <br /> &gt;&gt; This message has been automatically generated in response to the<b= r /> &gt;&gt; creation of a ticket regarding:<br /> <br /> &gt;&gt; ------------------------------------------------------------------= -------<br /> &gt;&gt; Subject: &quot;HPC Slurm/Software Issue: Module Not Available&quot= ;<br /> <br /> &gt;&gt; Message:<br /> <br /> &gt;&gt; First Name: Abdullah Al<br /> &gt;&gt; Last Name: Imran<br /> &gt;&gt; Email: wv96094@umbc.edu<br /> &gt;&gt; Campus ID: WV96094<br /> <br /> &gt;&gt; Request Type: High Performance Cluster<br /> <br /> <br /> &gt;&gt; Hello,<br /> <br /> &gt;&gt; I need MPICH, CGNS, PETSC modules to run my cases on hpcf cluster.=  But<br /> &gt;&gt; these modules are not available there to load! Could you please lo= ad<br /> &gt;&gt; the latest version of these three modules to the cluster and let m= e<br /> &gt;&gt; know when it&#39;s ready?<br /> <br /> &gt;&gt; Thanks,<br /> &gt;&gt; Imran<br /> <br /> <br /> &gt;&gt; ------------------------------------------------------------------= -------<br /> <br /> &gt;&gt; There is no need to reply to this message right now.<br /> <br /> &gt;&gt; Your ticket has been assigned an ID of [Research Computing #328735= 9] or<br /> &gt;&gt; you can go there directly by clicking the link below.<br /> <br /> &gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32873= 59 &gt;<br /> <br /> &gt;&gt; You can login to view your open tickets at any time by visiting<br=  /> &gt;&gt; http://my.umbc.edu and clicking on &quot;Help&quot; and &quot;Requ= est Help&quot;.<br /> <br /> &gt;&gt; Alternately you can click on http://my.umbc.edu/help<br /> <br /> &gt;&gt; Thank you<br /> <br /> <br /> <br /> ______________________________________<br /> <br /> Original Request:<br /> <br /> Requestors: Abdullah Al Imran<br /> <br /> First Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Abdullah=  Al<br /> Last Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Imr= an<br /> Email:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;=  &nbsp;wv96094@umbc.edu<br /> Campus ID:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;WV9= 6094<br /> <br /> Request Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; High Performa= nce Cluster<br /> <br /> <br /> Hello,<br /> <br /> I need MPICH, CGNS, PETSC modules to run my cases on hpcf cluster. But thes= e modules are not available there to load! Could you please load the latest=  version of these three modules to the cluster and let me know when it&#39;= s ready?<br /> <br /> Thanks,<br /> Imran<br /> <br /> &nbsp;</blockquote> </div> </blockquote> </div> "
3287359,72189122,Correspond,DoIT-Research-Computing,2025-10-09 16:56:20.0000000,HPC Slurm/Software Issue: Module Not Available,resolved,Danielle Esposito,desposi1,Abdullah Al Imran,wv96094,wv96094@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>I attempted to install CGNS as a custom module. You should be able to lo= ad the module with &#39;module load CGNS/4.5.0&#39;. Can you attempt to loa= d/use the module to verify it works as expected? If you run into issues wit= h the module, let me know.&nbsp;</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Thu Oct 09 12:05:07 2025, EJ47256 wrote: <blockquote> <p>Hi Imran,</p>  <p>You do not need to reinstall the software every time you log in/out. Aft= er you compile the software, it is installed to whichever directory of your=  choosing. From there, all you need to do is add the path to your install d= irectory to your PATH environment variable (or just run the binaries from i= nside the install directory).&nbsp;</p>  <p>The software is not readily supported by the module system, easybuild. T= here are some ways to possibly working around this, but for most use cases,=  to ensure compatibility it is recommended to just compile from source. I c= an take a crack at installing it as a custom module though.</p>  <p>Also, could you elaborate on what you mean by &quot;a bash file so that = I don&#39;t have to install all of the modules&quot;? All the modules have = already been installed, and just need to be loaded using &#39;module load $= MODULE_NAME&#39;.&nbsp;</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Thu Oct 09 11:52:15 2025, WV96094 wrote: <blockquote> <div> <div>Hi Danielle,</div>  <div>&nbsp;</div>  <div>Thank you very much for the update. Couldn&#39;t you please load the C= GNS also? Because I could be wrong as I am not that much familiar with this=  chip system yet. My assumption is, if I install it by myself, then I have = to do it again and again whenever I enter&nbsp;into the system, and run som= e cases. Please correct me&nbsp;if I am wrong! And I also need your help to=  make a bash file so that I don&#39;t have to install all of the modules th= at I need to run my cases every time separately. I need lots of modules to = run every single test case of mine.</div>  <div>&nbsp;</div>  <div> <div> <div> <div>Thanks,</div>  <div>Imran</div> </div> </div> </div> </div> &nbsp;  <div> <div>On Thu, Oct 9, 2025 at 11:27=E2=80=AFAM Danielle Esposito via RT &lt;U= MBCHelp@rt.umbc.edu&gt; wrote:</div>  <blockquote>If you agree your issue is resolved, please give us feedback on=  your experience by completing a brief satisfaction survey:<br /> <br /> https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&amp;customerema= il=3Dwv96094%40umbc.edu&amp;groupid=3DEIS&amp;ticketid=3D3287359&amp;ticket= owner=3Ddesposi1%40umbc.edu&amp;ticketsubject=3DHPC%20Slurm%2FSoftware%20Is= sue%3A%20Module%20Not%20Available<br /> <br /> If you believe your issue has not been resolved, please respond to this mes= sage, which will reopen your ticket. Note: A full record of your request ca= n be found at:&nbsp;<br /> <br /> Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287359 &gt;<b= r /> <br /> Thank You<br /> <br /> _________________________________________<br /> <br /> R e s o l u t i o n:<br /> =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D= &nbsp;<br /> <br /> Hi Imran,<br /> <br /> Yes, I was just about to update this ticket. I installed MPICH versions 4.2= .2<br /> and 4.2.1, along with PETSc 3.20.3. For CGNS, I recommend that you compile = it<br /> from source. The instructions for compiling CGNS from source are located on= <br /> their github page: https://github.com/CGNS/CGNS. You should just need to lo= ad<br /> HDF5 and Cmake, then configure the install directory to be the path to your= <br /> volumes, and you should be all set. If you run into any issues or have furt= her<br /> questions, let me know. Have a nice day!<br /> <br /> --<br /> <br /> Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker<br /> <br /> On Thu Oct 09 11:20:57 2025, WV96094 wrote:<br /> <br /> &gt; Hello, Is there any update on the ticket? Thanks,Imran On Tue, Oct 7, = 2025<br /> &gt; at 10:15 AM via RT &lt;UMBCHelp@rt.umbc.edu&gt; wrote:<br /> <br /> &gt;&gt; Greetings,<br /> <br /> &gt;&gt; This message has been automatically generated in response to the<b= r /> &gt;&gt; creation of a ticket regarding:<br /> <br /> &gt;&gt; ------------------------------------------------------------------= -------<br /> &gt;&gt; Subject: &quot;HPC Slurm/Software Issue: Module Not Available&quot= ;<br /> <br /> &gt;&gt; Message:<br /> <br /> &gt;&gt; First Name: Abdullah Al<br /> &gt;&gt; Last Name: Imran<br /> &gt;&gt; Email: wv96094@umbc.edu<br /> &gt;&gt; Campus ID: WV96094<br /> <br /> &gt;&gt; Request Type: High Performance Cluster<br /> <br /> <br /> &gt;&gt; Hello,<br /> <br /> &gt;&gt; I need MPICH, CGNS, PETSC modules to run my cases on hpcf cluster.=  But<br /> &gt;&gt; these modules are not available there to load! Could you please lo= ad<br /> &gt;&gt; the latest version of these three modules to the cluster and let m= e<br /> &gt;&gt; know when it&#39;s ready?<br /> <br /> &gt;&gt; Thanks,<br /> &gt;&gt; Imran<br /> <br /> <br /> &gt;&gt; ------------------------------------------------------------------= -------<br /> <br /> &gt;&gt; There is no need to reply to this message right now.<br /> <br /> &gt;&gt; Your ticket has been assigned an ID of [Research Computing #328735= 9] or<br /> &gt;&gt; you can go there directly by clicking the link below.<br /> <br /> &gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32873= 59 &gt;<br /> <br /> &gt;&gt; You can login to view your open tickets at any time by visiting<br=  /> &gt;&gt; http://my.umbc.edu and clicking on &quot;Help&quot; and &quot;Requ= est Help&quot;.<br /> <br /> &gt;&gt; Alternately you can click on http://my.umbc.edu/help<br /> <br /> &gt;&gt; Thank you<br /> <br /> <br /> <br /> ______________________________________<br /> <br /> Original Request:<br /> <br /> Requestors: Abdullah Al Imran<br /> <br /> First Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Abdullah=  Al<br /> Last Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Imr= an<br /> Email:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;=  &nbsp;wv96094@umbc.edu<br /> Campus ID:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;WV9= 6094<br /> <br /> Request Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; High Performa= nce Cluster<br /> <br /> <br /> Hello,<br /> <br /> I need MPICH, CGNS, PETSC modules to run my cases on hpcf cluster. But thes= e modules are not available there to load! Could you please load the latest=  version of these three modules to the cluster and let me know when it&#39;= s ready?<br /> <br /> Thanks,<br /> Imran<br /> <br /> &nbsp;</blockquote> </div> </blockquote> </div> </blockquote> </div> "
3287359,72191906,Correspond,DoIT-Research-Computing,2025-10-09 18:16:15.0000000,HPC Slurm/Software Issue: Module Not Available,resolved,Danielle Esposito,desposi1,Abdullah Al Imran,wv96094,wv96094@umbc.edu,Abdullah Al Imran,wv96094@umbc.edu,"Thanks! Yes, I loaded CGNS successfully. Now I am not sure how to use all of the modules for my code compilation and run the cases eventually. Could you please help me out with this?   On Thu, Oct 9, 2025 at 12:56=E2=80=AFPM Danielle Esposito via RT < UMBCHelp@rt.umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287359 > > > Last Update From Ticket: > > I attempted to install CGNS as a custom module. You should be able to load > the > module with 'module load CGNS/4.5.0'. Can you attempt to load/use the > module to > verify it works as expected? If you run into issues with the module, let = me > know. > > -- > > Kind regards, > Danielle Esposito (she/her/hers) > DoIT Unix Infra Student Worker > > On Thu Oct 09 12:05:07 2025, EJ47256 wrote: > > > Hi Imran, > > > You do not need to reinstall the software every time you log in/out. > After > > you compile the software, it is installed to whichever directory of your > > choosing. From there, all you need to do is add the path to your install > > directory to your PATH environment variable (or just run the binaries > from > > inside the install directory). > > > The software is not readily supported by the module system, easybuild. > > There are some ways to possibly working around this, but for most use > > cases, to ensure compatibility it is recommended to just compile from > > source. I can take a crack at installing it as a custom module though. > > > Also, could you elaborate on what you mean by ""a bash file so that I > don't > > have to install all of the modules""? All the modules have already been > > installed, and just need to be loaded using 'module load $MODULE_NAME'. > > > -- > > > Kind regards, > > Danielle Esposito (she/her/hers) > > DoIT Unix Infra Student Worker > > > On Thu Oct 09 11:52:15 2025, WV96094 wrote: > > >> Hi Danielle, Thank you very much for the update. Couldn't you please > >> load the CGNS also? Because I could be wrong as I am not that much > >> familiar with this chip system yet. My assumption is, if I install it > >> by myself, then I have to do it again and again whenever I enter into > >> the system, and run some cases. Please correct me if I am wrong! And I > >> also need your help to make a bash file so that I don't have to install > >> all of the modules that I need to run my cases every time separately. I > >> need lots of modules to run every single test case of mine. Thanks,Imr= an > >> On Thu, Oct 9, 2025 at 11:27 AM Danielle Esposito via RT > >> <UMBCHelp@rt.umbc.edu> wrote: > > >>> If you agree your issue is resolved, please give us feedback on > >>> your experience by completing a brief satisfaction survey: > > >>> > https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&customeremail= =3Dwv96094%40umbc.edu&groupid=3DEIS&ticketid=3D3287359&ticketowner=3Ddespos= i1%40umbc.edu&ticketsubject=3DHPC%20Slurm%2FSoftware%20Issue%3A%20Module%20= Not%20Available > > >>> If you believe your issue has not been resolved, please respond to > >>> this message, which will reopen your ticket. Note: A full record of > >>> your request can be found at: > > >>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287359 > > > >>> Thank You > > >>> _________________________________________ > > >>> R e s o l u t i o n: > >>> =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D = =3D =3D > > >>> Hi Imran, > > >>> Yes, I was just about to update this ticket. I installed MPICH > >>> versions 4.2.2 > >>> and 4.2.1, along with PETSc 3.20.3. For CGNS, I recommend that you > >>> compile it > >>> from source. The instructions for compiling CGNS from source are > >>> located on > >>> their github page: https://github.com/CGNS/CGNS. You should just > >>> need to load > >>> HDF5 and Cmake, then configure the install directory to be the path > >>> to your > >>> volumes, and you should be all set. If you run into any issues or > >>> have further > >>> questions, let me know. Have a nice day! > > >>> -- > > >>> Kind regards, > >>> Danielle Esposito (she/her/hers) > >>> DoIT Unix Infra Student Worker > > >>> On Thu Oct 09 11:20:57 2025, WV96094 wrote: > > >>> > Hello, Is there any update on the ticket? Thanks,Imran On Tue, > >>> Oct 7, 2025 > >>> > at 10:15 AM via RT <UMBCHelp@rt.umbc.edu> wrote: > > >>> >> Greetings, > > >>> >> This message has been automatically generated in response to the > >>> >> creation of a ticket regarding: > > >>> >> > >>> > ------------------------------------------------------------------------- > >>> >> Subject: ""HPC Slurm/Software Issue: Module Not Available"" > > >>> >> Message: > > >>> >> First Name: Abdullah Al > >>> >> Last Name: Imran > >>> >> Email: wv96094@umbc.edu > >>> >> Campus ID: WV96094 > > >>> >> Request Type: High Performance Cluster > > > >>> >> Hello, > > >>> >> I need MPICH, CGNS, PETSC modules to run my cases on hpcf > >>> cluster. But > >>> >> these modules are not available there to load! Could you please > >>> load > >>> >> the latest version of these three modules to the cluster and let > >>> me > >>> >> know when it's ready? > > >>> >> Thanks, > >>> >> Imran > > > >>> >> > >>> > ------------------------------------------------------------------------- > > >>> >> There is no need to reply to this message right now. > > >>> >> Your ticket has been assigned an ID of [Research Computing > >>> #3287359] or > >>> >> you can go there directly by clicking the link below. > > >>> >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287359 > >>> > > > >>> >> You can login to view your open tickets at any time by visiting > >>> >> http://my.umbc.edu and clicking on ""Help"" and ""Request Help"". > > >>> >> Alternately you can click on http://my.umbc.edu/help > > >>> >> Thank you > > > > >>> ______________________________________ > > >>> Original Request: > > >>> Requestors: Abdullah Al Imran > > >>> First Name: Abdullah Al > >>> Last Name: Imran > >>> Email: wv96094@umbc.edu > >>> Campus ID: WV96094 > > >>> Request Type: High Performance Cluster > > > >>> Hello, > > >>> I need MPICH, CGNS, PETSC modules to run my cases on hpcf cluster. > >>> But these modules are not available there to load! Could you please > >>> load the latest version of these three modules to the cluster and > >>> let me know when it's ready? > > >>> Thanks, > >>> Imran > > "
3287359,72191906,Correspond,DoIT-Research-Computing,2025-10-09 18:16:15.0000000,HPC Slurm/Software Issue: Module Not Available,resolved,Danielle Esposito,desposi1,Abdullah Al Imran,wv96094,wv96094@umbc.edu,Abdullah Al Imran,wv96094@umbc.edu,"<div dir=3D""auto"">Thanks! Yes, I loaded CGNS successfully. Now I am not sur= e how to use all of the modules for my code compilation and run the cases e= ventually. Could you please help me out with this?</div><div><br></div><div= ><br><div class=3D""gmail_quote gmail_quote_container""><div dir=3D""ltr"" clas= s=3D""gmail_attr"">On Thu, Oct 9, 2025 at 12:56=E2=80=AFPM Danielle Esposito = via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>= &gt; wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""margin:0px = 0px 0px 0.8ex;border-left-width:1px;border-left-style:solid;padding-left:1e= x;border-left-color:rgb(204,204,204)"">Ticket &lt;URL: <a href=3D""https://rt= .umbc.edu/Ticket/Display.html?id=3D3287359"" rel=3D""noreferrer"" target=3D""_b= lank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3287359</a> &gt;<br> <br> Last Update From Ticket:<br> <br> I attempted to install CGNS as a custom module. You should be able to load = the<br> module with &#39;module load CGNS/4.5.0&#39;. Can you attempt to load/use t= he module to<br> verify it works as expected? If you run into issues with the module, let me= <br> know.<br> <br> --<br> <br> Kind regards,<br> Danielle Esposito (she/her/hers)<br> DoIT Unix Infra Student Worker<br> <br> On Thu Oct 09 12:05:07 2025, EJ47256 wrote:<br> <br> &gt; Hi Imran,<br> <br> &gt; You do not need to reinstall the software every time you log in/out. A= fter<br> &gt; you compile the software, it is installed to whichever directory of yo= ur<br> &gt; choosing. From there, all you need to do is add the path to your insta= ll<br> &gt; directory to your PATH environment variable (or just run the binaries = from<br> &gt; inside the install directory).<br> <br> &gt; The software is not readily supported by the module system, easybuild.= <br> &gt; There are some ways to possibly working around this, but for most use<= br> &gt; cases, to ensure compatibility it is recommended to just compile from<= br> &gt; source. I can take a crack at installing it as a custom module though.= <br> <br> &gt; Also, could you elaborate on what you mean by &quot;a bash file so tha= t I don&#39;t<br> &gt; have to install all of the modules&quot;? All the modules have already=  been<br> &gt; installed, and just need to be loaded using &#39;module load $MODULE_N= AME&#39;.<br> <br> &gt; --<br> <br> &gt; Kind regards,<br> &gt; Danielle Esposito (she/her/hers)<br> &gt; DoIT Unix Infra Student Worker<br> <br> &gt; On Thu Oct 09 11:52:15 2025, WV96094 wrote:<br> <br> &gt;&gt; Hi Danielle, Thank you very much for the update. Couldn&#39;t you = please<br> &gt;&gt; load the CGNS also? Because I could be wrong as I am not that much= <br> &gt;&gt; familiar with this chip system yet. My assumption is, if I install=  it<br> &gt;&gt; by myself, then I have to do it again and again whenever I enter i= nto<br> &gt;&gt; the system, and run some cases. Please correct me if I am wrong! A= nd I<br> &gt;&gt; also need your help to make a bash file so that I don&#39;t have t= o install<br> &gt;&gt; all of the modules that I need to run my cases every time separate= ly. I<br> &gt;&gt; need lots of modules to run every single test case of mine. Thanks= ,Imran<br> &gt;&gt; On Thu, Oct 9, 2025 at 11:27 AM Danielle Esposito via RT<br> &gt;&gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBC= Help@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt;&gt; If you agree your issue is resolved, please give us feedback o= n<br> &gt;&gt;&gt; your experience by completing a brief satisfaction survey:<br> <br> &gt;&gt;&gt; <a href=3D""https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3= MTISF6Ly&amp;customeremail=3Dwv96094%40umbc.edu&amp;groupid=3DEIS&amp;ticke= tid=3D3287359&amp;ticketowner=3Ddesposi1%40umbc.edu&amp;ticketsubject=3DHPC= %20Slurm%2FSoftware%20Issue%3A%20Module%20Not%20Available"" rel=3D""noreferre= r"" target=3D""_blank"">https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTI= SF6Ly&amp;customeremail=3Dwv96094%40umbc.edu&amp;groupid=3DEIS&amp;ticketid= =3D3287359&amp;ticketowner=3Ddesposi1%40umbc.edu&amp;ticketsubject=3DHPC%20= Slurm%2FSoftware%20Issue%3A%20Module%20Not%20Available</a><br> <br> &gt;&gt;&gt; If you believe your issue has not been resolved, please respon= d to<br> &gt;&gt;&gt; this message, which will reopen your ticket. Note: A full reco= rd of<br> &gt;&gt;&gt; your request can be found at:<br> <br> &gt;&gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.= html?id=3D3287359"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu= /Ticket/Display.html?id=3D3287359</a> &gt;<br> <br> &gt;&gt;&gt; Thank You<br> <br> &gt;&gt;&gt; _________________________________________<br> <br> &gt;&gt;&gt; R e s o l u t i o n:<br> &gt;&gt;&gt; =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D = =3D =3D =3D =3D<br> <br> &gt;&gt;&gt; Hi Imran,<br> <br> &gt;&gt;&gt; Yes, I was just about to update this ticket. I installed MPICH= <br> &gt;&gt;&gt; versions 4.2.2<br> &gt;&gt;&gt; and 4.2.1, along with PETSc 3.20.3. For CGNS, I recommend that=  you<br> &gt;&gt;&gt; compile it<br> &gt;&gt;&gt; from source. The instructions for compiling CGNS from source a= re<br> &gt;&gt;&gt; located on<br> &gt;&gt;&gt; their github page: <a href=3D""https://github.com/CGNS/CGNS"" re= l=3D""noreferrer"" target=3D""_blank"">https://github.com/CGNS/CGNS</a>. You sh= ould just<br> &gt;&gt;&gt; need to load<br> &gt;&gt;&gt; HDF5 and Cmake, then configure the install directory to be the=  path<br> &gt;&gt;&gt; to your<br> &gt;&gt;&gt; volumes, and you should be all set. If you run into any issues=  or<br> &gt;&gt;&gt; have further<br> &gt;&gt;&gt; questions, let me know. Have a nice day!<br> <br> &gt;&gt;&gt; --<br> <br> &gt;&gt;&gt; Kind regards,<br> &gt;&gt;&gt; Danielle Esposito (she/her/hers)<br> &gt;&gt;&gt; DoIT Unix Infra Student Worker<br> <br> &gt;&gt;&gt; On Thu Oct 09 11:20:57 2025, WV96094 wrote:<br> <br> &gt;&gt;&gt; &gt; Hello, Is there any update on the ticket? Thanks,Imran On=  Tue,<br> &gt;&gt;&gt; Oct 7, 2025<br> &gt;&gt;&gt; &gt; at 10:15 AM via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc= .edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt;&gt; &gt;&gt; Greetings,<br> <br> &gt;&gt;&gt; &gt;&gt; This message has been automatically generated in resp= onse to the<br> &gt;&gt;&gt; &gt;&gt; creation of a ticket regarding:<br> <br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; --------------------------------------------------------------= -----------<br> &gt;&gt;&gt; &gt;&gt; Subject: &quot;HPC Slurm/Software Issue: Module Not A= vailable&quot;<br> <br> &gt;&gt;&gt; &gt;&gt; Message:<br> <br> &gt;&gt;&gt; &gt;&gt; First Name: Abdullah Al<br> &gt;&gt;&gt; &gt;&gt; Last Name: Imran<br> &gt;&gt;&gt; &gt;&gt; Email: <a href=3D""mailto:wv96094@umbc.edu"" target=3D""= _blank"">wv96094@umbc.edu</a><br> &gt;&gt;&gt; &gt;&gt; Campus ID: WV96094<br> <br> &gt;&gt;&gt; &gt;&gt; Request Type: High Performance Cluster<br> <br> <br> &gt;&gt;&gt; &gt;&gt; Hello,<br> <br> &gt;&gt;&gt; &gt;&gt; I need MPICH, CGNS, PETSC modules to run my cases on = hpcf<br> &gt;&gt;&gt; cluster. But<br> &gt;&gt;&gt; &gt;&gt; these modules are not available there to load! Could = you please<br> &gt;&gt;&gt; load<br> &gt;&gt;&gt; &gt;&gt; the latest version of these three modules to the clus= ter and let<br> &gt;&gt;&gt; me<br> &gt;&gt;&gt; &gt;&gt; know when it&#39;s ready?<br> <br> &gt;&gt;&gt; &gt;&gt; Thanks,<br> &gt;&gt;&gt; &gt;&gt; Imran<br> <br> <br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; --------------------------------------------------------------= -----------<br> <br> &gt;&gt;&gt; &gt;&gt; There is no need to reply to this message right now.<= br> <br> &gt;&gt;&gt; &gt;&gt; Your ticket has been assigned an ID of [Research Comp= uting<br> &gt;&gt;&gt; #3287359] or<br> &gt;&gt;&gt; &gt;&gt; you can go there directly by clicking the link below.= <br> <br> &gt;&gt;&gt; &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket= /Display.html?id=3D3287359"" rel=3D""noreferrer"" target=3D""_blank"">https://rt= .umbc.edu/Ticket/Display.html?id=3D3287359</a><br> &gt;&gt;&gt; &gt;<br> <br> &gt;&gt;&gt; &gt;&gt; You can login to view your open tickets at any time b= y visiting<br> &gt;&gt;&gt; &gt;&gt; <a href=3D""http://my.umbc.edu"" rel=3D""noreferrer"" tar= get=3D""_blank"">http://my.umbc.edu</a> and clicking on &quot;Help&quot; and = &quot;Request Help&quot;.<br> <br> &gt;&gt;&gt; &gt;&gt; Alternately you can click on <a href=3D""http://my.umb= c.edu/help"" rel=3D""noreferrer"" target=3D""_blank"">http://my.umbc.edu/help</a= ><br> <br> &gt;&gt;&gt; &gt;&gt; Thank you<br> <br> <br> <br> &gt;&gt;&gt; ______________________________________<br> <br> &gt;&gt;&gt; Original Request:<br> <br> &gt;&gt;&gt; Requestors: Abdullah Al Imran<br> <br> &gt;&gt;&gt; First Name: Abdullah Al<br> &gt;&gt;&gt; Last Name: Imran<br> &gt;&gt;&gt; Email: <a href=3D""mailto:wv96094@umbc.edu"" target=3D""_blank"">w= v96094@umbc.edu</a><br> &gt;&gt;&gt; Campus ID: WV96094<br> <br> &gt;&gt;&gt; Request Type: High Performance Cluster<br> <br> <br> &gt;&gt;&gt; Hello,<br> <br> &gt;&gt;&gt; I need MPICH, CGNS, PETSC modules to run my cases on hpcf clus= ter.<br> &gt;&gt;&gt; But these modules are not available there to load! Could you p= lease<br> &gt;&gt;&gt; load the latest version of these three modules to the cluster = and<br> &gt;&gt;&gt; let me know when it&#39;s ready?<br> <br> &gt;&gt;&gt; Thanks,<br> &gt;&gt;&gt; Imran<br> <br> </blockquote></div></div> "
3287359,72204785,Correspond,DoIT-Research-Computing,2025-10-10 14:26:50.0000000,HPC Slurm/Software Issue: Module Not Available,resolved,Danielle Esposito,desposi1,Abdullah Al Imran,wv96094,wv96094@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Imran,</p>  <p>All the documentation for these programs are publicly available online. = We are able to assist with HPC related issues you encounter along the way, = however we are unable to walk you through everything. We mainly provide sup= port with HPC/Slurm related issues, and utilizing all of those modules is a=  little out of scope. If you have any specific questions to help get starte= d, I can do my best to answer them. Otherwise, I recommend taking a look at=  the documentation for the modules you requested to utilize them.</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Thu Oct 09 14:16:15 2025, WV96094 wrote: <blockquote> <div>Thanks! Yes, I loaded CGNS successfully. Now I am not sure how to use = all of the modules for my code compilation and run the cases eventually. Co= uld you please help me out with this?</div>  <div>&nbsp;</div>  <div>&nbsp; <div> <div>On Thu, Oct 9, 2025 at 12:56=E2=80=AFPM Danielle Esposito via RT &lt;U= MBCHelp@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 87359 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> I attempted to install CGNS as a custom module. You should be able to load = the<br /> module with &#39;module load CGNS/4.5.0&#39;. Can you attempt to load/use t= he module to<br /> verify it works as expected? If you run into issues with the module, let me= <br /> know.<br /> <br /> --<br /> <br /> Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker<br /> <br /> On Thu Oct 09 12:05:07 2025, EJ47256 wrote:<br /> <br /> &gt; Hi Imran,<br /> <br /> &gt; You do not need to reinstall the software every time you log in/out. A= fter<br /> &gt; you compile the software, it is installed to whichever directory of yo= ur<br /> &gt; choosing. From there, all you need to do is add the path to your insta= ll<br /> &gt; directory to your PATH environment variable (or just run the binaries = from<br /> &gt; inside the install directory).<br /> <br /> &gt; The software is not readily supported by the module system, easybuild.= <br /> &gt; There are some ways to possibly working around this, but for most use<= br /> &gt; cases, to ensure compatibility it is recommended to just compile from<= br /> &gt; source. I can take a crack at installing it as a custom module though.= <br /> <br /> &gt; Also, could you elaborate on what you mean by &quot;a bash file so tha= t I don&#39;t<br /> &gt; have to install all of the modules&quot;? All the modules have already=  been<br /> &gt; installed, and just need to be loaded using &#39;module load $MODULE_N= AME&#39;.<br /> <br /> &gt; --<br /> <br /> &gt; Kind regards,<br /> &gt; Danielle Esposito (she/her/hers)<br /> &gt; DoIT Unix Infra Student Worker<br /> <br /> &gt; On Thu Oct 09 11:52:15 2025, WV96094 wrote:<br /> <br /> &gt;&gt; Hi Danielle, Thank you very much for the update. Couldn&#39;t you = please<br /> &gt;&gt; load the CGNS also? Because I could be wrong as I am not that much= <br /> &gt;&gt; familiar with this chip system yet. My assumption is, if I install=  it<br /> &gt;&gt; by myself, then I have to do it again and again whenever I enter i= nto<br /> &gt;&gt; the system, and run some cases. Please correct me if I am wrong! A= nd I<br /> &gt;&gt; also need your help to make a bash file so that I don&#39;t have t= o install<br /> &gt;&gt; all of the modules that I need to run my cases every time separate= ly. I<br /> &gt;&gt; need lots of modules to run every single test case of mine. Thanks= ,Imran<br /> &gt;&gt; On Thu, Oct 9, 2025 at 11:27 AM Danielle Esposito via RT<br /> &gt;&gt; &lt;UMBCHelp@rt.umbc.edu&gt; wrote:<br /> <br /> &gt;&gt;&gt; If you agree your issue is resolved, please give us feedback o= n<br /> &gt;&gt;&gt; your experience by completing a brief satisfaction survey:<br = /> <br /> &gt;&gt;&gt; https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&am= p;customeremail=3Dwv96094%40umbc.edu&amp;groupid=3DEIS&amp;ticketid=3D32873= 59&amp;ticketowner=3Ddesposi1%40umbc.edu&amp;ticketsubject=3DHPC%20Slurm%2F= Software%20Issue%3A%20Module%20Not%20Available<br /> <br /> &gt;&gt;&gt; If you believe your issue has not been resolved, please respon= d to<br /> &gt;&gt;&gt; this message, which will reopen your ticket. Note: A full reco= rd of<br /> &gt;&gt;&gt; your request can be found at:<br /> <br /> &gt;&gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3= 287359 &gt;<br /> <br /> &gt;&gt;&gt; Thank You<br /> <br /> &gt;&gt;&gt; _________________________________________<br /> <br /> &gt;&gt;&gt; R e s o l u t i o n:<br /> &gt;&gt;&gt; =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D = =3D =3D =3D =3D<br /> <br /> &gt;&gt;&gt; Hi Imran,<br /> <br /> &gt;&gt;&gt; Yes, I was just about to update this ticket. I installed MPICH= <br /> &gt;&gt;&gt; versions 4.2.2<br /> &gt;&gt;&gt; and 4.2.1, along with PETSc 3.20.3. For CGNS, I recommend that=  you<br /> &gt;&gt;&gt; compile it<br /> &gt;&gt;&gt; from source. The instructions for compiling CGNS from source a= re<br /> &gt;&gt;&gt; located on<br /> &gt;&gt;&gt; their github page: https://github.com/CGNS/CGNS. You should ju= st<br /> &gt;&gt;&gt; need to load<br /> &gt;&gt;&gt; HDF5 and Cmake, then configure the install directory to be the=  path<br /> &gt;&gt;&gt; to your<br /> &gt;&gt;&gt; volumes, and you should be all set. If you run into any issues=  or<br /> &gt;&gt;&gt; have further<br /> &gt;&gt;&gt; questions, let me know. Have a nice day!<br /> <br /> &gt;&gt;&gt; --<br /> <br /> &gt;&gt;&gt; Kind regards,<br /> &gt;&gt;&gt; Danielle Esposito (she/her/hers)<br /> &gt;&gt;&gt; DoIT Unix Infra Student Worker<br /> <br /> &gt;&gt;&gt; On Thu Oct 09 11:20:57 2025, WV96094 wrote:<br /> <br /> &gt;&gt;&gt; &gt; Hello, Is there any update on the ticket? Thanks,Imran On=  Tue,<br /> &gt;&gt;&gt; Oct 7, 2025<br /> &gt;&gt;&gt; &gt; at 10:15 AM via RT &lt;UMBCHelp@rt.umbc.edu&gt; wrote:<br=  /> <br /> &gt;&gt;&gt; &gt;&gt; Greetings,<br /> <br /> &gt;&gt;&gt; &gt;&gt; This message has been automatically generated in resp= onse to the<br /> &gt;&gt;&gt; &gt;&gt; creation of a ticket regarding:<br /> <br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; --------------------------------------------------------------= -----------<br /> &gt;&gt;&gt; &gt;&gt; Subject: &quot;HPC Slurm/Software Issue: Module Not A= vailable&quot;<br /> <br /> &gt;&gt;&gt; &gt;&gt; Message:<br /> <br /> &gt;&gt;&gt; &gt;&gt; First Name: Abdullah Al<br /> &gt;&gt;&gt; &gt;&gt; Last Name: Imran<br /> &gt;&gt;&gt; &gt;&gt; Email: wv96094@umbc.edu<br /> &gt;&gt;&gt; &gt;&gt; Campus ID: WV96094<br /> <br /> &gt;&gt;&gt; &gt;&gt; Request Type: High Performance Cluster<br /> <br /> <br /> &gt;&gt;&gt; &gt;&gt; Hello,<br /> <br /> &gt;&gt;&gt; &gt;&gt; I need MPICH, CGNS, PETSC modules to run my cases on = hpcf<br /> &gt;&gt;&gt; cluster. But<br /> &gt;&gt;&gt; &gt;&gt; these modules are not available there to load! Could = you please<br /> &gt;&gt;&gt; load<br /> &gt;&gt;&gt; &gt;&gt; the latest version of these three modules to the clus= ter and let<br /> &gt;&gt;&gt; me<br /> &gt;&gt;&gt; &gt;&gt; know when it&#39;s ready?<br /> <br /> &gt;&gt;&gt; &gt;&gt; Thanks,<br /> &gt;&gt;&gt; &gt;&gt; Imran<br /> <br /> <br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; --------------------------------------------------------------= -----------<br /> <br /> &gt;&gt;&gt; &gt;&gt; There is no need to reply to this message right now.<= br /> <br /> &gt;&gt;&gt; &gt;&gt; Your ticket has been assigned an ID of [Research Comp= uting<br /> &gt;&gt;&gt; #3287359] or<br /> &gt;&gt;&gt; &gt;&gt; you can go there directly by clicking the link below.= <br /> <br /> &gt;&gt;&gt; &gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.ht= ml?id=3D3287359<br /> &gt;&gt;&gt; &gt;<br /> <br /> &gt;&gt;&gt; &gt;&gt; You can login to view your open tickets at any time b= y visiting<br /> &gt;&gt;&gt; &gt;&gt; http://my.umbc.edu and clicking on &quot;Help&quot; a= nd &quot;Request Help&quot;.<br /> <br /> &gt;&gt;&gt; &gt;&gt; Alternately you can click on http://my.umbc.edu/help<= br /> <br /> &gt;&gt;&gt; &gt;&gt; Thank you<br /> <br /> <br /> <br /> &gt;&gt;&gt; ______________________________________<br /> <br /> &gt;&gt;&gt; Original Request:<br /> <br /> &gt;&gt;&gt; Requestors: Abdullah Al Imran<br /> <br /> &gt;&gt;&gt; First Name: Abdullah Al<br /> &gt;&gt;&gt; Last Name: Imran<br /> &gt;&gt;&gt; Email: wv96094@umbc.edu<br /> &gt;&gt;&gt; Campus ID: WV96094<br /> <br /> &gt;&gt;&gt; Request Type: High Performance Cluster<br /> <br /> <br /> &gt;&gt;&gt; Hello,<br /> <br /> &gt;&gt;&gt; I need MPICH, CGNS, PETSC modules to run my cases on hpcf clus= ter.<br /> &gt;&gt;&gt; But these modules are not available there to load! Could you p= lease<br /> &gt;&gt;&gt; load the latest version of these three modules to the cluster = and<br /> &gt;&gt;&gt; let me know when it&#39;s ready?<br /> <br /> &gt;&gt;&gt; Thanks,<br /> &gt;&gt;&gt; Imran<br /> &nbsp;</blockquote> </div> </div> </blockquote> </div> "
3287360,72132347,Create,DoIT-Research-Computing,2025-10-07 14:16:15.0000000,HPC Slurm/Software Issue: Job Recource Allocation Failing,resolved,Max Breitmeyer,mb17,Jacob Rubinstein,jrubins1,jrubins1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Jacob Last Name:                 Rubinstein Email:                     jrubins1@umbc.edu Campus ID:                 FK08578  Request Type:              High Performance Cluster   I was trying to get a conda environment set up, but after running conda create to make one, the next time I started an interactive job (command was: srun --cluster=chip-gpu --time=04:00:00 --mem=40000 --gres=gpu:1 --constraint=rtx_6000 --cpus-per-task=18 --pty /bin/bash) I noticed that instead of putting me into a node like normal, my termanal now says ""bash-5.1$"" instead of showing what node I'm on and when I try to run:      module load Anaconda3/2024.02-1 I get a note saying:     Note: Modules do not function on the login node If I then exit I am returned to the login node and I have to exit again to leave my ssh session.  "
3287360,72138619,Correspond,DoIT-Research-Computing,2025-10-07 15:24:26.0000000,HPC Slurm/Software Issue: Job Recource Allocation Failing,resolved,Max Breitmeyer,mb17,Jacob Rubinstein,jrubins1,jrubins1@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Jacob,</p>  <p>Your home directory is completely filled which can sometimes cause unexpected things to happen when creating new sessions (like what happens slurm starts a new interactive shell). Please remove some of the stuff from your home directory. If you&#39;re still having an issue after that, please let me know.&nbsp;</p>  <p>bash-5.1$ df -h /home/jrubins1/<br /> Filesystem &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Size &nbsp;Used Avail Use% Mounted on<br /> nfs.iss:/ifs/data/chip/home/jrubins1 &nbsp;500M &nbsp;500M &nbsp; &nbsp; 0 100% /home/jrubins1</p>  <p>On Tue Oct 07 10:16:15 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Jacob Last Name:                 Rubinstein Email:                     jrubins1@umbc.edu Campus ID:                 FK08578  Request Type:              High Performance Cluster   I was trying to get a conda environment set up, but after running conda create to make one, the next time I started an interactive job (command was: srun --cluster=chip-gpu --time=04:00:00 --mem=40000 --gres=gpu:1 --constraint=rtx_6000 --cpus-per-task=18 --pty /bin/bash) I noticed that instead of putting me into a node like normal, my termanal now says &quot;bash-5.1$&quot; instead of showing what node I&#39;m on and when I try to run:      module load Anaconda3/2024.02-1 I get a note saying:     Note: Modules do not function on the login node If I then exit I am returned to the login node and I have to exit again to leave my ssh session.  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3287360,72142203,Correspond,DoIT-Research-Computing,2025-10-07 17:08:07.0000000,HPC Slurm/Software Issue: Job Recource Allocation Failing,resolved,Max Breitmeyer,mb17,Jacob Rubinstein,jrubins1,jrubins1@umbc.edu,Jacob Rubinstein,jrubins1@umbc.edu,"Thank you for the quick reply,  I cleared out my home directory.  [jrubins1@chip-login2 ~]$ df -h /home/jrubins1/ Filesystem                            Size  Used Avail Use% Mounted on nfs.iss:/ifs/data/chip/home/jrubins1  500M     0  500M   0% /home/jrubins1  But when I try and run an interactive session I still have the same issue  [jrubins1@chip-login2 ~]$ srun --cluster=3Dchip-gpu --time=3D01:00:00 --mem=3D4000 --gres=3Dgpu:1 --cpus-per-task=3D1 --pty /bin/bash srun: job 101858 queued and waiting for resources srun: job 101858 has been allocated resources bash-5.1$  On Tue, Oct 7, 2025 at 11:24=E2=80=AFAM Max Breitmeyer via RT <UMBCHelp@rt.= umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287360 > > > Last Update From Ticket: > > Hi Jacob, > > Your home directory is completely filled which can sometimes cause > unexpected > things to happen when creating new sessions (like what happens slurm > starts a > new interactive shell). Please remove some of the stuff from your home > directory. If you're still having an issue after that, please let me know. > > bash-5.1$ df -h /home/jrubins1/ > Filesystem Size Used Avail Use% Mounted on > nfs.iss:/ifs/data/chip/home/jrubins1 500M 500M 0 100% /home/jrubins1 > > On Tue Oct 07 10:16:15 2025, ZZ99999 wrote: > > > First Name:                Jacob > > Last Name:                 Rubinstein > > Email:                     jrubins1@umbc.edu > > Campus ID:                 FK08578 > > > > Request Type:              High Performance Cluster > > > > > > I was trying to get a conda environment set up, but after running conda > create to make one, the next time I started an interactive job (command > was: srun --cluster=3Dchip-gpu --time=3D04:00:00 --mem=3D40000 --gres=3Dg= pu:1 > --constraint=3Drtx_6000 --cpus-per-task=3D18 --pty /bin/bash) I noticed t= hat > instead of putting me into a node like normal, my termanal now says > ""bash-5.1$"" instead of showing what node I'm on and when I try to run: > >>> module load Anaconda3/2024.02-1 > > I get a note saying: > >>> Note: Modules do not function on the login node > > If I then exit I am returned to the login node and I have to exit again > to leave my ssh session. > > > -- > > Best, > Max Breitmeyer > DOIT HPC System Administrator > > "
3287360,72142203,Correspond,DoIT-Research-Computing,2025-10-07 17:08:07.0000000,HPC Slurm/Software Issue: Job Recource Allocation Failing,resolved,Max Breitmeyer,mb17,Jacob Rubinstein,jrubins1,jrubins1@umbc.edu,Jacob Rubinstein,jrubins1@umbc.edu,"<div dir=3D""ltr""><div>Thank you for the quick reply,</div><div><br></div><d= iv>I cleared out my home directory.=C2=A0<br><br></div><div>[jrubins1@chip-= login2 ~]$ df -h /home/jrubins1/<br>Filesystem =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0Size = =C2=A0Used Avail Use% Mounted on<br>nfs.iss:/ifs/data/chip/home/jrubins1 = =C2=A0500M =C2=A0 =C2=A0 0 =C2=A0500M =C2=A0 0% /home/jrubins1</div><div><b= r></div><div>But when I try and run an interactive=C2=A0session=C2=A0I stil= l have the same issue=C2=A0</div><div><br></div><div>[jrubins1@chip-login2 = ~]$ srun --cluster=3Dchip-gpu --time=3D01:00:00 --mem=3D4000 --gres=3Dgpu:1=  --cpus-per-task=3D1 --pty /bin/bash<br>srun: job 101858 queued and waiting=  for resources<br>srun: job 101858 has been allocated resources<br>bash-5.1= $</div></div><br><div class=3D""gmail_quote gmail_quote_container""><div dir= =3D""ltr"" class=3D""gmail_attr"">On Tue, Oct 7, 2025 at 11:24=E2=80=AFAM Max B= reitmeyer via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.um= bc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""m= argin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left= :1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id= =3D3287360"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket= /Display.html?id=3D3287360</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Jacob,<br> <br> Your home directory is completely filled which can sometimes cause unexpect= ed<br> things to happen when creating new sessions (like what happens slurm starts=  a<br> new interactive shell). Please remove some of the stuff from your home<br> directory. If you&#39;re still having an issue after that, please let me kn= ow.<br> <br> bash-5.1$ df -h /home/jrubins1/<br> Filesystem Size Used Avail Use% Mounted on<br> nfs.iss:/ifs/data/chip/home/jrubins1 500M 500M 0 100% /home/jrubins1<br> <br> On Tue Oct 07 10:16:15 2025, ZZ99999 wrote:<br> <br> &gt; First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Jac= ob<br> &gt; Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0Rubinstein<br> &gt; Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 =C2=A0<a href=3D""mailto:jrubins1@umbc.edu"" target=3D""_blank"">jrubins= 1@umbc.edu</a><br> &gt; Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0FK08578<br> &gt; <br> &gt; Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Per= formance Cluster<br> &gt; <br> &gt; <br> &gt; I was trying to get a conda environment set up, but after running cond= a create to make one, the next time I started an interactive job (command w= as: srun --cluster=3Dchip-gpu --time=3D04:00:00 --mem=3D40000 --gres=3Dgpu:= 1 --constraint=3Drtx_6000 --cpus-per-task=3D18 --pty /bin/bash) I noticed t= hat instead of putting me into a node like normal, my termanal now says &qu= ot;bash-5.1$&quot; instead of showing what node I&#39;m on and when I try t= o run: <br> &gt;&gt;&gt; module load Anaconda3/2024.02-1<br> &gt; I get a note saying:<br> &gt;&gt;&gt; Note: Modules do not function on the login node<br> &gt; If I then exit I am returned to the login node and I have to exit agai= n to leave my ssh session.<br> <br> <br> --<br> <br> Best,<br> Max Breitmeyer<br> DOIT HPC System Administrator<br> <br> </blockquote></div> "
3287360,72144533,Correspond,DoIT-Research-Computing,2025-10-07 17:59:23.0000000,HPC Slurm/Software Issue: Job Recource Allocation Failing,resolved,Max Breitmeyer,mb17,Jacob Rubinstein,jrubins1,jrubins1@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Ha,</p>  <p>I think you cleared out *too much*. The file &quot;.bashrc&quot; is what= &#39;s generally in charge of creating your environment. If you just delete= d everything in your home directory, it doesn&#39;t know how to create your=  environment. I added it back in with the default set up we create for user= s in their bashrc and was able to confirm that your environment now looks n= ormal:</p>  <p>[jrubins1@chip-login2 ~]$ srun --cluster=3Dchip-gpu --time=3D01:00:00 --= mem=3D4000 --gres=3Dgpu:1 --cpus-per-task=3D1 --pty $SHELL<br /> srun: job 101868 queued and waiting for resources<br /> srun: job 101868 has been allocated resources<br /> [jrubins1@g24-01 ~]$</p>  <p>Let me know if you still have issues.&nbsp;</p>  <p>On Tue Oct 07 13:08:07 2025, FK08578 wrote:</p>  <blockquote> <div> <div>Thank you for the quick reply,</div>  <div>&nbsp;</div>  <div>I cleared out my home directory.&nbsp;<br /> &nbsp;</div>  <div>[jrubins1@chip-login2 ~]$ df -h /home/jrubins1/<br /> Filesystem &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &= nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Size &nbsp;Used Avail Use% Mounted on<br /> nfs.iss:/ifs/data/chip/home/jrubins1 &nbsp;500M &nbsp; &nbsp; 0 &nbsp;500M = &nbsp; 0% /home/jrubins1</div>  <div>&nbsp;</div>  <div>But when I try and run an interactive&nbsp;session&nbsp;I still have t= he same issue&nbsp;</div>  <div>&nbsp;</div>  <div>[jrubins1@chip-login2 ~]$ srun --cluster=3Dchip-gpu --time=3D01:00:00 = --mem=3D4000 --gres=3Dgpu:1 --cpus-per-task=3D1 --pty /bin/bash<br /> srun: job 101858 queued and waiting for resources<br /> srun: job 101858 has been allocated resources<br /> bash-5.1$</div> </div> &nbsp;  <div> <div>On Tue, Oct 7, 2025 at 11:24=E2=80=AFAM Max Breitmeyer via RT &lt;UMBC= Help@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 87360 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hi Jacob,<br /> <br /> Your home directory is completely filled which can sometimes cause unexpect= ed<br /> things to happen when creating new sessions (like what happens slurm starts=  a<br /> new interactive shell). Please remove some of the stuff from your home<br /> directory. If you&#39;re still having an issue after that, please let me kn= ow.<br /> <br /> bash-5.1$ df -h /home/jrubins1/<br /> Filesystem Size Used Avail Use% Mounted on<br /> nfs.iss:/ifs/data/chip/home/jrubins1 500M 500M 0 100% /home/jrubins1<br /> <br /> On Tue Oct 07 10:16:15 2025, ZZ99999 wrote:<br /> <br /> &gt; First Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Jac= ob<br /> &gt; Last Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p;Rubinstein<br /> &gt; Email:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &= nbsp; &nbsp;jrubins1@umbc.edu<br /> &gt; Campus ID:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p;FK08578<br /> &gt;<br /> &gt; Request Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; High Per= formance Cluster<br /> &gt;<br /> &gt;<br /> &gt; I was trying to get a conda environment set up, but after running cond= a create to make one, the next time I started an interactive job (command w= as: srun --cluster=3Dchip-gpu --time=3D04:00:00 --mem=3D40000 --gres=3Dgpu:= 1 --constraint=3Drtx_6000 --cpus-per-task=3D18 --pty /bin/bash) I noticed t= hat instead of putting me into a node like normal, my termanal now says &qu= ot;bash-5.1$&quot; instead of showing what node I&#39;m on and when I try t= o run:<br /> &gt;&gt;&gt; module load Anaconda3/2024.02-1<br /> &gt; I get a note saying:<br /> &gt;&gt;&gt; Note: Modules do not function on the login node<br /> &gt; If I then exit I am returned to the login node and I have to exit agai= n to leave my ssh session.<br /> <br /> <br /> --<br /> <br /> Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator<br /> &nbsp;</blockquote> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3287360,72144736,Correspond,DoIT-Research-Computing,2025-10-07 18:04:47.0000000,HPC Slurm/Software Issue: Job Recource Allocation Failing,resolved,Max Breitmeyer,mb17,Jacob Rubinstein,jrubins1,jrubins1@umbc.edu,Jacob Rubinstein,jrubins1@umbc.edu,"Seems to be working now. Thank you!  On Tue, Oct 7, 2025 at 1:59=E2=80=AFPM Max Breitmeyer via RT <UMBCHelp@rt.u= mbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287360 > > > Last Update From Ticket: > > Ha, > > I think you cleared out *too much*. The file "".bashrc"" is what's generally > in > charge of creating your environment. If you just deleted everything in yo= ur > home directory, it doesn't know how to create your environment. I added it > back > in with the default set up we create for users in their bashrc and was > able to > confirm that your environment now looks normal: > > [jrubins1@chip-login2 ~]$ srun --cluster=3Dchip-gpu --time=3D01:00:00 > --mem=3D4000 > --gres=3Dgpu:1 --cpus-per-task=3D1 --pty $SHELL > srun: job 101868 queued and waiting for resources > srun: job 101868 has been allocated resources > [jrubins1@g24-01 ~]$ > > Let me know if you still have issues. > > On Tue Oct 07 13:08:07 2025, FK08578 wrote: > > > Thank you for the quick reply, I cleared out my home directory. > > [jrubins1@chip-login2 ~]$ df -h /home/jrubins1/ > > Filesystem Size Used Avail Use% Mounted on > > nfs.iss:/ifs/data/chip/home/jrubins1 500M 0 500M 0% /home/jrubins1 But > when > > I try and run an interactive session I still have the same issue > > [jrubins1@chip-login2 ~]$ srun --cluster=3Dchip-gpu --time=3D01:00:00 > > --mem=3D4000 --gres=3Dgpu:1 --cpus-per-task=3D1 --pty /bin/bash > > srun: job 101858 queued and waiting for resources > > srun: job 101858 has been allocated resources > > bash-5.1$ On Tue, Oct 7, 2025 at 11:24 AM Max Breitmeyer via RT > > <UMBCHelp@rt.umbc.edu> wrote: > > >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287360 > > > >> Last Update From Ticket: > > >> Hi Jacob, > > >> Your home directory is completely filled which can sometimes cause > >> unexpected > >> things to happen when creating new sessions (like what happens slurm > >> starts a > >> new interactive shell). Please remove some of the stuff from your home > >> directory. If you're still having an issue after that, please let me > >> know. > > >> bash-5.1$ df -h /home/jrubins1/ > >> Filesystem Size Used Avail Use% Mounted on > >> nfs.iss:/ifs/data/chip/home/jrubins1 500M 500M 0 100% /home/jrubins1 > > >> On Tue Oct 07 10:16:15 2025, ZZ99999 wrote: > > >> > First Name: Jacob > >> > Last Name: Rubinstein > >> > Email: jrubins1@umbc.edu > >> > Campus ID: FK08578 > >> > > >> > Request Type: High Performance Cluster > >> > > >> > > >> > I was trying to get a conda environment set up, but after running > >> conda create to make one, the next time I started an interactive job > >> (command was: srun --cluster=3Dchip-gpu --time=3D04:00:00 --mem=3D40000 > >> --gres=3Dgpu:1 --constraint=3Drtx_6000 --cpus-per-task=3D18 --pty /bin= /bash) > >> I noticed that instead of putting me into a node like normal, my > >> termanal now says ""bash-5.1$"" instead of showing what node I'm on and > >> when I try to run: > >> >>> module load Anaconda3/2024.02-1 > >> > I get a note saying: > >> >>> Note: Modules do not function on the login node > >> > If I then exit I am returned to the login node and I have to exit > >> again to leave my ssh session. > > > >> -- > > >> Best, > >> Max Breitmeyer > >> DOIT HPC System Administrator > > -- > > Best, > Max Breitmeyer > DOIT HPC System Administrator > > "
3287360,72144736,Correspond,DoIT-Research-Computing,2025-10-07 18:04:47.0000000,HPC Slurm/Software Issue: Job Recource Allocation Failing,resolved,Max Breitmeyer,mb17,Jacob Rubinstein,jrubins1,jrubins1@umbc.edu,Jacob Rubinstein,jrubins1@umbc.edu,"<div dir=3D""ltr"">Seems to be=C2=A0working now. Thank you!</div><br><div cla= ss=3D""gmail_quote gmail_quote_container""><div dir=3D""ltr"" class=3D""gmail_at= tr"">On Tue, Oct 7, 2025 at 1:59=E2=80=AFPM Max Breitmeyer via RT &lt;<a hre= f=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></= div><blockquote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;bor= der-left:1px solid rgb(204,204,204);padding-left:1ex"">Ticket &lt;URL: <a hr= ef=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D3287360"" rel=3D""noreferr= er"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3287360<= /a> &gt;<br> <br> Last Update From Ticket:<br> <br> Ha,<br> <br> I think you cleared out *too much*. The file &quot;.bashrc&quot; is what&#3= 9;s generally in<br> charge of creating your environment. If you just deleted everything in your= <br> home directory, it doesn&#39;t know how to create your environment. I added=  it back<br> in with the default set up we create for users in their bashrc and was able=  to<br> confirm that your environment now looks normal:<br> <br> [jrubins1@chip-login2 ~]$ srun --cluster=3Dchip-gpu --time=3D01:00:00 --mem= =3D4000<br> --gres=3Dgpu:1 --cpus-per-task=3D1 --pty $SHELL<br> srun: job 101868 queued and waiting for resources<br> srun: job 101868 has been allocated resources<br> [jrubins1@g24-01 ~]$<br> <br> Let me know if you still have issues.<br> <br> On Tue Oct 07 13:08:07 2025, FK08578 wrote:<br> <br> &gt; Thank you for the quick reply, I cleared out my home directory.<br> &gt; [jrubins1@chip-login2 ~]$ df -h /home/jrubins1/<br> &gt; Filesystem Size Used Avail Use% Mounted on<br> &gt; nfs.iss:/ifs/data/chip/home/jrubins1 500M 0 500M 0% /home/jrubins1 But=  when<br> &gt; I try and run an interactive session I still have the same issue<br> &gt; [jrubins1@chip-login2 ~]$ srun --cluster=3Dchip-gpu --time=3D01:00:00<= br> &gt; --mem=3D4000 --gres=3Dgpu:1 --cpus-per-task=3D1 --pty /bin/bash<br> &gt; srun: job 101858 queued and waiting for resources<br> &gt; srun: job 101858 has been allocated resources<br> &gt; bash-5.1$ On Tue, Oct 7, 2025 at 11:24 AM Max Breitmeyer via RT<br> &gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp= @rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html= ?id=3D3287360"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tic= ket/Display.html?id=3D3287360</a> &gt;<br> <br> &gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt; Hi Jacob,<br> <br> &gt;&gt; Your home directory is completely filled which can sometimes cause= <br> &gt;&gt; unexpected<br> &gt;&gt; things to happen when creating new sessions (like what happens slu= rm<br> &gt;&gt; starts a<br> &gt;&gt; new interactive shell). Please remove some of the stuff from your = home<br> &gt;&gt; directory. If you&#39;re still having an issue after that, please = let me<br> &gt;&gt; know.<br> <br> &gt;&gt; bash-5.1$ df -h /home/jrubins1/<br> &gt;&gt; Filesystem Size Used Avail Use% Mounted on<br> &gt;&gt; nfs.iss:/ifs/data/chip/home/jrubins1 500M 500M 0 100% /home/jrubin= s1<br> <br> &gt;&gt; On Tue Oct 07 10:16:15 2025, ZZ99999 wrote:<br> <br> &gt;&gt; &gt; First Name: Jacob<br> &gt;&gt; &gt; Last Name: Rubinstein<br> &gt;&gt; &gt; Email: <a href=3D""mailto:jrubins1@umbc.edu"" target=3D""_blank""= >jrubins1@umbc.edu</a><br> &gt;&gt; &gt; Campus ID: FK08578<br> &gt;&gt; &gt;<br> &gt;&gt; &gt; Request Type: High Performance Cluster<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;<br> &gt;&gt; &gt; I was trying to get a conda environment set up, but after run= ning<br> &gt;&gt; conda create to make one, the next time I started an interactive j= ob<br> &gt;&gt; (command was: srun --cluster=3Dchip-gpu --time=3D04:00:00 --mem=3D= 40000<br> &gt;&gt; --gres=3Dgpu:1 --constraint=3Drtx_6000 --cpus-per-task=3D18 --pty = /bin/bash)<br> &gt;&gt; I noticed that instead of putting me into a node like normal, my<b= r> &gt;&gt; termanal now says &quot;bash-5.1$&quot; instead of showing what no= de I&#39;m on and<br> &gt;&gt; when I try to run:<br> &gt;&gt; &gt;&gt;&gt; module load Anaconda3/2024.02-1<br> &gt;&gt; &gt; I get a note saying:<br> &gt;&gt; &gt;&gt;&gt; Note: Modules do not function on the login node<br> &gt;&gt; &gt; If I then exit I am returned to the login node and I have to = exit<br> &gt;&gt; again to leave my ssh session.<br> <br> <br> &gt;&gt; --<br> <br> &gt;&gt; Best,<br> &gt;&gt; Max Breitmeyer<br> &gt;&gt; DOIT HPC System Administrator<br> <br> --<br> <br> Best,<br> Max Breitmeyer<br> DOIT HPC System Administrator<br> <br> </blockquote></div> "
3287721,72139463,Create,DoIT-Research-Computing,2025-10-07 15:44:22.0000000,HPC User Account: mrippin1 in oates,resolved,Beamlak Bekele,bbekele1,Madeline Rippin,mrippin1,mrippin1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Madeline Last Name:                 Rippin Email:                     mrippin1@umbc.edu Campus ID:                 SV92175  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    oates@cs.umbc.edu Existing Group:       oates Project Title:        Benchmarking MLLMs with Medical Datasets Project Abstract:     Multimodal LLMs (MLLMs) are gaining increasing awareness due to their ability to comprehensively process text, images, and videos. There are existing benchmarks that assess MLLMs ability to handle long-context memory problems; however, very few have been tested on medical data, which can have complex structures but important use case for tracking patients' medical history. This work aims to address this gap in performance metrics and determine areas of improvement for medical MLLMs memory capabilities.  First I'd like to perform benchmarking with existing datasets/benchmarks (LoCoMo, MileBench, LongHalQA, Video-MME, MMDocIR) to confirm the corresponding papers' results. Then I'd like to start benchmarking with medical datasets.  "
3287721,72139910,Correspond,DoIT-Research-Computing,2025-10-07 15:56:14.0000000,HPC User Account: mrippin1 in oates,resolved,Beamlak Bekele,bbekele1,Madeline Rippin,mrippin1,mrippin1@umbc.edu,Tim Oates,oates@cs.umbc.edu,"I approve.    On Tue, 7 Oct 2025, RT API via RT wrote:  > This e-mail is a notification that a UMBC user: Madeline Rippin <mrippin1@umbc.edu> has requested an account within UMBC's HPC environment in your group <oates>. As the PI, we request that you acknowledge and approve this account creation by replying to this message. Alternatively you can go to this link and review the ticket and indicate your decision here: > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3287721 > > > Once we have your approval, we will create the account and you and the new user will receive another e-mail notifying you that the account has been created. If you have any other questions or concerns please contact us. > > - UMBC DoIT Research Computing Support Staff >  --------------------------------------- Tim Oates, Professor Department of CS and EE University of Maryland Baltimore County (410) 455-3082 https://coral-lab.umbc.edu/oates/ "
3287721,72140988,Correspond,DoIT-Research-Computing,2025-10-07 16:32:20.0000000,HPC User Account: mrippin1 in oates,resolved,Beamlak Bekele,bbekele1,Madeline Rippin,mrippin1,mrippin1@umbc.edu,Beamlak Bekele,bbekele1@umbc.edu,"<div> <p>Hi Madeline</p>  <p>Your account (mrippin1) has been added to your primary group is pi_oates.</p>  <p>Please read through the documentation found at hpcf.umbc.edu &gt; User Support.<br /> All available modules can be viewed using the command &#39;module avail&#39;.<br /> Please submit additional questions or issues as separate tickets via the following link.<br /> (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/)</p>  <p>On Tue Oct 07 11:56:14 2025, oates@cs.umbc.edu wrote:</p>  <blockquote> <pre> I approve.    On Tue, 7 Oct 2025, RT API via RT wrote:  &gt; This e-mail is a notification that a UMBC user: Madeline Rippin &lt;mrippin1@umbc.edu&gt; has requested an account within UMBC&#39;s HPC environment in your group &lt;oates&gt;. As the PI, we request that you acknowledge and approve this account creation by replying to this message. Alternatively you can go to this link and review the ticket and indicate your decision here: &gt; &gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3287721 &gt; &gt; &gt; Once we have your approval, we will create the account and you and the new user will receive another e-mail notifying you that the account has been created. If you have any other questions or concerns please contact us. &gt; &gt; - UMBC DoIT Research Computing Support Staff &gt;  --------------------------------------- Tim Oates, Professor Department of CS and EE University of Maryland Baltimore County (410) 455-3082 https://coral-lab.umbc.edu/oates/ </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> "
3287907,72145655,Create,DoIT-Research-Computing,2025-10-07 18:28:03.0000000,URGENT: NVivo Access Policy Hindering Research - Request for Immediate Remote Desktop Access,resolved,Roy Prouty,proutyr1,----,jstsume1,jstsume1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                J=C3=A9<br /> Last Name:                 St Sume<br /> Email:                     jstsume1@umbc.edu<br /> Campus ID:                 ML69931<br /> <br /> Request Type:              Help with something else<br /> <br /> To Whom It May Concern, =0D<br /> =0D<br /> I am writing with an urgent request to secure NVivo access for one of my pa= id=0D<br /> undergraduate research assistants (URAs) working on an active, IRB-approved= =0D<br /> project (IRB #1700).=0D<br /> =0D<br /> The URA, Kendall Gitau (RQ47937), is currently unable to download the softw= are=0D<br /> because the license is restricted to graduate students. This is critically= =0D<br /> delaying a time-sensitive, grant-funded research initiative.=0D<br /> =0D<br /> Project Timeline and Financial Impact:=0D<br /> =E2=80=A2 Hard Deadline: The coding work for this project must be completed=  by December=0D<br /> 1st to allow for final data analysis and manuscript preparation.=0D<br /> =E2=80=A2 Financial Liability: Every day this access issue remains unresolv= ed directly=0D<br /> pushes back our schedule, creating a financial liability as I must continue=  to=0D<br /> pay the research assistants for tasks that cannot be completed.=0D<br /> =0D<br /> Requested Solution:=0D<br /> To resolve this immediately, could you please grant Kendall Gitau (RQ47937)= =0D<br /> access to NVivo via Remote Desktop?=0D<br /> =0D<br /> This solution would bypass the installation restrictions and ensure they ca= n=0D<br /> begin the mandatory Intercoder Reliability check this week.=0D<br /> =0D<br /> Thank you for your immediate attention to this matter, as the project's suc= cess=0D<br /> and budget depend on a quick resolution.=0D<br /> =0D<br /> Best regards,=0D<br /> Dr. St Sume=0D<br /> <br /> <br /> Attachment 1: <a href=3D""https://umbc.box.com/s/tpbusmv5djpnrk8563m4k5vgu2p= g8ty6"" target=3D""_blank"">URA Schedule and Deadline.pdf</a><br /> Attachment 2: <a href=3D""https://umbc.box.com/s/jh72und80z6qebewoc4q0ceqpa3= 575vz"" target=3D""_blank"">IRB #1700 Protocol.pdf</a><br /> "
3287907,72146834,Correspond,DoIT-Research-Computing,2025-10-07 18:57:13.0000000,URGENT: NVivo Access Policy Hindering Research - Request for Immediate Remote Desktop Access,resolved,Roy Prouty,proutyr1,----,jstsume1,jstsume1@umbc.edu,Roy Prouty,proutyr1@umbc.edu,"<p>Hi Dr. St Sume,</p>  <p>Could you have your student try this environment?&nbsp;</p>  <p>https://elum.in/umbc-desktop-sosc</p>  <p>I believe it offers the appropriate access.</p>  <p>--&nbsp;</p>  <p>Roy Prouty<br /> DoIT Research Computing Team</p> "
3287907,72146991,Correspond,DoIT-Research-Computing,2025-10-07 19:00:25.0000000,URGENT: NVivo Access Policy Hindering Research - Request for Immediate Remote Desktop Access,resolved,Roy Prouty,proutyr1,----,jstsume1,jstsume1@umbc.edu,Jé St Sume,jstsume1@umbc.edu,"Thank you for your prompt response! I will reach out to the student and update you accordingly.  With appreciation, Dr. S  On Tue, Oct 7, 2025 at 2:57=E2=80=AFPM Roy Prouty via RT <UMBCHelp@rt.umbc.= edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287907 > > > Last Update From Ticket: > > Hi Dr. St Sume, > > Could you have your student try this environment? > > https://elum.in/umbc-desktop-sosc > > I believe it offers the appropriate access. > > -- > > Roy Prouty > DoIT Research Computing Team > > "
3287907,72146991,Correspond,DoIT-Research-Computing,2025-10-07 19:00:25.0000000,URGENT: NVivo Access Policy Hindering Research - Request for Immediate Remote Desktop Access,resolved,Roy Prouty,proutyr1,----,jstsume1,jstsume1@umbc.edu,Jé St Sume,jstsume1@umbc.edu,"<div dir=3D""ltr"">Thank you for your prompt response! I will reach out to th= e student and update you accordingly.<div><br></div><div>With appreciation,= =C2=A0</div><div>Dr. S</div></div><br><div class=3D""gmail_quote gmail_quote= _container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Tue, Oct 7, 2025 at 2:= 57=E2=80=AFPM Roy Prouty via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu""= >UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_qu= ote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,20= 4);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/= Display.html?id=3D3287907"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.= umbc.edu/Ticket/Display.html?id=3D3287907</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Dr. St Sume,<br> <br> Could you have your student try this environment?<br> <br> <a href=3D""https://elum.in/umbc-desktop-sosc"" rel=3D""noreferrer"" target=3D""= _blank"">https://elum.in/umbc-desktop-sosc</a><br> <br> I believe it offers the appropriate access.<br> <br> --<br> <br> Roy Prouty<br> DoIT Research Computing Team<br> <br> </blockquote></div> "
3287907,72173481,Correspond,DoIT-Research-Computing,2025-10-08 19:59:49.0000000,URGENT: NVivo Access Policy Hindering Research - Request for Immediate Remote Desktop Access,resolved,Roy Prouty,proutyr1,----,jstsume1,jstsume1@umbc.edu,Jé St Sume,jstsume1@umbc.edu,"The student was able to download the software. Thank you for your help on this matter! I have no further requests.  Best, Dr. St Sume  On Tue, Oct 7, 2025 at 3:00=E2=80=AFPM J=C3=A9 St Sume <jstsume1@umbc.edu> = wrote:  > Thank you for your prompt response! I will reach out to the student and > update you accordingly. > > With appreciation, > Dr. S > > On Tue, Oct 7, 2025 at 2:57=E2=80=AFPM Roy Prouty via RT <UMBCHelp@rt.umb= c.edu> > wrote: > >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287907 > >> >> Last Update From Ticket: >> >> Hi Dr. St Sume, >> >> Could you have your student try this environment? >> >> https://elum.in/umbc-desktop-sosc >> >> I believe it offers the appropriate access. >> >> -- >> >> Roy Prouty >> DoIT Research Computing Team >> >> "
3287907,72173481,Correspond,DoIT-Research-Computing,2025-10-08 19:59:49.0000000,URGENT: NVivo Access Policy Hindering Research - Request for Immediate Remote Desktop Access,resolved,Roy Prouty,proutyr1,----,jstsume1,jstsume1@umbc.edu,Jé St Sume,jstsume1@umbc.edu,"<div dir=3D""auto"">The student was able to download the software. Thank you = for your help on this matter! I have no further requests.=C2=A0</div><div d= ir=3D""auto""><br></div><div dir=3D""auto"">Best,=C2=A0</div><div dir=3D""auto"">= Dr. St Sume</div><div dir=3D""auto""><br><div class=3D""gmail_quote gmail_quot= e_container"" dir=3D""auto""><div dir=3D""ltr"" class=3D""gmail_attr"">On Tue, Oct=  7, 2025 at 3:00=E2=80=AFPM J=C3=A9 St Sume &lt;<a href=3D""mailto:jstsume1@= umbc.edu"">jstsume1@umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gm= ail_quote"" style=3D""margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-le= ft:1ex""><div dir=3D""ltr"">Thank you for your prompt response! I will reach o= ut to the student and update you accordingly.<div><br></div><div>With appre= ciation,=C2=A0</div><div>Dr. S</div></div><br><div class=3D""gmail_quote""><d= iv dir=3D""ltr"" class=3D""gmail_attr"">On Tue, Oct 7, 2025 at 2:57=E2=80=AFPM = Roy Prouty via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_bl= ank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmai= l_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,20= 4,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Tic= ket/Display.html?id=3D3287907"" rel=3D""noreferrer"" target=3D""_blank"">https:/= /rt.umbc.edu/Ticket/Display.html?id=3D3287907</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Dr. St Sume,<br> <br> Could you have your student try this environment?<br> <br> <a href=3D""https://elum.in/umbc-desktop-sosc"" rel=3D""noreferrer"" target=3D""= _blank"">https://elum.in/umbc-desktop-sosc</a><br> <br> I believe it offers the appropriate access.<br> <br> --<br> <br> Roy Prouty<br> DoIT Research Computing Team<br> <br> </blockquote></div> </blockquote></div></div> "
3287907,72176056,Correspond,DoIT-Research-Computing,2025-10-09 00:52:09.0000000,URGENT: NVivo Access Policy Hindering Research - Request for Immediate Remote Desktop Access,resolved,Roy Prouty,proutyr1,----,jstsume1,jstsume1@umbc.edu,Roy Prouty,proutyr1@umbc.edu,"<div> <p>Excellent! Glad to hear this is working.</p>  <p>I&#39;ll mark this as resolved -- do reopen if there are related issues.= </p>  <p>&nbsp;</p>  <p>On Wed Oct 08 15:59:49 2025, ML69931 wrote:</p>  <blockquote> <div>The student was able to download the software. Thank you for your help=  on this matter! I have no further requests.&nbsp;</div>  <div>&nbsp;</div>  <div>Best,&nbsp;</div>  <div>Dr. St Sume</div>  <div>&nbsp; <div> <div>On Tue, Oct 7, 2025 at 3:00=E2=80=AFPM J&eacute; St Sume &lt;jstsume1@= umbc.edu&gt; wrote:</div>  <blockquote> <div>Thank you for your prompt response! I will reach out to the student an= d update you accordingly. <div>&nbsp;</div>  <div>With appreciation,&nbsp;</div>  <div>Dr. S</div> </div> &nbsp;  <div> <div>On Tue, Oct 7, 2025 at 2:57=E2=80=AFPM Roy Prouty via RT &lt;UMBCHelp@= rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 87907 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hi Dr. St Sume,<br /> <br /> Could you have your student try this environment?<br /> <br /> https://elum.in/umbc-desktop-sosc<br /> <br /> I believe it offers the appropriate access.<br /> <br /> --<br /> <br /> Roy Prouty<br /> DoIT Research Computing Team<br /> &nbsp;</blockquote> </div> </blockquote> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Roy Prouty<br /> DoIT Research Computing Team</p> "
3287915,72145993,Create,DoIT-Research-Computing,2025-10-07 18:36:54.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_brewster,stalled,Greg Ballantine,gballan1,Rachel Brewster,brewster,brewster@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Rachel,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a&nbsp;244.1 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/brewster&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_brewster&rdquo;.<br /> <br /> Thank you,</p>  <p>Elliot</p> "
3287915,72350574,Correspond,DoIT-Research-Computing,2025-10-17 16:13:50.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_brewster,stalled,Greg Ballantine,gballan1,Rachel Brewster,brewster,brewster@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Rachel,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 28th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3287915,72527859,Correspond,DoIT-Research-Computing,2025-10-28 14:49:13.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_brewster,stalled,Greg Ballantine,gballan1,Rachel Brewster,brewster,brewster@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Rachel,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 12:13:50 2025, IO12693 wrote:</p>  <blockquote> <p>Dear Rachel,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 28th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3287915,72539085,Correspond,DoIT-Research-Computing,2025-10-28 19:36:23.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_brewster,stalled,Greg Ballantine,gballan1,Rachel Brewster,brewster,brewster@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Rachel,<br /> <br /> We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note:</p>  <ul> 	<li>The path has changed, and is now available under <strong>/umbc/rs/pi_brewster</strong>.</li> 	<li>The alias used to reach the volume is now <strong>pi_brewster_common</strong> and <strong>pi_brewster_user</strong>.</li> 	<li>Your new volume has a quota of <strong>10TB</strong>.</li> </ul>  <p>When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good?<br /> <br /> Thank you,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Tue Oct 28 10:49:13 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Hello Rachel,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 12:13:50 2025, IO12693 wrote:</p>  <blockquote> <p>Dear Rachel,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 28th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3287918,72146122,Create,DoIT-Research-Computing,2025-10-07 18:40:58.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_chang,resolved,Greg Ballantine,gballan1,Elliot Gobbert,elliotg2,elliotg2@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear <strong>Richard</strong>,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 488.3 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/<strong>chang</strong>&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_chang&rdquo;.<br /> <br /> Thank you,<br /> <strong>Elliot</strong></p> "
3287921,72146184,Create,DoIT-Research-Computing,2025-10-07 18:42:15.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_chang,stalled,Greg Ballantine,gballan1,Richard Chang,chang,chang@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Richard,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 488.3 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/chang&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_chang&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3287921,72350814,Correspond,DoIT-Research-Computing,2025-10-17 16:21:16.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_chang,stalled,Greg Ballantine,gballan1,Richard Chang,chang,chang@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Richard,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 29th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3287921,72549344,Correspond,DoIT-Research-Computing,2025-10-29 13:47:56.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_chang,stalled,Greg Ballantine,gballan1,Richard Chang,chang,chang@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Richard,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 12:21:16 2025, IO12693 wrote:</p>  <blockquote> <p>Dear Richard,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 29th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3287921,72589143,Correspond,DoIT-Research-Computing,2025-10-29 21:28:31.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_chang,stalled,Greg Ballantine,gballan1,Richard Chang,chang,chang@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello again Richard,<br /> <br /> We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note:</p>  <ul> 	<li>The path has changed, and is now available under <strong>/umbc/rs/pi_chang</strong>.</li> 	<li>The alias used to reach the volume is now <strong>pi_chang_common</strong> and <strong>pi_chang_user</strong>.</li> 	<li>Your new volume has a quota of <strong>25TB</strong>.</li> </ul>  <p>When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good?<br /> <br /> Thank you,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Wed Oct 29 09:47:56 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Hello Richard,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 12:21:16 2025, IO12693 wrote:</p>  <blockquote> <p>Dear Richard,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 29th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3287925,72146358,Create,DoIT-Research-Computing,2025-10-07 18:44:50.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_cmckin1,stalled,Greg Ballantine,gballan1,Carlos E. McKinney,cmckin1,carlos@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Carlos,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 488.3 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/cmckin1&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_cmckin1&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3287925,72351074,Correspond,DoIT-Research-Computing,2025-10-17 16:28:50.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_cmckin1,stalled,Greg Ballantine,gballan1,Carlos E. McKinney,cmckin1,carlos@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Carlos,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 30th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3287925,72597817,Correspond,DoIT-Research-Computing,2025-10-30 14:33:54.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_cmckin1,stalled,Greg Ballantine,gballan1,Carlos E. McKinney,cmckin1,carlos@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Carlos,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 12:28:50 2025, IO12693 wrote:</p>  <blockquote> <p>Dear Carlos,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 30th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3287925,72612748,Correspond,DoIT-Research-Computing,2025-10-30 23:27:40.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_cmckin1,stalled,Greg Ballantine,gballan1,Carlos E. McKinney,cmckin1,carlos@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Carlos,<br /> <br /> We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note:</p>  <ul> 	<li>The path has changed, and is now available under <strong>/umbc/rs/pi_cmckin1</strong>.  	<ul> 		<li>The alias used to reach the volume is now <strong>pi_cmckin1_common</strong> and <strong>pi_cmckin1_user</strong>.</li> 	</ul> 	</li> 	<li>Your new volume has a quota of <strong>10TB</strong>.</li> </ul>  <p>When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good?<br /> <br /> Thank you,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Thu Oct 30 10:33:54 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Hello Carlos,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 12:28:50 2025, IO12693 wrote:</p>  <blockquote> <p>Dear Carlos,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 30th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3287928,72146453,Create,DoIT-Research-Computing,2025-10-07 18:47:03.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_deffner,resolved,Greg Ballantine,gballan1,Sebastian Deffner,deffner,deffner@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Sebastian,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 244.1 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/deffner&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_deffner&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3287928,72147586,Correspond,DoIT-Research-Computing,2025-10-07 19:15:59.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_deffner,resolved,Greg Ballantine,gballan1,Sebastian Deffner,deffner,deffner@umbc.edu,Sebastian Deffner,deffner@umbc.edu,"Any date is fine.  -Sebastian  ---------------------------------------------------------------------------= ------------- Dr. rer. nat. Sebastian Deffner Associate Professor Chair, Graduate Admissions Committee Department of Physics Quantum Science Institute UMBC (University of Maryland, Baltimore County) 1000 Hilltop Circle, Baltimore, MD 21250, USA  Fellow National Quantum Laboratory 4505 Campus Dr, College Park, MD 20740, USA  Chair-Elect and Program Chair Division of Statistical and Nonlinear Physics (DSNP) American Physical Society  Tel.: +1-410-455-1972 quthermo.umbc.edu <https://quthermo.umbc.edu/>  /UMBC was established upon the land of the Piscataway and Susquehannock=20 peoples. Over time, citizens of many more Indigenous nations have come=20 to reside in this region. We humbly offer our respect to all past,=20 present, and future Indigenous people connected to this place./  On 10/7/25 14:47, via RT wrote: > Greetings, > > This message has been automatically generated in response to the > creation of a ticket regarding: > > ------------------------------------------------------------------------- > Subject: ""Migrating Research Storage Volume to Ceph Cluster - pi_deffner"" > > Message: > > Dear Sebastian, > > As per the communication via myUMBC earlier this summer (June HPCF Newsle= tter > ), DoIT is in the process of migrating data off of an older storage serve= r to > our new RRStor Ceph storage cluster. Your group is using 0 GB of a 244.1 = GB > quota on the old storage server. > > To perform these migrations, we need to take individual storage volumes o= ffline > while we migrate them to the Ceph cluster. Thus we are reaching out to sc= hedule > a date where we can migrate your volume located at =E2=80=9C/umbc/rs/deff= ner=E2=80=9D. During > the migration, we will take your volume offline and will terminate any jo= bs > running on the chip compute cluster that are accessing this volume. > > Below we=E2=80=99ve listed two options for handling this data migration -=  please let us > know which of these you=E2=80=99d prefer. > > Option 1: Schedule a group-wide downtime date during standard business ho= urs, > which can be done by responding to this email with your preferred date(s)=  to > perform the migration. During this time, DoIT staff will work to migrate = your > volume to the Ceph storage cluster. DoIT staff will send an email alert o= n this > email thread when the migration has begun and when it has completed. For = most > storage volumes, this process should take less than a business day. > Option 2: If you don=E2=80=99t respond to this email by October 15th, DoI= T staff will > assign a day over the following month (October 16th through November 15th= ) to > migrate your volume. The day chosen will be random and will occur during > business hours. You will be notified of the date chosen to perform the > migration, and will be notified when the migration begins and completes. > > Note: After this process has completed, the new storage volume will have = a new > name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its data und= er =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, > or in your group=E2=80=99s case you will find your volume under =E2=80=9C= /umbc/rs/pi_deffner=E2=80=9D. > > Thank you, > Elliot > > > ------------------------------------------------------------------------- >=20=20=20 > There is no need to reply to this message right now. > > Your ticket has been assigned an ID of [Research Computing #3287928] or y= ou can go there directly by clicking the link below. > > Ticket <URL:https://rt.umbc.edu/Ticket/Display.html?id=3D3287928 > > > You can login to view your open tickets at any time by visitinghttp://my.= umbc.edu and clicking on ""Help"" and ""Request Help"". > > Alternately you can click onhttp://my.umbc.edu/help > >                          Thank you >= "
3287928,72147586,Correspond,DoIT-Research-Computing,2025-10-07 19:15:59.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_deffner,resolved,Greg Ballantine,gballan1,Sebastian Deffner,deffner,deffner@umbc.edu,Sebastian Deffner,deffner@umbc.edu,"<!DOCTYPE html> <html>   <head>     <meta http-equiv=3D""Content-Type"" content=3D""text/html; charset=3DUTF-8= "">   </head>   <body>     <p>Any date is fine.</p>     <p>-Sebastian</p>     <div class=3D""moz-signature"">------------------------------------------= ----------------------------------------------<br>       Dr. rer. nat. Sebastian Deffner<br>       Associate Professor<br>       Chair, Graduate Admissions Committee<br>       Department of Physics<br>       Quantum Science Institute<br>       UMBC (University of Maryland, Baltimore County)<br>       1000 Hilltop Circle, Baltimore, MD 21250, USA<br>       <br>       Fellow<br>       National Quantum Laboratory<br>       4505 Campus Dr, College Park, MD 20740, USA<br>       <br>       Chair-Elect and Program Chair<br>       Division of Statistical and Nonlinear Physics (DSNP)<br>       American Physical Society<br>       <br>       Tel.: +1-410-455-1972<br>       <a href=3D""https://quthermo.umbc.edu/"">quthermo.umbc.edu</a><br>       <br>       <i>UMBC was established upon the land of the Piscataway and         Susquehannock peoples. Over time, citizens of many more         Indigenous nations have come to reside in this region. We humbly         offer our respect to all past, present, and future Indigenous         people connected to this place.</i>       <br>       <p style=3D""text-align: left"">         <img src=3D""cid:part1.rulh7IZQ.HQupY4aI@umbc.edu"" alt=3D""""           width=3D""300"" height=3D""96"">       </p>     </div>     <div class=3D""moz-cite-prefix"">On 10/7/25 14:47, via RT wrote:<br>     </div>     <blockquote type=3D""cite""       cite=3D""mid:rt-5.0.5-28886-1759862824-163.3287928-3-0@rt.umbc.edu"">       <pre wrap=3D"""" class=3D""moz-quote-pre"">Greetings,  This message has been automatically generated in response to the creation of a ticket regarding:  ------------------------------------------------------------------------- Subject: ""Migrating Research Storage Volume to Ceph Cluster - pi_deffner""  Message:=20  Dear Sebastian,  As per the communication via myUMBC earlier this summer (June HPCF Newslett= er ), DoIT is in the process of migrating data off of an older storage server = to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 244.1 GB quota on the old storage server.  To perform these migrations, we need to take individual storage volumes off= line while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule a date where we can migrate your volume located at =E2=80=9C/umbc/rs/deffne= r=E2=80=9D. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.  Below we=E2=80=99ve listed two options for handling this data migration - p= lease let us know which of these you=E2=80=99d prefer.  Option 1: Schedule a group-wide downtime date during standard business hour= s, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate yo= ur volume to the Ceph storage cluster. DoIT staff will send an email alert on = this email thread when the migration has begun and when it has completed. For mo= st storage volumes, this process should take less than a business day. Option 2: If you don=E2=80=99t respond to this email by October 15th, DoIT = staff will assign a day over the following month (October 16th through November 15th) = to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.  Note: After this process has completed, the new storage volume will have a = new name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its data under=  =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, or in your group=E2=80=99s case you will find your volume under =E2=80=9C/u= mbc/rs/pi_deffner=E2=80=9D.  Thank you, Elliot   ------------------------------------------------------------------------- =20 There is no need to reply to this message right now.=20=20  Your ticket has been assigned an ID of [Research Computing #3287928] or you=  can go there directly by clicking the link below.  Ticket &lt;URL: <a class=3D""moz-txt-link-freetext"" href=3D""https://rt.umbc.= edu/Ticket/Display.html?id=3D3287928"">https://rt.umbc.edu/Ticket/Display.ht= ml?id=3D3287928</a> &gt;  You can login to view your open tickets at any time by visiting <a class=3D= ""moz-txt-link-freetext"" href=3D""http://my.umbc.edu"">http://my.umbc.edu</a> = and clicking on ""Help"" and ""Request Help"".=20  Alternately you can click on <a class=3D""moz-txt-link-freetext"" href=3D""htt= p://my.umbc.edu/help"">http://my.umbc.edu/help</a>                          Thank you  </pre>     </blockquote>   </body> </html>= "
3287928,72210852,Correspond,DoIT-Research-Computing,2025-10-10 17:14:46.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_deffner,resolved,Greg Ballantine,gballan1,Sebastian Deffner,deffner,deffner@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Sebastian,<br /> <br /> We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note:</p>  <ul> 	<li>The path has changed, and is now available under <strong>/umbc/rs/pi_deffner</strong>.</li> 	<li>The alias used to reach the volume is now <strong>pi_deffner_common</strong> and <strong>pi_deffner_user</strong>.</li> 	<li>Your new volume has a quota of <strong>10TB</strong>.</li> </ul>  <p>When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good?<br /> <br /> Thank you,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Tue Oct 07 15:15:59 2025, KU92175 wrote:</p>  <blockquote>deffner</blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3287931,72146530,Create,DoIT-Research-Computing,2025-10-07 18:49:19.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_delgado,stalled,Greg Ballantine,gballan1,Ruben Delgado,delgado,delgado@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Ruben,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 488.3 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/delgado&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_delgado&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3287931,72351763,Correspond,DoIT-Research-Computing,2025-10-17 16:49:18.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_delgado,stalled,Greg Ballantine,gballan1,Ruben Delgado,delgado,delgado@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Ruben,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 31st for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3287931,72621342,Correspond,DoIT-Research-Computing,2025-10-31 14:42:15.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_delgado,stalled,Greg Ballantine,gballan1,Ruben Delgado,delgado,delgado@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Good morning Ruben,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 12:49:18 2025, IO12693 wrote:</p>  <blockquote> <p>Dear Ruben,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 31st for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3287931,72639759,Correspond,DoIT-Research-Computing,2025-10-31 18:20:24.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_delgado,stalled,Greg Ballantine,gballan1,Ruben Delgado,delgado,delgado@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Ruben,<br /> <br /> We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note:</p>  <ul> 	<li>The path has changed, and is now available under <strong>/umbc/rs/pi_delgado</strong>.</li> 	<li>The alias used to reach the volume is now <strong>pi_delgado_common</strong> and <strong>pi_delgado_user</strong>.</li> 	<li>Your new volume has a quota of <strong>10TB</strong>.</li> </ul>  <p>When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good?<br /> <br /> Thank you,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 31 10:42:15 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Good morning Ruben,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 12:49:18 2025, IO12693 wrote:</p>  <blockquote> <p>Dear Ruben,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of October 31st for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3287935,72146641,Create,DoIT-Research-Computing,2025-10-07 18:52:42.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_georgano,stalled,Greg Ballantine,gballan1,Markos Georganopoulos,georgano,georgano@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Markos,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 488.3 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/georgano&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_georgano&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3287935,72352529,Correspond,DoIT-Research-Computing,2025-10-17 17:10:28.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_georgano,stalled,Greg Ballantine,gballan1,Markos Georganopoulos,georgano,georgano@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Markos,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 5th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3287940,72146732,Create,DoIT-Research-Computing,2025-10-07 18:55:06.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_hwkang,stalled,Greg Ballantine,gballan1,Hye-Won Kang,hwkang,hwkang@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Hye-Won,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB&nbsp;of a 488.3 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/hwkang&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_hwkang&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3287940,72397472,Correspond,DoIT-Research-Computing,2025-10-21 15:50:01.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_hwkang,stalled,Greg Ballantine,gballan1,Hye-Won Kang,hwkang,hwkang@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Hye-Won,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 6th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3287940,72397567,Correspond,DoIT-Research-Computing,2025-10-21 15:52:25.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_hwkang,stalled,Greg Ballantine,gballan1,Hye-Won Kang,hwkang,hwkang@umbc.edu,Hye-Won Kang,hwkang@umbc.edu,"<div>On Tue Oct 21 11:50:01 2025, IO12693 wrote: <blockquote> <p>Dear Hye-Won,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 6th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> </blockquote> </div>  <p>Dear Elliot,</p>  <p>Thank you for your email and for letting me know. It sounds good to me.</p>  <p>Best regards,</p>  <p>Hye-Won&nbsp;</p> "
3287942,72146849,Create,DoIT-Research-Computing,2025-10-07 18:57:16.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ithorpe,stalled,Greg Ballantine,gballan1,Ian Thorpe,ithorpe,ithorpe@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Ian,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB&nbsp;of a 488.3 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/ithorpe&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_ithorpe&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3287942,72397526,Correspond,DoIT-Research-Computing,2025-10-21 15:51:13.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ithorpe,stalled,Greg Ballantine,gballan1,Ian Thorpe,ithorpe,ithorpe@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Ian,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 6th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3287945,72146926,Create,DoIT-Research-Computing,2025-10-07 18:59:12.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_jgong,stalled,Greg Ballantine,gballan1,Jiaqi Gong,jgong,jgong@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Jiaqi,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 488.3 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/jgong&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_jgong&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3287945,72397781,Correspond,DoIT-Research-Computing,2025-10-21 15:56:02.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_jgong,stalled,Greg Ballantine,gballan1,Jiaqi Gong,jgong,jgong@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Jiaqi,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 6th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3287949,72147020,Create,DoIT-Research-Computing,2025-10-07 19:00:55.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_jliebman,stalled,Greg Ballantine,gballan1,Joel Liebman,jliebman,jliebman@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Joel,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB&nbsp;of a 488.3 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/jliebman&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_jliebman&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3287949,72397924,Correspond,DoIT-Research-Computing,2025-10-21 15:58:29.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_jliebman,stalled,Greg Ballantine,gballan1,Joel Liebman,jliebman,jliebman@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Joel,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 7th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3287950,72147090,Create,DoIT-Research-Computing,2025-10-07 19:03:37.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_kak,resolved,Greg Ballantine,gballan1,Kaur Kullman,kak,kak@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Kaur,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 244.1 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/kak&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_kak&quot;.<br /> <br /> Thank you,<br /> Elliot</p> "
3287950,72148570,Correspond,DoIT-Research-Computing,2025-10-07 19:42:37.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_kak,resolved,Greg Ballantine,gballan1,Kaur Kullman,kak,kak@umbc.edu,Kaur Kullman,kak@umbc.edu,"Hello,  Please go ahead with the migration as you see fit. AFAIK i do not have any = data in that folder currently.  Best regards, Kaur  > On Oct 7, 2025, at 21:03, via RT <UMBCHelp@rt.umbc.edu> wrote: >=20 > =EF=BB=BFGreetings, >=20 > This message has been automatically generated in response to the > creation of a ticket regarding: >=20 > ------------------------------------------------------------------------- > Subject: ""Migrating Research Storage Volume to Ceph Cluster - pi_kak"" >=20 > Message: >=20 > Dear Kaur, >=20 > As per the communication via myUMBC earlier this summer (June HPCF Newsle= tter > ), DoIT is in the process of migrating data off of an older storage serve= r to > our new RRStor Ceph storage cluster. Your group is using 0 GB of a 244.1 = GB > quota on the old storage server. >=20 > To perform these migrations, we need to take individual storage volumes o= ffline > while we migrate them to the Ceph cluster. Thus we are reaching out to sc= hedule > a date where we can migrate your volume located at =E2=80=9C/umbc/rs/kak= =E2=80=9D. During the > migration, we will take your volume offline and will terminate any jobs r= unning > on the chip compute cluster that are accessing this volume. >=20 > Below we=E2=80=99ve listed two options for handling this data migration -=  please let us > know which of these you=E2=80=99d prefer. >=20 > Option 1: Schedule a group-wide downtime date during standard business ho= urs, > which can be done by responding to this email with your preferred date(s)=  to > perform the migration. During this time, DoIT staff will work to migrate = your > volume to the Ceph storage cluster. DoIT staff will send an email alert o= n this > email thread when the migration has begun and when it has completed. For = most > storage volumes, this process should take less than a business day. > Option 2: If you don=E2=80=99t respond to this email by October 15th, DoI= T staff will > assign a day over the following month (October 16th through November 15th= ) to > migrate your volume. The day chosen will be random and will occur during > business hours. You will be notified of the date chosen to perform the > migration, and will be notified when the migration begins and completes. >=20 > Note: After this process has completed, the new storage volume will have = a new > name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its data und= er =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, > or in your group=E2=80=99s case you will find your volume under =E2=80=9C= /umbc/rs/pi_kak"". >=20 > Thank you, > Elliot >=20 >=20 > ------------------------------------------------------------------------- >=20 > There is no need to reply to this message right now.=20=20 >=20 > Your ticket has been assigned an ID of [Research Computing #3287950] or y= ou can go there directly by clicking the link below. >=20 > Ticket <URL: https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Di= splay.html?id%3D3287950&source=3Dgmail-imap&ust=3D1760468623000000&usg=3DAO= vVaw01nFIGJOwmwRbKuFOVjfNg > >=20 > You can login to view your open tickets at any time by visiting https://w= ww.google.com/url?q=3Dhttp://my.umbc.edu&source=3Dgmail-imap&ust=3D17604686= 23000000&usg=3DAOvVaw3Xs6z1SKtCJYX0R7YE054L and clicking on ""Help"" and ""Req= uest Help"". >=20 > Alternately you can click on https://www.google.com/url?q=3Dhttp://my.umb= c.edu/help&source=3Dgmail-imap&ust=3D1760468623000000&usg=3DAOvVaw1E2ghKVip= K0SsvWqfSuNUq >=20 >                        Thank you >=20 "
3287950,72148570,Correspond,DoIT-Research-Computing,2025-10-07 19:42:37.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_kak,resolved,Greg Ballantine,gballan1,Kaur Kullman,kak,kak@umbc.edu,Kaur Kullman,kak@umbc.edu,"<html><head><meta http-equiv=3D""content-type"" content=3D""text/html; charset= =3Dutf-8""></head><body dir=3D""auto"">Hello,<div><br></div><div>Please go ahe= ad with the migration as you see fit. AFAIK i do not have any data in that = folder currently.</div><div><br id=3D""lineBreakAtBeginningOfSignature""><div=  dir=3D""ltr""><p class=3D""MsoNormal"" style=3D""margin: 0cm 0cm 0.0001pt;"">Bes= t regards,</p><p class=3D""MsoNormal"" style=3D""margin: 0cm 0cm 0.0001pt;"">Ka= ur</p></div><div dir=3D""ltr""><br><blockquote type=3D""cite"">On Oct 7, 2025, = at 21:03, via RT &lt;UMBCHelp@rt.umbc.edu&gt; wrote:<br><br></blockquote></= div><blockquote type=3D""cite""><div dir=3D""ltr"">=EF=BB=BF<span>Greetings,</s= pan><br><span></span><br><span>This message has been automatically generate= d in response to the</span><br><span>creation of a ticket regarding:</span>= <br><span></span><br><span>------------------------------------------------= -------------------------</span><br><span>Subject: ""Migrating Research Stor= age Volume to Ceph Cluster - pi_kak""</span><br><span></span><br><span>Messa= ge: </span><br><span></span><br><span>Dear Kaur,</span><br><span></span><br= ><span>As per the communication via myUMBC earlier this summer (June HPCF N= ewsletter</span><br><span>), DoIT is in the process of migrating data off o= f an older storage server to</span><br><span>our new RRStor Ceph storage cl= uster. Your group is using 0 GB of a 244.1 GB</span><br><span>quota on the = old storage server.</span><br><span></span><br><span>To perform these migra= tions, we need to take individual storage volumes offline</span><br><span>w= hile we migrate them to the Ceph cluster. Thus we are reaching out to sched= ule</span><br><span>a date where we can migrate your volume located at =E2= =80=9C/umbc/rs/kak=E2=80=9D. During the</span><br><span>migration, we will = take your volume offline and will terminate any jobs running</span><br><spa= n>on the chip compute cluster that are accessing this volume.</span><br><sp= an></span><br><span>Below we=E2=80=99ve listed two options for handling thi= s data migration - please let us</span><br><span>know which of these you=E2= =80=99d prefer.</span><br><span></span><br><span>Option 1: Schedule a group= -wide downtime date during standard business hours,</span><br><span>which c= an be done by responding to this email with your preferred date(s) to</span= ><br><span>perform the migration. During this time, DoIT staff will work to=  migrate your</span><br><span>volume to the Ceph storage cluster. DoIT staf= f will send an email alert on this</span><br><span>email thread when the mi= gration has begun and when it has completed. For most</span><br><span>stora= ge volumes, this process should take less than a business day.</span><br><s= pan>Option 2: If you don=E2=80=99t respond to this email by October 15th, D= oIT staff will</span><br><span>assign a day over the following month (Octob= er 16th through November 15th) to</span><br><span>migrate your volume. The = day chosen will be random and will occur during</span><br><span>business ho= urs. You will be notified of the date chosen to perform the</span><br><span= >migration, and will be notified when the migration begins and completes.</= span><br><span></span><br><span>Note: After this process has completed, the=  new storage volume will have a new</span><br><span>name. For example, grou= p =E2=80=9Cpi_doit=E2=80=9D will find its data under =E2=80=9C/umbc/rs/pi_d= oit=E2=80=9D,</span><br><span>or in your group=E2=80=99s case you will find=  your volume under =E2=80=9C/umbc/rs/pi_kak"".</span><br><span></span><br><s= pan>Thank you,</span><br><span>Elliot</span><br><span></span><br><span></sp= an><br><span>--------------------------------------------------------------= -----------</span><br><span></span><br><span>There is no need to reply to t= his message right now. &nbsp;</span><br><span></span><br><span>Your ticket = has been assigned an ID of [Research Computing #3287950] or you can go ther= e directly by clicking the link below.</span><br><span></span><br><span>Tic= ket &lt;URL: https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Disp= lay.html?id%3D3287950&amp;source=3Dgmail-imap&amp;ust=3D1760468623000000&am= p;usg=3DAOvVaw01nFIGJOwmwRbKuFOVjfNg &gt;</span><br><span></span><br><span>= You can login to view your open tickets at any time by visiting https://www= .google.com/url?q=3Dhttp://my.umbc.edu&amp;source=3Dgmail-imap&amp;ust=3D17= 60468623000000&amp;usg=3DAOvVaw3Xs6z1SKtCJYX0R7YE054L and clicking on ""Help= "" and ""Request Help"". </span><br><span></span><br><span>Alternately you can=  click on https://www.google.com/url?q=3Dhttp://my.umbc.edu/help&amp;source= =3Dgmail-imap&amp;ust=3D1760468623000000&amp;usg=3DAOvVaw1E2ghKVipK0SsvWqfS= uNUq</span><br><span></span><br><span> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nb= sp;&nbsp;&nbsp;&nbsp;&nbsp;Thank you</span><br><span></span><br></div></blo= ckquote></div></body></html>= "
3287950,72210894,Correspond,DoIT-Research-Computing,2025-10-10 17:15:32.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_kak,resolved,Greg Ballantine,gballan1,Kaur Kullman,kak,kak@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Kaur,<br /> <br /> We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note:</p>  <ul> 	<li>The path has changed, and is now available under <strong>/umbc/rs/pi_kak</strong>.</li> 	<li>The alias used to reach the volume is now <strong>pi_kak_common</strong> and <strong>pi_kak_user</strong>.</li> 	<li>Your new volume has a quota of <strong>10TB</strong>.</li> </ul>  <p>When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good?<br /> <br /> Thank you,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Tue Oct 07 15:42:37 2025, EC42736 wrote:</p>  <blockquote>kak</blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3287957,72147360,Create,DoIT-Research-Computing,2025-10-07 19:08:41.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_kjoshi1,stalled,Greg Ballantine,gballan1,Karuna Pande Joshi,kjoshi1,kjoshi1@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Karuna,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 488.3 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/kjoshi1&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_kjoshi1&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3287957,72398056,Correspond,DoIT-Research-Computing,2025-10-21 16:02:08.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_kjoshi1,stalled,Greg Ballantine,gballan1,Karuna Pande Joshi,kjoshi1,kjoshi1@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Karuna,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 10th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3287957,72398525,Correspond,DoIT-Research-Computing,2025-10-21 16:04:53.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_kjoshi1,stalled,Greg Ballantine,gballan1,Karuna Pande Joshi,kjoshi1,kjoshi1@umbc.edu,Karuna Pande Joshi,kjoshi1@umbc.edu,"Yes, please proceed. ____________________________________________________ Dr. Karuna Pande Joshi UMBC Director, Center for Accelerated Real Time Analytics (CARTA) Professor, Information Systems Department ITE 424, UMBC 410-455-8775 Karuna.Joshi@umbc.edu Personal Webex room: https://umbc.webex.com/meet/kjoshi1      On Tue, Oct 21, 2025 at 12:02=E2=80=AFPM Elliot Gobbert via RT <UMBCHelp@rt= .umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287957 > > > Last Update From Ticket: > > Dear Karuna, > > As per our previous communications, since you did not schedule a date by > October 15, we will be going with Option 2, randomly assigning a date > between > October 16 and November 15 for your migration. > > We have assigned the date of November 10th for your migration. Let us know > if > there is a better day for you, and within reason, we can reschedule that > date. > > You will be notified when the migration begins and completes. > > Thank you, > > Elliot > > "
3287957,72398525,Correspond,DoIT-Research-Computing,2025-10-21 16:04:53.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_kjoshi1,stalled,Greg Ballantine,gballan1,Karuna Pande Joshi,kjoshi1,kjoshi1@umbc.edu,Karuna Pande Joshi,kjoshi1@umbc.edu,"<div dir=3D""ltr""><div>Yes, please proceed.=C2=A0</div><div><div dir=3D""ltr""=  class=3D""gmail_signature"" data-smartmail=3D""gmail_signature""><div dir=3D""l= tr""><div dir=3D""ltr""><div dir=3D""ltr""><div dir=3D""ltr""><div dir=3D""ltr""><di= v dir=3D""ltr""><div dir=3D""ltr""><div dir=3D""ltr""><div dir=3D""ltr""><font face= =3D""georgia, serif""><font color=3D""#666666""><div><span style=3D""font-size:1= 2.8px"">____________________________________________________</span><br></div= ></font></font><div><font color=3D""#666666""><font face=3D""georgia, serif"">D= r. Karuna Pande Joshi</font></font><div><font face=3D""georgia, serif"" color= =3D""#666666"">UMBC Director, Center for Accelerated Real Time Analytics (CAR= TA)</font></div><div><font face=3D""georgia, serif"" color=3D""#666666"">Profes= sor,=C2=A0</font><span style=3D""color:rgb(102,102,102);font-family:georgia,= serif"">Information Systems Department</span></div><div><div><font color=3D""= #666666""><span style=3D""font-family:georgia,serif"">ITE 424, UMBC</span><br>= </font></div></div><div><font face=3D""georgia, serif"" color=3D""#666666"">410= -455-8775</font></div><div><font face=3D""georgia, serif"" color=3D""#666666"">= <a href=3D""mailto:Karuna.Joshi@umbc.edu"" target=3D""_blank"">Karuna.Joshi@umb= c.edu</a></font></div><div><font face=3D""georgia, serif"" style=3D""backgroun= d-color:rgb(255,255,255)"" color=3D""#666666"">Personal Webex room:=C2=A0<a hr= ef=3D""https://umbc.webex.com/meet/kjoshi1"" target=3D""_blank"">https://umbc.w= ebex.com/meet/kjoshi1</a></font></div><div><img src=3D""https://docs.google.= com/uc?export=3Ddownload&amp;id=3D1Kj_p3qnlUFIgys3N3Td6EDDz0PSl51xZ&amp;rev= id=3D0B64vWNvw1HpvanNIcDM4MFRncUZSbGNFcll3clRDNnVsRDcwPQ"" width=3D""200"" hei= ght=3D""70""><br></div><div><font face=3D""georgia, serif""><br><br></font></di= v></div></div></div></div></div></div></div></div></div></div></div></div><= br></div><br><div class=3D""gmail_quote gmail_quote_container""><div dir=3D""l= tr"" class=3D""gmail_attr"">On Tue, Oct 21, 2025 at 12:02=E2=80=AFPM Elliot Go= bbert via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.e= du</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""margi= n:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex= "">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D3= 287957"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Dis= play.html?id=3D3287957</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Dear Karuna,<br> <br> As per our previous communications, since you did not schedule a date by<br> October 15, we will be going with Option 2, randomly assigning a date betwe= en<br> October 16 and November 15 for your migration.<br> <br> We have assigned the date of November 10th for your migration. Let us know = if<br> there is a better day for you, and within reason, we can reschedule that da= te.<br> <br> You will be notified when the migration begins and completes.<br> <br> Thank you,<br> <br> Elliot<br> <br> </blockquote></div> "
3287959,72147432,Create,DoIT-Research-Computing,2025-10-07 19:10:47.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_kofi,stalled,Greg Ballantine,gballan1,Kofi Adragni,kofi,kofi@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Kofi,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB&nbsp;of a 488.3 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/kofi&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_kofi&rdquo;.<br /> <br /> Thank you,</p>  <p>Elliot</p> "
3287959,72398124,Correspond,DoIT-Research-Computing,2025-10-21 16:03:03.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_kofi,stalled,Greg Ballantine,gballan1,Kofi Adragni,kofi,kofi@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Kofi,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 10th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3287961,72147596,Create,DoIT-Research-Computing,2025-10-07 19:16:01.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_kogan,stalled,Greg Ballantine,gballan1,Jacob Kogan," ",IT55893@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Jacob,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 488.3 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/kogan&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_kogan&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3287961,72398297,Correspond,DoIT-Research-Computing,2025-10-21 16:04:01.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_kogan,stalled,Greg Ballantine,gballan1,Jacob Kogan," ",IT55893@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Jacob,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 10th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3287965,72147676,Create,DoIT-Research-Computing,2025-10-07 19:18:06.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ksolaima,stalled,Greg Ballantine,gballan1,Khaled Solaiman,ksolaima,ksolaima@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Khaled,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 244.1 GB&nbsp;quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/ksolaima&gt;&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_ksolaima&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3287965,72398543,Correspond,DoIT-Research-Computing,2025-10-21 16:04:58.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ksolaima,stalled,Greg Ballantine,gballan1,Khaled Solaiman,ksolaima,ksolaima@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Khaled,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 11th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3287965,72460914,Correspond,DoIT-Research-Computing,2025-10-23 17:34:06.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ksolaima,stalled,Greg Ballantine,gballan1,Khaled Solaiman,ksolaima,ksolaima@umbc.edu,Khaled Solaiman,ksolaima@umbc.edu,"Hello Elliot  Thank you for informing me. I do have a query though. Most of our storage is in /umbc/rs/ksolaima. Are those going to be moved somewhere else? My understanding was you are moving from a destination which has 0 KB of data in there to /umbc/rs/ksolaima. Am I correct?  On Tue, Oct 21, 2025 at 12:05=E2=80=AFPM Elliot Gobbert via RT <UMBCHelp@rt= .umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287965 > > > Last Update From Ticket: > > Dear Khaled, > > As per our previous communications, since you did not schedule a date by > October 15, we will be going with Option 2, randomly assigning a date > between > October 16 and November 15 for your migration. > > We have assigned the date of November 11th for your migration. Let us know > if > there is a better day for you, and within reason, we can reschedule that > date. > > You will be notified when the migration begins and completes. > > Thank you, > > Elliot > >  --=20 Best Regards, KMA Solaiman, PhD Assistant Teaching Professor Department of Computer Science and Electrical Engineering University of Maryland, Baltimore County (765) 775-8230 "
3287965,72460914,Correspond,DoIT-Research-Computing,2025-10-23 17:34:06.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ksolaima,stalled,Greg Ballantine,gballan1,Khaled Solaiman,ksolaima,ksolaima@umbc.edu,Khaled Solaiman,ksolaima@umbc.edu,"<div dir=3D""ltr"">Hello Elliot<div><br></div><div>Thank you for informing me= . I do have a query though.=C2=A0<br>Most of our storage is in /umbc/rs/kso= laima. Are those going to be moved=C2=A0somewhere else? <br>My understandin= g was you are moving from a destination which has 0 KB of data in there to = /umbc/rs/ksolaima. Am I correct?</div></div><br><div class=3D""gmail_quote g= mail_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Tue, Oct 21,=  2025 at 12:05=E2=80=AFPM Elliot Gobbert via RT &lt;<a href=3D""mailto:UMBCH= elp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote c= lass=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px soli= d rgb(204,204,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.= umbc.edu/Ticket/Display.html?id=3D3287965"" rel=3D""noreferrer"" target=3D""_bl= ank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3287965</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Dear Khaled,<br> <br> As per our previous communications, since you did not schedule a date by<br> October 15, we will be going with Option 2, randomly assigning a date betwe= en<br> October 16 and November 15 for your migration.<br> <br> We have assigned the date of November 11th for your migration. Let us know = if<br> there is a better day for you, and within reason, we can reschedule that da= te.<br> <br> You will be notified when the migration begins and completes.<br> <br> Thank you,<br> <br> Elliot<br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><font color=3D""#000000"">Best Regards,<br>KMA Sol= aiman, PhD</font><div><div><font color=3D""#000000"">Assistant Teaching Profe= ssor</font></div><div><font color=3D""#000000"">Department of Computer Scienc= e and Electrical Engineering<br>University of Maryland, Baltimore County<br= >(765) 775-8230</font><br></div></div></div></div> "
3287965,72527037,Comment,DoIT-Research-Computing,2025-10-28 14:35:35.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ksolaima,stalled,Greg Ballantine,gballan1,Khaled Solaiman,ksolaima,ksolaima@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hello,</p>  <p>Yes, the data you currently have will be moved to&nbsp;&ldquo;/umbc/rs/pi_ksolaima&rdquo;.</p>  <p>Looking at your storage personally, apologies, it seems our estimate was off, it seems your pi actually has around 69 GB used in &quot;/umbc/rs/ksolaima&quot;, and will be moved to&nbsp;&ldquo;/umbc/rs/pi_ksolaima&rdquo;. No worries, the transfer process will still be very quick.</p>  <p>Best,</p>  <p>Elliot</p>  <p>&nbsp;</p> "
3287965,72527039,CommentEmailRecord,DoIT-Research-Computing,2025-10-28 14:35:37.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ksolaima,stalled,Greg Ballantine,gballan1,Khaled Solaiman,ksolaima,ksolaima@umbc.edu,The RT System itself,NULL,"Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287965 >  Comment just added.=20=20  Hello,  Yes, the data you currently have will be moved to =E2=80=9C/umbc/rs/pi_ksol= aima=E2=80=9D.  Looking at your storage personally, apologies, it seems our estimate was of= f, it seems your pi actually has around 69 GB used in ""/umbc/rs/ksolaima"", and will be moved to =E2=80=9C/umbc/rs/pi_ksolaima=E2=80=9D. No worries, the tr= ansfer process will still be very quick.  Best,  Elliot  "
3287967,72147796,Create,DoIT-Research-Computing,2025-10-07 19:20:31.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos,open,Greg Ballantine,gballan1,Zhibo Zhang,zzbatmos,zzbatmos@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Zhibo,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 458.94 TB of a 505.0000 TB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/zzbatmos&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_zzbatmos&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3287967,72152380,Correspond,DoIT-Research-Computing,2025-10-08 01:29:13.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos,open,Greg Ballantine,gballan1,Zhibo Zhang,zzbatmos,zzbatmos@umbc.edu,Zhibo Zhang,zzbatmos@umbc.edu,"Hi Elliot      We will choose option 1 Schedule a group-wide downtime date during standard business hours,. I need to determine the exact date with my team members though.  Can I get back to you by Friday? Thanks -Zhibo  On Tue, Oct 7, 2025 at 3:20=E2=80=AFPM via RT <UMBCHelp@rt.umbc.edu> wrote:  > Greetings, > > This message has been automatically generated in response to the > creation of a ticket regarding: > > ------------------------------------------------------------------------- > Subject: ""Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos"" > > Message: > > Dear Zhibo, > > As per the communication via myUMBC earlier this summer (June HPCF > Newsletter > ), DoIT is in the process of migrating data off of an older storage server > to > our new RRStor Ceph storage cluster. Your group is using 458.94 TB of a > 505.0000 TB quota on the old storage server. > > To perform these migrations, we need to take individual storage volumes > offline > while we migrate them to the Ceph cluster. Thus we are reaching out to > schedule > a date where we can migrate your volume located at =E2=80=9C/umbc/rs/zzba= tmos=E2=80=9D. > During > the migration, we will take your volume offline and will terminate any jo= bs > running on the chip compute cluster that are accessing this volume. > > Below we=E2=80=99ve listed two options for handling this data migration -=  please > let us > know which of these you=E2=80=99d prefer. > > Option 1: Schedule a group-wide downtime date during standard business > hours, > which can be done by responding to this email with your preferred date(s) > to > perform the migration. During this time, DoIT staff will work to migrate > your > volume to the Ceph storage cluster. DoIT staff will send an email alert on > this > email thread when the migration has begun and when it has completed. For > most > storage volumes, this process should take less than a business day. > Option 2: If you don=E2=80=99t respond to this email by October 15th, DoI= T staff > will > assign a day over the following month (October 16th through November 15th) > to > migrate your volume. The day chosen will be random and will occur during > business hours. You will be notified of the date chosen to perform the > migration, and will be notified when the migration begins and completes. > > Note: After this process has completed, the new storage volume will have a > new > name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its data und= er > =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, > or in your group=E2=80=99s case you will find your volume under > =E2=80=9C/umbc/rs/pi_zzbatmos=E2=80=9D. > > Thank you, > Elliot > > > ------------------------------------------------------------------------- > > There is no need to reply to this message right now. > > Your ticket has been assigned an ID of [Research Computing #3287967] or > you can go there directly by clicking the link below. > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > > > You can login to view your open tickets at any time by visiting > http://my.umbc.edu and clicking on ""Help"" and ""Request Help"". > > Alternately you can click on http://my.umbc.edu/help > >                         Thank you > > "
3287967,72152380,Correspond,DoIT-Research-Computing,2025-10-08 01:29:13.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos,open,Greg Ballantine,gballan1,Zhibo Zhang,zzbatmos,zzbatmos@umbc.edu,Zhibo Zhang,zzbatmos@umbc.edu,"<div dir=3D""ltr""><div class=3D""gmail_default"" style=3D""font-family:arial,he= lvetica,sans-serif;font-size:small"">Hi Elliot=C2=A0</div><div class=3D""gmai= l_default"" style=3D""font-family:arial,helvetica,sans-serif;font-size:small""= >=C2=A0 =C2=A0 =C2=A0We will choose option 1=C2=A0<span style=3D""font-famil= y:Arial,Helvetica,sans-serif"">Schedule a group-wide downtime date during st= andard business hours,. I need to determine the exact date with my team mem= bers though.=C2=A0 Can I get back to you by Friday? Thanks</span></div><div=  class=3D""gmail_default"" style=3D""font-family:arial,helvetica,sans-serif;fo= nt-size:small""><span style=3D""font-family:Arial,Helvetica,sans-serif"">-Zhib= o</span></div></div><br><div class=3D""gmail_quote gmail_quote_container""><d= iv dir=3D""ltr"" class=3D""gmail_attr"">On Tue, Oct 7, 2025 at 3:20=E2=80=AFPM = via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>= &gt; wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""margin:0px = 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"">Gree= tings,<br> <br> This message has been automatically generated in response to the<br> creation of a ticket regarding:<br> <br> -------------------------------------------------------------------------<b= r> Subject: &quot;Migrating Research Storage Volume to Ceph Cluster - pi_zzbat= mos&quot;<br> <br> Message: <br> <br> Dear Zhibo,<br> <br> As per the communication via myUMBC earlier this summer (June HPCF Newslett= er<br> ), DoIT is in the process of migrating data off of an older storage server = to<br> our new RRStor Ceph storage cluster. Your group is using 458.94 TB of a<br> 505.0000 TB quota on the old storage server.<br> <br> To perform these migrations, we need to take individual storage volumes off= line<br> while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule<br> a date where we can migrate your volume located at =E2=80=9C/umbc/rs/zzbatm= os=E2=80=9D. During<br> the migration, we will take your volume offline and will terminate any jobs= <br> running on the chip compute cluster that are accessing this volume.<br> <br> Below we=E2=80=99ve listed two options for handling this data migration - p= lease let us<br> know which of these you=E2=80=99d prefer.<br> <br> Option 1: Schedule a group-wide downtime date during standard business hour= s,<br> which can be done by responding to this email with your preferred date(s) t= o<br> perform the migration. During this time, DoIT staff will work to migrate yo= ur<br> volume to the Ceph storage cluster. DoIT staff will send an email alert on = this<br> email thread when the migration has begun and when it has completed. For mo= st<br> storage volumes, this process should take less than a business day.<br> Option 2: If you don=E2=80=99t respond to this email by October 15th, DoIT = staff will<br> assign a day over the following month (October 16th through November 15th) = to<br> migrate your volume. The day chosen will be random and will occur during<br> business hours. You will be notified of the date chosen to perform the<br> migration, and will be notified when the migration begins and completes.<br> <br> Note: After this process has completed, the new storage volume will have a = new<br> name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its data under=  =E2=80=9C/umbc/rs/pi_doit=E2=80=9D,<br> or in your group=E2=80=99s case you will find your volume under =E2=80=9C/u= mbc/rs/pi_zzbatmos=E2=80=9D.<br> <br> Thank you,<br> Elliot<br> <br> <br> -------------------------------------------------------------------------<b= r> <br> There is no need to reply to this message right now.=C2=A0 <br> <br> Your ticket has been assigned an ID of [Research Computing #3287967] or you=  can go there directly by clicking the link below.<br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D328= 7967"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3287967</a> &gt;<br> <br> You can login to view your open tickets at any time by visiting <a href=3D""= http://my.umbc.edu"" rel=3D""noreferrer"" target=3D""_blank"">http://my.umbc.edu= </a> and clicking on &quot;Help&quot; and &quot;Request Help&quot;. <br> <br> Alternately you can click on <a href=3D""http://my.umbc.edu/help"" rel=3D""nor= eferrer"" target=3D""_blank"">http://my.umbc.edu/help</a><br> <br> =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 Thank you<br> <br> </blockquote></div> "
3287967,72165037,Correspond,DoIT-Research-Computing,2025-10-08 16:42:05.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos,open,Greg Ballantine,gballan1,Zhibo Zhang,zzbatmos,zzbatmos@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Zhibo,<br /> <br /> Yes, a response by Friday is fine.<br /> <br /> Thank you,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Tue Oct 07 21:29:13 2025, OX20655 wrote:</p>  <blockquote> <div> <div>Hi Elliot&nbsp;</div>  <div>&nbsp; &nbsp; &nbsp;We will choose option 1&nbsp;Schedule a group-wide=  downtime date during standard business hours,. I need to determine the exa= ct date with my team members though.&nbsp; Can I get back to you by Friday?=  Thanks</div>  <div>-Zhibo</div> </div> &nbsp;  <div> <div>On Tue, Oct 7, 2025 at 3:20=E2=80=AFPM via RT &lt;UMBCHelp@rt.umbc.edu= &gt; wrote:</div>  <blockquote>Greetings,<br /> <br /> This message has been automatically generated in response to the<br /> creation of a ticket regarding:<br /> <br /> -------------------------------------------------------------------------<b= r /> Subject: &quot;Migrating Research Storage Volume to Ceph Cluster - pi_zzbat= mos&quot;<br /> <br /> Message:<br /> <br /> Dear Zhibo,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newslett= er<br /> ), DoIT is in the process of migrating data off of an older storage server = to<br /> our new RRStor Ceph storage cluster. Your group is using 458.94 TB of a<br = /> 505.0000 TB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes off= line<br /> while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule<br /> a date where we can migrate your volume located at &ldquo;/umbc/rs/zzbatmos= &rdquo;. During<br /> the migration, we will take your volume offline and will terminate any jobs= <br /> running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - ple= ase let us<br /> know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hour= s,<br /> which can be done by responding to this email with your preferred date(s) t= o<br /> perform the migration. During this time, DoIT staff will work to migrate yo= ur<br /> volume to the Ceph storage cluster. DoIT staff will send an email alert on = this<br /> email thread when the migration has begun and when it has completed. For mo= st<br /> storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT st= aff will<br /> assign a day over the following month (October 16th through November 15th) = to<br /> migrate your volume. The day chosen will be random and will occur during<br=  /> business hours. You will be notified of the date chosen to perform the<br /> migration, and will be notified when the migration begins and completes.<br=  /> <br /> Note: After this process has completed, the new storage volume will have a = new<br /> name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ld= quo;/umbc/rs/pi_doit&rdquo;,<br /> or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/= rs/pi_zzbatmos&rdquo;.<br /> <br /> Thank you,<br /> Elliot<br /> <br /> <br /> -------------------------------------------------------------------------<b= r /> <br /> There is no need to reply to this message right now.&nbsp;<br /> <br /> Your ticket has been assigned an ID of [Research Computing #3287967] or you=  can go there directly by clicking the link below.<br /> <br /> Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 &gt;<b= r /> <br /> You can login to view your open tickets at any time by visiting http://my.u= mbc.edu and clicking on &quot;Help&quot; and &quot;Request Help&quot;.<br /> <br /> Alternately you can click on http://my.umbc.edu/help<br /> <br /> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp= ; &nbsp; Thank you<br /> &nbsp;</blockquote> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3287967,72425300,Correspond,DoIT-Research-Computing,2025-10-22 14:14:02.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos,open,Greg Ballantine,gballan1,Zhibo Zhang,zzbatmos,zzbatmos@umbc.edu,Zhibo Zhang,zzbatmos@umbc.edu,"Hi Sorry for the late reply. Our group has decided to make the disk migration *on Oct. 31 Friday*. Would that work for you? Thanks.  On Wed, Oct 8, 2025 at 12:42=E2=80=AFPM Greg Ballantine via RT <UMBCHelp@rt= .umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > > > Last Update From Ticket: > > Hello Zhibo, > > Yes, a response by Friday is fine. > > Thank you, > Greg > > On Tue Oct 07 21:29:13 2025, OX20655 wrote: > > > Hi Elliot We will choose option 1 Schedule a group-wide downtime date > > during standard business hours,. I need to determine the exact date with > my > > team members though. Can I get back to you by Friday? Thanks-Zhibo On > Tue, > > Oct 7, 2025 at 3:20 PM via RT <UMBCHelp@rt.umbc.edu> wrote: > > >> Greetings, > > >> This message has been automatically generated in response to the > >> creation of a ticket regarding: > > >> > ------------------------------------------------------------------------- > >> Subject: ""Migrating Research Storage Volume to Ceph Cluster - > >> pi_zzbatmos"" > > >> Message: > > >> Dear Zhibo, > > >> As per the communication via myUMBC earlier this summer (June HPCF > >> Newsletter > >> ), DoIT is in the process of migrating data off of an older storage > >> server to > >> our new RRStor Ceph storage cluster. Your group is using 458.94 TB of a > >> 505.0000 TB quota on the old storage server. > > >> To perform these migrations, we need to take individual storage volumes > >> offline > >> while we migrate them to the Ceph cluster. Thus we are reaching out to > >> schedule > >> a date where we can migrate your volume located at =E2=80=9C/umbc/rs/z= zbatmos=E2=80=9D. > >> During > >> the migration, we will take your volume offline and will terminate any > >> jobs > >> running on the chip compute cluster that are accessing this volume. > > >> Below we=E2=80=99ve listed two options for handling this data migratio= n - > >> please let us > >> know which of these you=E2=80=99d prefer. > > >> Option 1: Schedule a group-wide downtime date during standard business > >> hours, > >> which can be done by responding to this email with your preferred > >> date(s) to > >> perform the migration. During this time, DoIT staff will work to > >> migrate your > >> volume to the Ceph storage cluster. DoIT staff will send an email alert > >> on this > >> email thread when the migration has begun and when it has completed. > >> For most > >> storage volumes, this process should take less than a business day. > >> Option 2: If you don=E2=80=99t respond to this email by October 15th, = DoIT > >> staff will > >> assign a day over the following month (October 16th through November > >> 15th) to > >> migrate your volume. The day chosen will be random and will occur > >> during > >> business hours. You will be notified of the date chosen to perform the > >> migration, and will be notified when the migration begins and > >> completes. > > >> Note: After this process has completed, the new storage volume will > >> have a new > >> name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its data = under > >> =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, > >> or in your group=E2=80=99s case you will find your volume under > >> =E2=80=9C/umbc/rs/pi_zzbatmos=E2=80=9D. > > >> Thank you, > >> Elliot > > > >> > ------------------------------------------------------------------------- > > >> There is no need to reply to this message right now. > > >> Your ticket has been assigned an ID of [Research Computing #3287967] or > >> you can go there directly by clicking the link below. > > >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > > > >> You can login to view your open tickets at any time by visiting > >> http://my.umbc.edu and clicking on ""Help"" and ""Request Help"". > > >> Alternately you can click on http://my.umbc.edu/help > > >> Thank you > > -- > > Gregory BallantineSystem Administrator for Research and Enterprise > ComputingUMBC > - DoIT > > "
3287967,72425300,Correspond,DoIT-Research-Computing,2025-10-22 14:14:02.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos,open,Greg Ballantine,gballan1,Zhibo Zhang,zzbatmos,zzbatmos@umbc.edu,Zhibo Zhang,zzbatmos@umbc.edu,"<div dir=3D""ltr""><div class=3D""gmail_default"" style=3D""font-family:arial,he= lvetica,sans-serif;font-size:small"">Hi Sorry for the late reply. Our group = has decided to make the disk migration <b>on Oct. 31 Friday</b>. Would that=  work for you? Thanks.</div></div><br><div class=3D""gmail_quote gmail_quote= _container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Wed, Oct 8, 2025 at 12= :42=E2=80=AFPM Greg Ballantine via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umb= c.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gm= ail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,= 204,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/T= icket/Display.html?id=3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">https= ://rt.umbc.edu/Ticket/Display.html?id=3D3287967</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hello Zhibo,<br> <br> Yes, a response by Friday is fine.<br> <br> Thank you,<br> Greg<br> <br> On Tue Oct 07 21:29:13 2025, OX20655 wrote:<br> <br> &gt; Hi Elliot We will choose option 1 Schedule a group-wide downtime date<= br> &gt; during standard business hours,. I need to determine the exact date wi= th my<br> &gt; team members though. Can I get back to you by Friday? Thanks-Zhibo On = Tue,<br> &gt; Oct 7, 2025 at 3:20 PM via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.e= du"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; Greetings,<br> <br> &gt;&gt; This message has been automatically generated in response to the<b= r> &gt;&gt; creation of a ticket regarding:<br> <br> &gt;&gt; ------------------------------------------------------------------= -------<br> &gt;&gt; Subject: &quot;Migrating Research Storage Volume to Ceph Cluster -= <br> &gt;&gt; pi_zzbatmos&quot;<br> <br> &gt;&gt; Message:<br> <br> &gt;&gt; Dear Zhibo,<br> <br> &gt;&gt; As per the communication via myUMBC earlier this summer (June HPCF= <br> &gt;&gt; Newsletter<br> &gt;&gt; ), DoIT is in the process of migrating data off of an older storag= e<br> &gt;&gt; server to<br> &gt;&gt; our new RRStor Ceph storage cluster. Your group is using 458.94 TB=  of a<br> &gt;&gt; 505.0000 TB quota on the old storage server.<br> <br> &gt;&gt; To perform these migrations, we need to take individual storage vo= lumes<br> &gt;&gt; offline<br> &gt;&gt; while we migrate them to the Ceph cluster. Thus we are reaching ou= t to<br> &gt;&gt; schedule<br> &gt;&gt; a date where we can migrate your volume located at =E2=80=9C/umbc/= rs/zzbatmos=E2=80=9D.<br> &gt;&gt; During<br> &gt;&gt; the migration, we will take your volume offline and will terminate=  any<br> &gt;&gt; jobs<br> &gt;&gt; running on the chip compute cluster that are accessing this volume= .<br> <br> &gt;&gt; Below we=E2=80=99ve listed two options for handling this data migr= ation -<br> &gt;&gt; please let us<br> &gt;&gt; know which of these you=E2=80=99d prefer.<br> <br> &gt;&gt; Option 1: Schedule a group-wide downtime date during standard busi= ness<br> &gt;&gt; hours,<br> &gt;&gt; which can be done by responding to this email with your preferred<= br> &gt;&gt; date(s) to<br> &gt;&gt; perform the migration. During this time, DoIT staff will work to<b= r> &gt;&gt; migrate your<br> &gt;&gt; volume to the Ceph storage cluster. DoIT staff will send an email = alert<br> &gt;&gt; on this<br> &gt;&gt; email thread when the migration has begun and when it has complete= d.<br> &gt;&gt; For most<br> &gt;&gt; storage volumes, this process should take less than a business day= .<br> &gt;&gt; Option 2: If you don=E2=80=99t respond to this email by October 15= th, DoIT<br> &gt;&gt; staff will<br> &gt;&gt; assign a day over the following month (October 16th through Novemb= er<br> &gt;&gt; 15th) to<br> &gt;&gt; migrate your volume. The day chosen will be random and will occur<= br> &gt;&gt; during<br> &gt;&gt; business hours. You will be notified of the date chosen to perform=  the<br> &gt;&gt; migration, and will be notified when the migration begins and<br> &gt;&gt; completes.<br> <br> &gt;&gt; Note: After this process has completed, the new storage volume wil= l<br> &gt;&gt; have a new<br> &gt;&gt; name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its d= ata under<br> &gt;&gt; =E2=80=9C/umbc/rs/pi_doit=E2=80=9D,<br> &gt;&gt; or in your group=E2=80=99s case you will find your volume under<br> &gt;&gt; =E2=80=9C/umbc/rs/pi_zzbatmos=E2=80=9D.<br> <br> &gt;&gt; Thank you,<br> &gt;&gt; Elliot<br> <br> <br> &gt;&gt; ------------------------------------------------------------------= -------<br> <br> &gt;&gt; There is no need to reply to this message right now.<br> <br> &gt;&gt; Your ticket has been assigned an ID of [Research Computing #328796= 7] or<br> &gt;&gt; you can go there directly by clicking the link below.<br> <br> &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html= ?id=3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tic= ket/Display.html?id=3D3287967</a> &gt;<br> <br> &gt;&gt; You can login to view your open tickets at any time by visiting<br> &gt;&gt; <a href=3D""http://my.umbc.edu"" rel=3D""noreferrer"" target=3D""_blank= "">http://my.umbc.edu</a> and clicking on &quot;Help&quot; and &quot;Request=  Help&quot;.<br> <br> &gt;&gt; Alternately you can click on <a href=3D""http://my.umbc.edu/help"" r= el=3D""noreferrer"" target=3D""_blank"">http://my.umbc.edu/help</a><br> <br> &gt;&gt; Thank you<br> <br> --<br> <br> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br> - DoIT<br> <br> </blockquote></div> "
3287967,72504327,Correspond,DoIT-Research-Computing,2025-10-27 15:21:28.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos,open,Greg Ballantine,gballan1,Zhibo Zhang,zzbatmos,zzbatmos@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Good morning Zhibo,<br /> <br /> Yes, Friday the 31st will work for starting your migration. However, since = your storage volume is currently very large (~482TB as of this morning) the= re is a good chance that your migration will take longer than the business = day that we typically plan for. We&#39;ve started copying your data to the = new storage server to minimize downtime, but I would still expect the final=  migration to run through the weekend and likely into the following week.<b= r /> <br /> Will that time frame still work for your group?<br /> <br /> Thank you,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Wed Oct 22 10:14:02 2025, OX20655 wrote:</p>  <blockquote> <div> <div>Hi Sorry for the late reply. Our group has decided to make the disk mi= gration <strong>on Oct. 31 Friday</strong>. Would that work for you? Thanks= .</div> </div> &nbsp;  <div> <div>On Wed, Oct 8, 2025 at 12:42=E2=80=AFPM Greg Ballantine via RT &lt;UMB= CHelp@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 87967 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hello Zhibo,<br /> <br /> Yes, a response by Friday is fine.<br /> <br /> Thank you,<br /> Greg<br /> <br /> On Tue Oct 07 21:29:13 2025, OX20655 wrote:<br /> <br /> &gt; Hi Elliot We will choose option 1 Schedule a group-wide downtime date<= br /> &gt; during standard business hours,. I need to determine the exact date wi= th my<br /> &gt; team members though. Can I get back to you by Friday? Thanks-Zhibo On = Tue,<br /> &gt; Oct 7, 2025 at 3:20 PM via RT &lt;UMBCHelp@rt.umbc.edu&gt; wrote:<br /> <br /> &gt;&gt; Greetings,<br /> <br /> &gt;&gt; This message has been automatically generated in response to the<b= r /> &gt;&gt; creation of a ticket regarding:<br /> <br /> &gt;&gt; ------------------------------------------------------------------= -------<br /> &gt;&gt; Subject: &quot;Migrating Research Storage Volume to Ceph Cluster -= <br /> &gt;&gt; pi_zzbatmos&quot;<br /> <br /> &gt;&gt; Message:<br /> <br /> &gt;&gt; Dear Zhibo,<br /> <br /> &gt;&gt; As per the communication via myUMBC earlier this summer (June HPCF= <br /> &gt;&gt; Newsletter<br /> &gt;&gt; ), DoIT is in the process of migrating data off of an older storag= e<br /> &gt;&gt; server to<br /> &gt;&gt; our new RRStor Ceph storage cluster. Your group is using 458.94 TB=  of a<br /> &gt;&gt; 505.0000 TB quota on the old storage server.<br /> <br /> &gt;&gt; To perform these migrations, we need to take individual storage vo= lumes<br /> &gt;&gt; offline<br /> &gt;&gt; while we migrate them to the Ceph cluster. Thus we are reaching ou= t to<br /> &gt;&gt; schedule<br /> &gt;&gt; a date where we can migrate your volume located at &ldquo;/umbc/rs= /zzbatmos&rdquo;.<br /> &gt;&gt; During<br /> &gt;&gt; the migration, we will take your volume offline and will terminate=  any<br /> &gt;&gt; jobs<br /> &gt;&gt; running on the chip compute cluster that are accessing this volume= .<br /> <br /> &gt;&gt; Below we&rsquo;ve listed two options for handling this data migrat= ion -<br /> &gt;&gt; please let us<br /> &gt;&gt; know which of these you&rsquo;d prefer.<br /> <br /> &gt;&gt; Option 1: Schedule a group-wide downtime date during standard busi= ness<br /> &gt;&gt; hours,<br /> &gt;&gt; which can be done by responding to this email with your preferred<= br /> &gt;&gt; date(s) to<br /> &gt;&gt; perform the migration. During this time, DoIT staff will work to<b= r /> &gt;&gt; migrate your<br /> &gt;&gt; volume to the Ceph storage cluster. DoIT staff will send an email = alert<br /> &gt;&gt; on this<br /> &gt;&gt; email thread when the migration has begun and when it has complete= d.<br /> &gt;&gt; For most<br /> &gt;&gt; storage volumes, this process should take less than a business day= .<br /> &gt;&gt; Option 2: If you don&rsquo;t respond to this email by October 15th= , DoIT<br /> &gt;&gt; staff will<br /> &gt;&gt; assign a day over the following month (October 16th through Novemb= er<br /> &gt;&gt; 15th) to<br /> &gt;&gt; migrate your volume. The day chosen will be random and will occur<= br /> &gt;&gt; during<br /> &gt;&gt; business hours. You will be notified of the date chosen to perform=  the<br /> &gt;&gt; migration, and will be notified when the migration begins and<br /> &gt;&gt; completes.<br /> <br /> &gt;&gt; Note: After this process has completed, the new storage volume wil= l<br /> &gt;&gt; have a new<br /> &gt;&gt; name. For example, group &ldquo;pi_doit&rdquo; will find its data = under<br /> &gt;&gt; &ldquo;/umbc/rs/pi_doit&rdquo;,<br /> &gt;&gt; or in your group&rsquo;s case you will find your volume under<br /> &gt;&gt; &ldquo;/umbc/rs/pi_zzbatmos&rdquo;.<br /> <br /> &gt;&gt; Thank you,<br /> &gt;&gt; Elliot<br /> <br /> <br /> &gt;&gt; ------------------------------------------------------------------= -------<br /> <br /> &gt;&gt; There is no need to reply to this message right now.<br /> <br /> &gt;&gt; Your ticket has been assigned an ID of [Research Computing #328796= 7] or<br /> &gt;&gt; you can go there directly by clicking the link below.<br /> <br /> &gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32879= 67 &gt;<br /> <br /> &gt;&gt; You can login to view your open tickets at any time by visiting<br=  /> &gt;&gt; http://my.umbc.edu and clicking on &quot;Help&quot; and &quot;Requ= est Help&quot;.<br /> <br /> &gt;&gt; Alternately you can click on http://my.umbc.edu/help<br /> <br /> &gt;&gt; Thank you<br /> <br /> --<br /> <br /> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br /> - DoIT<br /> &nbsp;</blockquote> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3287967,72504401,Correspond,DoIT-Research-Computing,2025-10-27 15:23:17.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos,open,Greg Ballantine,gballan1,Zhibo Zhang,zzbatmos,zzbatmos@umbc.edu,Zhibo Zhang,zzbatmos@umbc.edu,"Yes, that will be fine. thanks, Greg  On Mon, Oct 27, 2025 at 11:21=E2=80=AFAM Greg Ballantine via RT < UMBCHelp@rt.umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > > > Last Update From Ticket: > > Good morning Zhibo, > > Yes, Friday the 31st will work for starting your migration. However, since > your > storage volume is currently very large (~482TB as of this morning) there > is a > good chance that your migration will take longer than the business day > that we > typically plan for. We've started copying your data to the new storage > server > to minimize downtime, but I would still expect the final migration to run > through the weekend and likely into the following week. > > Will that time frame still work for your group? > > Thank you, > Greg > > On Wed Oct 22 10:14:02 2025, OX20655 wrote: > > > Hi Sorry for the late reply. Our group has decided to make the disk > > migration on Oct. 31 Friday. Would that work for you? Thanks. On Wed, O= ct > > 8, 2025 at 12:42 PM Greg Ballantine via RT <UMBCHelp@rt.umbc.edu> wrote: > > >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > > > >> Last Update From Ticket: > > >> Hello Zhibo, > > >> Yes, a response by Friday is fine. > > >> Thank you, > >> Greg > > >> On Tue Oct 07 21:29:13 2025, OX20655 wrote: > > >> > Hi Elliot We will choose option 1 Schedule a group-wide downtime date > >> > during standard business hours,. I need to determine the exact date > >> with my > >> > team members though. Can I get back to you by Friday? Thanks-Zhibo On > >> Tue, > >> > Oct 7, 2025 at 3:20 PM via RT <UMBCHelp@rt.umbc.edu> wrote: > > >> >> Greetings, > > >> >> This message has been automatically generated in response to the > >> >> creation of a ticket regarding: > > >> >> > >> > ------------------------------------------------------------------------- > >> >> Subject: ""Migrating Research Storage Volume to Ceph Cluster - > >> >> pi_zzbatmos"" > > >> >> Message: > > >> >> Dear Zhibo, > > >> >> As per the communication via myUMBC earlier this summer (June HPCF > >> >> Newsletter > >> >> ), DoIT is in the process of migrating data off of an older storage > >> >> server to > >> >> our new RRStor Ceph storage cluster. Your group is using 458.94 TB > >> of a > >> >> 505.0000 TB quota on the old storage server. > > >> >> To perform these migrations, we need to take individual storage > >> volumes > >> >> offline > >> >> while we migrate them to the Ceph cluster. Thus we are reaching out > >> to > >> >> schedule > >> >> a date where we can migrate your volume located at > >> =E2=80=9C/umbc/rs/zzbatmos=E2=80=9D. > >> >> During > >> >> the migration, we will take your volume offline and will terminate > >> any > >> >> jobs > >> >> running on the chip compute cluster that are accessing this volume. > > >> >> Below we=E2=80=99ve listed two options for handling this data migra= tion - > >> >> please let us > >> >> know which of these you=E2=80=99d prefer. > > >> >> Option 1: Schedule a group-wide downtime date during standard > >> business > >> >> hours, > >> >> which can be done by responding to this email with your preferred > >> >> date(s) to > >> >> perform the migration. During this time, DoIT staff will work to > >> >> migrate your > >> >> volume to the Ceph storage cluster. DoIT staff will send an email > >> alert > >> >> on this > >> >> email thread when the migration has begun and when it has completed. > >> >> For most > >> >> storage volumes, this process should take less than a business day. > >> >> Option 2: If you don=E2=80=99t respond to this email by October 15t= h, DoIT > >> >> staff will > >> >> assign a day over the following month (October 16th through November > >> >> 15th) to > >> >> migrate your volume. The day chosen will be random and will occur > >> >> during > >> >> business hours. You will be notified of the date chosen to perform > >> the > >> >> migration, and will be notified when the migration begins and > >> >> completes. > > >> >> Note: After this process has completed, the new storage volume will > >> >> have a new > >> >> name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its da= ta under > >> >> =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, > >> >> or in your group=E2=80=99s case you will find your volume under > >> >> =E2=80=9C/umbc/rs/pi_zzbatmos=E2=80=9D. > > >> >> Thank you, > >> >> Elliot > > > >> >> > >> > ------------------------------------------------------------------------- > > >> >> There is no need to reply to this message right now. > > >> >> Your ticket has been assigned an ID of [Research Computing #3287967] > >> or > >> >> you can go there directly by clicking the link below. > > >> >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > > > >> >> You can login to view your open tickets at any time by visiting > >> >> http://my.umbc.edu and clicking on ""Help"" and ""Request Help"". > > >> >> Alternately you can click on http://my.umbc.edu/help > > >> >> Thank you > > >> -- > > >> Gregory BallantineSystem Administrator for Research and Enterprise > >> ComputingUMBC > >> - DoIT > > -- > > Gregory BallantineSystem Administrator for Research and Enterprise > ComputingUMBC > - DoIT > > "
3287967,72504401,Correspond,DoIT-Research-Computing,2025-10-27 15:23:17.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos,open,Greg Ballantine,gballan1,Zhibo Zhang,zzbatmos,zzbatmos@umbc.edu,Zhibo Zhang,zzbatmos@umbc.edu,"<div dir=3D""ltr""><div class=3D""gmail_default"" style=3D""font-family:arial,he= lvetica,sans-serif;font-size:small"">Yes, that will be fine. thanks, Greg</d= iv></div><br><div class=3D""gmail_quote gmail_quote_container""><div dir=3D""l= tr"" class=3D""gmail_attr"">On Mon, Oct 27, 2025 at 11:21=E2=80=AFAM Greg Ball= antine via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.= edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""marg= in:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1e= x"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D= 3287967"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Di= splay.html?id=3D3287967</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Good morning Zhibo,<br> <br> Yes, Friday the 31st will work for starting your migration. However, since = your<br> storage volume is currently very large (~482TB as of this morning) there is=  a<br> good chance that your migration will take longer than the business day that=  we<br> typically plan for. We&#39;ve started copying your data to the new storage = server<br> to minimize downtime, but I would still expect the final migration to run<b= r> through the weekend and likely into the following week.<br> <br> Will that time frame still work for your group?<br> <br> Thank you,<br> Greg<br> <br> On Wed Oct 22 10:14:02 2025, OX20655 wrote:<br> <br> &gt; Hi Sorry for the late reply. Our group has decided to make the disk<br> &gt; migration on Oct. 31 Friday. Would that work for you? Thanks. On Wed, = Oct<br> &gt; 8, 2025 at 12:42 PM Greg Ballantine via RT &lt;<a href=3D""mailto:UMBCH= elp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html= ?id=3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tic= ket/Display.html?id=3D3287967</a> &gt;<br> <br> &gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt; Hello Zhibo,<br> <br> &gt;&gt; Yes, a response by Friday is fine.<br> <br> &gt;&gt; Thank you,<br> &gt;&gt; Greg<br> <br> &gt;&gt; On Tue Oct 07 21:29:13 2025, OX20655 wrote:<br> <br> &gt;&gt; &gt; Hi Elliot We will choose option 1 Schedule a group-wide downt= ime date<br> &gt;&gt; &gt; during standard business hours,. I need to determine the exac= t date<br> &gt;&gt; with my<br> &gt;&gt; &gt; team members though. Can I get back to you by Friday? Thanks-= Zhibo On<br> &gt;&gt; Tue,<br> &gt;&gt; &gt; Oct 7, 2025 at 3:20 PM via RT &lt;<a href=3D""mailto:UMBCHelp@= rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; &gt;&gt; Greetings,<br> <br> &gt;&gt; &gt;&gt; This message has been automatically generated in response=  to the<br> &gt;&gt; &gt;&gt; creation of a ticket regarding:<br> <br> &gt;&gt; &gt;&gt;<br> &gt;&gt; ------------------------------------------------------------------= -------<br> &gt;&gt; &gt;&gt; Subject: &quot;Migrating Research Storage Volume to Ceph = Cluster -<br> &gt;&gt; &gt;&gt; pi_zzbatmos&quot;<br> <br> &gt;&gt; &gt;&gt; Message:<br> <br> &gt;&gt; &gt;&gt; Dear Zhibo,<br> <br> &gt;&gt; &gt;&gt; As per the communication via myUMBC earlier this summer (= June HPCF<br> &gt;&gt; &gt;&gt; Newsletter<br> &gt;&gt; &gt;&gt; ), DoIT is in the process of migrating data off of an old= er storage<br> &gt;&gt; &gt;&gt; server to<br> &gt;&gt; &gt;&gt; our new RRStor Ceph storage cluster. Your group is using = 458.94 TB<br> &gt;&gt; of a<br> &gt;&gt; &gt;&gt; 505.0000 TB quota on the old storage server.<br> <br> &gt;&gt; &gt;&gt; To perform these migrations, we need to take individual s= torage<br> &gt;&gt; volumes<br> &gt;&gt; &gt;&gt; offline<br> &gt;&gt; &gt;&gt; while we migrate them to the Ceph cluster. Thus we are re= aching out<br> &gt;&gt; to<br> &gt;&gt; &gt;&gt; schedule<br> &gt;&gt; &gt;&gt; a date where we can migrate your volume located at<br> &gt;&gt; =E2=80=9C/umbc/rs/zzbatmos=E2=80=9D.<br> &gt;&gt; &gt;&gt; During<br> &gt;&gt; &gt;&gt; the migration, we will take your volume offline and will = terminate<br> &gt;&gt; any<br> &gt;&gt; &gt;&gt; jobs<br> &gt;&gt; &gt;&gt; running on the chip compute cluster that are accessing th= is volume.<br> <br> &gt;&gt; &gt;&gt; Below we=E2=80=99ve listed two options for handling this = data migration -<br> &gt;&gt; &gt;&gt; please let us<br> &gt;&gt; &gt;&gt; know which of these you=E2=80=99d prefer.<br> <br> &gt;&gt; &gt;&gt; Option 1: Schedule a group-wide downtime date during stan= dard<br> &gt;&gt; business<br> &gt;&gt; &gt;&gt; hours,<br> &gt;&gt; &gt;&gt; which can be done by responding to this email with your p= referred<br> &gt;&gt; &gt;&gt; date(s) to<br> &gt;&gt; &gt;&gt; perform the migration. During this time, DoIT staff will = work to<br> &gt;&gt; &gt;&gt; migrate your<br> &gt;&gt; &gt;&gt; volume to the Ceph storage cluster. DoIT staff will send = an email<br> &gt;&gt; alert<br> &gt;&gt; &gt;&gt; on this<br> &gt;&gt; &gt;&gt; email thread when the migration has begun and when it has=  completed.<br> &gt;&gt; &gt;&gt; For most<br> &gt;&gt; &gt;&gt; storage volumes, this process should take less than a bus= iness day.<br> &gt;&gt; &gt;&gt; Option 2: If you don=E2=80=99t respond to this email by O= ctober 15th, DoIT<br> &gt;&gt; &gt;&gt; staff will<br> &gt;&gt; &gt;&gt; assign a day over the following month (October 16th throu= gh November<br> &gt;&gt; &gt;&gt; 15th) to<br> &gt;&gt; &gt;&gt; migrate your volume. The day chosen will be random and wi= ll occur<br> &gt;&gt; &gt;&gt; during<br> &gt;&gt; &gt;&gt; business hours. You will be notified of the date chosen t= o perform<br> &gt;&gt; the<br> &gt;&gt; &gt;&gt; migration, and will be notified when the migration begins=  and<br> &gt;&gt; &gt;&gt; completes.<br> <br> &gt;&gt; &gt;&gt; Note: After this process has completed, the new storage v= olume will<br> &gt;&gt; &gt;&gt; have a new<br> &gt;&gt; &gt;&gt; name. For example, group =E2=80=9Cpi_doit=E2=80=9D will f= ind its data under<br> &gt;&gt; &gt;&gt; =E2=80=9C/umbc/rs/pi_doit=E2=80=9D,<br> &gt;&gt; &gt;&gt; or in your group=E2=80=99s case you will find your volume=  under<br> &gt;&gt; &gt;&gt; =E2=80=9C/umbc/rs/pi_zzbatmos=E2=80=9D.<br> <br> &gt;&gt; &gt;&gt; Thank you,<br> &gt;&gt; &gt;&gt; Elliot<br> <br> <br> &gt;&gt; &gt;&gt;<br> &gt;&gt; ------------------------------------------------------------------= -------<br> <br> &gt;&gt; &gt;&gt; There is no need to reply to this message right now.<br> <br> &gt;&gt; &gt;&gt; Your ticket has been assigned an ID of [Research Computin= g #3287967]<br> &gt;&gt; or<br> &gt;&gt; &gt;&gt; you can go there directly by clicking the link below.<br> <br> &gt;&gt; &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Dis= play.html?id=3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umb= c.edu/Ticket/Display.html?id=3D3287967</a> &gt;<br> <br> &gt;&gt; &gt;&gt; You can login to view your open tickets at any time by vi= siting<br> &gt;&gt; &gt;&gt; <a href=3D""http://my.umbc.edu"" rel=3D""noreferrer"" target= =3D""_blank"">http://my.umbc.edu</a> and clicking on &quot;Help&quot; and &qu= ot;Request Help&quot;.<br> <br> &gt;&gt; &gt;&gt; Alternately you can click on <a href=3D""http://my.umbc.ed= u/help"" rel=3D""noreferrer"" target=3D""_blank"">http://my.umbc.edu/help</a><br> <br> &gt;&gt; &gt;&gt; Thank you<br> <br> &gt;&gt; --<br> <br> &gt;&gt; Gregory BallantineSystem Administrator for Research and Enterprise= <br> &gt;&gt; ComputingUMBC<br> &gt;&gt; - DoIT<br> <br> --<br> <br> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br> - DoIT<br> <br> </blockquote></div> "
3287967,72610461,Correspond,DoIT-Research-Computing,2025-10-30 20:22:50.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos,open,Greg Ballantine,gballan1,Zhibo Zhang,zzbatmos,zzbatmos@umbc.edu,Zhibo Zhang,zzbatmos@umbc.edu,"Hi All        This is a reminder that DOIT will help us with the Chip disk migration tomorrow Oct. 31st. During this period, please do not use the Chip server until we are notified that the migration process is finished. Thanks  -Zhibo  On Mon, Oct 27, 2025 at 11:23=E2=80=AFAM Zhibo Zhang <zzbatmos@umbc.edu> wr= ote:  > Yes, that will be fine. thanks, Greg > > On Mon, Oct 27, 2025 at 11:21=E2=80=AFAM Greg Ballantine via RT < > UMBCHelp@rt.umbc.edu> wrote: > >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > >> >> Last Update From Ticket: >> >> Good morning Zhibo, >> >> Yes, Friday the 31st will work for starting your migration. However, >> since your >> storage volume is currently very large (~482TB as of this morning) there >> is a >> good chance that your migration will take longer than the business day >> that we >> typically plan for. We've started copying your data to the new storage >> server >> to minimize downtime, but I would still expect the final migration to run >> through the weekend and likely into the following week. >> >> Will that time frame still work for your group? >> >> Thank you, >> Greg >> >> On Wed Oct 22 10:14:02 2025, OX20655 wrote: >> >> > Hi Sorry for the late reply. Our group has decided to make the disk >> > migration on Oct. 31 Friday. Would that work for you? Thanks. On Wed, >> Oct >> > 8, 2025 at 12:42 PM Greg Ballantine via RT <UMBCHelp@rt.umbc.edu> >> wrote: >> >> >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > >> >> >> Last Update From Ticket: >> >> >> Hello Zhibo, >> >> >> Yes, a response by Friday is fine. >> >> >> Thank you, >> >> Greg >> >> >> On Tue Oct 07 21:29:13 2025, OX20655 wrote: >> >> >> > Hi Elliot We will choose option 1 Schedule a group-wide downtime da= te >> >> > during standard business hours,. I need to determine the exact date >> >> with my >> >> > team members though. Can I get back to you by Friday? Thanks-Zhibo = On >> >> Tue, >> >> > Oct 7, 2025 at 3:20 PM via RT <UMBCHelp@rt.umbc.edu> wrote: >> >> >> >> Greetings, >> >> >> >> This message has been automatically generated in response to the >> >> >> creation of a ticket regarding: >> >> >> >> >> >> >> ------------------------------------------------------------------------- >> >> >> Subject: ""Migrating Research Storage Volume to Ceph Cluster - >> >> >> pi_zzbatmos"" >> >> >> >> Message: >> >> >> >> Dear Zhibo, >> >> >> >> As per the communication via myUMBC earlier this summer (June HPCF >> >> >> Newsletter >> >> >> ), DoIT is in the process of migrating data off of an older storage >> >> >> server to >> >> >> our new RRStor Ceph storage cluster. Your group is using 458.94 TB >> >> of a >> >> >> 505.0000 TB quota on the old storage server. >> >> >> >> To perform these migrations, we need to take individual storage >> >> volumes >> >> >> offline >> >> >> while we migrate them to the Ceph cluster. Thus we are reaching out >> >> to >> >> >> schedule >> >> >> a date where we can migrate your volume located at >> >> =E2=80=9C/umbc/rs/zzbatmos=E2=80=9D. >> >> >> During >> >> >> the migration, we will take your volume offline and will terminate >> >> any >> >> >> jobs >> >> >> running on the chip compute cluster that are accessing this volume. >> >> >> >> Below we=E2=80=99ve listed two options for handling this data migr= ation - >> >> >> please let us >> >> >> know which of these you=E2=80=99d prefer. >> >> >> >> Option 1: Schedule a group-wide downtime date during standard >> >> business >> >> >> hours, >> >> >> which can be done by responding to this email with your preferred >> >> >> date(s) to >> >> >> perform the migration. During this time, DoIT staff will work to >> >> >> migrate your >> >> >> volume to the Ceph storage cluster. DoIT staff will send an email >> >> alert >> >> >> on this >> >> >> email thread when the migration has begun and when it has complete= d. >> >> >> For most >> >> >> storage volumes, this process should take less than a business day. >> >> >> Option 2: If you don=E2=80=99t respond to this email by October 15= th, DoIT >> >> >> staff will >> >> >> assign a day over the following month (October 16th through Novemb= er >> >> >> 15th) to >> >> >> migrate your volume. The day chosen will be random and will occur >> >> >> during >> >> >> business hours. You will be notified of the date chosen to perform >> >> the >> >> >> migration, and will be notified when the migration begins and >> >> >> completes. >> >> >> >> Note: After this process has completed, the new storage volume will >> >> >> have a new >> >> >> name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its d= ata under >> >> >> =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, >> >> >> or in your group=E2=80=99s case you will find your volume under >> >> >> =E2=80=9C/umbc/rs/pi_zzbatmos=E2=80=9D. >> >> >> >> Thank you, >> >> >> Elliot >> >> >> >> >> >> >> >> ------------------------------------------------------------------------- >> >> >> >> There is no need to reply to this message right now. >> >> >> >> Your ticket has been assigned an ID of [Research Computing #328796= 7] >> >> or >> >> >> you can go there directly by clicking the link below. >> >> >> >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > >> >> >> >> You can login to view your open tickets at any time by visiting >> >> >> http://my.umbc.edu and clicking on ""Help"" and ""Request Help"". >> >> >> >> Alternately you can click on http://my.umbc.edu/help >> >> >> >> Thank you >> >> >> -- >> >> >> Gregory BallantineSystem Administrator for Research and Enterprise >> >> ComputingUMBC >> >> - DoIT >> >> -- >> >> Gregory BallantineSystem Administrator for Research and Enterprise >> ComputingUMBC >> - DoIT >> >> "
3287967,72610461,Correspond,DoIT-Research-Computing,2025-10-30 20:22:50.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos,open,Greg Ballantine,gballan1,Zhibo Zhang,zzbatmos,zzbatmos@umbc.edu,Zhibo Zhang,zzbatmos@umbc.edu,"<div dir=3D""ltr""><div class=3D""gmail_default"" style=3D""font-family:arial,he= lvetica,sans-serif;font-size:small"">Hi All</div><div class=3D""gmail_default= "" style=3D""font-family:arial,helvetica,sans-serif;font-size:small"">=C2=A0 = =C2=A0 =C2=A0 =C2=A0This is a reminder that DOIT will help us with the Chip=  disk migration tomorrow Oct. 31st. During this period, please do not use t= he Chip server until we are notified that the migration process is finished= . Thanks</div><div class=3D""gmail_default"" style=3D""font-family:arial,helve= tica,sans-serif;font-size:small""><br></div><div class=3D""gmail_default"" sty= le=3D""font-family:arial,helvetica,sans-serif;font-size:small"">-Zhibo</div><= /div><br><div class=3D""gmail_quote gmail_quote_container""><div dir=3D""ltr"" = class=3D""gmail_attr"">On Mon, Oct 27, 2025 at 11:23=E2=80=AFAM Zhibo Zhang &= lt;<a href=3D""mailto:zzbatmos@umbc.edu"">zzbatmos@umbc.edu</a>&gt; wrote:<br= ></div><blockquote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;= border-left:1px solid rgb(204,204,204);padding-left:1ex""><div dir=3D""ltr""><= div class=3D""gmail_default"" style=3D""font-family:arial,helvetica,sans-serif= ;font-size:small"">Yes, that will be fine. thanks, Greg</div></div><br><div = class=3D""gmail_quote""><div dir=3D""ltr"" class=3D""gmail_attr"">On Mon, Oct 27,=  2025 at 11:21=E2=80=AFAM Greg Ballantine via RT &lt;<a href=3D""mailto:UMBC= Help@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br>= </div><blockquote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;b= order-left:1px solid rgb(204,204,204);padding-left:1ex"">Ticket &lt;URL: <a = href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D3287967"" rel=3D""norefe= rrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D328796= 7</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Good morning Zhibo,<br> <br> Yes, Friday the 31st will work for starting your migration. However, since = your<br> storage volume is currently very large (~482TB as of this morning) there is=  a<br> good chance that your migration will take longer than the business day that=  we<br> typically plan for. We&#39;ve started copying your data to the new storage = server<br> to minimize downtime, but I would still expect the final migration to run<b= r> through the weekend and likely into the following week.<br> <br> Will that time frame still work for your group?<br> <br> Thank you,<br> Greg<br> <br> On Wed Oct 22 10:14:02 2025, OX20655 wrote:<br> <br> &gt; Hi Sorry for the late reply. Our group has decided to make the disk<br> &gt; migration on Oct. 31 Friday. Would that work for you? Thanks. On Wed, = Oct<br> &gt; 8, 2025 at 12:42 PM Greg Ballantine via RT &lt;<a href=3D""mailto:UMBCH= elp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html= ?id=3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tic= ket/Display.html?id=3D3287967</a> &gt;<br> <br> &gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt; Hello Zhibo,<br> <br> &gt;&gt; Yes, a response by Friday is fine.<br> <br> &gt;&gt; Thank you,<br> &gt;&gt; Greg<br> <br> &gt;&gt; On Tue Oct 07 21:29:13 2025, OX20655 wrote:<br> <br> &gt;&gt; &gt; Hi Elliot We will choose option 1 Schedule a group-wide downt= ime date<br> &gt;&gt; &gt; during standard business hours,. I need to determine the exac= t date<br> &gt;&gt; with my<br> &gt;&gt; &gt; team members though. Can I get back to you by Friday? Thanks-= Zhibo On<br> &gt;&gt; Tue,<br> &gt;&gt; &gt; Oct 7, 2025 at 3:20 PM via RT &lt;<a href=3D""mailto:UMBCHelp@= rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; &gt;&gt; Greetings,<br> <br> &gt;&gt; &gt;&gt; This message has been automatically generated in response=  to the<br> &gt;&gt; &gt;&gt; creation of a ticket regarding:<br> <br> &gt;&gt; &gt;&gt;<br> &gt;&gt; ------------------------------------------------------------------= -------<br> &gt;&gt; &gt;&gt; Subject: &quot;Migrating Research Storage Volume to Ceph = Cluster -<br> &gt;&gt; &gt;&gt; pi_zzbatmos&quot;<br> <br> &gt;&gt; &gt;&gt; Message:<br> <br> &gt;&gt; &gt;&gt; Dear Zhibo,<br> <br> &gt;&gt; &gt;&gt; As per the communication via myUMBC earlier this summer (= June HPCF<br> &gt;&gt; &gt;&gt; Newsletter<br> &gt;&gt; &gt;&gt; ), DoIT is in the process of migrating data off of an old= er storage<br> &gt;&gt; &gt;&gt; server to<br> &gt;&gt; &gt;&gt; our new RRStor Ceph storage cluster. Your group is using = 458.94 TB<br> &gt;&gt; of a<br> &gt;&gt; &gt;&gt; 505.0000 TB quota on the old storage server.<br> <br> &gt;&gt; &gt;&gt; To perform these migrations, we need to take individual s= torage<br> &gt;&gt; volumes<br> &gt;&gt; &gt;&gt; offline<br> &gt;&gt; &gt;&gt; while we migrate them to the Ceph cluster. Thus we are re= aching out<br> &gt;&gt; to<br> &gt;&gt; &gt;&gt; schedule<br> &gt;&gt; &gt;&gt; a date where we can migrate your volume located at<br> &gt;&gt; =E2=80=9C/umbc/rs/zzbatmos=E2=80=9D.<br> &gt;&gt; &gt;&gt; During<br> &gt;&gt; &gt;&gt; the migration, we will take your volume offline and will = terminate<br> &gt;&gt; any<br> &gt;&gt; &gt;&gt; jobs<br> &gt;&gt; &gt;&gt; running on the chip compute cluster that are accessing th= is volume.<br> <br> &gt;&gt; &gt;&gt; Below we=E2=80=99ve listed two options for handling this = data migration -<br> &gt;&gt; &gt;&gt; please let us<br> &gt;&gt; &gt;&gt; know which of these you=E2=80=99d prefer.<br> <br> &gt;&gt; &gt;&gt; Option 1: Schedule a group-wide downtime date during stan= dard<br> &gt;&gt; business<br> &gt;&gt; &gt;&gt; hours,<br> &gt;&gt; &gt;&gt; which can be done by responding to this email with your p= referred<br> &gt;&gt; &gt;&gt; date(s) to<br> &gt;&gt; &gt;&gt; perform the migration. During this time, DoIT staff will = work to<br> &gt;&gt; &gt;&gt; migrate your<br> &gt;&gt; &gt;&gt; volume to the Ceph storage cluster. DoIT staff will send = an email<br> &gt;&gt; alert<br> &gt;&gt; &gt;&gt; on this<br> &gt;&gt; &gt;&gt; email thread when the migration has begun and when it has=  completed.<br> &gt;&gt; &gt;&gt; For most<br> &gt;&gt; &gt;&gt; storage volumes, this process should take less than a bus= iness day.<br> &gt;&gt; &gt;&gt; Option 2: If you don=E2=80=99t respond to this email by O= ctober 15th, DoIT<br> &gt;&gt; &gt;&gt; staff will<br> &gt;&gt; &gt;&gt; assign a day over the following month (October 16th throu= gh November<br> &gt;&gt; &gt;&gt; 15th) to<br> &gt;&gt; &gt;&gt; migrate your volume. The day chosen will be random and wi= ll occur<br> &gt;&gt; &gt;&gt; during<br> &gt;&gt; &gt;&gt; business hours. You will be notified of the date chosen t= o perform<br> &gt;&gt; the<br> &gt;&gt; &gt;&gt; migration, and will be notified when the migration begins=  and<br> &gt;&gt; &gt;&gt; completes.<br> <br> &gt;&gt; &gt;&gt; Note: After this process has completed, the new storage v= olume will<br> &gt;&gt; &gt;&gt; have a new<br> &gt;&gt; &gt;&gt; name. For example, group =E2=80=9Cpi_doit=E2=80=9D will f= ind its data under<br> &gt;&gt; &gt;&gt; =E2=80=9C/umbc/rs/pi_doit=E2=80=9D,<br> &gt;&gt; &gt;&gt; or in your group=E2=80=99s case you will find your volume=  under<br> &gt;&gt; &gt;&gt; =E2=80=9C/umbc/rs/pi_zzbatmos=E2=80=9D.<br> <br> &gt;&gt; &gt;&gt; Thank you,<br> &gt;&gt; &gt;&gt; Elliot<br> <br> <br> &gt;&gt; &gt;&gt;<br> &gt;&gt; ------------------------------------------------------------------= -------<br> <br> &gt;&gt; &gt;&gt; There is no need to reply to this message right now.<br> <br> &gt;&gt; &gt;&gt; Your ticket has been assigned an ID of [Research Computin= g #3287967]<br> &gt;&gt; or<br> &gt;&gt; &gt;&gt; you can go there directly by clicking the link below.<br> <br> &gt;&gt; &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Dis= play.html?id=3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umb= c.edu/Ticket/Display.html?id=3D3287967</a> &gt;<br> <br> &gt;&gt; &gt;&gt; You can login to view your open tickets at any time by vi= siting<br> &gt;&gt; &gt;&gt; <a href=3D""http://my.umbc.edu"" rel=3D""noreferrer"" target= =3D""_blank"">http://my.umbc.edu</a> and clicking on &quot;Help&quot; and &qu= ot;Request Help&quot;.<br> <br> &gt;&gt; &gt;&gt; Alternately you can click on <a href=3D""http://my.umbc.ed= u/help"" rel=3D""noreferrer"" target=3D""_blank"">http://my.umbc.edu/help</a><br> <br> &gt;&gt; &gt;&gt; Thank you<br> <br> &gt;&gt; --<br> <br> &gt;&gt; Gregory BallantineSystem Administrator for Research and Enterprise= <br> &gt;&gt; ComputingUMBC<br> &gt;&gt; - DoIT<br> <br> --<br> <br> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br> - DoIT<br> <br> </blockquote></div> </blockquote></div> "
3287967,72621309,Correspond,DoIT-Research-Computing,2025-10-31 14:41:44.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos,open,Greg Ballantine,gballan1,Zhibo Zhang,zzbatmos,zzbatmos@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Good morning Zhibo,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to=  the Ceph storage cluster today.&nbsp;During this time, please ensure there=  are not any jobs being run in your research group, otherwise these may be = terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Thu Oct 30 16:22:50 2025, OX20655 wrote:</p>  <blockquote> <div> <div>Hi All</div>  <div>&nbsp; &nbsp; &nbsp; &nbsp;This is a reminder that DOIT will help us w= ith the Chip disk migration tomorrow Oct. 31st. During this period, please = do not use the Chip server until we are notified that the migration process=  is finished. Thanks</div>  <div>&nbsp;</div>  <div>-Zhibo</div> </div> &nbsp;  <div> <div>On Mon, Oct 27, 2025 at 11:23=E2=80=AFAM Zhibo Zhang &lt;zzbatmos@umbc= .edu&gt; wrote:</div>  <blockquote> <div> <div>Yes, that will be fine. thanks, Greg</div> </div> &nbsp;  <div> <div>On Mon, Oct 27, 2025 at 11:21=E2=80=AFAM Greg Ballantine via RT &lt;UM= BCHelp@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 87967 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Good morning Zhibo,<br /> <br /> Yes, Friday the 31st will work for starting your migration. However, since = your<br /> storage volume is currently very large (~482TB as of this morning) there is=  a<br /> good chance that your migration will take longer than the business day that=  we<br /> typically plan for. We&#39;ve started copying your data to the new storage = server<br /> to minimize downtime, but I would still expect the final migration to run<b= r /> through the weekend and likely into the following week.<br /> <br /> Will that time frame still work for your group?<br /> <br /> Thank you,<br /> Greg<br /> <br /> On Wed Oct 22 10:14:02 2025, OX20655 wrote:<br /> <br /> &gt; Hi Sorry for the late reply. Our group has decided to make the disk<br=  /> &gt; migration on Oct. 31 Friday. Would that work for you? Thanks. On Wed, = Oct<br /> &gt; 8, 2025 at 12:42 PM Greg Ballantine via RT &lt;UMBCHelp@rt.umbc.edu&gt= ; wrote:<br /> <br /> &gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32879= 67 &gt;<br /> <br /> &gt;&gt; Last Update From Ticket:<br /> <br /> &gt;&gt; Hello Zhibo,<br /> <br /> &gt;&gt; Yes, a response by Friday is fine.<br /> <br /> &gt;&gt; Thank you,<br /> &gt;&gt; Greg<br /> <br /> &gt;&gt; On Tue Oct 07 21:29:13 2025, OX20655 wrote:<br /> <br /> &gt;&gt; &gt; Hi Elliot We will choose option 1 Schedule a group-wide downt= ime date<br /> &gt;&gt; &gt; during standard business hours,. I need to determine the exac= t date<br /> &gt;&gt; with my<br /> &gt;&gt; &gt; team members though. Can I get back to you by Friday? Thanks-= Zhibo On<br /> &gt;&gt; Tue,<br /> &gt;&gt; &gt; Oct 7, 2025 at 3:20 PM via RT &lt;UMBCHelp@rt.umbc.edu&gt; wr= ote:<br /> <br /> &gt;&gt; &gt;&gt; Greetings,<br /> <br /> &gt;&gt; &gt;&gt; This message has been automatically generated in response=  to the<br /> &gt;&gt; &gt;&gt; creation of a ticket regarding:<br /> <br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; ------------------------------------------------------------------= -------<br /> &gt;&gt; &gt;&gt; Subject: &quot;Migrating Research Storage Volume to Ceph = Cluster -<br /> &gt;&gt; &gt;&gt; pi_zzbatmos&quot;<br /> <br /> &gt;&gt; &gt;&gt; Message:<br /> <br /> &gt;&gt; &gt;&gt; Dear Zhibo,<br /> <br /> &gt;&gt; &gt;&gt; As per the communication via myUMBC earlier this summer (= June HPCF<br /> &gt;&gt; &gt;&gt; Newsletter<br /> &gt;&gt; &gt;&gt; ), DoIT is in the process of migrating data off of an old= er storage<br /> &gt;&gt; &gt;&gt; server to<br /> &gt;&gt; &gt;&gt; our new RRStor Ceph storage cluster. Your group is using = 458.94 TB<br /> &gt;&gt; of a<br /> &gt;&gt; &gt;&gt; 505.0000 TB quota on the old storage server.<br /> <br /> &gt;&gt; &gt;&gt; To perform these migrations, we need to take individual s= torage<br /> &gt;&gt; volumes<br /> &gt;&gt; &gt;&gt; offline<br /> &gt;&gt; &gt;&gt; while we migrate them to the Ceph cluster. Thus we are re= aching out<br /> &gt;&gt; to<br /> &gt;&gt; &gt;&gt; schedule<br /> &gt;&gt; &gt;&gt; a date where we can migrate your volume located at<br /> &gt;&gt; &ldquo;/umbc/rs/zzbatmos&rdquo;.<br /> &gt;&gt; &gt;&gt; During<br /> &gt;&gt; &gt;&gt; the migration, we will take your volume offline and will = terminate<br /> &gt;&gt; any<br /> &gt;&gt; &gt;&gt; jobs<br /> &gt;&gt; &gt;&gt; running on the chip compute cluster that are accessing th= is volume.<br /> <br /> &gt;&gt; &gt;&gt; Below we&rsquo;ve listed two options for handling this da= ta migration -<br /> &gt;&gt; &gt;&gt; please let us<br /> &gt;&gt; &gt;&gt; know which of these you&rsquo;d prefer.<br /> <br /> &gt;&gt; &gt;&gt; Option 1: Schedule a group-wide downtime date during stan= dard<br /> &gt;&gt; business<br /> &gt;&gt; &gt;&gt; hours,<br /> &gt;&gt; &gt;&gt; which can be done by responding to this email with your p= referred<br /> &gt;&gt; &gt;&gt; date(s) to<br /> &gt;&gt; &gt;&gt; perform the migration. During this time, DoIT staff will = work to<br /> &gt;&gt; &gt;&gt; migrate your<br /> &gt;&gt; &gt;&gt; volume to the Ceph storage cluster. DoIT staff will send = an email<br /> &gt;&gt; alert<br /> &gt;&gt; &gt;&gt; on this<br /> &gt;&gt; &gt;&gt; email thread when the migration has begun and when it has=  completed.<br /> &gt;&gt; &gt;&gt; For most<br /> &gt;&gt; &gt;&gt; storage volumes, this process should take less than a bus= iness day.<br /> &gt;&gt; &gt;&gt; Option 2: If you don&rsquo;t respond to this email by Oct= ober 15th, DoIT<br /> &gt;&gt; &gt;&gt; staff will<br /> &gt;&gt; &gt;&gt; assign a day over the following month (October 16th throu= gh November<br /> &gt;&gt; &gt;&gt; 15th) to<br /> &gt;&gt; &gt;&gt; migrate your volume. The day chosen will be random and wi= ll occur<br /> &gt;&gt; &gt;&gt; during<br /> &gt;&gt; &gt;&gt; business hours. You will be notified of the date chosen t= o perform<br /> &gt;&gt; the<br /> &gt;&gt; &gt;&gt; migration, and will be notified when the migration begins=  and<br /> &gt;&gt; &gt;&gt; completes.<br /> <br /> &gt;&gt; &gt;&gt; Note: After this process has completed, the new storage v= olume will<br /> &gt;&gt; &gt;&gt; have a new<br /> &gt;&gt; &gt;&gt; name. For example, group &ldquo;pi_doit&rdquo; will find = its data under<br /> &gt;&gt; &gt;&gt; &ldquo;/umbc/rs/pi_doit&rdquo;,<br /> &gt;&gt; &gt;&gt; or in your group&rsquo;s case you will find your volume u= nder<br /> &gt;&gt; &gt;&gt; &ldquo;/umbc/rs/pi_zzbatmos&rdquo;.<br /> <br /> &gt;&gt; &gt;&gt; Thank you,<br /> &gt;&gt; &gt;&gt; Elliot<br /> <br /> <br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; ------------------------------------------------------------------= -------<br /> <br /> &gt;&gt; &gt;&gt; There is no need to reply to this message right now.<br /> <br /> &gt;&gt; &gt;&gt; Your ticket has been assigned an ID of [Research Computin= g #3287967]<br /> &gt;&gt; or<br /> &gt;&gt; &gt;&gt; you can go there directly by clicking the link below.<br = /> <br /> &gt;&gt; &gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?i= d=3D3287967 &gt;<br /> <br /> &gt;&gt; &gt;&gt; You can login to view your open tickets at any time by vi= siting<br /> &gt;&gt; &gt;&gt; http://my.umbc.edu and clicking on &quot;Help&quot; and &= quot;Request Help&quot;.<br /> <br /> &gt;&gt; &gt;&gt; Alternately you can click on http://my.umbc.edu/help<br /> <br /> &gt;&gt; &gt;&gt; Thank you<br /> <br /> &gt;&gt; --<br /> <br /> &gt;&gt; Gregory BallantineSystem Administrator for Research and Enterprise= <br /> &gt;&gt; ComputingUMBC<br /> &gt;&gt; - DoIT<br /> <br /> --<br /> <br /> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br /> - DoIT<br /> &nbsp;</blockquote> </div> </blockquote> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3287967,72650948,Correspond,DoIT-Research-Computing,2025-11-03 00:44:36.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos,open,Greg Ballantine,gballan1,Zhibo Zhang,zzbatmos,zzbatmos@umbc.edu,Zhibo Zhang,zzbatmos@umbc.edu,"Greg      Please let our group know if the disk migration is completed and if we need to do something (e.g., new configurations) to use the new disk space. Thanks -Zhibo  On Fri, Oct 31, 2025 at 10:41=E2=80=AFAM Greg Ballantine via RT < UMBCHelp@rt.umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > > > Last Update From Ticket: > > Good morning Zhibo, > > This is a reminder that we will be performing your group's migration to t= he > Ceph storage cluster today. During this time, please ensure there are not > any > jobs being run in your research group, otherwise these may be terminated. > > We will provide an update once completed. > > Best, > Greg > > On Thu Oct 30 16:22:50 2025, OX20655 wrote: > > > Hi All This is a reminder that DOIT will help us with the Chip disk > > migration tomorrow Oct. 31st. During this period, please do not use the > > Chip server until we are notified that the migration process is finishe= d. > > Thanks -Zhibo On Mon, Oct 27, 2025 at 11:23 AM Zhibo Zhang > > <zzbatmos@umbc.edu> wrote: > > >> Yes, that will be fine. thanks, Greg On Mon, Oct 27, 2025 at 11:21 AM > >> Greg Ballantine via RT <UMBCHelp@rt.umbc.edu> wrote: > > >>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > > > >>> Last Update From Ticket: > > >>> Good morning Zhibo, > > >>> Yes, Friday the 31st will work for starting your migration. > >>> However, since your > >>> storage volume is currently very large (~482TB as of this morning) > >>> there is a > >>> good chance that your migration will take longer than the business > >>> day that we > >>> typically plan for. We've started copying your data to the new > >>> storage server > >>> to minimize downtime, but I would still expect the final migration > >>> to run > >>> through the weekend and likely into the following week. > > >>> Will that time frame still work for your group? > > >>> Thank you, > >>> Greg > > >>> On Wed Oct 22 10:14:02 2025, OX20655 wrote: > > >>> > Hi Sorry for the late reply. Our group has decided to make the > >>> disk > >>> > migration on Oct. 31 Friday. Would that work for you? Thanks. On > >>> Wed, Oct > >>> > 8, 2025 at 12:42 PM Greg Ballantine via RT <UMBCHelp@rt.umbc.edu> > >>> wrote: > > >>> >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > >>> > > > >>> >> Last Update From Ticket: > > >>> >> Hello Zhibo, > > >>> >> Yes, a response by Friday is fine. > > >>> >> Thank you, > >>> >> Greg > > >>> >> On Tue Oct 07 21:29:13 2025, OX20655 wrote: > > >>> >> > Hi Elliot We will choose option 1 Schedule a group-wide > >>> downtime date > >>> >> > during standard business hours,. I need to determine the exact > >>> date > >>> >> with my > >>> >> > team members though. Can I get back to you by Friday? > >>> Thanks-Zhibo On > >>> >> Tue, > >>> >> > Oct 7, 2025 at 3:20 PM via RT <UMBCHelp@rt.umbc.edu> wrote: > > >>> >> >> Greetings, > > >>> >> >> This message has been automatically generated in response to > >>> the > >>> >> >> creation of a ticket regarding: > > >>> >> >> > >>> >> > >>> > ------------------------------------------------------------------------- > >>> >> >> Subject: ""Migrating Research Storage Volume to Ceph Cluster - > >>> >> >> pi_zzbatmos"" > > >>> >> >> Message: > > >>> >> >> Dear Zhibo, > > >>> >> >> As per the communication via myUMBC earlier this summer (June > >>> HPCF > >>> >> >> Newsletter > >>> >> >> ), DoIT is in the process of migrating data off of an older > >>> storage > >>> >> >> server to > >>> >> >> our new RRStor Ceph storage cluster. Your group is using > >>> 458.94 TB > >>> >> of a > >>> >> >> 505.0000 TB quota on the old storage server. > > >>> >> >> To perform these migrations, we need to take individual > >>> storage > >>> >> volumes > >>> >> >> offline > >>> >> >> while we migrate them to the Ceph cluster. Thus we are > >>> reaching out > >>> >> to > >>> >> >> schedule > >>> >> >> a date where we can migrate your volume located at > >>> >> =E2=80=9C/umbc/rs/zzbatmos=E2=80=9D. > >>> >> >> During > >>> >> >> the migration, we will take your volume offline and will > >>> terminate > >>> >> any > >>> >> >> jobs > >>> >> >> running on the chip compute cluster that are accessing this > >>> volume. > > >>> >> >> Below we=E2=80=99ve listed two options for handling this data > >>> migration - > >>> >> >> please let us > >>> >> >> know which of these you=E2=80=99d prefer. > > >>> >> >> Option 1: Schedule a group-wide downtime date during standard > >>> >> business > >>> >> >> hours, > >>> >> >> which can be done by responding to this email with your > >>> preferred > >>> >> >> date(s) to > >>> >> >> perform the migration. During this time, DoIT staff will work > >>> to > >>> >> >> migrate your > >>> >> >> volume to the Ceph storage cluster. DoIT staff will send an > >>> email > >>> >> alert > >>> >> >> on this > >>> >> >> email thread when the migration has begun and when it has > >>> completed. > >>> >> >> For most > >>> >> >> storage volumes, this process should take less than a > >>> business day. > >>> >> >> Option 2: If you don=E2=80=99t respond to this email by October=  15th, > >>> DoIT > >>> >> >> staff will > >>> >> >> assign a day over the following month (October 16th through > >>> November > >>> >> >> 15th) to > >>> >> >> migrate your volume. The day chosen will be random and will > >>> occur > >>> >> >> during > >>> >> >> business hours. You will be notified of the date chosen to > >>> perform > >>> >> the > >>> >> >> migration, and will be notified when the migration begins and > >>> >> >> completes. > > >>> >> >> Note: After this process has completed, the new storage > >>> volume will > >>> >> >> have a new > >>> >> >> name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find it= s data under > >>> >> >> =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, > >>> >> >> or in your group=E2=80=99s case you will find your volume under > >>> >> >> =E2=80=9C/umbc/rs/pi_zzbatmos=E2=80=9D. > > >>> >> >> Thank you, > >>> >> >> Elliot > > > >>> >> >> > >>> >> > >>> > ------------------------------------------------------------------------- > > >>> >> >> There is no need to reply to this message right now. > > >>> >> >> Your ticket has been assigned an ID of [Research Computing > >>> #3287967] > >>> >> or > >>> >> >> you can go there directly by clicking the link below. > > >>> >> >> Ticket <URL: > >>> https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > > > >>> >> >> You can login to view your open tickets at any time by > >>> visiting > >>> >> >> http://my.umbc.edu and clicking on ""Help"" and ""Request Help"". > > >>> >> >> Alternately you can click on http://my.umbc.edu/help > > >>> >> >> Thank you > > >>> >> -- > > >>> >> Gregory BallantineSystem Administrator for Research and > >>> Enterprise > >>> >> ComputingUMBC > >>> >> - DoIT > > >>> -- > > >>> Gregory BallantineSystem Administrator for Research and Enterprise > >>> ComputingUMBC > >>> - DoIT > > -- > > Gregory BallantineSystem Administrator for Research and Enterprise > ComputingUMBC > - DoIT > > "
3287967,72650948,Correspond,DoIT-Research-Computing,2025-11-03 00:44:36.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos,open,Greg Ballantine,gballan1,Zhibo Zhang,zzbatmos,zzbatmos@umbc.edu,Zhibo Zhang,zzbatmos@umbc.edu,"<div dir=3D""ltr""><div class=3D""gmail_default"" style=3D""font-family:arial,he= lvetica,sans-serif;font-size:small"">Greg</div><div class=3D""gmail_default"" = style=3D""font-family:arial,helvetica,sans-serif;font-size:small"">=C2=A0 =C2= =A0 =C2=A0Please let our group know if the disk migration is completed and = if we need to do something=C2=A0(e.g., new configurations) to use the new d= isk space. Thanks</div><div class=3D""gmail_default"" style=3D""font-family:ar= ial,helvetica,sans-serif;font-size:small"">-Zhibo</div></div><br><div class= =3D""gmail_quote gmail_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr= "">On Fri, Oct 31, 2025 at 10:41=E2=80=AFAM Greg Ballantine via RT &lt;<a hr= ef=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br><= /div><blockquote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;bo= rder-left:1px solid rgb(204,204,204);padding-left:1ex"">Ticket &lt;URL: <a h= ref=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D3287967"" rel=3D""norefer= rer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3287967= </a> &gt;<br> <br> Last Update From Ticket:<br> <br> Good morning Zhibo,<br> <br> This is a reminder that we will be performing your group&#39;s migration to=  the<br> Ceph storage cluster today. During this time, please ensure there are not a= ny<br> jobs being run in your research group, otherwise these may be terminated.<b= r> <br> We will provide an update once completed.<br> <br> Best,<br> Greg<br> <br> On Thu Oct 30 16:22:50 2025, OX20655 wrote:<br> <br> &gt; Hi All This is a reminder that DOIT will help us with the Chip disk<br> &gt; migration tomorrow Oct. 31st. During this period, please do not use th= e<br> &gt; Chip server until we are notified that the migration process is finish= ed.<br> &gt; Thanks -Zhibo On Mon, Oct 27, 2025 at 11:23 AM Zhibo Zhang<br> &gt; &lt;<a href=3D""mailto:zzbatmos@umbc.edu"" target=3D""_blank"">zzbatmos@um= bc.edu</a>&gt; wrote:<br> <br> &gt;&gt; Yes, that will be fine. thanks, Greg On Mon, Oct 27, 2025 at 11:21=  AM<br> &gt;&gt; Greg Ballantine via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu""=  target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.= html?id=3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu= /Ticket/Display.html?id=3D3287967</a> &gt;<br> <br> &gt;&gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt;&gt; Good morning Zhibo,<br> <br> &gt;&gt;&gt; Yes, Friday the 31st will work for starting your migration.<br> &gt;&gt;&gt; However, since your<br> &gt;&gt;&gt; storage volume is currently very large (~482TB as of this morn= ing)<br> &gt;&gt;&gt; there is a<br> &gt;&gt;&gt; good chance that your migration will take longer than the busi= ness<br> &gt;&gt;&gt; day that we<br> &gt;&gt;&gt; typically plan for. We&#39;ve started copying your data to the=  new<br> &gt;&gt;&gt; storage server<br> &gt;&gt;&gt; to minimize downtime, but I would still expect the final migra= tion<br> &gt;&gt;&gt; to run<br> &gt;&gt;&gt; through the weekend and likely into the following week.<br> <br> &gt;&gt;&gt; Will that time frame still work for your group?<br> <br> &gt;&gt;&gt; Thank you,<br> &gt;&gt;&gt; Greg<br> <br> &gt;&gt;&gt; On Wed Oct 22 10:14:02 2025, OX20655 wrote:<br> <br> &gt;&gt;&gt; &gt; Hi Sorry for the late reply. Our group has decided to mak= e the<br> &gt;&gt;&gt; disk<br> &gt;&gt;&gt; &gt; migration on Oct. 31 Friday. Would that work for you? Tha= nks. On<br> &gt;&gt;&gt; Wed, Oct<br> &gt;&gt;&gt; &gt; 8, 2025 at 12:42 PM Greg Ballantine via RT &lt;<a href=3D= ""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt= ;<br> &gt;&gt;&gt; wrote:<br> <br> &gt;&gt;&gt; &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket= /Display.html?id=3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">https://rt= .umbc.edu/Ticket/Display.html?id=3D3287967</a><br> &gt;&gt;&gt; &gt;<br> <br> &gt;&gt;&gt; &gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt;&gt; &gt;&gt; Hello Zhibo,<br> <br> &gt;&gt;&gt; &gt;&gt; Yes, a response by Friday is fine.<br> <br> &gt;&gt;&gt; &gt;&gt; Thank you,<br> &gt;&gt;&gt; &gt;&gt; Greg<br> <br> &gt;&gt;&gt; &gt;&gt; On Tue Oct 07 21:29:13 2025, OX20655 wrote:<br> <br> &gt;&gt;&gt; &gt;&gt; &gt; Hi Elliot We will choose option 1 Schedule a gro= up-wide<br> &gt;&gt;&gt; downtime date<br> &gt;&gt;&gt; &gt;&gt; &gt; during standard business hours,. I need to deter= mine the exact<br> &gt;&gt;&gt; date<br> &gt;&gt;&gt; &gt;&gt; with my<br> &gt;&gt;&gt; &gt;&gt; &gt; team members though. Can I get back to you by Fr= iday?<br> &gt;&gt;&gt; Thanks-Zhibo On<br> &gt;&gt;&gt; &gt;&gt; Tue,<br> &gt;&gt;&gt; &gt;&gt; &gt; Oct 7, 2025 at 3:20 PM via RT &lt;<a href=3D""mai= lto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt; wr= ote:<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Greetings,<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; This message has been automatically generate= d in response to<br> &gt;&gt;&gt; the<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; creation of a ticket regarding:<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; --------------------------------------------------------------= -----------<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Subject: &quot;Migrating Research Storage Vo= lume to Ceph Cluster -<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; pi_zzbatmos&quot;<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Message:<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Dear Zhibo,<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; As per the communication via myUMBC earlier = this summer (June<br> &gt;&gt;&gt; HPCF<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Newsletter<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; ), DoIT is in the process of migrating data = off of an older<br> &gt;&gt;&gt; storage<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; server to<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; our new RRStor Ceph storage cluster. Your gr= oup is using<br> &gt;&gt;&gt; 458.94 TB<br> &gt;&gt;&gt; &gt;&gt; of a<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; 505.0000 TB quota on the old storage server.= <br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; To perform these migrations, we need to take=  individual<br> &gt;&gt;&gt; storage<br> &gt;&gt;&gt; &gt;&gt; volumes<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; offline<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; while we migrate them to the Ceph cluster. T= hus we are<br> &gt;&gt;&gt; reaching out<br> &gt;&gt;&gt; &gt;&gt; to<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; schedule<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; a date where we can migrate your volume loca= ted at<br> &gt;&gt;&gt; &gt;&gt; =E2=80=9C/umbc/rs/zzbatmos=E2=80=9D.<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; During<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; the migration, we will take your volume offl= ine and will<br> &gt;&gt;&gt; terminate<br> &gt;&gt;&gt; &gt;&gt; any<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; jobs<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; running on the chip compute cluster that are=  accessing this<br> &gt;&gt;&gt; volume.<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Below we=E2=80=99ve listed two options for h= andling this data<br> &gt;&gt;&gt; migration -<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; please let us<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; know which of these you=E2=80=99d prefer.<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Option 1: Schedule a group-wide downtime dat= e during standard<br> &gt;&gt;&gt; &gt;&gt; business<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; hours,<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; which can be done by responding to this emai= l with your<br> &gt;&gt;&gt; preferred<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; date(s) to<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; perform the migration. During this time, DoI= T staff will work<br> &gt;&gt;&gt; to<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; migrate your<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; volume to the Ceph storage cluster. DoIT sta= ff will send an<br> &gt;&gt;&gt; email<br> &gt;&gt;&gt; &gt;&gt; alert<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; on this<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; email thread when the migration has begun an= d when it has<br> &gt;&gt;&gt; completed.<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; For most<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; storage volumes, this process should take le= ss than a<br> &gt;&gt;&gt; business day.<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Option 2: If you don=E2=80=99t respond to th= is email by October 15th,<br> &gt;&gt;&gt; DoIT<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; staff will<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; assign a day over the following month (Octob= er 16th through<br> &gt;&gt;&gt; November<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; 15th) to<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; migrate your volume. The day chosen will be = random and will<br> &gt;&gt;&gt; occur<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; during<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; business hours. You will be notified of the = date chosen to<br> &gt;&gt;&gt; perform<br> &gt;&gt;&gt; &gt;&gt; the<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; migration, and will be notified when the mig= ration begins and<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; completes.<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Note: After this process has completed, the = new storage<br> &gt;&gt;&gt; volume will<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; have a new<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; name. For example, group =E2=80=9Cpi_doit=E2= =80=9D will find its data under<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; =E2=80=9C/umbc/rs/pi_doit=E2=80=9D,<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; or in your group=E2=80=99s case you will fin= d your volume under<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; =E2=80=9C/umbc/rs/pi_zzbatmos=E2=80=9D.<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thank you,<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Elliot<br> <br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; --------------------------------------------------------------= -----------<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; There is no need to reply to this message ri= ght now.<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Your ticket has been assigned an ID of [Rese= arch Computing<br> &gt;&gt;&gt; #3287967]<br> &gt;&gt;&gt; &gt;&gt; or<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; you can go there directly by clicking the li= nk below.<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Ticket &lt;URL:<br> &gt;&gt;&gt; <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D328796= 7"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.= html?id=3D3287967</a> &gt;<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; You can login to view your open tickets at a= ny time by<br> &gt;&gt;&gt; visiting<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; <a href=3D""http://my.umbc.edu"" rel=3D""norefe= rrer"" target=3D""_blank"">http://my.umbc.edu</a> and clicking on &quot;Help&q= uot; and &quot;Request Help&quot;.<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Alternately you can click on <a href=3D""http= ://my.umbc.edu/help"" rel=3D""noreferrer"" target=3D""_blank"">http://my.umbc.ed= u/help</a><br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thank you<br> <br> &gt;&gt;&gt; &gt;&gt; --<br> <br> &gt;&gt;&gt; &gt;&gt; Gregory BallantineSystem Administrator for Research a= nd<br> &gt;&gt;&gt; Enterprise<br> &gt;&gt;&gt; &gt;&gt; ComputingUMBC<br> &gt;&gt;&gt; &gt;&gt; - DoIT<br> <br> &gt;&gt;&gt; --<br> <br> &gt;&gt;&gt; Gregory BallantineSystem Administrator for Research and Enterp= rise<br> &gt;&gt;&gt; ComputingUMBC<br> &gt;&gt;&gt; - DoIT<br> <br> --<br> <br> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br> - DoIT<br> <br> </blockquote></div> "
3287967,72658062,Correspond,DoIT-Research-Computing,2025-11-03 14:42:53.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos,open,Greg Ballantine,gballan1,Zhibo Zhang,zzbatmos,zzbatmos@umbc.edu,Zhibo Zhang,zzbatmos@umbc.edu,"Hi Greg,      Would you please give us some updates about the disk migration process? We have a large group of students and postdocs waiting for the use of the server again. Could you please give us an estimate of completion time? Thanks -Zhibo  On Sun, Nov 2, 2025 at 7:44=E2=80=AFPM Zhibo Zhang <zzbatmos@umbc.edu> wrot= e:  > Greg >      Please let our group know if the disk migration is completed and if > we need to do something (e.g., new configurations) to use the new disk > space. Thanks > -Zhibo > > On Fri, Oct 31, 2025 at 10:41=E2=80=AFAM Greg Ballantine via RT < > UMBCHelp@rt.umbc.edu> wrote: > >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > >> >> Last Update From Ticket: >> >> Good morning Zhibo, >> >> This is a reminder that we will be performing your group's migration to >> the >> Ceph storage cluster today. During this time, please ensure there are not >> any >> jobs being run in your research group, otherwise these may be terminated. >> >> We will provide an update once completed. >> >> Best, >> Greg >> >> On Thu Oct 30 16:22:50 2025, OX20655 wrote: >> >> > Hi All This is a reminder that DOIT will help us with the Chip disk >> > migration tomorrow Oct. 31st. During this period, please do not use the >> > Chip server until we are notified that the migration process is >> finished. >> > Thanks -Zhibo On Mon, Oct 27, 2025 at 11:23 AM Zhibo Zhang >> > <zzbatmos@umbc.edu> wrote: >> >> >> Yes, that will be fine. thanks, Greg On Mon, Oct 27, 2025 at 11:21 AM >> >> Greg Ballantine via RT <UMBCHelp@rt.umbc.edu> wrote: >> >> >>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > >> >> >>> Last Update From Ticket: >> >> >>> Good morning Zhibo, >> >> >>> Yes, Friday the 31st will work for starting your migration. >> >>> However, since your >> >>> storage volume is currently very large (~482TB as of this morning) >> >>> there is a >> >>> good chance that your migration will take longer than the business >> >>> day that we >> >>> typically plan for. We've started copying your data to the new >> >>> storage server >> >>> to minimize downtime, but I would still expect the final migration >> >>> to run >> >>> through the weekend and likely into the following week. >> >> >>> Will that time frame still work for your group? >> >> >>> Thank you, >> >>> Greg >> >> >>> On Wed Oct 22 10:14:02 2025, OX20655 wrote: >> >> >>> > Hi Sorry for the late reply. Our group has decided to make the >> >>> disk >> >>> > migration on Oct. 31 Friday. Would that work for you? Thanks. On >> >>> Wed, Oct >> >>> > 8, 2025 at 12:42 PM Greg Ballantine via RT <UMBCHelp@rt.umbc.edu> >> >>> wrote: >> >> >>> >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 >> >>> > >> >> >>> >> Last Update From Ticket: >> >> >>> >> Hello Zhibo, >> >> >>> >> Yes, a response by Friday is fine. >> >> >>> >> Thank you, >> >>> >> Greg >> >> >>> >> On Tue Oct 07 21:29:13 2025, OX20655 wrote: >> >> >>> >> > Hi Elliot We will choose option 1 Schedule a group-wide >> >>> downtime date >> >>> >> > during standard business hours,. I need to determine the exact >> >>> date >> >>> >> with my >> >>> >> > team members though. Can I get back to you by Friday? >> >>> Thanks-Zhibo On >> >>> >> Tue, >> >>> >> > Oct 7, 2025 at 3:20 PM via RT <UMBCHelp@rt.umbc.edu> wrote: >> >> >>> >> >> Greetings, >> >> >>> >> >> This message has been automatically generated in response to >> >>> the >> >>> >> >> creation of a ticket regarding: >> >> >>> >> >> >> >>> >> >> >>> >> ------------------------------------------------------------------------- >> >>> >> >> Subject: ""Migrating Research Storage Volume to Ceph Cluster - >> >>> >> >> pi_zzbatmos"" >> >> >>> >> >> Message: >> >> >>> >> >> Dear Zhibo, >> >> >>> >> >> As per the communication via myUMBC earlier this summer (June >> >>> HPCF >> >>> >> >> Newsletter >> >>> >> >> ), DoIT is in the process of migrating data off of an older >> >>> storage >> >>> >> >> server to >> >>> >> >> our new RRStor Ceph storage cluster. Your group is using >> >>> 458.94 TB >> >>> >> of a >> >>> >> >> 505.0000 TB quota on the old storage server. >> >> >>> >> >> To perform these migrations, we need to take individual >> >>> storage >> >>> >> volumes >> >>> >> >> offline >> >>> >> >> while we migrate them to the Ceph cluster. Thus we are >> >>> reaching out >> >>> >> to >> >>> >> >> schedule >> >>> >> >> a date where we can migrate your volume located at >> >>> >> =E2=80=9C/umbc/rs/zzbatmos=E2=80=9D. >> >>> >> >> During >> >>> >> >> the migration, we will take your volume offline and will >> >>> terminate >> >>> >> any >> >>> >> >> jobs >> >>> >> >> running on the chip compute cluster that are accessing this >> >>> volume. >> >> >>> >> >> Below we=E2=80=99ve listed two options for handling this data >> >>> migration - >> >>> >> >> please let us >> >>> >> >> know which of these you=E2=80=99d prefer. >> >> >>> >> >> Option 1: Schedule a group-wide downtime date during standard >> >>> >> business >> >>> >> >> hours, >> >>> >> >> which can be done by responding to this email with your >> >>> preferred >> >>> >> >> date(s) to >> >>> >> >> perform the migration. During this time, DoIT staff will work >> >>> to >> >>> >> >> migrate your >> >>> >> >> volume to the Ceph storage cluster. DoIT staff will send an >> >>> email >> >>> >> alert >> >>> >> >> on this >> >>> >> >> email thread when the migration has begun and when it has >> >>> completed. >> >>> >> >> For most >> >>> >> >> storage volumes, this process should take less than a >> >>> business day. >> >>> >> >> Option 2: If you don=E2=80=99t respond to this email by Octobe= r 15th, >> >>> DoIT >> >>> >> >> staff will >> >>> >> >> assign a day over the following month (October 16th through >> >>> November >> >>> >> >> 15th) to >> >>> >> >> migrate your volume. The day chosen will be random and will >> >>> occur >> >>> >> >> during >> >>> >> >> business hours. You will be notified of the date chosen to >> >>> perform >> >>> >> the >> >>> >> >> migration, and will be notified when the migration begins and >> >>> >> >> completes. >> >> >>> >> >> Note: After this process has completed, the new storage >> >>> volume will >> >>> >> >> have a new >> >>> >> >> name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find i= ts data under >> >>> >> >> =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, >> >>> >> >> or in your group=E2=80=99s case you will find your volume under >> >>> >> >> =E2=80=9C/umbc/rs/pi_zzbatmos=E2=80=9D. >> >> >>> >> >> Thank you, >> >>> >> >> Elliot >> >> >> >>> >> >> >> >>> >> >> >>> >> ------------------------------------------------------------------------- >> >> >>> >> >> There is no need to reply to this message right now. >> >> >>> >> >> Your ticket has been assigned an ID of [Research Computing >> >>> #3287967] >> >>> >> or >> >>> >> >> you can go there directly by clicking the link below. >> >> >>> >> >> Ticket <URL: >> >>> https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > >> >> >>> >> >> You can login to view your open tickets at any time by >> >>> visiting >> >>> >> >> http://my.umbc.edu and clicking on ""Help"" and ""Request Help"". >> >> >>> >> >> Alternately you can click on http://my.umbc.edu/help >> >> >>> >> >> Thank you >> >> >>> >> -- >> >> >>> >> Gregory BallantineSystem Administrator for Research and >> >>> Enterprise >> >>> >> ComputingUMBC >> >>> >> - DoIT >> >> >>> -- >> >> >>> Gregory BallantineSystem Administrator for Research and Enterprise >> >>> ComputingUMBC >> >>> - DoIT >> >> -- >> >> Gregory BallantineSystem Administrator for Research and Enterprise >> ComputingUMBC >> - DoIT >> >> "
3287967,72658062,Correspond,DoIT-Research-Computing,2025-11-03 14:42:53.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos,open,Greg Ballantine,gballan1,Zhibo Zhang,zzbatmos,zzbatmos@umbc.edu,Zhibo Zhang,zzbatmos@umbc.edu,"<div dir=3D""ltr""><div class=3D""gmail_default"" style=3D""font-family:arial,he= lvetica,sans-serif;font-size:small"">Hi Greg,</div><div class=3D""gmail_defau= lt"" style=3D""font-family:arial,helvetica,sans-serif;font-size:small"">=C2=A0=  =C2=A0 =C2=A0Would you please give us some updates about the disk migratio= n process? We have a large group of students and postdocs=C2=A0waiting=C2= =A0for the use of the server again. Could you please give us an estimate of=  completion=C2=A0time? Thanks</div><div class=3D""gmail_default"" style=3D""fo= nt-family:arial,helvetica,sans-serif;font-size:small"">-Zhibo</div></div><br= ><div class=3D""gmail_quote gmail_quote_container""><div dir=3D""ltr"" class=3D= ""gmail_attr"">On Sun, Nov 2, 2025 at 7:44=E2=80=AFPM Zhibo Zhang &lt;<a href= =3D""mailto:zzbatmos@umbc.edu"">zzbatmos@umbc.edu</a>&gt; wrote:<br></div><bl= ockquote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-lef= t:1px solid rgb(204,204,204);padding-left:1ex""><div dir=3D""ltr""><div class= =3D""gmail_default"" style=3D""font-family:arial,helvetica,sans-serif;font-siz= e:small"">Greg</div><div class=3D""gmail_default"" style=3D""font-family:arial,= helvetica,sans-serif;font-size:small"">=C2=A0 =C2=A0 =C2=A0Please let our gr= oup know if the disk migration is completed and if we need to do something= =C2=A0(e.g., new configurations) to use the new disk space. Thanks</div><di= v class=3D""gmail_default"" style=3D""font-family:arial,helvetica,sans-serif;f= ont-size:small"">-Zhibo</div></div><br><div class=3D""gmail_quote""><div dir= =3D""ltr"" class=3D""gmail_attr"">On Fri, Oct 31, 2025 at 10:41=E2=80=AFAM Greg=  Ballantine via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_b= lank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gma= il_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,2= 04,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ti= cket/Display.html?id=3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">https:= //rt.umbc.edu/Ticket/Display.html?id=3D3287967</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Good morning Zhibo,<br> <br> This is a reminder that we will be performing your group&#39;s migration to=  the<br> Ceph storage cluster today. During this time, please ensure there are not a= ny<br> jobs being run in your research group, otherwise these may be terminated.<b= r> <br> We will provide an update once completed.<br> <br> Best,<br> Greg<br> <br> On Thu Oct 30 16:22:50 2025, OX20655 wrote:<br> <br> &gt; Hi All This is a reminder that DOIT will help us with the Chip disk<br> &gt; migration tomorrow Oct. 31st. During this period, please do not use th= e<br> &gt; Chip server until we are notified that the migration process is finish= ed.<br> &gt; Thanks -Zhibo On Mon, Oct 27, 2025 at 11:23 AM Zhibo Zhang<br> &gt; &lt;<a href=3D""mailto:zzbatmos@umbc.edu"" target=3D""_blank"">zzbatmos@um= bc.edu</a>&gt; wrote:<br> <br> &gt;&gt; Yes, that will be fine. thanks, Greg On Mon, Oct 27, 2025 at 11:21=  AM<br> &gt;&gt; Greg Ballantine via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu""=  target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.= html?id=3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu= /Ticket/Display.html?id=3D3287967</a> &gt;<br> <br> &gt;&gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt;&gt; Good morning Zhibo,<br> <br> &gt;&gt;&gt; Yes, Friday the 31st will work for starting your migration.<br> &gt;&gt;&gt; However, since your<br> &gt;&gt;&gt; storage volume is currently very large (~482TB as of this morn= ing)<br> &gt;&gt;&gt; there is a<br> &gt;&gt;&gt; good chance that your migration will take longer than the busi= ness<br> &gt;&gt;&gt; day that we<br> &gt;&gt;&gt; typically plan for. We&#39;ve started copying your data to the=  new<br> &gt;&gt;&gt; storage server<br> &gt;&gt;&gt; to minimize downtime, but I would still expect the final migra= tion<br> &gt;&gt;&gt; to run<br> &gt;&gt;&gt; through the weekend and likely into the following week.<br> <br> &gt;&gt;&gt; Will that time frame still work for your group?<br> <br> &gt;&gt;&gt; Thank you,<br> &gt;&gt;&gt; Greg<br> <br> &gt;&gt;&gt; On Wed Oct 22 10:14:02 2025, OX20655 wrote:<br> <br> &gt;&gt;&gt; &gt; Hi Sorry for the late reply. Our group has decided to mak= e the<br> &gt;&gt;&gt; disk<br> &gt;&gt;&gt; &gt; migration on Oct. 31 Friday. Would that work for you? Tha= nks. On<br> &gt;&gt;&gt; Wed, Oct<br> &gt;&gt;&gt; &gt; 8, 2025 at 12:42 PM Greg Ballantine via RT &lt;<a href=3D= ""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt= ;<br> &gt;&gt;&gt; wrote:<br> <br> &gt;&gt;&gt; &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket= /Display.html?id=3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">https://rt= .umbc.edu/Ticket/Display.html?id=3D3287967</a><br> &gt;&gt;&gt; &gt;<br> <br> &gt;&gt;&gt; &gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt;&gt; &gt;&gt; Hello Zhibo,<br> <br> &gt;&gt;&gt; &gt;&gt; Yes, a response by Friday is fine.<br> <br> &gt;&gt;&gt; &gt;&gt; Thank you,<br> &gt;&gt;&gt; &gt;&gt; Greg<br> <br> &gt;&gt;&gt; &gt;&gt; On Tue Oct 07 21:29:13 2025, OX20655 wrote:<br> <br> &gt;&gt;&gt; &gt;&gt; &gt; Hi Elliot We will choose option 1 Schedule a gro= up-wide<br> &gt;&gt;&gt; downtime date<br> &gt;&gt;&gt; &gt;&gt; &gt; during standard business hours,. I need to deter= mine the exact<br> &gt;&gt;&gt; date<br> &gt;&gt;&gt; &gt;&gt; with my<br> &gt;&gt;&gt; &gt;&gt; &gt; team members though. Can I get back to you by Fr= iday?<br> &gt;&gt;&gt; Thanks-Zhibo On<br> &gt;&gt;&gt; &gt;&gt; Tue,<br> &gt;&gt;&gt; &gt;&gt; &gt; Oct 7, 2025 at 3:20 PM via RT &lt;<a href=3D""mai= lto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt; wr= ote:<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Greetings,<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; This message has been automatically generate= d in response to<br> &gt;&gt;&gt; the<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; creation of a ticket regarding:<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; --------------------------------------------------------------= -----------<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Subject: &quot;Migrating Research Storage Vo= lume to Ceph Cluster -<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; pi_zzbatmos&quot;<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Message:<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Dear Zhibo,<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; As per the communication via myUMBC earlier = this summer (June<br> &gt;&gt;&gt; HPCF<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Newsletter<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; ), DoIT is in the process of migrating data = off of an older<br> &gt;&gt;&gt; storage<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; server to<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; our new RRStor Ceph storage cluster. Your gr= oup is using<br> &gt;&gt;&gt; 458.94 TB<br> &gt;&gt;&gt; &gt;&gt; of a<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; 505.0000 TB quota on the old storage server.= <br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; To perform these migrations, we need to take=  individual<br> &gt;&gt;&gt; storage<br> &gt;&gt;&gt; &gt;&gt; volumes<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; offline<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; while we migrate them to the Ceph cluster. T= hus we are<br> &gt;&gt;&gt; reaching out<br> &gt;&gt;&gt; &gt;&gt; to<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; schedule<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; a date where we can migrate your volume loca= ted at<br> &gt;&gt;&gt; &gt;&gt; =E2=80=9C/umbc/rs/zzbatmos=E2=80=9D.<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; During<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; the migration, we will take your volume offl= ine and will<br> &gt;&gt;&gt; terminate<br> &gt;&gt;&gt; &gt;&gt; any<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; jobs<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; running on the chip compute cluster that are=  accessing this<br> &gt;&gt;&gt; volume.<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Below we=E2=80=99ve listed two options for h= andling this data<br> &gt;&gt;&gt; migration -<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; please let us<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; know which of these you=E2=80=99d prefer.<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Option 1: Schedule a group-wide downtime dat= e during standard<br> &gt;&gt;&gt; &gt;&gt; business<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; hours,<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; which can be done by responding to this emai= l with your<br> &gt;&gt;&gt; preferred<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; date(s) to<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; perform the migration. During this time, DoI= T staff will work<br> &gt;&gt;&gt; to<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; migrate your<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; volume to the Ceph storage cluster. DoIT sta= ff will send an<br> &gt;&gt;&gt; email<br> &gt;&gt;&gt; &gt;&gt; alert<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; on this<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; email thread when the migration has begun an= d when it has<br> &gt;&gt;&gt; completed.<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; For most<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; storage volumes, this process should take le= ss than a<br> &gt;&gt;&gt; business day.<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Option 2: If you don=E2=80=99t respond to th= is email by October 15th,<br> &gt;&gt;&gt; DoIT<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; staff will<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; assign a day over the following month (Octob= er 16th through<br> &gt;&gt;&gt; November<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; 15th) to<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; migrate your volume. The day chosen will be = random and will<br> &gt;&gt;&gt; occur<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; during<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; business hours. You will be notified of the = date chosen to<br> &gt;&gt;&gt; perform<br> &gt;&gt;&gt; &gt;&gt; the<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; migration, and will be notified when the mig= ration begins and<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; completes.<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Note: After this process has completed, the = new storage<br> &gt;&gt;&gt; volume will<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; have a new<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; name. For example, group =E2=80=9Cpi_doit=E2= =80=9D will find its data under<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; =E2=80=9C/umbc/rs/pi_doit=E2=80=9D,<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; or in your group=E2=80=99s case you will fin= d your volume under<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; =E2=80=9C/umbc/rs/pi_zzbatmos=E2=80=9D.<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thank you,<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Elliot<br> <br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; --------------------------------------------------------------= -----------<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; There is no need to reply to this message ri= ght now.<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Your ticket has been assigned an ID of [Rese= arch Computing<br> &gt;&gt;&gt; #3287967]<br> &gt;&gt;&gt; &gt;&gt; or<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; you can go there directly by clicking the li= nk below.<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Ticket &lt;URL:<br> &gt;&gt;&gt; <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D328796= 7"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.= html?id=3D3287967</a> &gt;<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; You can login to view your open tickets at a= ny time by<br> &gt;&gt;&gt; visiting<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; <a href=3D""http://my.umbc.edu"" rel=3D""norefe= rrer"" target=3D""_blank"">http://my.umbc.edu</a> and clicking on &quot;Help&q= uot; and &quot;Request Help&quot;.<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Alternately you can click on <a href=3D""http= ://my.umbc.edu/help"" rel=3D""noreferrer"" target=3D""_blank"">http://my.umbc.ed= u/help</a><br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thank you<br> <br> &gt;&gt;&gt; &gt;&gt; --<br> <br> &gt;&gt;&gt; &gt;&gt; Gregory BallantineSystem Administrator for Research a= nd<br> &gt;&gt;&gt; Enterprise<br> &gt;&gt;&gt; &gt;&gt; ComputingUMBC<br> &gt;&gt;&gt; &gt;&gt; - DoIT<br> <br> &gt;&gt;&gt; --<br> <br> &gt;&gt;&gt; Gregory BallantineSystem Administrator for Research and Enterp= rise<br> &gt;&gt;&gt; ComputingUMBC<br> &gt;&gt;&gt; - DoIT<br> <br> --<br> <br> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br> - DoIT<br> <br> </blockquote></div> </blockquote></div> "
3287967,72659475,Correspond,DoIT-Research-Computing,2025-11-03 15:00:27.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos,open,Greg Ballantine,gballan1,Zhibo Zhang,zzbatmos,zzbatmos@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Good morning Zhibo,<br /> <br /> Your group&#39;s migration is still copying data to the new Ceph storage cl= uster. I don&#39;t currently have a good estimate for completion, but it lo= oks like it&#39;s currently churning through the user directories, and ever= ything else has been copied.<br /> <br /> When it is completed, your data will be available under /umbc/rs/pi_zzbatmo= s, otherwise there aren&#39;t any other changes you&#39;ll need to make on = your end.<br /> <br /> I apologize for the lack of communication on this matter Friday evening and=  over the weekend. I will send you an update later today with the current s= tatus.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Sun Nov 02 19:44:36 2025, OX20655 wrote:</p>  <blockquote> <div> <div>Greg</div>  <div>&nbsp; &nbsp; &nbsp;Please let our group know if the disk migration is=  completed and if we need to do something&nbsp;(e.g., new configurations) t= o use the new disk space. Thanks</div>  <div>-Zhibo</div> </div> &nbsp;  <div> <div>On Fri, Oct 31, 2025 at 10:41=E2=80=AFAM Greg Ballantine via RT &lt;UM= BCHelp@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 87967 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Good morning Zhibo,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to=  the<br /> Ceph storage cluster today. During this time, please ensure there are not a= ny<br /> jobs being run in your research group, otherwise these may be terminated.<b= r /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg<br /> <br /> On Thu Oct 30 16:22:50 2025, OX20655 wrote:<br /> <br /> &gt; Hi All This is a reminder that DOIT will help us with the Chip disk<br=  /> &gt; migration tomorrow Oct. 31st. During this period, please do not use th= e<br /> &gt; Chip server until we are notified that the migration process is finish= ed.<br /> &gt; Thanks -Zhibo On Mon, Oct 27, 2025 at 11:23 AM Zhibo Zhang<br /> &gt; &lt;zzbatmos@umbc.edu&gt; wrote:<br /> <br /> &gt;&gt; Yes, that will be fine. thanks, Greg On Mon, Oct 27, 2025 at 11:21=  AM<br /> &gt;&gt; Greg Ballantine via RT &lt;UMBCHelp@rt.umbc.edu&gt; wrote:<br /> <br /> &gt;&gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3= 287967 &gt;<br /> <br /> &gt;&gt;&gt; Last Update From Ticket:<br /> <br /> &gt;&gt;&gt; Good morning Zhibo,<br /> <br /> &gt;&gt;&gt; Yes, Friday the 31st will work for starting your migration.<br=  /> &gt;&gt;&gt; However, since your<br /> &gt;&gt;&gt; storage volume is currently very large (~482TB as of this morn= ing)<br /> &gt;&gt;&gt; there is a<br /> &gt;&gt;&gt; good chance that your migration will take longer than the busi= ness<br /> &gt;&gt;&gt; day that we<br /> &gt;&gt;&gt; typically plan for. We&#39;ve started copying your data to the=  new<br /> &gt;&gt;&gt; storage server<br /> &gt;&gt;&gt; to minimize downtime, but I would still expect the final migra= tion<br /> &gt;&gt;&gt; to run<br /> &gt;&gt;&gt; through the weekend and likely into the following week.<br /> <br /> &gt;&gt;&gt; Will that time frame still work for your group?<br /> <br /> &gt;&gt;&gt; Thank you,<br /> &gt;&gt;&gt; Greg<br /> <br /> &gt;&gt;&gt; On Wed Oct 22 10:14:02 2025, OX20655 wrote:<br /> <br /> &gt;&gt;&gt; &gt; Hi Sorry for the late reply. Our group has decided to mak= e the<br /> &gt;&gt;&gt; disk<br /> &gt;&gt;&gt; &gt; migration on Oct. 31 Friday. Would that work for you? Tha= nks. On<br /> &gt;&gt;&gt; Wed, Oct<br /> &gt;&gt;&gt; &gt; 8, 2025 at 12:42 PM Greg Ballantine via RT &lt;UMBCHelp@r= t.umbc.edu&gt;<br /> &gt;&gt;&gt; wrote:<br /> <br /> &gt;&gt;&gt; &gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.ht= ml?id=3D3287967<br /> &gt;&gt;&gt; &gt;<br /> <br /> &gt;&gt;&gt; &gt;&gt; Last Update From Ticket:<br /> <br /> &gt;&gt;&gt; &gt;&gt; Hello Zhibo,<br /> <br /> &gt;&gt;&gt; &gt;&gt; Yes, a response by Friday is fine.<br /> <br /> &gt;&gt;&gt; &gt;&gt; Thank you,<br /> &gt;&gt;&gt; &gt;&gt; Greg<br /> <br /> &gt;&gt;&gt; &gt;&gt; On Tue Oct 07 21:29:13 2025, OX20655 wrote:<br /> <br /> &gt;&gt;&gt; &gt;&gt; &gt; Hi Elliot We will choose option 1 Schedule a gro= up-wide<br /> &gt;&gt;&gt; downtime date<br /> &gt;&gt;&gt; &gt;&gt; &gt; during standard business hours,. I need to deter= mine the exact<br /> &gt;&gt;&gt; date<br /> &gt;&gt;&gt; &gt;&gt; with my<br /> &gt;&gt;&gt; &gt;&gt; &gt; team members though. Can I get back to you by Fr= iday?<br /> &gt;&gt;&gt; Thanks-Zhibo On<br /> &gt;&gt;&gt; &gt;&gt; Tue,<br /> &gt;&gt;&gt; &gt;&gt; &gt; Oct 7, 2025 at 3:20 PM via RT &lt;UMBCHelp@rt.um= bc.edu&gt; wrote:<br /> <br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Greetings,<br /> <br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; This message has been automatically generate= d in response to<br /> &gt;&gt;&gt; the<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; creation of a ticket regarding:<br /> <br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; --------------------------------------------------------------= -----------<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Subject: &quot;Migrating Research Storage Vo= lume to Ceph Cluster -<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; pi_zzbatmos&quot;<br /> <br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Message:<br /> <br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Dear Zhibo,<br /> <br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; As per the communication via myUMBC earlier = this summer (June<br /> &gt;&gt;&gt; HPCF<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Newsletter<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; ), DoIT is in the process of migrating data = off of an older<br /> &gt;&gt;&gt; storage<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; server to<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; our new RRStor Ceph storage cluster. Your gr= oup is using<br /> &gt;&gt;&gt; 458.94 TB<br /> &gt;&gt;&gt; &gt;&gt; of a<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; 505.0000 TB quota on the old storage server.= <br /> <br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; To perform these migrations, we need to take=  individual<br /> &gt;&gt;&gt; storage<br /> &gt;&gt;&gt; &gt;&gt; volumes<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; offline<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; while we migrate them to the Ceph cluster. T= hus we are<br /> &gt;&gt;&gt; reaching out<br /> &gt;&gt;&gt; &gt;&gt; to<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; schedule<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; a date where we can migrate your volume loca= ted at<br /> &gt;&gt;&gt; &gt;&gt; &ldquo;/umbc/rs/zzbatmos&rdquo;.<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; During<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; the migration, we will take your volume offl= ine and will<br /> &gt;&gt;&gt; terminate<br /> &gt;&gt;&gt; &gt;&gt; any<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; jobs<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; running on the chip compute cluster that are=  accessing this<br /> &gt;&gt;&gt; volume.<br /> <br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Below we&rsquo;ve listed two options for han= dling this data<br /> &gt;&gt;&gt; migration -<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; please let us<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; know which of these you&rsquo;d prefer.<br /> <br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Option 1: Schedule a group-wide downtime dat= e during standard<br /> &gt;&gt;&gt; &gt;&gt; business<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; hours,<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; which can be done by responding to this emai= l with your<br /> &gt;&gt;&gt; preferred<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; date(s) to<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; perform the migration. During this time, DoI= T staff will work<br /> &gt;&gt;&gt; to<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; migrate your<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; volume to the Ceph storage cluster. DoIT sta= ff will send an<br /> &gt;&gt;&gt; email<br /> &gt;&gt;&gt; &gt;&gt; alert<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; on this<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; email thread when the migration has begun an= d when it has<br /> &gt;&gt;&gt; completed.<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; For most<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; storage volumes, this process should take le= ss than a<br /> &gt;&gt;&gt; business day.<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Option 2: If you don&rsquo;t respond to this=  email by October 15th,<br /> &gt;&gt;&gt; DoIT<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; staff will<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; assign a day over the following month (Octob= er 16th through<br /> &gt;&gt;&gt; November<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; 15th) to<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; migrate your volume. The day chosen will be = random and will<br /> &gt;&gt;&gt; occur<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; during<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; business hours. You will be notified of the = date chosen to<br /> &gt;&gt;&gt; perform<br /> &gt;&gt;&gt; &gt;&gt; the<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; migration, and will be notified when the mig= ration begins and<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; completes.<br /> <br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Note: After this process has completed, the = new storage<br /> &gt;&gt;&gt; volume will<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; have a new<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; name. For example, group &ldquo;pi_doit&rdqu= o; will find its data under<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &ldquo;/umbc/rs/pi_doit&rdquo;,<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; or in your group&rsquo;s case you will find = your volume under<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &ldquo;/umbc/rs/pi_zzbatmos&rdquo;.<br /> <br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thank you,<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Elliot<br /> <br /> <br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; --------------------------------------------------------------= -----------<br /> <br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; There is no need to reply to this message ri= ght now.<br /> <br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Your ticket has been assigned an ID of [Rese= arch Computing<br /> &gt;&gt;&gt; #3287967]<br /> &gt;&gt;&gt; &gt;&gt; or<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; you can go there directly by clicking the li= nk below.<br /> <br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Ticket &lt;URL:<br /> &gt;&gt;&gt; https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 &gt;<br /> <br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; You can login to view your open tickets at a= ny time by<br /> &gt;&gt;&gt; visiting<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; http://my.umbc.edu and clicking on &quot;Hel= p&quot; and &quot;Request Help&quot;.<br /> <br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Alternately you can click on http://my.umbc.= edu/help<br /> <br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thank you<br /> <br /> &gt;&gt;&gt; &gt;&gt; --<br /> <br /> &gt;&gt;&gt; &gt;&gt; Gregory BallantineSystem Administrator for Research a= nd<br /> &gt;&gt;&gt; Enterprise<br /> &gt;&gt;&gt; &gt;&gt; ComputingUMBC<br /> &gt;&gt;&gt; &gt;&gt; - DoIT<br /> <br /> &gt;&gt;&gt; --<br /> <br /> &gt;&gt;&gt; Gregory BallantineSystem Administrator for Research and Enterp= rise<br /> &gt;&gt;&gt; ComputingUMBC<br /> &gt;&gt;&gt; - DoIT<br /> <br /> --<br /> <br /> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br /> - DoIT<br /> &nbsp;</blockquote> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3287967,72660799,Correspond,DoIT-Research-Computing,2025-11-03 15:22:02.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos,open,Greg Ballantine,gballan1,Zhibo Zhang,zzbatmos,zzbatmos@umbc.edu,Zhibo Zhang,zzbatmos@umbc.edu,"Ok, Thanks for the update. Please keep us updated. -Zhibo  On Mon, Nov 3, 2025 at 10:00=E2=80=AFAM Greg Ballantine via RT <UMBCHelp@rt= .umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > > > Last Update From Ticket: > > Good morning Zhibo, > > Your group's migration is still copying data to the new Ceph storage > cluster. I > don't currently have a good estimate for completion, but it looks like it= 's > currently churning through the user directories, and everything else has > been > copied. > > When it is completed, your data will be available under > /umbc/rs/pi_zzbatmos, > otherwise there aren't any other changes you'll need to make on your end. > > I apologize for the lack of communication on this matter Friday evening a= nd > over the weekend. I will send you an update later today with the current > status. > > Best, > Greg > > On Sun Nov 02 19:44:36 2025, OX20655 wrote: > > > Greg Please let our group know if the disk migration is completed and if > we > > need to do something (e.g., new configurations) to use the new disk > space. > > Thanks-Zhibo On Fri, Oct 31, 2025 at 10:41 AM Greg Ballantine via RT > > <UMBCHelp@rt.umbc.edu> wrote: > > >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > > > >> Last Update From Ticket: > > >> Good morning Zhibo, > > >> This is a reminder that we will be performing your group's migration to > >> the > >> Ceph storage cluster today. During this time, please ensure there are > >> not any > >> jobs being run in your research group, otherwise these may be > >> terminated. > > >> We will provide an update once completed. > > >> Best, > >> Greg > > >> On Thu Oct 30 16:22:50 2025, OX20655 wrote: > > >> > Hi All This is a reminder that DOIT will help us with the Chip disk > >> > migration tomorrow Oct. 31st. During this period, please do not use > >> the > >> > Chip server until we are notified that the migration process is > >> finished. > >> > Thanks -Zhibo On Mon, Oct 27, 2025 at 11:23 AM Zhibo Zhang > >> > <zzbatmos@umbc.edu> wrote: > > >> >> Yes, that will be fine. thanks, Greg On Mon, Oct 27, 2025 at 11:21 > >> AM > >> >> Greg Ballantine via RT <UMBCHelp@rt.umbc.edu> wrote: > > >> >>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > > > >> >>> Last Update From Ticket: > > >> >>> Good morning Zhibo, > > >> >>> Yes, Friday the 31st will work for starting your migration. > >> >>> However, since your > >> >>> storage volume is currently very large (~482TB as of this morning) > >> >>> there is a > >> >>> good chance that your migration will take longer than the business > >> >>> day that we > >> >>> typically plan for. We've started copying your data to the new > >> >>> storage server > >> >>> to minimize downtime, but I would still expect the final migration > >> >>> to run > >> >>> through the weekend and likely into the following week. > > >> >>> Will that time frame still work for your group? > > >> >>> Thank you, > >> >>> Greg > > >> >>> On Wed Oct 22 10:14:02 2025, OX20655 wrote: > > >> >>> > Hi Sorry for the late reply. Our group has decided to make the > >> >>> disk > >> >>> > migration on Oct. 31 Friday. Would that work for you? Thanks. On > >> >>> Wed, Oct > >> >>> > 8, 2025 at 12:42 PM Greg Ballantine via RT <UMBCHelp@rt.umbc.edu> > >> >>> wrote: > > >> >>> >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32879= 67 > >> >>> > > > >> >>> >> Last Update From Ticket: > > >> >>> >> Hello Zhibo, > > >> >>> >> Yes, a response by Friday is fine. > > >> >>> >> Thank you, > >> >>> >> Greg > > >> >>> >> On Tue Oct 07 21:29:13 2025, OX20655 wrote: > > >> >>> >> > Hi Elliot We will choose option 1 Schedule a group-wide > >> >>> downtime date > >> >>> >> > during standard business hours,. I need to determine the exact > >> >>> date > >> >>> >> with my > >> >>> >> > team members though. Can I get back to you by Friday? > >> >>> Thanks-Zhibo On > >> >>> >> Tue, > >> >>> >> > Oct 7, 2025 at 3:20 PM via RT <UMBCHelp@rt.umbc.edu> wrote: > > >> >>> >> >> Greetings, > > >> >>> >> >> This message has been automatically generated in response to > >> >>> the > >> >>> >> >> creation of a ticket regarding: > > >> >>> >> >> > >> >>> >> > >> >>> > >> > ------------------------------------------------------------------------- > >> >>> >> >> Subject: ""Migrating Research Storage Volume to Ceph Cluster - > >> >>> >> >> pi_zzbatmos"" > > >> >>> >> >> Message: > > >> >>> >> >> Dear Zhibo, > > >> >>> >> >> As per the communication via myUMBC earlier this summer (June > >> >>> HPCF > >> >>> >> >> Newsletter > >> >>> >> >> ), DoIT is in the process of migrating data off of an older > >> >>> storage > >> >>> >> >> server to > >> >>> >> >> our new RRStor Ceph storage cluster. Your group is using > >> >>> 458.94 TB > >> >>> >> of a > >> >>> >> >> 505.0000 TB quota on the old storage server. > > >> >>> >> >> To perform these migrations, we need to take individual > >> >>> storage > >> >>> >> volumes > >> >>> >> >> offline > >> >>> >> >> while we migrate them to the Ceph cluster. Thus we are > >> >>> reaching out > >> >>> >> to > >> >>> >> >> schedule > >> >>> >> >> a date where we can migrate your volume located at > >> >>> >> =E2=80=9C/umbc/rs/zzbatmos=E2=80=9D. > >> >>> >> >> During > >> >>> >> >> the migration, we will take your volume offline and will > >> >>> terminate > >> >>> >> any > >> >>> >> >> jobs > >> >>> >> >> running on the chip compute cluster that are accessing this > >> >>> volume. > > >> >>> >> >> Below we=E2=80=99ve listed two options for handling this data > >> >>> migration - > >> >>> >> >> please let us > >> >>> >> >> know which of these you=E2=80=99d prefer. > > >> >>> >> >> Option 1: Schedule a group-wide downtime date during standard > >> >>> >> business > >> >>> >> >> hours, > >> >>> >> >> which can be done by responding to this email with your > >> >>> preferred > >> >>> >> >> date(s) to > >> >>> >> >> perform the migration. During this time, DoIT staff will work > >> >>> to > >> >>> >> >> migrate your > >> >>> >> >> volume to the Ceph storage cluster. DoIT staff will send an > >> >>> email > >> >>> >> alert > >> >>> >> >> on this > >> >>> >> >> email thread when the migration has begun and when it has > >> >>> completed. > >> >>> >> >> For most > >> >>> >> >> storage volumes, this process should take less than a > >> >>> business day. > >> >>> >> >> Option 2: If you don=E2=80=99t respond to this email by Octo= ber 15th, > >> >>> DoIT > >> >>> >> >> staff will > >> >>> >> >> assign a day over the following month (October 16th through > >> >>> November > >> >>> >> >> 15th) to > >> >>> >> >> migrate your volume. The day chosen will be random and will > >> >>> occur > >> >>> >> >> during > >> >>> >> >> business hours. You will be notified of the date chosen to > >> >>> perform > >> >>> >> the > >> >>> >> >> migration, and will be notified when the migration begins and > >> >>> >> >> completes. > > >> >>> >> >> Note: After this process has completed, the new storage > >> >>> volume will > >> >>> >> >> have a new > >> >>> >> >> name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find=  its data under > >> >>> >> >> =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, > >> >>> >> >> or in your group=E2=80=99s case you will find your volume un= der > >> >>> >> >> =E2=80=9C/umbc/rs/pi_zzbatmos=E2=80=9D. > > >> >>> >> >> Thank you, > >> >>> >> >> Elliot > > > >> >>> >> >> > >> >>> >> > >> >>> > >> > ------------------------------------------------------------------------- > > >> >>> >> >> There is no need to reply to this message right now. > > >> >>> >> >> Your ticket has been assigned an ID of [Research Computing > >> >>> #3287967] > >> >>> >> or > >> >>> >> >> you can go there directly by clicking the link below. > > >> >>> >> >> Ticket <URL: > >> >>> https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > > > >> >>> >> >> You can login to view your open tickets at any time by > >> >>> visiting > >> >>> >> >> http://my.umbc.edu and clicking on ""Help"" and ""Request Help"". > > >> >>> >> >> Alternately you can click on http://my.umbc.edu/help > > >> >>> >> >> Thank you > > >> >>> >> -- > > >> >>> >> Gregory BallantineSystem Administrator for Research and > >> >>> Enterprise > >> >>> >> ComputingUMBC > >> >>> >> - DoIT > > >> >>> -- > > >> >>> Gregory BallantineSystem Administrator for Research and Enterprise > >> >>> ComputingUMBC > >> >>> - DoIT > > >> -- > > >> Gregory BallantineSystem Administrator for Research and Enterprise > >> ComputingUMBC > >> - DoIT > > -- > > Gregory BallantineSystem Administrator for Research and Enterprise > ComputingUMBC > - DoIT > > "
3287967,72660799,Correspond,DoIT-Research-Computing,2025-11-03 15:22:02.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos,open,Greg Ballantine,gballan1,Zhibo Zhang,zzbatmos,zzbatmos@umbc.edu,Zhibo Zhang,zzbatmos@umbc.edu,"<div dir=3D""ltr""><div class=3D""gmail_default"" style=3D""font-family:arial,he= lvetica,sans-serif;font-size:small"">Ok, Thanks for the update. Please keep = us updated. -Zhibo</div></div><br><div class=3D""gmail_quote gmail_quote_con= tainer""><div dir=3D""ltr"" class=3D""gmail_attr"">On Mon, Nov 3, 2025 at 10:00= =E2=80=AFAM Greg Ballantine via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.e= du"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail= _quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204= ,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Tick= et/Display.html?id=3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">https://= rt.umbc.edu/Ticket/Display.html?id=3D3287967</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Good morning Zhibo,<br> <br> Your group&#39;s migration is still copying data to the new Ceph storage cl= uster. I<br> don&#39;t currently have a good estimate for completion, but it looks like = it&#39;s<br> currently churning through the user directories, and everything else has be= en<br> copied.<br> <br> When it is completed, your data will be available under /umbc/rs/pi_zzbatmo= s,<br> otherwise there aren&#39;t any other changes you&#39;ll need to make on you= r end.<br> <br> I apologize for the lack of communication on this matter Friday evening and= <br> over the weekend. I will send you an update later today with the current<br> status.<br> <br> Best,<br> Greg<br> <br> On Sun Nov 02 19:44:36 2025, OX20655 wrote:<br> <br> &gt; Greg Please let our group know if the disk migration is completed and = if we<br> &gt; need to do something (e.g., new configurations) to use the new disk sp= ace.<br> &gt; Thanks-Zhibo On Fri, Oct 31, 2025 at 10:41 AM Greg Ballantine via RT<b= r> &gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp= @rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html= ?id=3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tic= ket/Display.html?id=3D3287967</a> &gt;<br> <br> &gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt; Good morning Zhibo,<br> <br> &gt;&gt; This is a reminder that we will be performing your group&#39;s mig= ration to<br> &gt;&gt; the<br> &gt;&gt; Ceph storage cluster today. During this time, please ensure there = are<br> &gt;&gt; not any<br> &gt;&gt; jobs being run in your research group, otherwise these may be<br> &gt;&gt; terminated.<br> <br> &gt;&gt; We will provide an update once completed.<br> <br> &gt;&gt; Best,<br> &gt;&gt; Greg<br> <br> &gt;&gt; On Thu Oct 30 16:22:50 2025, OX20655 wrote:<br> <br> &gt;&gt; &gt; Hi All This is a reminder that DOIT will help us with the Chi= p disk<br> &gt;&gt; &gt; migration tomorrow Oct. 31st. During this period, please do n= ot use<br> &gt;&gt; the<br> &gt;&gt; &gt; Chip server until we are notified that the migration process = is<br> &gt;&gt; finished.<br> &gt;&gt; &gt; Thanks -Zhibo On Mon, Oct 27, 2025 at 11:23 AM Zhibo Zhang<br> &gt;&gt; &gt; &lt;<a href=3D""mailto:zzbatmos@umbc.edu"" target=3D""_blank"">zz= batmos@umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; &gt;&gt; Yes, that will be fine. thanks, Greg On Mon, Oct 27, 2025=  at 11:21<br> &gt;&gt; AM<br> &gt;&gt; &gt;&gt; Greg Ballantine via RT &lt;<a href=3D""mailto:UMBCHelp@rt.= umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; &gt;&gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket= /Display.html?id=3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">https://rt= .umbc.edu/Ticket/Display.html?id=3D3287967</a> &gt;<br> <br> &gt;&gt; &gt;&gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt; &gt;&gt;&gt; Good morning Zhibo,<br> <br> &gt;&gt; &gt;&gt;&gt; Yes, Friday the 31st will work for starting your migr= ation.<br> &gt;&gt; &gt;&gt;&gt; However, since your<br> &gt;&gt; &gt;&gt;&gt; storage volume is currently very large (~482TB as of = this morning)<br> &gt;&gt; &gt;&gt;&gt; there is a<br> &gt;&gt; &gt;&gt;&gt; good chance that your migration will take longer than=  the business<br> &gt;&gt; &gt;&gt;&gt; day that we<br> &gt;&gt; &gt;&gt;&gt; typically plan for. We&#39;ve started copying your da= ta to the new<br> &gt;&gt; &gt;&gt;&gt; storage server<br> &gt;&gt; &gt;&gt;&gt; to minimize downtime, but I would still expect the fi= nal migration<br> &gt;&gt; &gt;&gt;&gt; to run<br> &gt;&gt; &gt;&gt;&gt; through the weekend and likely into the following wee= k.<br> <br> &gt;&gt; &gt;&gt;&gt; Will that time frame still work for your group?<br> <br> &gt;&gt; &gt;&gt;&gt; Thank you,<br> &gt;&gt; &gt;&gt;&gt; Greg<br> <br> &gt;&gt; &gt;&gt;&gt; On Wed Oct 22 10:14:02 2025, OX20655 wrote:<br> <br> &gt;&gt; &gt;&gt;&gt; &gt; Hi Sorry for the late reply. Our group has decid= ed to make the<br> &gt;&gt; &gt;&gt;&gt; disk<br> &gt;&gt; &gt;&gt;&gt; &gt; migration on Oct. 31 Friday. Would that work for=  you? Thanks. On<br> &gt;&gt; &gt;&gt;&gt; Wed, Oct<br> &gt;&gt; &gt;&gt;&gt; &gt; 8, 2025 at 12:42 PM Greg Ballantine via RT &lt;<= a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.e= du</a>&gt;<br> &gt;&gt; &gt;&gt;&gt; wrote:<br> <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.e= du/Ticket/Display.html?id=3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">h= ttps://rt.umbc.edu/Ticket/Display.html?id=3D3287967</a><br> &gt;&gt; &gt;&gt;&gt; &gt;<br> <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; Hello Zhibo,<br> <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; Yes, a response by Friday is fine.<br> <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; Thank you,<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; Greg<br> <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; On Tue Oct 07 21:29:13 2025, OX20655 wrote:<= br> <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Hi Elliot We will choose option 1 Sched= ule a group-wide<br> &gt;&gt; &gt;&gt;&gt; downtime date<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; during standard business hours,. I need=  to determine the exact<br> &gt;&gt; &gt;&gt;&gt; date<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; with my<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; team members though. Can I get back to = you by Friday?<br> &gt;&gt; &gt;&gt;&gt; Thanks-Zhibo On<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; Tue,<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Oct 7, 2025 at 3:20 PM via RT &lt;<a hr= ef=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</= a>&gt; wrote:<br> <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Greetings,<br> <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; This message has been automatically=  generated in response to<br> &gt;&gt; &gt;&gt;&gt; the<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; creation of a ticket regarding:<br> <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt;<br> &gt;&gt; ------------------------------------------------------------------= -------<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Subject: &quot;Migrating Research S= torage Volume to Ceph Cluster -<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; pi_zzbatmos&quot;<br> <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Message:<br> <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Dear Zhibo,<br> <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; As per the communication via myUMBC=  earlier this summer (June<br> &gt;&gt; &gt;&gt;&gt; HPCF<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Newsletter<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; ), DoIT is in the process of migrat= ing data off of an older<br> &gt;&gt; &gt;&gt;&gt; storage<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; server to<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; our new RRStor Ceph storage cluster= . Your group is using<br> &gt;&gt; &gt;&gt;&gt; 458.94 TB<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; of a<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; 505.0000 TB quota on the old storag= e server.<br> <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; To perform these migrations, we nee= d to take individual<br> &gt;&gt; &gt;&gt;&gt; storage<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; volumes<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; offline<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; while we migrate them to the Ceph c= luster. Thus we are<br> &gt;&gt; &gt;&gt;&gt; reaching out<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; to<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; schedule<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; a date where we can migrate your vo= lume located at<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; =E2=80=9C/umbc/rs/zzbatmos=E2=80=9D.<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; During<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; the migration, we will take your vo= lume offline and will<br> &gt;&gt; &gt;&gt;&gt; terminate<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; any<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; jobs<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; running on the chip compute cluster=  that are accessing this<br> &gt;&gt; &gt;&gt;&gt; volume.<br> <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Below we=E2=80=99ve listed two opti= ons for handling this data<br> &gt;&gt; &gt;&gt;&gt; migration -<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; please let us<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; know which of these you=E2=80=99d p= refer.<br> <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Option 1: Schedule a group-wide dow= ntime date during standard<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; business<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; hours,<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; which can be done by responding to = this email with your<br> &gt;&gt; &gt;&gt;&gt; preferred<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; date(s) to<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; perform the migration. During this = time, DoIT staff will work<br> &gt;&gt; &gt;&gt;&gt; to<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; migrate your<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; volume to the Ceph storage cluster.=  DoIT staff will send an<br> &gt;&gt; &gt;&gt;&gt; email<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; alert<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; on this<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; email thread when the migration has=  begun and when it has<br> &gt;&gt; &gt;&gt;&gt; completed.<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; For most<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; storage volumes, this process shoul= d take less than a<br> &gt;&gt; &gt;&gt;&gt; business day.<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Option 2: If you don=E2=80=99t resp= ond to this email by October 15th,<br> &gt;&gt; &gt;&gt;&gt; DoIT<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; staff will<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; assign a day over the following mon= th (October 16th through<br> &gt;&gt; &gt;&gt;&gt; November<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; 15th) to<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; migrate your volume. The day chosen=  will be random and will<br> &gt;&gt; &gt;&gt;&gt; occur<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; during<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; business hours. You will be notifie= d of the date chosen to<br> &gt;&gt; &gt;&gt;&gt; perform<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; the<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; migration, and will be notified whe= n the migration begins and<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; completes.<br> <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Note: After this process has comple= ted, the new storage<br> &gt;&gt; &gt;&gt;&gt; volume will<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; have a new<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; name. For example, group =E2=80=9Cp= i_doit=E2=80=9D will find its data under<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; =E2=80=9C/umbc/rs/pi_doit=E2=80=9D,= <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; or in your group=E2=80=99s case you=  will find your volume under<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; =E2=80=9C/umbc/rs/pi_zzbatmos=E2=80= =9D.<br> <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thank you,<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Elliot<br> <br> <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt;<br> &gt;&gt; ------------------------------------------------------------------= -------<br> <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; There is no need to reply to this m= essage right now.<br> <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Your ticket has been assigned an ID=  of [Research Computing<br> &gt;&gt; &gt;&gt;&gt; #3287967]<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; or<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; you can go there directly by clicki= ng the link below.<br> <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Ticket &lt;URL:<br> &gt;&gt; &gt;&gt;&gt; <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id= =3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket= /Display.html?id=3D3287967</a> &gt;<br> <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; You can login to view your open tic= kets at any time by<br> &gt;&gt; &gt;&gt;&gt; visiting<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; <a href=3D""http://my.umbc.edu"" rel= =3D""noreferrer"" target=3D""_blank"">http://my.umbc.edu</a> and clicking on &q= uot;Help&quot; and &quot;Request Help&quot;.<br> <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Alternately you can click on <a hre= f=3D""http://my.umbc.edu/help"" rel=3D""noreferrer"" target=3D""_blank"">http://m= y.umbc.edu/help</a><br> <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thank you<br> <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; --<br> <br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; Gregory BallantineSystem Administrator for R= esearch and<br> &gt;&gt; &gt;&gt;&gt; Enterprise<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; ComputingUMBC<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; - DoIT<br> <br> &gt;&gt; &gt;&gt;&gt; --<br> <br> &gt;&gt; &gt;&gt;&gt; Gregory BallantineSystem Administrator for Research a= nd Enterprise<br> &gt;&gt; &gt;&gt;&gt; ComputingUMBC<br> &gt;&gt; &gt;&gt;&gt; - DoIT<br> <br> &gt;&gt; --<br> <br> &gt;&gt; Gregory BallantineSystem Administrator for Research and Enterprise= <br> &gt;&gt; ComputingUMBC<br> &gt;&gt; - DoIT<br> <br> --<br> <br> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br> - DoIT<br> <br> </blockquote></div> "
3287967,72697138,Correspond,DoIT-Research-Computing,2025-11-03 21:39:40.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos,open,Greg Ballantine,gballan1,Zhibo Zhang,zzbatmos,zzbatmos@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Zhibo,<br /> <br /> Just to give you an update: the migration is still going, and my best guess=  currently is it will take another 2-3 days to finish.&nbsp;<br /> <br /> I&#39;ll leave it running and will send you an update tomorrow morning with=  where it&#39;s at.<br /> <br /> Best,</p>  <p>Greg</p>  <p>&nbsp;</p>  <p>On Mon Nov 03 10:22:02 2025, OX20655 wrote:</p>  <blockquote> <div> <div>Ok, Thanks for the update. Please keep us updated. -Zhibo</div> </div> &nbsp;  <div> <div>On Mon, Nov 3, 2025 at 10:00=E2=80=AFAM Greg Ballantine via RT &lt;UMB= CHelp@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 87967 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Good morning Zhibo,<br /> <br /> Your group&#39;s migration is still copying data to the new Ceph storage cl= uster. I<br /> don&#39;t currently have a good estimate for completion, but it looks like = it&#39;s<br /> currently churning through the user directories, and everything else has be= en<br /> copied.<br /> <br /> When it is completed, your data will be available under /umbc/rs/pi_zzbatmo= s,<br /> otherwise there aren&#39;t any other changes you&#39;ll need to make on you= r end.<br /> <br /> I apologize for the lack of communication on this matter Friday evening and= <br /> over the weekend. I will send you an update later today with the current<br=  /> status.<br /> <br /> Best,<br /> Greg<br /> <br /> On Sun Nov 02 19:44:36 2025, OX20655 wrote:<br /> <br /> &gt; Greg Please let our group know if the disk migration is completed and = if we<br /> &gt; need to do something (e.g., new configurations) to use the new disk sp= ace.<br /> &gt; Thanks-Zhibo On Fri, Oct 31, 2025 at 10:41 AM Greg Ballantine via RT<b= r /> &gt; &lt;UMBCHelp@rt.umbc.edu&gt; wrote:<br /> <br /> &gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32879= 67 &gt;<br /> <br /> &gt;&gt; Last Update From Ticket:<br /> <br /> &gt;&gt; Good morning Zhibo,<br /> <br /> &gt;&gt; This is a reminder that we will be performing your group&#39;s mig= ration to<br /> &gt;&gt; the<br /> &gt;&gt; Ceph storage cluster today. During this time, please ensure there = are<br /> &gt;&gt; not any<br /> &gt;&gt; jobs being run in your research group, otherwise these may be<br /> &gt;&gt; terminated.<br /> <br /> &gt;&gt; We will provide an update once completed.<br /> <br /> &gt;&gt; Best,<br /> &gt;&gt; Greg<br /> <br /> &gt;&gt; On Thu Oct 30 16:22:50 2025, OX20655 wrote:<br /> <br /> &gt;&gt; &gt; Hi All This is a reminder that DOIT will help us with the Chi= p disk<br /> &gt;&gt; &gt; migration tomorrow Oct. 31st. During this period, please do n= ot use<br /> &gt;&gt; the<br /> &gt;&gt; &gt; Chip server until we are notified that the migration process = is<br /> &gt;&gt; finished.<br /> &gt;&gt; &gt; Thanks -Zhibo On Mon, Oct 27, 2025 at 11:23 AM Zhibo Zhang<br=  /> &gt;&gt; &gt; &lt;zzbatmos@umbc.edu&gt; wrote:<br /> <br /> &gt;&gt; &gt;&gt; Yes, that will be fine. thanks, Greg On Mon, Oct 27, 2025=  at 11:21<br /> &gt;&gt; AM<br /> &gt;&gt; &gt;&gt; Greg Ballantine via RT &lt;UMBCHelp@rt.umbc.edu&gt; wrote= :<br /> <br /> &gt;&gt; &gt;&gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.ht= ml?id=3D3287967 &gt;<br /> <br /> &gt;&gt; &gt;&gt;&gt; Last Update From Ticket:<br /> <br /> &gt;&gt; &gt;&gt;&gt; Good morning Zhibo,<br /> <br /> &gt;&gt; &gt;&gt;&gt; Yes, Friday the 31st will work for starting your migr= ation.<br /> &gt;&gt; &gt;&gt;&gt; However, since your<br /> &gt;&gt; &gt;&gt;&gt; storage volume is currently very large (~482TB as of = this morning)<br /> &gt;&gt; &gt;&gt;&gt; there is a<br /> &gt;&gt; &gt;&gt;&gt; good chance that your migration will take longer than=  the business<br /> &gt;&gt; &gt;&gt;&gt; day that we<br /> &gt;&gt; &gt;&gt;&gt; typically plan for. We&#39;ve started copying your da= ta to the new<br /> &gt;&gt; &gt;&gt;&gt; storage server<br /> &gt;&gt; &gt;&gt;&gt; to minimize downtime, but I would still expect the fi= nal migration<br /> &gt;&gt; &gt;&gt;&gt; to run<br /> &gt;&gt; &gt;&gt;&gt; through the weekend and likely into the following wee= k.<br /> <br /> &gt;&gt; &gt;&gt;&gt; Will that time frame still work for your group?<br /> <br /> &gt;&gt; &gt;&gt;&gt; Thank you,<br /> &gt;&gt; &gt;&gt;&gt; Greg<br /> <br /> &gt;&gt; &gt;&gt;&gt; On Wed Oct 22 10:14:02 2025, OX20655 wrote:<br /> <br /> &gt;&gt; &gt;&gt;&gt; &gt; Hi Sorry for the late reply. Our group has decid= ed to make the<br /> &gt;&gt; &gt;&gt;&gt; disk<br /> &gt;&gt; &gt;&gt;&gt; &gt; migration on Oct. 31 Friday. Would that work for=  you? Thanks. On<br /> &gt;&gt; &gt;&gt;&gt; Wed, Oct<br /> &gt;&gt; &gt;&gt;&gt; &gt; 8, 2025 at 12:42 PM Greg Ballantine via RT &lt;U= MBCHelp@rt.umbc.edu&gt;<br /> &gt;&gt; &gt;&gt;&gt; wrote:<br /> <br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/D= isplay.html?id=3D3287967<br /> &gt;&gt; &gt;&gt;&gt; &gt;<br /> <br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; Last Update From Ticket:<br /> <br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; Hello Zhibo,<br /> <br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; Yes, a response by Friday is fine.<br /> <br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; Thank you,<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; Greg<br /> <br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; On Tue Oct 07 21:29:13 2025, OX20655 wrote:<= br /> <br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Hi Elliot We will choose option 1 Sched= ule a group-wide<br /> &gt;&gt; &gt;&gt;&gt; downtime date<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; during standard business hours,. I need=  to determine the exact<br /> &gt;&gt; &gt;&gt;&gt; date<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; with my<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; team members though. Can I get back to = you by Friday?<br /> &gt;&gt; &gt;&gt;&gt; Thanks-Zhibo On<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; Tue,<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Oct 7, 2025 at 3:20 PM via RT &lt;UMBCH= elp@rt.umbc.edu&gt; wrote:<br /> <br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Greetings,<br /> <br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; This message has been automatically=  generated in response to<br /> &gt;&gt; &gt;&gt;&gt; the<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; creation of a ticket regarding:<br = /> <br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt;<br /> &gt;&gt; ------------------------------------------------------------------= -------<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Subject: &quot;Migrating Research S= torage Volume to Ceph Cluster -<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; pi_zzbatmos&quot;<br /> <br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Message:<br /> <br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Dear Zhibo,<br /> <br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; As per the communication via myUMBC=  earlier this summer (June<br /> &gt;&gt; &gt;&gt;&gt; HPCF<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Newsletter<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; ), DoIT is in the process of migrat= ing data off of an older<br /> &gt;&gt; &gt;&gt;&gt; storage<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; server to<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; our new RRStor Ceph storage cluster= . Your group is using<br /> &gt;&gt; &gt;&gt;&gt; 458.94 TB<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; of a<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; 505.0000 TB quota on the old storag= e server.<br /> <br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; To perform these migrations, we nee= d to take individual<br /> &gt;&gt; &gt;&gt;&gt; storage<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; volumes<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; offline<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; while we migrate them to the Ceph c= luster. Thus we are<br /> &gt;&gt; &gt;&gt;&gt; reaching out<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; to<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; schedule<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; a date where we can migrate your vo= lume located at<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &ldquo;/umbc/rs/zzbatmos&rdquo;.<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; During<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; the migration, we will take your vo= lume offline and will<br /> &gt;&gt; &gt;&gt;&gt; terminate<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; any<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; jobs<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; running on the chip compute cluster=  that are accessing this<br /> &gt;&gt; &gt;&gt;&gt; volume.<br /> <br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Below we&rsquo;ve listed two option= s for handling this data<br /> &gt;&gt; &gt;&gt;&gt; migration -<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; please let us<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; know which of these you&rsquo;d pre= fer.<br /> <br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Option 1: Schedule a group-wide dow= ntime date during standard<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; business<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; hours,<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; which can be done by responding to = this email with your<br /> &gt;&gt; &gt;&gt;&gt; preferred<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; date(s) to<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; perform the migration. During this = time, DoIT staff will work<br /> &gt;&gt; &gt;&gt;&gt; to<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; migrate your<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; volume to the Ceph storage cluster.=  DoIT staff will send an<br /> &gt;&gt; &gt;&gt;&gt; email<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; alert<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; on this<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; email thread when the migration has=  begun and when it has<br /> &gt;&gt; &gt;&gt;&gt; completed.<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; For most<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; storage volumes, this process shoul= d take less than a<br /> &gt;&gt; &gt;&gt;&gt; business day.<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Option 2: If you don&rsquo;t respon= d to this email by October 15th,<br /> &gt;&gt; &gt;&gt;&gt; DoIT<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; staff will<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; assign a day over the following mon= th (October 16th through<br /> &gt;&gt; &gt;&gt;&gt; November<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; 15th) to<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; migrate your volume. The day chosen=  will be random and will<br /> &gt;&gt; &gt;&gt;&gt; occur<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; during<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; business hours. You will be notifie= d of the date chosen to<br /> &gt;&gt; &gt;&gt;&gt; perform<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; the<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; migration, and will be notified whe= n the migration begins and<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; completes.<br /> <br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Note: After this process has comple= ted, the new storage<br /> &gt;&gt; &gt;&gt;&gt; volume will<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; have a new<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; name. For example, group &ldquo;pi_= doit&rdquo; will find its data under<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &ldquo;/umbc/rs/pi_doit&rdquo;,<br = /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; or in your group&rsquo;s case you w= ill find your volume under<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &ldquo;/umbc/rs/pi_zzbatmos&rdquo;.= <br /> <br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thank you,<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Elliot<br /> <br /> <br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt;<br /> &gt;&gt; ------------------------------------------------------------------= -------<br /> <br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; There is no need to reply to this m= essage right now.<br /> <br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Your ticket has been assigned an ID=  of [Research Computing<br /> &gt;&gt; &gt;&gt;&gt; #3287967]<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; or<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; you can go there directly by clicki= ng the link below.<br /> <br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Ticket &lt;URL:<br /> &gt;&gt; &gt;&gt;&gt; https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 = &gt;<br /> <br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; You can login to view your open tic= kets at any time by<br /> &gt;&gt; &gt;&gt;&gt; visiting<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; http://my.umbc.edu and clicking on = &quot;Help&quot; and &quot;Request Help&quot;.<br /> <br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Alternately you can click on http:/= /my.umbc.edu/help<br /> <br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thank you<br /> <br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; --<br /> <br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; Gregory BallantineSystem Administrator for R= esearch and<br /> &gt;&gt; &gt;&gt;&gt; Enterprise<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; ComputingUMBC<br /> &gt;&gt; &gt;&gt;&gt; &gt;&gt; - DoIT<br /> <br /> &gt;&gt; &gt;&gt;&gt; --<br /> <br /> &gt;&gt; &gt;&gt;&gt; Gregory BallantineSystem Administrator for Research a= nd Enterprise<br /> &gt;&gt; &gt;&gt;&gt; ComputingUMBC<br /> &gt;&gt; &gt;&gt;&gt; - DoIT<br /> <br /> &gt;&gt; --<br /> <br /> &gt;&gt; Gregory BallantineSystem Administrator for Research and Enterprise= <br /> &gt;&gt; ComputingUMBC<br /> &gt;&gt; - DoIT<br /> <br /> --<br /> <br /> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br /> - DoIT<br /> &nbsp;</blockquote> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3287967,72698622,Correspond,DoIT-Research-Computing,2025-11-03 22:48:03.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos,open,Greg Ballantine,gballan1,Zhibo Zhang,zzbatmos,zzbatmos@umbc.edu,Zhibo Zhang,zzbatmos@umbc.edu,"Thanks for the update, Greg. We will wait until the migration finishes to resume our usage in Chip. Please keep us posted. -Zhibo  On Mon, Nov 3, 2025 at 4:39=E2=80=AFPM Greg Ballantine via RT <UMBCHelp@rt.= umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > > > Last Update From Ticket: > > Hello Zhibo, > > Just to give you an update: the migration is still going, and my best gue= ss > currently is it will take another 2-3 days to finish. > > I'll leave it running and will send you an update tomorrow morning with > where > it's at. > > Best, > > Greg > > On Mon Nov 03 10:22:02 2025, OX20655 wrote: > > > Ok, Thanks for the update. Please keep us updated. -Zhibo On Mon, Nov 3, > > 2025 at 10:00 AM Greg Ballantine via RT <UMBCHelp@rt.umbc.edu> wrote: > > >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > > > >> Last Update From Ticket: > > >> Good morning Zhibo, > > >> Your group's migration is still copying data to the new Ceph storage > >> cluster. I > >> don't currently have a good estimate for completion, but it looks like > >> it's > >> currently churning through the user directories, and everything else > >> has been > >> copied. > > >> When it is completed, your data will be available under > >> /umbc/rs/pi_zzbatmos, > >> otherwise there aren't any other changes you'll need to make on your > >> end. > > >> I apologize for the lack of communication on this matter Friday evening > >> and > >> over the weekend. I will send you an update later today with the > >> current > >> status. > > >> Best, > >> Greg > > >> On Sun Nov 02 19:44:36 2025, OX20655 wrote: > > >> > Greg Please let our group know if the disk migration is completed and > >> if we > >> > need to do something (e.g., new configurations) to use the new disk > >> space. > >> > Thanks-Zhibo On Fri, Oct 31, 2025 at 10:41 AM Greg Ballantine via RT > >> > <UMBCHelp@rt.umbc.edu> wrote: > > >> >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > > > >> >> Last Update From Ticket: > > >> >> Good morning Zhibo, > > >> >> This is a reminder that we will be performing your group's migration > >> to > >> >> the > >> >> Ceph storage cluster today. During this time, please ensure there > >> are > >> >> not any > >> >> jobs being run in your research group, otherwise these may be > >> >> terminated. > > >> >> We will provide an update once completed. > > >> >> Best, > >> >> Greg > > >> >> On Thu Oct 30 16:22:50 2025, OX20655 wrote: > > >> >> > Hi All This is a reminder that DOIT will help us with the Chip > >> disk > >> >> > migration tomorrow Oct. 31st. During this period, please do not > >> use > >> >> the > >> >> > Chip server until we are notified that the migration process is > >> >> finished. > >> >> > Thanks -Zhibo On Mon, Oct 27, 2025 at 11:23 AM Zhibo Zhang > >> >> > <zzbatmos@umbc.edu> wrote: > > >> >> >> Yes, that will be fine. thanks, Greg On Mon, Oct 27, 2025 at > >> 11:21 > >> >> AM > >> >> >> Greg Ballantine via RT <UMBCHelp@rt.umbc.edu> wrote: > > >> >> >>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32879= 67 > >> > > > >> >> >>> Last Update From Ticket: > > >> >> >>> Good morning Zhibo, > > >> >> >>> Yes, Friday the 31st will work for starting your migration. > >> >> >>> However, since your > >> >> >>> storage volume is currently very large (~482TB as of this > >> morning) > >> >> >>> there is a > >> >> >>> good chance that your migration will take longer than the > >> business > >> >> >>> day that we > >> >> >>> typically plan for. We've started copying your data to the new > >> >> >>> storage server > >> >> >>> to minimize downtime, but I would still expect the final > >> migration > >> >> >>> to run > >> >> >>> through the weekend and likely into the following week. > > >> >> >>> Will that time frame still work for your group? > > >> >> >>> Thank you, > >> >> >>> Greg > > >> >> >>> On Wed Oct 22 10:14:02 2025, OX20655 wrote: > > >> >> >>> > Hi Sorry for the late reply. Our group has decided to make the > >> >> >>> disk > >> >> >>> > migration on Oct. 31 Friday. Would that work for you? Thanks. > >> On > >> >> >>> Wed, Oct > >> >> >>> > 8, 2025 at 12:42 PM Greg Ballantine via RT > >> <UMBCHelp@rt.umbc.edu> > >> >> >>> wrote: > > >> >> >>> >> Ticket <URL: > >> https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > >> >> >>> > > > >> >> >>> >> Last Update From Ticket: > > >> >> >>> >> Hello Zhibo, > > >> >> >>> >> Yes, a response by Friday is fine. > > >> >> >>> >> Thank you, > >> >> >>> >> Greg > > >> >> >>> >> On Tue Oct 07 21:29:13 2025, OX20655 wrote: > > >> >> >>> >> > Hi Elliot We will choose option 1 Schedule a group-wide > >> >> >>> downtime date > >> >> >>> >> > during standard business hours,. I need to determine the > >> exact > >> >> >>> date > >> >> >>> >> with my > >> >> >>> >> > team members though. Can I get back to you by Friday? > >> >> >>> Thanks-Zhibo On > >> >> >>> >> Tue, > >> >> >>> >> > Oct 7, 2025 at 3:20 PM via RT <UMBCHelp@rt.umbc.edu> wrote: > > >> >> >>> >> >> Greetings, > > >> >> >>> >> >> This message has been automatically generated in response > >> to > >> >> >>> the > >> >> >>> >> >> creation of a ticket regarding: > > >> >> >>> >> >> > >> >> >>> >> > >> >> >>> > >> >> > >> > ------------------------------------------------------------------------- > >> >> >>> >> >> Subject: ""Migrating Research Storage Volume to Ceph > >> Cluster - > >> >> >>> >> >> pi_zzbatmos"" > > >> >> >>> >> >> Message: > > >> >> >>> >> >> Dear Zhibo, > > >> >> >>> >> >> As per the communication via myUMBC earlier this summer > >> (June > >> >> >>> HPCF > >> >> >>> >> >> Newsletter > >> >> >>> >> >> ), DoIT is in the process of migrating data off of an > >> older > >> >> >>> storage > >> >> >>> >> >> server to > >> >> >>> >> >> our new RRStor Ceph storage cluster. Your group is using > >> >> >>> 458.94 TB > >> >> >>> >> of a > >> >> >>> >> >> 505.0000 TB quota on the old storage server. > > >> >> >>> >> >> To perform these migrations, we need to take individual > >> >> >>> storage > >> >> >>> >> volumes > >> >> >>> >> >> offline > >> >> >>> >> >> while we migrate them to the Ceph cluster. Thus we are > >> >> >>> reaching out > >> >> >>> >> to > >> >> >>> >> >> schedule > >> >> >>> >> >> a date where we can migrate your volume located at > >> >> >>> >> =E2=80=9C/umbc/rs/zzbatmos=E2=80=9D. > >> >> >>> >> >> During > >> >> >>> >> >> the migration, we will take your volume offline and will > >> >> >>> terminate > >> >> >>> >> any > >> >> >>> >> >> jobs > >> >> >>> >> >> running on the chip compute cluster that are accessing > >> this > >> >> >>> volume. > > >> >> >>> >> >> Below we=E2=80=99ve listed two options for handling this = data > >> >> >>> migration - > >> >> >>> >> >> please let us > >> >> >>> >> >> know which of these you=E2=80=99d prefer. > > >> >> >>> >> >> Option 1: Schedule a group-wide downtime date during > >> standard > >> >> >>> >> business > >> >> >>> >> >> hours, > >> >> >>> >> >> which can be done by responding to this email with your > >> >> >>> preferred > >> >> >>> >> >> date(s) to > >> >> >>> >> >> perform the migration. During this time, DoIT staff will > >> work > >> >> >>> to > >> >> >>> >> >> migrate your > >> >> >>> >> >> volume to the Ceph storage cluster. DoIT staff will send > >> an > >> >> >>> email > >> >> >>> >> alert > >> >> >>> >> >> on this > >> >> >>> >> >> email thread when the migration has begun and when it has > >> >> >>> completed. > >> >> >>> >> >> For most > >> >> >>> >> >> storage volumes, this process should take less than a > >> >> >>> business day. > >> >> >>> >> >> Option 2: If you don=E2=80=99t respond to this email by O= ctober > >> 15th, > >> >> >>> DoIT > >> >> >>> >> >> staff will > >> >> >>> >> >> assign a day over the following month (October 16th > >> through > >> >> >>> November > >> >> >>> >> >> 15th) to > >> >> >>> >> >> migrate your volume. The day chosen will be random and > >> will > >> >> >>> occur > >> >> >>> >> >> during > >> >> >>> >> >> business hours. You will be notified of the date chosen to > >> >> >>> perform > >> >> >>> >> the > >> >> >>> >> >> migration, and will be notified when the migration begins > >> and > >> >> >>> >> >> completes. > > >> >> >>> >> >> Note: After this process has completed, the new storage > >> >> >>> volume will > >> >> >>> >> >> have a new > >> >> >>> >> >> name. For example, group =E2=80=9Cpi_doit=E2=80=9D will f= ind its data > >> under > >> >> >>> >> >> =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, > >> >> >>> >> >> or in your group=E2=80=99s case you will find your volume=  under > >> >> >>> >> >> =E2=80=9C/umbc/rs/pi_zzbatmos=E2=80=9D. > > >> >> >>> >> >> Thank you, > >> >> >>> >> >> Elliot > > > >> >> >>> >> >> > >> >> >>> >> > >> >> >>> > >> >> > >> > ------------------------------------------------------------------------- > > >> >> >>> >> >> There is no need to reply to this message right now. > > >> >> >>> >> >> Your ticket has been assigned an ID of [Research Computing > >> >> >>> #3287967] > >> >> >>> >> or > >> >> >>> >> >> you can go there directly by clicking the link below. > > >> >> >>> >> >> Ticket <URL: > >> >> >>> https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > > > >> >> >>> >> >> You can login to view your open tickets at any time by > >> >> >>> visiting > >> >> >>> >> >> http://my.umbc.edu and clicking on ""Help"" and ""Request > >> Help"". > > >> >> >>> >> >> Alternately you can click on http://my.umbc.edu/help > > >> >> >>> >> >> Thank you > > >> >> >>> >> -- > > >> >> >>> >> Gregory BallantineSystem Administrator for Research and > >> >> >>> Enterprise > >> >> >>> >> ComputingUMBC > >> >> >>> >> - DoIT > > >> >> >>> -- > > >> >> >>> Gregory BallantineSystem Administrator for Research and > >> Enterprise > >> >> >>> ComputingUMBC > >> >> >>> - DoIT > > >> >> -- > > >> >> Gregory BallantineSystem Administrator for Research and Enterprise > >> >> ComputingUMBC > >> >> - DoIT > > >> -- > > >> Gregory BallantineSystem Administrator for Research and Enterprise > >> ComputingUMBC > >> - DoIT > > -- > > Gregory BallantineSystem Administrator for Research and Enterprise > ComputingUMBC > - DoIT > > "
3287967,72698622,Correspond,DoIT-Research-Computing,2025-11-03 22:48:03.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos,open,Greg Ballantine,gballan1,Zhibo Zhang,zzbatmos,zzbatmos@umbc.edu,Zhibo Zhang,zzbatmos@umbc.edu,"<div dir=3D""ltr""><div class=3D""gmail_default"" style=3D""font-family:arial,he= lvetica,sans-serif;font-size:small"">Thanks=C2=A0for the update, Greg. We wi= ll wait until=C2=A0the migration finishes to resume our usage in Chip. Plea= se keep us posted. -Zhibo</div></div><br><div class=3D""gmail_quote gmail_qu= ote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Mon, Nov 3, 2025 at=  4:39=E2=80=AFPM Greg Ballantine via RT &lt;<a href=3D""mailto:UMBCHelp@rt.u= mbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""= gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(20= 4,204,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu= /Ticket/Display.html?id=3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">htt= ps://rt.umbc.edu/Ticket/Display.html?id=3D3287967</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hello Zhibo,<br> <br> Just to give you an update: the migration is still going, and my best guess= <br> currently is it will take another 2-3 days to finish.<br> <br> I&#39;ll leave it running and will send you an update tomorrow morning with=  where<br> it&#39;s at.<br> <br> Best,<br> <br> Greg<br> <br> On Mon Nov 03 10:22:02 2025, OX20655 wrote:<br> <br> &gt; Ok, Thanks for the update. Please keep us updated. -Zhibo On Mon, Nov = 3,<br> &gt; 2025 at 10:00 AM Greg Ballantine via RT &lt;<a href=3D""mailto:UMBCHelp= @rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html= ?id=3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tic= ket/Display.html?id=3D3287967</a> &gt;<br> <br> &gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt; Good morning Zhibo,<br> <br> &gt;&gt; Your group&#39;s migration is still copying data to the new Ceph s= torage<br> &gt;&gt; cluster. I<br> &gt;&gt; don&#39;t currently have a good estimate for completion, but it lo= oks like<br> &gt;&gt; it&#39;s<br> &gt;&gt; currently churning through the user directories, and everything el= se<br> &gt;&gt; has been<br> &gt;&gt; copied.<br> <br> &gt;&gt; When it is completed, your data will be available under<br> &gt;&gt; /umbc/rs/pi_zzbatmos,<br> &gt;&gt; otherwise there aren&#39;t any other changes you&#39;ll need to ma= ke on your<br> &gt;&gt; end.<br> <br> &gt;&gt; I apologize for the lack of communication on this matter Friday ev= ening<br> &gt;&gt; and<br> &gt;&gt; over the weekend. I will send you an update later today with the<b= r> &gt;&gt; current<br> &gt;&gt; status.<br> <br> &gt;&gt; Best,<br> &gt;&gt; Greg<br> <br> &gt;&gt; On Sun Nov 02 19:44:36 2025, OX20655 wrote:<br> <br> &gt;&gt; &gt; Greg Please let our group know if the disk migration is compl= eted and<br> &gt;&gt; if we<br> &gt;&gt; &gt; need to do something (e.g., new configurations) to use the ne= w disk<br> &gt;&gt; space.<br> &gt;&gt; &gt; Thanks-Zhibo On Fri, Oct 31, 2025 at 10:41 AM Greg Ballantine=  via RT<br> &gt;&gt; &gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank""= >UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Dis= play.html?id=3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umb= c.edu/Ticket/Display.html?id=3D3287967</a> &gt;<br> <br> &gt;&gt; &gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt; &gt;&gt; Good morning Zhibo,<br> <br> &gt;&gt; &gt;&gt; This is a reminder that we will be performing your group&= #39;s migration<br> &gt;&gt; to<br> &gt;&gt; &gt;&gt; the<br> &gt;&gt; &gt;&gt; Ceph storage cluster today. During this time, please ensu= re there<br> &gt;&gt; are<br> &gt;&gt; &gt;&gt; not any<br> &gt;&gt; &gt;&gt; jobs being run in your research group, otherwise these ma= y be<br> &gt;&gt; &gt;&gt; terminated.<br> <br> &gt;&gt; &gt;&gt; We will provide an update once completed.<br> <br> &gt;&gt; &gt;&gt; Best,<br> &gt;&gt; &gt;&gt; Greg<br> <br> &gt;&gt; &gt;&gt; On Thu Oct 30 16:22:50 2025, OX20655 wrote:<br> <br> &gt;&gt; &gt;&gt; &gt; Hi All This is a reminder that DOIT will help us wit= h the Chip<br> &gt;&gt; disk<br> &gt;&gt; &gt;&gt; &gt; migration tomorrow Oct. 31st. During this period, pl= ease do not<br> &gt;&gt; use<br> &gt;&gt; &gt;&gt; the<br> &gt;&gt; &gt;&gt; &gt; Chip server until we are notified that the migration=  process is<br> &gt;&gt; &gt;&gt; finished.<br> &gt;&gt; &gt;&gt; &gt; Thanks -Zhibo On Mon, Oct 27, 2025 at 11:23 AM Zhibo=  Zhang<br> &gt;&gt; &gt;&gt; &gt; &lt;<a href=3D""mailto:zzbatmos@umbc.edu"" target=3D""_= blank"">zzbatmos@umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; &gt;&gt; &gt;&gt; Yes, that will be fine. thanks, Greg On Mon, Oct=  27, 2025 at<br> &gt;&gt; 11:21<br> &gt;&gt; &gt;&gt; AM<br> &gt;&gt; &gt;&gt; &gt;&gt; Greg Ballantine via RT &lt;<a href=3D""mailto:UMB= CHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.e= du/Ticket/Display.html?id=3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">h= ttps://rt.umbc.edu/Ticket/Display.html?id=3D3287967</a><br> &gt;&gt; &gt;<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Good morning Zhibo,<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Yes, Friday the 31st will work for starting = your migration.<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; However, since your<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; storage volume is currently very large (~482= TB as of this<br> &gt;&gt; morning)<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; there is a<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; good chance that your migration will take lo= nger than the<br> &gt;&gt; business<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; day that we<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; typically plan for. We&#39;ve started copyin= g your data to the new<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; storage server<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; to minimize downtime, but I would still expe= ct the final<br> &gt;&gt; migration<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; to run<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; through the weekend and likely into the foll= owing week.<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Will that time frame still work for your gro= up?<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Thank you,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Greg<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; On Wed Oct 22 10:14:02 2025, OX20655 wrote:<= br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Hi Sorry for the late reply. Our group = has decided to make the<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; disk<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; migration on Oct. 31 Friday. Would that=  work for you? Thanks.<br> &gt;&gt; On<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Wed, Oct<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; 8, 2025 at 12:42 PM Greg Ballantine via=  RT<br> &gt;&gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBC= Help@rt.umbc.edu</a>&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; wrote:<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Ticket &lt;URL:<br> &gt;&gt; <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D3287967"" r= el=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html= ?id=3D3287967</a><br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Hello Zhibo,<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Yes, a response by Friday is fine.<= br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Thank you,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Greg<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; On Tue Oct 07 21:29:13 2025, OX2065= 5 wrote:<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Hi Elliot We will choose optio= n 1 Schedule a group-wide<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; downtime date<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; during standard business hours= ,. I need to determine the<br> &gt;&gt; exact<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; date<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; with my<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; team members though. Can I get=  back to you by Friday?<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Thanks-Zhibo On<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Tue,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Oct 7, 2025 at 3:20 PM via RT = &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.u= mbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Greetings,<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; This message has been auto= matically generated in response<br> &gt;&gt; to<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; the<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; creation of a ticket regar= ding:<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt;<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; ------------------------------------------------------------------= -------<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Subject: &quot;Migrating R= esearch Storage Volume to Ceph<br> &gt;&gt; Cluster -<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; pi_zzbatmos&quot;<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Message:<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Dear Zhibo,<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; As per the communication v= ia myUMBC earlier this summer<br> &gt;&gt; (June<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; HPCF<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Newsletter<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; ), DoIT is in the process = of migrating data off of an<br> &gt;&gt; older<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; storage<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; server to<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; our new RRStor Ceph storag= e cluster. Your group is using<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; 458.94 TB<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; of a<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; 505.0000 TB quota on the o= ld storage server.<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; To perform these migration= s, we need to take individual<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; storage<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; volumes<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; offline<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; while we migrate them to t= he Ceph cluster. Thus we are<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; reaching out<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; to<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; schedule<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; a date where we can migrat= e your volume located at<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; =E2=80=9C/umbc/rs/zzbatmos=E2=80=9D= .<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; During<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; the migration, we will tak= e your volume offline and will<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; terminate<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; any<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; jobs<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; running on the chip comput= e cluster that are accessing<br> &gt;&gt; this<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; volume.<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Below we=E2=80=99ve listed=  two options for handling this data<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; migration -<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; please let us<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; know which of these you=E2= =80=99d prefer.<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Option 1: Schedule a group= -wide downtime date during<br> &gt;&gt; standard<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; business<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; hours,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; which can be done by respo= nding to this email with your<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; preferred<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; date(s) to<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; perform the migration. Dur= ing this time, DoIT staff will<br> &gt;&gt; work<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; to<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; migrate your<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; volume to the Ceph storage=  cluster. DoIT staff will send<br> &gt;&gt; an<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; email<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; alert<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; on this<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; email thread when the migr= ation has begun and when it has<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; completed.<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; For most<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; storage volumes, this proc= ess should take less than a<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; business day.<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Option 2: If you don=E2=80= =99t respond to this email by October<br> &gt;&gt; 15th,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; DoIT<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; staff will<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; assign a day over the foll= owing month (October 16th<br> &gt;&gt; through<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; November<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; 15th) to<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; migrate your volume. The d= ay chosen will be random and<br> &gt;&gt; will<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; occur<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; during<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; business hours. You will b= e notified of the date chosen to<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; perform<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; the<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; migration, and will be not= ified when the migration begins<br> &gt;&gt; and<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; completes.<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Note: After this process h= as completed, the new storage<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; volume will<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; have a new<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; name. For example, group = =E2=80=9Cpi_doit=E2=80=9D will find its data<br> &gt;&gt; under<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; =E2=80=9C/umbc/rs/pi_doit= =E2=80=9D,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; or in your group=E2=80=99s=  case you will find your volume under<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; =E2=80=9C/umbc/rs/pi_zzbat= mos=E2=80=9D.<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thank you,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Elliot<br> <br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt;<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; ------------------------------------------------------------------= -------<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; There is no need to reply = to this message right now.<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Your ticket has been assig= ned an ID of [Research Computing<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; #3287967]<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; or<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; you can go there directly = by clicking the link below.<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Ticket &lt;URL:<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; <a href=3D""https://rt.umbc.edu/Ticket/Displa= y.html?id=3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.e= du/Ticket/Display.html?id=3D3287967</a> &gt;<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; You can login to view your=  open tickets at any time by<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; visiting<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; <a href=3D""http://my.umbc.= edu"" rel=3D""noreferrer"" target=3D""_blank"">http://my.umbc.edu</a> and clicki= ng on &quot;Help&quot; and &quot;Request<br> &gt;&gt; Help&quot;.<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Alternately you can click = on <a href=3D""http://my.umbc.edu/help"" rel=3D""noreferrer"" target=3D""_blank""= >http://my.umbc.edu/help</a><br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thank you<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; --<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Gregory BallantineSystem Administra= tor for Research and<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Enterprise<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; ComputingUMBC<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; - DoIT<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; --<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Gregory BallantineSystem Administrator for R= esearch and<br> &gt;&gt; Enterprise<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; ComputingUMBC<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; - DoIT<br> <br> &gt;&gt; &gt;&gt; --<br> <br> &gt;&gt; &gt;&gt; Gregory BallantineSystem Administrator for Research and E= nterprise<br> &gt;&gt; &gt;&gt; ComputingUMBC<br> &gt;&gt; &gt;&gt; - DoIT<br> <br> &gt;&gt; --<br> <br> &gt;&gt; Gregory BallantineSystem Administrator for Research and Enterprise= <br> &gt;&gt; ComputingUMBC<br> &gt;&gt; - DoIT<br> <br> --<br> <br> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br> - DoIT<br> <br> </blockquote></div> "
3287967,72705855,Correspond,DoIT-Research-Computing,2025-11-04 14:33:10.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos,open,Greg Ballantine,gballan1,Zhibo Zhang,zzbatmos,zzbatmos@umbc.edu,Zhibo Zhang,zzbatmos@umbc.edu,"Hi Greg       I just check the new disk volume /umbc/rs/pi_zzbatmos using the command df -h | grep zzbatmos. I saw that the new disk volume is almost up to 100%. I remember that our old volume had 20-30 TB of free space before the migration. I'm wondering what is going on and if the migration process is still going well? Thanks.   [image: image.png]  On Mon, Nov 3, 2025 at 5:47=E2=80=AFPM Zhibo Zhang <zzbatmos@umbc.edu> wrot= e:  > Thanks for the update, Greg. We will wait until the migration finishes to > resume our usage in Chip. Please keep us posted. -Zhibo > > On Mon, Nov 3, 2025 at 4:39=E2=80=AFPM Greg Ballantine via RT < > UMBCHelp@rt.umbc.edu> wrote: > >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > >> >> Last Update From Ticket: >> >> Hello Zhibo, >> >> Just to give you an update: the migration is still going, and my best >> guess >> currently is it will take another 2-3 days to finish. >> >> I'll leave it running and will send you an update tomorrow morning with >> where >> it's at. >> >> Best, >> >> Greg >> >> On Mon Nov 03 10:22:02 2025, OX20655 wrote: >> >> > Ok, Thanks for the update. Please keep us updated. -Zhibo On Mon, Nov = 3, >> > 2025 at 10:00 AM Greg Ballantine via RT <UMBCHelp@rt.umbc.edu> wrote: >> >> >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > >> >> >> Last Update From Ticket: >> >> >> Good morning Zhibo, >> >> >> Your group's migration is still copying data to the new Ceph storage >> >> cluster. I >> >> don't currently have a good estimate for completion, but it looks like >> >> it's >> >> currently churning through the user directories, and everything else >> >> has been >> >> copied. >> >> >> When it is completed, your data will be available under >> >> /umbc/rs/pi_zzbatmos, >> >> otherwise there aren't any other changes you'll need to make on your >> >> end. >> >> >> I apologize for the lack of communication on this matter Friday eveni= ng >> >> and >> >> over the weekend. I will send you an update later today with the >> >> current >> >> status. >> >> >> Best, >> >> Greg >> >> >> On Sun Nov 02 19:44:36 2025, OX20655 wrote: >> >> >> > Greg Please let our group know if the disk migration is completed a= nd >> >> if we >> >> > need to do something (e.g., new configurations) to use the new disk >> >> space. >> >> > Thanks-Zhibo On Fri, Oct 31, 2025 at 10:41 AM Greg Ballantine via RT >> >> > <UMBCHelp@rt.umbc.edu> wrote: >> >> >> >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > >> >> >> >> Last Update From Ticket: >> >> >> >> Good morning Zhibo, >> >> >> >> This is a reminder that we will be performing your group's migrati= on >> >> to >> >> >> the >> >> >> Ceph storage cluster today. During this time, please ensure there >> >> are >> >> >> not any >> >> >> jobs being run in your research group, otherwise these may be >> >> >> terminated. >> >> >> >> We will provide an update once completed. >> >> >> >> Best, >> >> >> Greg >> >> >> >> On Thu Oct 30 16:22:50 2025, OX20655 wrote: >> >> >> >> > Hi All This is a reminder that DOIT will help us with the Chip >> >> disk >> >> >> > migration tomorrow Oct. 31st. During this period, please do not >> >> use >> >> >> the >> >> >> > Chip server until we are notified that the migration process is >> >> >> finished. >> >> >> > Thanks -Zhibo On Mon, Oct 27, 2025 at 11:23 AM Zhibo Zhang >> >> >> > <zzbatmos@umbc.edu> wrote: >> >> >> >> >> Yes, that will be fine. thanks, Greg On Mon, Oct 27, 2025 at >> >> 11:21 >> >> >> AM >> >> >> >> Greg Ballantine via RT <UMBCHelp@rt.umbc.edu> wrote: >> >> >> >> >>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287= 967 >> >> > >> >> >> >> >>> Last Update From Ticket: >> >> >> >> >>> Good morning Zhibo, >> >> >> >> >>> Yes, Friday the 31st will work for starting your migration. >> >> >> >>> However, since your >> >> >> >>> storage volume is currently very large (~482TB as of this >> >> morning) >> >> >> >>> there is a >> >> >> >>> good chance that your migration will take longer than the >> >> business >> >> >> >>> day that we >> >> >> >>> typically plan for. We've started copying your data to the new >> >> >> >>> storage server >> >> >> >>> to minimize downtime, but I would still expect the final >> >> migration >> >> >> >>> to run >> >> >> >>> through the weekend and likely into the following week. >> >> >> >> >>> Will that time frame still work for your group? >> >> >> >> >>> Thank you, >> >> >> >>> Greg >> >> >> >> >>> On Wed Oct 22 10:14:02 2025, OX20655 wrote: >> >> >> >> >>> > Hi Sorry for the late reply. Our group has decided to make t= he >> >> >> >>> disk >> >> >> >>> > migration on Oct. 31 Friday. Would that work for you? Thanks. >> >> On >> >> >> >>> Wed, Oct >> >> >> >>> > 8, 2025 at 12:42 PM Greg Ballantine via RT >> >> <UMBCHelp@rt.umbc.edu> >> >> >> >>> wrote: >> >> >> >> >>> >> Ticket <URL: >> >> https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 >> >> >> >>> > >> >> >> >> >>> >> Last Update From Ticket: >> >> >> >> >>> >> Hello Zhibo, >> >> >> >> >>> >> Yes, a response by Friday is fine. >> >> >> >> >>> >> Thank you, >> >> >> >>> >> Greg >> >> >> >> >>> >> On Tue Oct 07 21:29:13 2025, OX20655 wrote: >> >> >> >> >>> >> > Hi Elliot We will choose option 1 Schedule a group-wide >> >> >> >>> downtime date >> >> >> >>> >> > during standard business hours,. I need to determine the >> >> exact >> >> >> >>> date >> >> >> >>> >> with my >> >> >> >>> >> > team members though. Can I get back to you by Friday? >> >> >> >>> Thanks-Zhibo On >> >> >> >>> >> Tue, >> >> >> >>> >> > Oct 7, 2025 at 3:20 PM via RT <UMBCHelp@rt.umbc.edu> >> wrote: >> >> >> >> >>> >> >> Greetings, >> >> >> >> >>> >> >> This message has been automatically generated in response >> >> to >> >> >> >>> the >> >> >> >>> >> >> creation of a ticket regarding: >> >> >> >> >>> >> >> >> >> >> >>> >> >> >> >> >>> >> >> >> >> >> >> ------------------------------------------------------------------------- >> >> >> >>> >> >> Subject: ""Migrating Research Storage Volume to Ceph >> >> Cluster - >> >> >> >>> >> >> pi_zzbatmos"" >> >> >> >> >>> >> >> Message: >> >> >> >> >>> >> >> Dear Zhibo, >> >> >> >> >>> >> >> As per the communication via myUMBC earlier this summer >> >> (June >> >> >> >>> HPCF >> >> >> >>> >> >> Newsletter >> >> >> >>> >> >> ), DoIT is in the process of migrating data off of an >> >> older >> >> >> >>> storage >> >> >> >>> >> >> server to >> >> >> >>> >> >> our new RRStor Ceph storage cluster. Your group is using >> >> >> >>> 458.94 TB >> >> >> >>> >> of a >> >> >> >>> >> >> 505.0000 TB quota on the old storage server. >> >> >> >> >>> >> >> To perform these migrations, we need to take individual >> >> >> >>> storage >> >> >> >>> >> volumes >> >> >> >>> >> >> offline >> >> >> >>> >> >> while we migrate them to the Ceph cluster. Thus we are >> >> >> >>> reaching out >> >> >> >>> >> to >> >> >> >>> >> >> schedule >> >> >> >>> >> >> a date where we can migrate your volume located at >> >> >> >>> >> =E2=80=9C/umbc/rs/zzbatmos=E2=80=9D. >> >> >> >>> >> >> During >> >> >> >>> >> >> the migration, we will take your volume offline and will >> >> >> >>> terminate >> >> >> >>> >> any >> >> >> >>> >> >> jobs >> >> >> >>> >> >> running on the chip compute cluster that are accessing >> >> this >> >> >> >>> volume. >> >> >> >> >>> >> >> Below we=E2=80=99ve listed two options for handling this=  data >> >> >> >>> migration - >> >> >> >>> >> >> please let us >> >> >> >>> >> >> know which of these you=E2=80=99d prefer. >> >> >> >> >>> >> >> Option 1: Schedule a group-wide downtime date during >> >> standard >> >> >> >>> >> business >> >> >> >>> >> >> hours, >> >> >> >>> >> >> which can be done by responding to this email with your >> >> >> >>> preferred >> >> >> >>> >> >> date(s) to >> >> >> >>> >> >> perform the migration. During this time, DoIT staff will >> >> work >> >> >> >>> to >> >> >> >>> >> >> migrate your >> >> >> >>> >> >> volume to the Ceph storage cluster. DoIT staff will send >> >> an >> >> >> >>> email >> >> >> >>> >> alert >> >> >> >>> >> >> on this >> >> >> >>> >> >> email thread when the migration has begun and when it has >> >> >> >>> completed. >> >> >> >>> >> >> For most >> >> >> >>> >> >> storage volumes, this process should take less than a >> >> >> >>> business day. >> >> >> >>> >> >> Option 2: If you don=E2=80=99t respond to this email by = October >> >> 15th, >> >> >> >>> DoIT >> >> >> >>> >> >> staff will >> >> >> >>> >> >> assign a day over the following month (October 16th >> >> through >> >> >> >>> November >> >> >> >>> >> >> 15th) to >> >> >> >>> >> >> migrate your volume. The day chosen will be random and >> >> will >> >> >> >>> occur >> >> >> >>> >> >> during >> >> >> >>> >> >> business hours. You will be notified of the date chosen = to >> >> >> >>> perform >> >> >> >>> >> the >> >> >> >>> >> >> migration, and will be notified when the migration begins >> >> and >> >> >> >>> >> >> completes. >> >> >> >> >>> >> >> Note: After this process has completed, the new storage >> >> >> >>> volume will >> >> >> >>> >> >> have a new >> >> >> >>> >> >> name. For example, group =E2=80=9Cpi_doit=E2=80=9D will = find its data >> >> under >> >> >> >>> >> >> =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, >> >> >> >>> >> >> or in your group=E2=80=99s case you will find your volum= e under >> >> >> >>> >> >> =E2=80=9C/umbc/rs/pi_zzbatmos=E2=80=9D. >> >> >> >> >>> >> >> Thank you, >> >> >> >>> >> >> Elliot >> >> >> >> >> >>> >> >> >> >> >> >>> >> >> >> >> >>> >> >> >> >> >> >> ------------------------------------------------------------------------- >> >> >> >> >>> >> >> There is no need to reply to this message right now. >> >> >> >> >>> >> >> Your ticket has been assigned an ID of [Research Computi= ng >> >> >> >>> #3287967] >> >> >> >>> >> or >> >> >> >>> >> >> you can go there directly by clicking the link below. >> >> >> >> >>> >> >> Ticket <URL: >> >> >> >>> https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > >> >> >> >> >>> >> >> You can login to view your open tickets at any time by >> >> >> >>> visiting >> >> >> >>> >> >> http://my.umbc.edu and clicking on ""Help"" and ""Request >> >> Help"". >> >> >> >> >>> >> >> Alternately you can click on http://my.umbc.edu/help >> >> >> >> >>> >> >> Thank you >> >> >> >> >>> >> -- >> >> >> >> >>> >> Gregory BallantineSystem Administrator for Research and >> >> >> >>> Enterprise >> >> >> >>> >> ComputingUMBC >> >> >> >>> >> - DoIT >> >> >> >> >>> -- >> >> >> >> >>> Gregory BallantineSystem Administrator for Research and >> >> Enterprise >> >> >> >>> ComputingUMBC >> >> >> >>> - DoIT >> >> >> >> -- >> >> >> >> Gregory BallantineSystem Administrator for Research and Enterprise >> >> >> ComputingUMBC >> >> >> - DoIT >> >> >> -- >> >> >> Gregory BallantineSystem Administrator for Research and Enterprise >> >> ComputingUMBC >> >> - DoIT >> >> -- >> >> Gregory BallantineSystem Administrator for Research and Enterprise >> ComputingUMBC >> - DoIT >> >> "
3287967,72705855,Correspond,DoIT-Research-Computing,2025-11-04 14:33:10.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos,open,Greg Ballantine,gballan1,Zhibo Zhang,zzbatmos,zzbatmos@umbc.edu,Zhibo Zhang,zzbatmos@umbc.edu,"<div dir=3D""ltr""><div class=3D""gmail_default"" style=3D""font-family:arial,he= lvetica,sans-serif;font-size:small"">Hi Greg</div><div class=3D""gmail_defaul= t"" style=3D""font-family:arial,helvetica,sans-serif;font-size:small"">=C2=A0 = =C2=A0 =C2=A0 I just check the new disk volume=C2=A0/umbc/rs/pi_zzbatmos us= ing the command df -h | grep zzbatmos. I saw that the new disk volume is al= most up to 100%. I remember that our old volume had 20-30 TB of free space = before the migration. I&#39;m wondering=C2=A0what is going on and if the mi= gration process is still going=C2=A0well? Thanks.</div><div class=3D""gmail_= default"" style=3D""font-family:arial,helvetica,sans-serif;font-size:small""><= br></div><div class=3D""gmail_default"" style=3D""font-family:arial,helvetica,= sans-serif;font-size:small""><br></div><div class=3D""gmail_default"" style=3D= ""font-family:arial,helvetica,sans-serif;font-size:small""><img src=3D""cid:ii= _mhko23xl0"" alt=3D""image.png"" width=3D""562"" height=3D""31""><br></div></div><= br><div class=3D""gmail_quote gmail_quote_container""><div dir=3D""ltr"" class= =3D""gmail_attr"">On Mon, Nov 3, 2025 at 5:47=E2=80=AFPM Zhibo Zhang &lt;<a h= ref=3D""mailto:zzbatmos@umbc.edu"">zzbatmos@umbc.edu</a>&gt; wrote:<br></div>= <blockquote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-= left:1px solid rgb(204,204,204);padding-left:1ex""><div dir=3D""ltr""><div cla= ss=3D""gmail_default"" style=3D""font-family:arial,helvetica,sans-serif;font-s= ize:small"">Thanks=C2=A0for the update, Greg. We will wait until=C2=A0the mi= gration finishes to resume our usage in Chip. Please keep us posted. -Zhibo= </div></div><br><div class=3D""gmail_quote""><div dir=3D""ltr"" class=3D""gmail_= attr"">On Mon, Nov 3, 2025 at 4:39=E2=80=AFPM Greg Ballantine via RT &lt;<a = href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu= </a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""margin:= 0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"">= Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D328= 7967"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3287967</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hello Zhibo,<br> <br> Just to give you an update: the migration is still going, and my best guess= <br> currently is it will take another 2-3 days to finish.<br> <br> I&#39;ll leave it running and will send you an update tomorrow morning with=  where<br> it&#39;s at.<br> <br> Best,<br> <br> Greg<br> <br> On Mon Nov 03 10:22:02 2025, OX20655 wrote:<br> <br> &gt; Ok, Thanks for the update. Please keep us updated. -Zhibo On Mon, Nov = 3,<br> &gt; 2025 at 10:00 AM Greg Ballantine via RT &lt;<a href=3D""mailto:UMBCHelp= @rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html= ?id=3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tic= ket/Display.html?id=3D3287967</a> &gt;<br> <br> &gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt; Good morning Zhibo,<br> <br> &gt;&gt; Your group&#39;s migration is still copying data to the new Ceph s= torage<br> &gt;&gt; cluster. I<br> &gt;&gt; don&#39;t currently have a good estimate for completion, but it lo= oks like<br> &gt;&gt; it&#39;s<br> &gt;&gt; currently churning through the user directories, and everything el= se<br> &gt;&gt; has been<br> &gt;&gt; copied.<br> <br> &gt;&gt; When it is completed, your data will be available under<br> &gt;&gt; /umbc/rs/pi_zzbatmos,<br> &gt;&gt; otherwise there aren&#39;t any other changes you&#39;ll need to ma= ke on your<br> &gt;&gt; end.<br> <br> &gt;&gt; I apologize for the lack of communication on this matter Friday ev= ening<br> &gt;&gt; and<br> &gt;&gt; over the weekend. I will send you an update later today with the<b= r> &gt;&gt; current<br> &gt;&gt; status.<br> <br> &gt;&gt; Best,<br> &gt;&gt; Greg<br> <br> &gt;&gt; On Sun Nov 02 19:44:36 2025, OX20655 wrote:<br> <br> &gt;&gt; &gt; Greg Please let our group know if the disk migration is compl= eted and<br> &gt;&gt; if we<br> &gt;&gt; &gt; need to do something (e.g., new configurations) to use the ne= w disk<br> &gt;&gt; space.<br> &gt;&gt; &gt; Thanks-Zhibo On Fri, Oct 31, 2025 at 10:41 AM Greg Ballantine=  via RT<br> &gt;&gt; &gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank""= >UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Dis= play.html?id=3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umb= c.edu/Ticket/Display.html?id=3D3287967</a> &gt;<br> <br> &gt;&gt; &gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt; &gt;&gt; Good morning Zhibo,<br> <br> &gt;&gt; &gt;&gt; This is a reminder that we will be performing your group&= #39;s migration<br> &gt;&gt; to<br> &gt;&gt; &gt;&gt; the<br> &gt;&gt; &gt;&gt; Ceph storage cluster today. During this time, please ensu= re there<br> &gt;&gt; are<br> &gt;&gt; &gt;&gt; not any<br> &gt;&gt; &gt;&gt; jobs being run in your research group, otherwise these ma= y be<br> &gt;&gt; &gt;&gt; terminated.<br> <br> &gt;&gt; &gt;&gt; We will provide an update once completed.<br> <br> &gt;&gt; &gt;&gt; Best,<br> &gt;&gt; &gt;&gt; Greg<br> <br> &gt;&gt; &gt;&gt; On Thu Oct 30 16:22:50 2025, OX20655 wrote:<br> <br> &gt;&gt; &gt;&gt; &gt; Hi All This is a reminder that DOIT will help us wit= h the Chip<br> &gt;&gt; disk<br> &gt;&gt; &gt;&gt; &gt; migration tomorrow Oct. 31st. During this period, pl= ease do not<br> &gt;&gt; use<br> &gt;&gt; &gt;&gt; the<br> &gt;&gt; &gt;&gt; &gt; Chip server until we are notified that the migration=  process is<br> &gt;&gt; &gt;&gt; finished.<br> &gt;&gt; &gt;&gt; &gt; Thanks -Zhibo On Mon, Oct 27, 2025 at 11:23 AM Zhibo=  Zhang<br> &gt;&gt; &gt;&gt; &gt; &lt;<a href=3D""mailto:zzbatmos@umbc.edu"" target=3D""_= blank"">zzbatmos@umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; &gt;&gt; &gt;&gt; Yes, that will be fine. thanks, Greg On Mon, Oct=  27, 2025 at<br> &gt;&gt; 11:21<br> &gt;&gt; &gt;&gt; AM<br> &gt;&gt; &gt;&gt; &gt;&gt; Greg Ballantine via RT &lt;<a href=3D""mailto:UMB= CHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.e= du/Ticket/Display.html?id=3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">h= ttps://rt.umbc.edu/Ticket/Display.html?id=3D3287967</a><br> &gt;&gt; &gt;<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Good morning Zhibo,<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Yes, Friday the 31st will work for starting = your migration.<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; However, since your<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; storage volume is currently very large (~482= TB as of this<br> &gt;&gt; morning)<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; there is a<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; good chance that your migration will take lo= nger than the<br> &gt;&gt; business<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; day that we<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; typically plan for. We&#39;ve started copyin= g your data to the new<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; storage server<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; to minimize downtime, but I would still expe= ct the final<br> &gt;&gt; migration<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; to run<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; through the weekend and likely into the foll= owing week.<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Will that time frame still work for your gro= up?<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Thank you,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Greg<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; On Wed Oct 22 10:14:02 2025, OX20655 wrote:<= br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Hi Sorry for the late reply. Our group = has decided to make the<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; disk<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; migration on Oct. 31 Friday. Would that=  work for you? Thanks.<br> &gt;&gt; On<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Wed, Oct<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; 8, 2025 at 12:42 PM Greg Ballantine via=  RT<br> &gt;&gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBC= Help@rt.umbc.edu</a>&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; wrote:<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Ticket &lt;URL:<br> &gt;&gt; <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D3287967"" r= el=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html= ?id=3D3287967</a><br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Hello Zhibo,<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Yes, a response by Friday is fine.<= br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Thank you,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Greg<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; On Tue Oct 07 21:29:13 2025, OX2065= 5 wrote:<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Hi Elliot We will choose optio= n 1 Schedule a group-wide<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; downtime date<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; during standard business hours= ,. I need to determine the<br> &gt;&gt; exact<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; date<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; with my<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; team members though. Can I get=  back to you by Friday?<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Thanks-Zhibo On<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Tue,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Oct 7, 2025 at 3:20 PM via RT = &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.u= mbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Greetings,<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; This message has been auto= matically generated in response<br> &gt;&gt; to<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; the<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; creation of a ticket regar= ding:<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt;<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; ------------------------------------------------------------------= -------<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Subject: &quot;Migrating R= esearch Storage Volume to Ceph<br> &gt;&gt; Cluster -<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; pi_zzbatmos&quot;<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Message:<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Dear Zhibo,<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; As per the communication v= ia myUMBC earlier this summer<br> &gt;&gt; (June<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; HPCF<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Newsletter<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; ), DoIT is in the process = of migrating data off of an<br> &gt;&gt; older<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; storage<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; server to<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; our new RRStor Ceph storag= e cluster. Your group is using<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; 458.94 TB<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; of a<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; 505.0000 TB quota on the o= ld storage server.<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; To perform these migration= s, we need to take individual<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; storage<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; volumes<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; offline<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; while we migrate them to t= he Ceph cluster. Thus we are<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; reaching out<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; to<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; schedule<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; a date where we can migrat= e your volume located at<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; =E2=80=9C/umbc/rs/zzbatmos=E2=80=9D= .<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; During<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; the migration, we will tak= e your volume offline and will<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; terminate<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; any<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; jobs<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; running on the chip comput= e cluster that are accessing<br> &gt;&gt; this<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; volume.<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Below we=E2=80=99ve listed=  two options for handling this data<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; migration -<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; please let us<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; know which of these you=E2= =80=99d prefer.<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Option 1: Schedule a group= -wide downtime date during<br> &gt;&gt; standard<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; business<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; hours,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; which can be done by respo= nding to this email with your<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; preferred<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; date(s) to<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; perform the migration. Dur= ing this time, DoIT staff will<br> &gt;&gt; work<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; to<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; migrate your<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; volume to the Ceph storage=  cluster. DoIT staff will send<br> &gt;&gt; an<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; email<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; alert<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; on this<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; email thread when the migr= ation has begun and when it has<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; completed.<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; For most<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; storage volumes, this proc= ess should take less than a<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; business day.<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Option 2: If you don=E2=80= =99t respond to this email by October<br> &gt;&gt; 15th,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; DoIT<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; staff will<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; assign a day over the foll= owing month (October 16th<br> &gt;&gt; through<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; November<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; 15th) to<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; migrate your volume. The d= ay chosen will be random and<br> &gt;&gt; will<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; occur<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; during<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; business hours. You will b= e notified of the date chosen to<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; perform<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; the<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; migration, and will be not= ified when the migration begins<br> &gt;&gt; and<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; completes.<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Note: After this process h= as completed, the new storage<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; volume will<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; have a new<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; name. For example, group = =E2=80=9Cpi_doit=E2=80=9D will find its data<br> &gt;&gt; under<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; =E2=80=9C/umbc/rs/pi_doit= =E2=80=9D,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; or in your group=E2=80=99s=  case you will find your volume under<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; =E2=80=9C/umbc/rs/pi_zzbat= mos=E2=80=9D.<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thank you,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Elliot<br> <br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt;<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; ------------------------------------------------------------------= -------<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; There is no need to reply = to this message right now.<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Your ticket has been assig= ned an ID of [Research Computing<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; #3287967]<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; or<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; you can go there directly = by clicking the link below.<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Ticket &lt;URL:<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; <a href=3D""https://rt.umbc.edu/Ticket/Displa= y.html?id=3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.e= du/Ticket/Display.html?id=3D3287967</a> &gt;<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; You can login to view your=  open tickets at any time by<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; visiting<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; <a href=3D""http://my.umbc.= edu"" rel=3D""noreferrer"" target=3D""_blank"">http://my.umbc.edu</a> and clicki= ng on &quot;Help&quot; and &quot;Request<br> &gt;&gt; Help&quot;.<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Alternately you can click = on <a href=3D""http://my.umbc.edu/help"" rel=3D""noreferrer"" target=3D""_blank""= >http://my.umbc.edu/help</a><br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thank you<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; --<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Gregory BallantineSystem Administra= tor for Research and<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Enterprise<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; ComputingUMBC<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; - DoIT<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; --<br> <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Gregory BallantineSystem Administrator for R= esearch and<br> &gt;&gt; Enterprise<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; ComputingUMBC<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; - DoIT<br> <br> &gt;&gt; &gt;&gt; --<br> <br> &gt;&gt; &gt;&gt; Gregory BallantineSystem Administrator for Research and E= nterprise<br> &gt;&gt; &gt;&gt; ComputingUMBC<br> &gt;&gt; &gt;&gt; - DoIT<br> <br> &gt;&gt; --<br> <br> &gt;&gt; Gregory BallantineSystem Administrator for Research and Enterprise= <br> &gt;&gt; ComputingUMBC<br> &gt;&gt; - DoIT<br> <br> --<br> <br> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br> - DoIT<br> <br> </blockquote></div> </blockquote></div> "
3287967,72708399,Correspond,DoIT-Research-Computing,2025-11-04 15:18:45.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos,open,Greg Ballantine,gballan1,Zhibo Zhang,zzbatmos,zzbatmos@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Zhibo,<br /> <br /> The migration is still moving along (thankfully), and I can confirm that yo= ur new volume was almost full. The Ceph storage server may have some slight=  differences in how it compresses and de-duplicates data, which would be th= e most likely reason why the storage usage number is different from what wa= s on the old server.<br /> <br /> On your old storage volume your group was using 486TB out of a 505TB quota = - we will make sure you have at least 20TB of free space on the new storage=  server when the migration completes.<br /> <br /> Please let me know if you have any more questions or concerns!<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Tue Nov 04 09:33:10 2025, OX20655 wrote:</p>  <blockquote> <div> <div>Hi Greg</div>  <div>&nbsp; &nbsp; &nbsp; I just check the new disk volume&nbsp;/umbc/rs/pi= _zzbatmos using the command df -h | grep zzbatmos. I saw that the new disk = volume is almost up to 100%. I remember that our old volume had 20-30 TB of=  free space before the migration. I&#39;m wondering&nbsp;what is going on a= nd if the migration process is still going&nbsp;well? Thanks.</div>  <div>&nbsp;</div>  <div>&nbsp;</div>  <div>&nbsp;</div> </div> &nbsp;  <div> <div>On Mon, Nov 3, 2025 at 5:47=E2=80=AFPM Zhibo Zhang &lt;zzbatmos@umbc.e= du&gt; wrote:</div>  <blockquote> <div> <div>Thanks&nbsp;for the update, Greg. We will wait until&nbsp;the migratio= n finishes to resume our usage in Chip. Please keep us posted. -Zhibo</div> </div> &nbsp;  <div> <div>On Mon, Nov 3, 2025 at 4:39=E2=80=AFPM Greg Ballantine via RT &lt;UMBC= Help@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 87967 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hello Zhibo,<br /> <br /> Just to give you an update: the migration is still going, and my best guess= <br /> currently is it will take another 2-3 days to finish.<br /> <br /> I&#39;ll leave it running and will send you an update tomorrow morning with=  where<br /> it&#39;s at.<br /> <br /> Best,<br /> <br /> Greg<br /> <br /> On Mon Nov 03 10:22:02 2025, OX20655 wrote:<br /> <br /> &gt; Ok, Thanks for the update. Please keep us updated. -Zhibo On Mon, Nov = 3,<br /> &gt; 2025 at 10:00 AM Greg Ballantine via RT &lt;UMBCHelp@rt.umbc.edu&gt; w= rote:<br /> <br /> &gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32879= 67 &gt;<br /> <br /> &gt;&gt; Last Update From Ticket:<br /> <br /> &gt;&gt; Good morning Zhibo,<br /> <br /> &gt;&gt; Your group&#39;s migration is still copying data to the new Ceph s= torage<br /> &gt;&gt; cluster. I<br /> &gt;&gt; don&#39;t currently have a good estimate for completion, but it lo= oks like<br /> &gt;&gt; it&#39;s<br /> &gt;&gt; currently churning through the user directories, and everything el= se<br /> &gt;&gt; has been<br /> &gt;&gt; copied.<br /> <br /> &gt;&gt; When it is completed, your data will be available under<br /> &gt;&gt; /umbc/rs/pi_zzbatmos,<br /> &gt;&gt; otherwise there aren&#39;t any other changes you&#39;ll need to ma= ke on your<br /> &gt;&gt; end.<br /> <br /> &gt;&gt; I apologize for the lack of communication on this matter Friday ev= ening<br /> &gt;&gt; and<br /> &gt;&gt; over the weekend. I will send you an update later today with the<b= r /> &gt;&gt; current<br /> &gt;&gt; status.<br /> <br /> &gt;&gt; Best,<br /> &gt;&gt; Greg<br /> <br /> &gt;&gt; On Sun Nov 02 19:44:36 2025, OX20655 wrote:<br /> <br /> &gt;&gt; &gt; Greg Please let our group know if the disk migration is compl= eted and<br /> &gt;&gt; if we<br /> &gt;&gt; &gt; need to do something (e.g., new configurations) to use the ne= w disk<br /> &gt;&gt; space.<br /> &gt;&gt; &gt; Thanks-Zhibo On Fri, Oct 31, 2025 at 10:41 AM Greg Ballantine=  via RT<br /> &gt;&gt; &gt; &lt;UMBCHelp@rt.umbc.edu&gt; wrote:<br /> <br /> &gt;&gt; &gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?i= d=3D3287967 &gt;<br /> <br /> &gt;&gt; &gt;&gt; Last Update From Ticket:<br /> <br /> &gt;&gt; &gt;&gt; Good morning Zhibo,<br /> <br /> &gt;&gt; &gt;&gt; This is a reminder that we will be performing your group&= #39;s migration<br /> &gt;&gt; to<br /> &gt;&gt; &gt;&gt; the<br /> &gt;&gt; &gt;&gt; Ceph storage cluster today. During this time, please ensu= re there<br /> &gt;&gt; are<br /> &gt;&gt; &gt;&gt; not any<br /> &gt;&gt; &gt;&gt; jobs being run in your research group, otherwise these ma= y be<br /> &gt;&gt; &gt;&gt; terminated.<br /> <br /> &gt;&gt; &gt;&gt; We will provide an update once completed.<br /> <br /> &gt;&gt; &gt;&gt; Best,<br /> &gt;&gt; &gt;&gt; Greg<br /> <br /> &gt;&gt; &gt;&gt; On Thu Oct 30 16:22:50 2025, OX20655 wrote:<br /> <br /> &gt;&gt; &gt;&gt; &gt; Hi All This is a reminder that DOIT will help us wit= h the Chip<br /> &gt;&gt; disk<br /> &gt;&gt; &gt;&gt; &gt; migration tomorrow Oct. 31st. During this period, pl= ease do not<br /> &gt;&gt; use<br /> &gt;&gt; &gt;&gt; the<br /> &gt;&gt; &gt;&gt; &gt; Chip server until we are notified that the migration=  process is<br /> &gt;&gt; &gt;&gt; finished.<br /> &gt;&gt; &gt;&gt; &gt; Thanks -Zhibo On Mon, Oct 27, 2025 at 11:23 AM Zhibo=  Zhang<br /> &gt;&gt; &gt;&gt; &gt; &lt;zzbatmos@umbc.edu&gt; wrote:<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt; Yes, that will be fine. thanks, Greg On Mon, Oct=  27, 2025 at<br /> &gt;&gt; 11:21<br /> &gt;&gt; &gt;&gt; AM<br /> &gt;&gt; &gt;&gt; &gt;&gt; Greg Ballantine via RT &lt;UMBCHelp@rt.umbc.edu&= gt; wrote:<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/D= isplay.html?id=3D3287967<br /> &gt;&gt; &gt;<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Last Update From Ticket:<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Good morning Zhibo,<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Yes, Friday the 31st will work for starting = your migration.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; However, since your<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; storage volume is currently very large (~482= TB as of this<br /> &gt;&gt; morning)<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; there is a<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; good chance that your migration will take lo= nger than the<br /> &gt;&gt; business<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; day that we<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; typically plan for. We&#39;ve started copyin= g your data to the new<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; storage server<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; to minimize downtime, but I would still expe= ct the final<br /> &gt;&gt; migration<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; to run<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; through the weekend and likely into the foll= owing week.<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Will that time frame still work for your gro= up?<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Thank you,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Greg<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; On Wed Oct 22 10:14:02 2025, OX20655 wrote:<= br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Hi Sorry for the late reply. Our group = has decided to make the<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; disk<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; migration on Oct. 31 Friday. Would that=  work for you? Thanks.<br /> &gt;&gt; On<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Wed, Oct<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; 8, 2025 at 12:42 PM Greg Ballantine via=  RT<br /> &gt;&gt; &lt;UMBCHelp@rt.umbc.edu&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; wrote:<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Ticket &lt;URL:<br /> &gt;&gt; https://rt.umbc.edu/Ticket/Display.html?id=3D3287967<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Last Update From Ticket:<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Hello Zhibo,<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Yes, a response by Friday is fine.<= br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Thank you,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Greg<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; On Tue Oct 07 21:29:13 2025, OX2065= 5 wrote:<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Hi Elliot We will choose optio= n 1 Schedule a group-wide<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; downtime date<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; during standard business hours= ,. I need to determine the<br /> &gt;&gt; exact<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; date<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; with my<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; team members though. Can I get=  back to you by Friday?<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Thanks-Zhibo On<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Tue,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Oct 7, 2025 at 3:20 PM via RT = &lt;UMBCHelp@rt.umbc.edu&gt; wrote:<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Greetings,<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; This message has been auto= matically generated in response<br /> &gt;&gt; to<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; the<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; creation of a ticket regar= ding:<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; ------------------------------------------------------------------= -------<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Subject: &quot;Migrating R= esearch Storage Volume to Ceph<br /> &gt;&gt; Cluster -<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; pi_zzbatmos&quot;<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Message:<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Dear Zhibo,<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; As per the communication v= ia myUMBC earlier this summer<br /> &gt;&gt; (June<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; HPCF<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Newsletter<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; ), DoIT is in the process = of migrating data off of an<br /> &gt;&gt; older<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; storage<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; server to<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; our new RRStor Ceph storag= e cluster. Your group is using<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; 458.94 TB<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; of a<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; 505.0000 TB quota on the o= ld storage server.<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; To perform these migration= s, we need to take individual<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; storage<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; volumes<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; offline<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; while we migrate them to t= he Ceph cluster. Thus we are<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; reaching out<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; to<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; schedule<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; a date where we can migrat= e your volume located at<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &ldquo;/umbc/rs/zzbatmos&rdquo;.<br=  /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; During<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; the migration, we will tak= e your volume offline and will<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; terminate<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; any<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; jobs<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; running on the chip comput= e cluster that are accessing<br /> &gt;&gt; this<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; volume.<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Below we&rsquo;ve listed t= wo options for handling this data<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; migration -<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; please let us<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; know which of these you&rs= quo;d prefer.<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Option 1: Schedule a group= -wide downtime date during<br /> &gt;&gt; standard<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; business<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; hours,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; which can be done by respo= nding to this email with your<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; preferred<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; date(s) to<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; perform the migration. Dur= ing this time, DoIT staff will<br /> &gt;&gt; work<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; to<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; migrate your<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; volume to the Ceph storage=  cluster. DoIT staff will send<br /> &gt;&gt; an<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; email<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; alert<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; on this<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; email thread when the migr= ation has begun and when it has<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; completed.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; For most<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; storage volumes, this proc= ess should take less than a<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; business day.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Option 2: If you don&rsquo= ;t respond to this email by October<br /> &gt;&gt; 15th,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; DoIT<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; staff will<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; assign a day over the foll= owing month (October 16th<br /> &gt;&gt; through<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; November<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; 15th) to<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; migrate your volume. The d= ay chosen will be random and<br /> &gt;&gt; will<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; occur<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; during<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; business hours. You will b= e notified of the date chosen to<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; perform<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; the<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; migration, and will be not= ified when the migration begins<br /> &gt;&gt; and<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; completes.<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Note: After this process h= as completed, the new storage<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; volume will<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; have a new<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; name. For example, group &= ldquo;pi_doit&rdquo; will find its data<br /> &gt;&gt; under<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &ldquo;/umbc/rs/pi_doit&rd= quo;,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; or in your group&rsquo;s c= ase you will find your volume under<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; &ldquo;/umbc/rs/pi_zzbatmo= s&rdquo;.<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thank you,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Elliot<br /> <br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; ------------------------------------------------------------------= -------<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; There is no need to reply = to this message right now.<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Your ticket has been assig= ned an ID of [Research Computing<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; #3287967]<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; or<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; you can go there directly = by clicking the link below.<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Ticket &lt;URL:<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; https://rt.umbc.edu/Ticket/Display.html?id= =3D3287967 &gt;<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; You can login to view your=  open tickets at any time by<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; visiting<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; http://my.umbc.edu and cli= cking on &quot;Help&quot; and &quot;Request<br /> &gt;&gt; Help&quot;.<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Alternately you can click = on http://my.umbc.edu/help<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thank you<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; --<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Gregory BallantineSystem Administra= tor for Research and<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Enterprise<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; ComputingUMBC<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; - DoIT<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; --<br /> <br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Gregory BallantineSystem Administrator for R= esearch and<br /> &gt;&gt; Enterprise<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; ComputingUMBC<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; - DoIT<br /> <br /> &gt;&gt; &gt;&gt; --<br /> <br /> &gt;&gt; &gt;&gt; Gregory BallantineSystem Administrator for Research and E= nterprise<br /> &gt;&gt; &gt;&gt; ComputingUMBC<br /> &gt;&gt; &gt;&gt; - DoIT<br /> <br /> &gt;&gt; --<br /> <br /> &gt;&gt; Gregory BallantineSystem Administrator for Research and Enterprise= <br /> &gt;&gt; ComputingUMBC<br /> &gt;&gt; - DoIT<br /> <br /> --<br /> <br /> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br /> - DoIT<br /> &nbsp;</blockquote> </div> </blockquote> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3287967,72731652,Correspond,DoIT-Research-Computing,2025-11-04 20:25:12.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos,open,Greg Ballantine,gballan1,Zhibo Zhang,zzbatmos,zzbatmos@umbc.edu,Zhibo Zhang,zzbatmos@umbc.edu,"Hi Greg      Thanks a lot for the update. I saw that some additional disk has been allocated to our disk partition. I really appreciate it. Hopefully the migration process can finish soon and successfully. -Zhibo  On Tue, Nov 4, 2025 at 10:18=E2=80=AFAM Greg Ballantine via RT <UMBCHelp@rt= .umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > > > Last Update From Ticket: > > Hello Zhibo, > > The migration is still moving along (thankfully), and I can confirm that > your > new volume was almost full. The Ceph storage server may have some slight > differences in how it compresses and de-duplicates data, which would be t= he > most likely reason why the storage usage number is different from what was > on > the old server. > > On your old storage volume your group was using 486TB out of a 505TB quota > - we > will make sure you have at least 20TB of free space on the new storage > server > when the migration completes. > > Please let me know if you have any more questions or concerns! > > Best, > Greg > > On Tue Nov 04 09:33:10 2025, OX20655 wrote: > > > Hi Greg I just check the new disk volume /umbc/rs/pi_zzbatmos using the > > command df -h | grep zzbatmos. I saw that the new disk volume is almost > up > > to 100%. I remember that our old volume had 20-30 TB of free space befo= re > > the migration. I'm wondering what is going on and if the migration > process > > is still going well? Thanks. On Mon, Nov 3, 2025 at 5:47 PM Zhibo Zhang > > <zzbatmos@umbc.edu> wrote: > > >> Thanks for the update, Greg. We will wait until the migration finishes > >> to resume our usage in Chip. Please keep us posted. -Zhibo On Mon, Nov > >> 3, 2025 at 4:39 PM Greg Ballantine via RT <UMBCHelp@rt.umbc.edu> wrote: > > >>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > > > >>> Last Update From Ticket: > > >>> Hello Zhibo, > > >>> Just to give you an update: the migration is still going, and my > >>> best guess > >>> currently is it will take another 2-3 days to finish. > > >>> I'll leave it running and will send you an update tomorrow morning > >>> with where > >>> it's at. > > >>> Best, > > >>> Greg > > >>> On Mon Nov 03 10:22:02 2025, OX20655 wrote: > > >>> > Ok, Thanks for the update. Please keep us updated. -Zhibo On Mon, > >>> Nov 3, > >>> > 2025 at 10:00 AM Greg Ballantine via RT <UMBCHelp@rt.umbc.edu> > >>> wrote: > > >>> >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > >>> > > > >>> >> Last Update From Ticket: > > >>> >> Good morning Zhibo, > > >>> >> Your group's migration is still copying data to the new Ceph > >>> storage > >>> >> cluster. I > >>> >> don't currently have a good estimate for completion, but it > >>> looks like > >>> >> it's > >>> >> currently churning through the user directories, and everything > >>> else > >>> >> has been > >>> >> copied. > > >>> >> When it is completed, your data will be available under > >>> >> /umbc/rs/pi_zzbatmos, > >>> >> otherwise there aren't any other changes you'll need to make on > >>> your > >>> >> end. > > >>> >> I apologize for the lack of communication on this matter Friday > >>> evening > >>> >> and > >>> >> over the weekend. I will send you an update later today with the > >>> >> current > >>> >> status. > > >>> >> Best, > >>> >> Greg > > >>> >> On Sun Nov 02 19:44:36 2025, OX20655 wrote: > > >>> >> > Greg Please let our group know if the disk migration is > >>> completed and > >>> >> if we > >>> >> > need to do something (e.g., new configurations) to use the new > >>> disk > >>> >> space. > >>> >> > Thanks-Zhibo On Fri, Oct 31, 2025 at 10:41 AM Greg Ballantine > >>> via RT > >>> >> > <UMBCHelp@rt.umbc.edu> wrote: > > >>> >> >> Ticket <URL: > >>> https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > > > >>> >> >> Last Update From Ticket: > > >>> >> >> Good morning Zhibo, > > >>> >> >> This is a reminder that we will be performing your group's > >>> migration > >>> >> to > >>> >> >> the > >>> >> >> Ceph storage cluster today. During this time, please ensure > >>> there > >>> >> are > >>> >> >> not any > >>> >> >> jobs being run in your research group, otherwise these may be > >>> >> >> terminated. > > >>> >> >> We will provide an update once completed. > > >>> >> >> Best, > >>> >> >> Greg > > >>> >> >> On Thu Oct 30 16:22:50 2025, OX20655 wrote: > > >>> >> >> > Hi All This is a reminder that DOIT will help us with the > >>> Chip > >>> >> disk > >>> >> >> > migration tomorrow Oct. 31st. During this period, please do > >>> not > >>> >> use > >>> >> >> the > >>> >> >> > Chip server until we are notified that the migration > >>> process is > >>> >> >> finished. > >>> >> >> > Thanks -Zhibo On Mon, Oct 27, 2025 at 11:23 AM Zhibo Zhang > >>> >> >> > <zzbatmos@umbc.edu> wrote: > > >>> >> >> >> Yes, that will be fine. thanks, Greg On Mon, Oct 27, 2025 > >>> at > >>> >> 11:21 > >>> >> >> AM > >>> >> >> >> Greg Ballantine via RT <UMBCHelp@rt.umbc.edu> wrote: > > >>> >> >> >>> Ticket <URL: > >>> https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > >>> >> > > > >>> >> >> >>> Last Update From Ticket: > > >>> >> >> >>> Good morning Zhibo, > > >>> >> >> >>> Yes, Friday the 31st will work for starting your > >>> migration. > >>> >> >> >>> However, since your > >>> >> >> >>> storage volume is currently very large (~482TB as of this > >>> >> morning) > >>> >> >> >>> there is a > >>> >> >> >>> good chance that your migration will take longer than the > >>> >> business > >>> >> >> >>> day that we > >>> >> >> >>> typically plan for. We've started copying your data to > >>> the new > >>> >> >> >>> storage server > >>> >> >> >>> to minimize downtime, but I would still expect the final > >>> >> migration > >>> >> >> >>> to run > >>> >> >> >>> through the weekend and likely into the following week. > > >>> >> >> >>> Will that time frame still work for your group? > > >>> >> >> >>> Thank you, > >>> >> >> >>> Greg > > >>> >> >> >>> On Wed Oct 22 10:14:02 2025, OX20655 wrote: > > >>> >> >> >>> > Hi Sorry for the late reply. Our group has decided to > >>> make the > >>> >> >> >>> disk > >>> >> >> >>> > migration on Oct. 31 Friday. Would that work for you? > >>> Thanks. > >>> >> On > >>> >> >> >>> Wed, Oct > >>> >> >> >>> > 8, 2025 at 12:42 PM Greg Ballantine via RT > >>> >> <UMBCHelp@rt.umbc.edu> > >>> >> >> >>> wrote: > > >>> >> >> >>> >> Ticket <URL: > >>> >> https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > >>> >> >> >>> > > > >>> >> >> >>> >> Last Update From Ticket: > > >>> >> >> >>> >> Hello Zhibo, > > >>> >> >> >>> >> Yes, a response by Friday is fine. > > >>> >> >> >>> >> Thank you, > >>> >> >> >>> >> Greg > > >>> >> >> >>> >> On Tue Oct 07 21:29:13 2025, OX20655 wrote: > > >>> >> >> >>> >> > Hi Elliot We will choose option 1 Schedule a > >>> group-wide > >>> >> >> >>> downtime date > >>> >> >> >>> >> > during standard business hours,. I need to determine > >>> the > >>> >> exact > >>> >> >> >>> date > >>> >> >> >>> >> with my > >>> >> >> >>> >> > team members though. Can I get back to you by > >>> Friday? > >>> >> >> >>> Thanks-Zhibo On > >>> >> >> >>> >> Tue, > >>> >> >> >>> >> > Oct 7, 2025 at 3:20 PM via RT <UMBCHelp@rt.umbc.edu> > >>> wrote: > > >>> >> >> >>> >> >> Greetings, > > >>> >> >> >>> >> >> This message has been automatically generated in > >>> response > >>> >> to > >>> >> >> >>> the > >>> >> >> >>> >> >> creation of a ticket regarding: > > >>> >> >> >>> >> >> > >>> >> >> >>> >> > >>> >> >> >>> > >>> >> >> > >>> >> > >>> > ------------------------------------------------------------------------- > >>> >> >> >>> >> >> Subject: ""Migrating Research Storage Volume to Ceph > >>> >> Cluster - > >>> >> >> >>> >> >> pi_zzbatmos"" > > >>> >> >> >>> >> >> Message: > > >>> >> >> >>> >> >> Dear Zhibo, > > >>> >> >> >>> >> >> As per the communication via myUMBC earlier this > >>> summer > >>> >> (June > >>> >> >> >>> HPCF > >>> >> >> >>> >> >> Newsletter > >>> >> >> >>> >> >> ), DoIT is in the process of migrating data off of > >>> an > >>> >> older > >>> >> >> >>> storage > >>> >> >> >>> >> >> server to > >>> >> >> >>> >> >> our new RRStor Ceph storage cluster. Your group is > >>> using > >>> >> >> >>> 458.94 TB > >>> >> >> >>> >> of a > >>> >> >> >>> >> >> 505.0000 TB quota on the old storage server. > > >>> >> >> >>> >> >> To perform these migrations, we need to take > >>> individual > >>> >> >> >>> storage > >>> >> >> >>> >> volumes > >>> >> >> >>> >> >> offline > >>> >> >> >>> >> >> while we migrate them to the Ceph cluster. Thus we > >>> are > >>> >> >> >>> reaching out > >>> >> >> >>> >> to > >>> >> >> >>> >> >> schedule > >>> >> >> >>> >> >> a date where we can migrate your volume located at > >>> >> >> >>> >> =E2=80=9C/umbc/rs/zzbatmos=E2=80=9D. > >>> >> >> >>> >> >> During > >>> >> >> >>> >> >> the migration, we will take your volume offline and > >>> will > >>> >> >> >>> terminate > >>> >> >> >>> >> any > >>> >> >> >>> >> >> jobs > >>> >> >> >>> >> >> running on the chip compute cluster that are > >>> accessing > >>> >> this > >>> >> >> >>> volume. > > >>> >> >> >>> >> >> Below we=E2=80=99ve listed two options for handling t= his > >>> data > >>> >> >> >>> migration - > >>> >> >> >>> >> >> please let us > >>> >> >> >>> >> >> know which of these you=E2=80=99d prefer. > > >>> >> >> >>> >> >> Option 1: Schedule a group-wide downtime date > >>> during > >>> >> standard > >>> >> >> >>> >> business > >>> >> >> >>> >> >> hours, > >>> >> >> >>> >> >> which can be done by responding to this email with > >>> your > >>> >> >> >>> preferred > >>> >> >> >>> >> >> date(s) to > >>> >> >> >>> >> >> perform the migration. During this time, DoIT staff > >>> will > >>> >> work > >>> >> >> >>> to > >>> >> >> >>> >> >> migrate your > >>> >> >> >>> >> >> volume to the Ceph storage cluster. DoIT staff will > >>> send > >>> >> an > >>> >> >> >>> email > >>> >> >> >>> >> alert > >>> >> >> >>> >> >> on this > >>> >> >> >>> >> >> email thread when the migration has begun and when > >>> it has > >>> >> >> >>> completed. > >>> >> >> >>> >> >> For most > >>> >> >> >>> >> >> storage volumes, this process should take less than > >>> a > >>> >> >> >>> business day. > >>> >> >> >>> >> >> Option 2: If you don=E2=80=99t respond to this email = by > >>> October > >>> >> 15th, > >>> >> >> >>> DoIT > >>> >> >> >>> >> >> staff will > >>> >> >> >>> >> >> assign a day over the following month (October 16th > >>> >> through > >>> >> >> >>> November > >>> >> >> >>> >> >> 15th) to > >>> >> >> >>> >> >> migrate your volume. The day chosen will be random > >>> and > >>> >> will > >>> >> >> >>> occur > >>> >> >> >>> >> >> during > >>> >> >> >>> >> >> business hours. You will be notified of the date > >>> chosen to > >>> >> >> >>> perform > >>> >> >> >>> >> the > >>> >> >> >>> >> >> migration, and will be notified when the migration > >>> begins > >>> >> and > >>> >> >> >>> >> >> completes. > > >>> >> >> >>> >> >> Note: After this process has completed, the new > >>> storage > >>> >> >> >>> volume will > >>> >> >> >>> >> >> have a new > >>> >> >> >>> >> >> name. For example, group =E2=80=9Cpi_doit=E2=80=9D wi= ll find its > >>> data > >>> >> under > >>> >> >> >>> >> >> =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, > >>> >> >> >>> >> >> or in your group=E2=80=99s case you will find your vo= lume > >>> under > >>> >> >> >>> >> >> =E2=80=9C/umbc/rs/pi_zzbatmos=E2=80=9D. > > >>> >> >> >>> >> >> Thank you, > >>> >> >> >>> >> >> Elliot > > > >>> >> >> >>> >> >> > >>> >> >> >>> >> > >>> >> >> >>> > >>> >> >> > >>> >> > >>> > ------------------------------------------------------------------------- > > >>> >> >> >>> >> >> There is no need to reply to this message right > >>> now. > > >>> >> >> >>> >> >> Your ticket has been assigned an ID of [Research > >>> Computing > >>> >> >> >>> #3287967] > >>> >> >> >>> >> or > >>> >> >> >>> >> >> you can go there directly by clicking the link > >>> below. > > >>> >> >> >>> >> >> Ticket <URL: > >>> >> >> >>> https://rt.umbc.edu/Ticket/Display.html?id=3D3287967 > > > >>> >> >> >>> >> >> You can login to view your open tickets at any time > >>> by > >>> >> >> >>> visiting > >>> >> >> >>> >> >> http://my.umbc.edu and clicking on ""Help"" and > >>> ""Request > >>> >> Help"". > > >>> >> >> >>> >> >> Alternately you can click on > >>> http://my.umbc.edu/help > > >>> >> >> >>> >> >> Thank you > > >>> >> >> >>> >> -- > > >>> >> >> >>> >> Gregory BallantineSystem Administrator for Research > >>> and > >>> >> >> >>> Enterprise > >>> >> >> >>> >> ComputingUMBC > >>> >> >> >>> >> - DoIT > > >>> >> >> >>> -- > > >>> >> >> >>> Gregory BallantineSystem Administrator for Research and > >>> >> Enterprise > >>> >> >> >>> ComputingUMBC > >>> >> >> >>> - DoIT > > >>> >> >> -- > > >>> >> >> Gregory BallantineSystem Administrator for Research and > >>> Enterprise > >>> >> >> ComputingUMBC > >>> >> >> - DoIT > > >>> >> -- > > >>> >> Gregory BallantineSystem Administrator for Research and > >>> Enterprise > >>> >> ComputingUMBC > >>> >> - DoIT > > >>> -- > > >>> Gregory BallantineSystem Administrator for Research and Enterprise > >>> ComputingUMBC > >>> - DoIT > > -- > > Gregory BallantineSystem Administrator for Research and Enterprise > ComputingUMBC > - DoIT > > "
3287967,72731652,Correspond,DoIT-Research-Computing,2025-11-04 20:25:12.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zzbatmos,open,Greg Ballantine,gballan1,Zhibo Zhang,zzbatmos,zzbatmos@umbc.edu,Zhibo Zhang,zzbatmos@umbc.edu,"<div dir=3D""ltr""><div class=3D""gmail_default"" style=3D""font-family:arial,he= lvetica,sans-serif;font-size:small"">Hi Greg</div><div class=3D""gmail_defaul= t"" style=3D""font-family:arial,helvetica,sans-serif;font-size:small"">=C2=A0 = =C2=A0 =C2=A0Thanks a lot for the=C2=A0update. I saw that some additional= =C2=A0disk has been allocated to our disk partition. I really appreciate=C2= =A0it. Hopefully the migration process can finish soon and successfully.=C2= =A0</div><div class=3D""gmail_default"" style=3D""font-family:arial,helvetica,= sans-serif;font-size:small"">-Zhibo</div></div><br><div class=3D""gmail_quote=  gmail_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Tue, Nov 4= , 2025 at 10:18=E2=80=AFAM Greg Ballantine via RT &lt;<a href=3D""mailto:UMB= CHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote=  class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px so= lid rgb(204,204,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://r= t.umbc.edu/Ticket/Display.html?id=3D3287967"" rel=3D""noreferrer"" target=3D""_= blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3287967</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hello Zhibo,<br> <br> The migration is still moving along (thankfully), and I can confirm that yo= ur<br> new volume was almost full. The Ceph storage server may have some slight<br> differences in how it compresses and de-duplicates data, which would be the= <br> most likely reason why the storage usage number is different from what was = on<br> the old server.<br> <br> On your old storage volume your group was using 486TB out of a 505TB quota = - we<br> will make sure you have at least 20TB of free space on the new storage serv= er<br> when the migration completes.<br> <br> Please let me know if you have any more questions or concerns!<br> <br> Best,<br> Greg<br> <br> On Tue Nov 04 09:33:10 2025, OX20655 wrote:<br> <br> &gt; Hi Greg I just check the new disk volume /umbc/rs/pi_zzbatmos using th= e<br> &gt; command df -h | grep zzbatmos. I saw that the new disk volume is almos= t up<br> &gt; to 100%. I remember that our old volume had 20-30 TB of free space bef= ore<br> &gt; the migration. I&#39;m wondering what is going on and if the migration=  process<br> &gt; is still going well? Thanks. On Mon, Nov 3, 2025 at 5:47 PM Zhibo Zhan= g<br> &gt; &lt;<a href=3D""mailto:zzbatmos@umbc.edu"" target=3D""_blank"">zzbatmos@um= bc.edu</a>&gt; wrote:<br> <br> &gt;&gt; Thanks for the update, Greg. We will wait until the migration fini= shes<br> &gt;&gt; to resume our usage in Chip. Please keep us posted. -Zhibo On Mon,=  Nov<br> &gt;&gt; 3, 2025 at 4:39 PM Greg Ballantine via RT &lt;<a href=3D""mailto:UM= BCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<b= r> <br> &gt;&gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.= html?id=3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu= /Ticket/Display.html?id=3D3287967</a> &gt;<br> <br> &gt;&gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt;&gt; Hello Zhibo,<br> <br> &gt;&gt;&gt; Just to give you an update: the migration is still going, and = my<br> &gt;&gt;&gt; best guess<br> &gt;&gt;&gt; currently is it will take another 2-3 days to finish.<br> <br> &gt;&gt;&gt; I&#39;ll leave it running and will send you an update tomorrow=  morning<br> &gt;&gt;&gt; with where<br> &gt;&gt;&gt; it&#39;s at.<br> <br> &gt;&gt;&gt; Best,<br> <br> &gt;&gt;&gt; Greg<br> <br> &gt;&gt;&gt; On Mon Nov 03 10:22:02 2025, OX20655 wrote:<br> <br> &gt;&gt;&gt; &gt; Ok, Thanks for the update. Please keep us updated. -Zhibo=  On Mon,<br> &gt;&gt;&gt; Nov 3,<br> &gt;&gt;&gt; &gt; 2025 at 10:00 AM Greg Ballantine via RT &lt;<a href=3D""ma= ilto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt;<b= r> &gt;&gt;&gt; wrote:<br> <br> &gt;&gt;&gt; &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket= /Display.html?id=3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">https://rt= .umbc.edu/Ticket/Display.html?id=3D3287967</a><br> &gt;&gt;&gt; &gt;<br> <br> &gt;&gt;&gt; &gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt;&gt; &gt;&gt; Good morning Zhibo,<br> <br> &gt;&gt;&gt; &gt;&gt; Your group&#39;s migration is still copying data to t= he new Ceph<br> &gt;&gt;&gt; storage<br> &gt;&gt;&gt; &gt;&gt; cluster. I<br> &gt;&gt;&gt; &gt;&gt; don&#39;t currently have a good estimate for completi= on, but it<br> &gt;&gt;&gt; looks like<br> &gt;&gt;&gt; &gt;&gt; it&#39;s<br> &gt;&gt;&gt; &gt;&gt; currently churning through the user directories, and = everything<br> &gt;&gt;&gt; else<br> &gt;&gt;&gt; &gt;&gt; has been<br> &gt;&gt;&gt; &gt;&gt; copied.<br> <br> &gt;&gt;&gt; &gt;&gt; When it is completed, your data will be available und= er<br> &gt;&gt;&gt; &gt;&gt; /umbc/rs/pi_zzbatmos,<br> &gt;&gt;&gt; &gt;&gt; otherwise there aren&#39;t any other changes you&#39;= ll need to make on<br> &gt;&gt;&gt; your<br> &gt;&gt;&gt; &gt;&gt; end.<br> <br> &gt;&gt;&gt; &gt;&gt; I apologize for the lack of communication on this mat= ter Friday<br> &gt;&gt;&gt; evening<br> &gt;&gt;&gt; &gt;&gt; and<br> &gt;&gt;&gt; &gt;&gt; over the weekend. I will send you an update later tod= ay with the<br> &gt;&gt;&gt; &gt;&gt; current<br> &gt;&gt;&gt; &gt;&gt; status.<br> <br> &gt;&gt;&gt; &gt;&gt; Best,<br> &gt;&gt;&gt; &gt;&gt; Greg<br> <br> &gt;&gt;&gt; &gt;&gt; On Sun Nov 02 19:44:36 2025, OX20655 wrote:<br> <br> &gt;&gt;&gt; &gt;&gt; &gt; Greg Please let our group know if the disk migra= tion is<br> &gt;&gt;&gt; completed and<br> &gt;&gt;&gt; &gt;&gt; if we<br> &gt;&gt;&gt; &gt;&gt; &gt; need to do something (e.g., new configurations) = to use the new<br> &gt;&gt;&gt; disk<br> &gt;&gt;&gt; &gt;&gt; space.<br> &gt;&gt;&gt; &gt;&gt; &gt; Thanks-Zhibo On Fri, Oct 31, 2025 at 10:41 AM Gr= eg Ballantine<br> &gt;&gt;&gt; via RT<br> &gt;&gt;&gt; &gt;&gt; &gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" targ= et=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Ticket &lt;URL:<br> &gt;&gt;&gt; <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D328796= 7"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.= html?id=3D3287967</a> &gt;<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Good morning Zhibo,<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; This is a reminder that we will be performin= g your group&#39;s<br> &gt;&gt;&gt; migration<br> &gt;&gt;&gt; &gt;&gt; to<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; the<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Ceph storage cluster today. During this time= , please ensure<br> &gt;&gt;&gt; there<br> &gt;&gt;&gt; &gt;&gt; are<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; not any<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; jobs being run in your research group, other= wise these may be<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; terminated.<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; We will provide an update once completed.<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Best,<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Greg<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; On Thu Oct 30 16:22:50 2025, OX20655 wrote:<= br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt; Hi All This is a reminder that DOIT wil= l help us with the<br> &gt;&gt;&gt; Chip<br> &gt;&gt;&gt; &gt;&gt; disk<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt; migration tomorrow Oct. 31st. During th= is period, please do<br> &gt;&gt;&gt; not<br> &gt;&gt;&gt; &gt;&gt; use<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; the<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt; Chip server until we are notified that = the migration<br> &gt;&gt;&gt; process is<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; finished.<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt; Thanks -Zhibo On Mon, Oct 27, 2025 at 1= 1:23 AM Zhibo Zhang<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &lt;<a href=3D""mailto:zzbatmos@umbc.edu= "" target=3D""_blank"">zzbatmos@umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Yes, that will be fine. thanks, Gre= g On Mon, Oct 27, 2025<br> &gt;&gt;&gt; at<br> &gt;&gt;&gt; &gt;&gt; 11:21<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; AM<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Greg Ballantine via RT &lt;<a href= =3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>= &gt; wrote:<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; Ticket &lt;URL:<br> &gt;&gt;&gt; <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D328796= 7"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.= html?id=3D3287967</a><br> &gt;&gt;&gt; &gt;&gt; &gt;<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; Good morning Zhibo,<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; Yes, Friday the 31st will work = for starting your<br> &gt;&gt;&gt; migration.<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; However, since your<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; storage volume is currently ver= y large (~482TB as of this<br> &gt;&gt;&gt; &gt;&gt; morning)<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; there is a<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; good chance that your migration=  will take longer than the<br> &gt;&gt;&gt; &gt;&gt; business<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; day that we<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; typically plan for. We&#39;ve s= tarted copying your data to<br> &gt;&gt;&gt; the new<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; storage server<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; to minimize downtime, but I wou= ld still expect the final<br> &gt;&gt;&gt; &gt;&gt; migration<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; to run<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; through the weekend and likely = into the following week.<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; Will that time frame still work=  for your group?<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; Thank you,<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; Greg<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; On Wed Oct 22 10:14:02 2025, OX= 20655 wrote:<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Hi Sorry for the late repl= y. Our group has decided to<br> &gt;&gt;&gt; make the<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; disk<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; migration on Oct. 31 Frida= y. Would that work for you?<br> &gt;&gt;&gt; Thanks.<br> &gt;&gt;&gt; &gt;&gt; On<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; Wed, Oct<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; 8, 2025 at 12:42 PM Greg B= allantine via RT<br> &gt;&gt;&gt; &gt;&gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D= ""_blank"">UMBCHelp@rt.umbc.edu</a>&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; wrote:<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Ticket &lt;URL:<br> &gt;&gt;&gt; &gt;&gt; <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id= =3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket= /Display.html?id=3D3287967</a><br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Last Update From Ticke= t:<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Hello Zhibo,<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Yes, a response by Fri= day is fine.<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Thank you,<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Greg<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; On Tue Oct 07 21:29:13=  2025, OX20655 wrote:<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Hi Elliot We will=  choose option 1 Schedule a<br> &gt;&gt;&gt; group-wide<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; downtime date<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; during standard b= usiness hours,. I need to determine<br> &gt;&gt;&gt; the<br> &gt;&gt;&gt; &gt;&gt; exact<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; date<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; with my<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; team members thou= gh. Can I get back to you by<br> &gt;&gt;&gt; Friday?<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; Thanks-Zhibo On<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Tue,<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Oct 7, 2025 at 3:= 20 PM via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">= UMBCHelp@rt.umbc.edu</a>&gt;<br> &gt;&gt;&gt; wrote:<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Greetings,<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; This message = has been automatically generated in<br> &gt;&gt;&gt; response<br> &gt;&gt;&gt; &gt;&gt; to<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; the<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; creation of a=  ticket regarding:<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; --------------------------------------------------------------= -----------<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Subject: &quo= t;Migrating Research Storage Volume to Ceph<br> &gt;&gt;&gt; &gt;&gt; Cluster -<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; pi_zzbatmos&q= uot;<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Message:<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Dear Zhibo,<b= r> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; As per the co= mmunication via myUMBC earlier this<br> &gt;&gt;&gt; summer<br> &gt;&gt;&gt; &gt;&gt; (June<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; HPCF<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Newsletter<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; ), DoIT is in=  the process of migrating data off of<br> &gt;&gt;&gt; an<br> &gt;&gt;&gt; &gt;&gt; older<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; storage<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; server to<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; our new RRSto= r Ceph storage cluster. Your group is<br> &gt;&gt;&gt; using<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; 458.94 TB<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; of a<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; 505.0000 TB q= uota on the old storage server.<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; To perform th= ese migrations, we need to take<br> &gt;&gt;&gt; individual<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; storage<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; volumes<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; offline<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; while we migr= ate them to the Ceph cluster. Thus we<br> &gt;&gt;&gt; are<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; reaching out<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; to<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; schedule<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; a date where = we can migrate your volume located at<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; =E2=80=9C/umbc/rs/zzba= tmos=E2=80=9D.<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; During<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; the migration= , we will take your volume offline and<br> &gt;&gt;&gt; will<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; terminate<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; any<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; jobs<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; running on th= e chip compute cluster that are<br> &gt;&gt;&gt; accessing<br> &gt;&gt;&gt; &gt;&gt; this<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; volume.<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Below we=E2= =80=99ve listed two options for handling this<br> &gt;&gt;&gt; data<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; migration -<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; please let us= <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; know which of=  these you=E2=80=99d prefer.<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Option 1: Sch= edule a group-wide downtime date<br> &gt;&gt;&gt; during<br> &gt;&gt;&gt; &gt;&gt; standard<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; business<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; hours,<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; which can be = done by responding to this email with<br> &gt;&gt;&gt; your<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; preferred<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; date(s) to<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; perform the m= igration. During this time, DoIT staff<br> &gt;&gt;&gt; will<br> &gt;&gt;&gt; &gt;&gt; work<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; to<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; migrate your<= br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; volume to the=  Ceph storage cluster. DoIT staff will<br> &gt;&gt;&gt; send<br> &gt;&gt;&gt; &gt;&gt; an<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; email<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; alert<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; on this<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; email thread = when the migration has begun and when<br> &gt;&gt;&gt; it has<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; completed.<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; For most<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; storage volum= es, this process should take less than<br> &gt;&gt;&gt; a<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; business day.<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Option 2: If = you don=E2=80=99t respond to this email by<br> &gt;&gt;&gt; October<br> &gt;&gt;&gt; &gt;&gt; 15th,<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; DoIT<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; staff will<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; assign a day = over the following month (October 16th<br> &gt;&gt;&gt; &gt;&gt; through<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; November<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; 15th) to<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; migrate your = volume. The day chosen will be random<br> &gt;&gt;&gt; and<br> &gt;&gt;&gt; &gt;&gt; will<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; occur<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; during<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; business hour= s. You will be notified of the date<br> &gt;&gt;&gt; chosen to<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; perform<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; the<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; migration, an= d will be notified when the migration<br> &gt;&gt;&gt; begins<br> &gt;&gt;&gt; &gt;&gt; and<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; completes.<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Note: After t= his process has completed, the new<br> &gt;&gt;&gt; storage<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; volume will<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; have a new<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; name. For exa= mple, group =E2=80=9Cpi_doit=E2=80=9D will find its<br> &gt;&gt;&gt; data<br> &gt;&gt;&gt; &gt;&gt; under<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; =E2=80=9C/umb= c/rs/pi_doit=E2=80=9D,<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; or in your gr= oup=E2=80=99s case you will find your volume<br> &gt;&gt;&gt; under<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; =E2=80=9C/umb= c/rs/pi_zzbatmos=E2=80=9D.<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thank you,<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Elliot<br> <br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; --------------------------------------------------------------= -----------<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; There is no n= eed to reply to this message right<br> &gt;&gt;&gt; now.<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Your ticket h= as been assigned an ID of [Research<br> &gt;&gt;&gt; Computing<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; #3287967]<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; or<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; you can go th= ere directly by clicking the link<br> &gt;&gt;&gt; below.<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Ticket &lt;UR= L:<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; <a href=3D""https://rt.umbc.edu/= Ticket/Display.html?id=3D3287967"" rel=3D""noreferrer"" target=3D""_blank"">http= s://rt.umbc.edu/Ticket/Display.html?id=3D3287967</a> &gt;<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; You can login=  to view your open tickets at any time<br> &gt;&gt;&gt; by<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; visiting<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; <a href=3D""ht= tp://my.umbc.edu"" rel=3D""noreferrer"" target=3D""_blank"">http://my.umbc.edu</= a> and clicking on &quot;Help&quot; and<br> &gt;&gt;&gt; &quot;Request<br> &gt;&gt;&gt; &gt;&gt; Help&quot;.<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Alternately y= ou can click on<br> &gt;&gt;&gt; <a href=3D""http://my.umbc.edu/help"" rel=3D""noreferrer"" target= =3D""_blank"">http://my.umbc.edu/help</a><br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thank you<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; --<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Gregory BallantineSyst= em Administrator for Research<br> &gt;&gt;&gt; and<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; Enterprise<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; ComputingUMBC<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; - DoIT<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; --<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; Gregory BallantineSystem Admini= strator for Research and<br> &gt;&gt;&gt; &gt;&gt; Enterprise<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; ComputingUMBC<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&gt; - DoIT<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; --<br> <br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Gregory BallantineSystem Administrator for R= esearch and<br> &gt;&gt;&gt; Enterprise<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; ComputingUMBC<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; - DoIT<br> <br> &gt;&gt;&gt; &gt;&gt; --<br> <br> &gt;&gt;&gt; &gt;&gt; Gregory BallantineSystem Administrator for Research a= nd<br> &gt;&gt;&gt; Enterprise<br> &gt;&gt;&gt; &gt;&gt; ComputingUMBC<br> &gt;&gt;&gt; &gt;&gt; - DoIT<br> <br> &gt;&gt;&gt; --<br> <br> &gt;&gt;&gt; Gregory BallantineSystem Administrator for Research and Enterp= rise<br> &gt;&gt;&gt; ComputingUMBC<br> &gt;&gt;&gt; - DoIT<br> <br> --<br> <br> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br> - DoIT<br> <br> </blockquote></div> "
3287969,72147922,Create,DoIT-Research-Computing,2025-10-07 19:22:40.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zweck,stalled,Greg Ballantine,gballan1,John Zweck,zweck,zweck@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear John,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 36.2 GB of a 97.7 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/zweck&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_zweck&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3287969,72147958,Correspond,DoIT-Research-Computing,2025-10-07 19:23:47.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zweck,stalled,Greg Ballantine,gballan1,John Zweck,zweck,zweck@umbc.edu,NULL,exchangeadmins@utdallas.edu,"Delivery has failed to these recipients or groups:  zweck@umbc.edu<mailto:zweck@umbc.edu> The email address you entered couldn't be found. Please check the recipient's email address and try to resend the message. If the problem continues, please contact your email admin.         Diagnostic information for administrators:  Generating server: UTDEX04.campus.ad.utdallas.edu  zweck@umbc.edu Remote Server returned '550 5.1.10 RESOLVER.ADR.RecipientNotFound; Recipient not found by SMTP address lookup'  Original message headers:  Received: from UTDEX03.campus.ad.utdallas.edu (10.182.72.101) by  UTDEX04.campus.ad.utdallas.edu (10.182.72.102) with Microsoft SMTP Server  (version=TLS1_2, cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id  15.2.1748.26; Tue, 7 Oct 2025 14:23:30 -0500 Received: from DS2PR08CU001.outbound.protection.outlook.com (10.44.46.82) by  UTDEX03.campus.ad.utdallas.edu (10.182.72.101) with Microsoft SMTP Server  (version=TLS1_2, cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id  15.2.1748.26 via Frontend Transport; Tue, 7 Oct 2025 14:23:30 -0500 ARC-Seal: i=1; a=rsa-sha256; s=arcselector10001; d=microsoft.com; cv=none;  b=earlPCQ+h2uTDovbozVcitWG42O2+rv175kQcYTlHkVYvmBR/Wu50/uMPTyVIhOnGsZh3+c+u0C+2+ea2HA/7hprNKVhEemaKG3Vdvrs1LmNun7XDb23rIjECkBnXCq2QENIrUmz/vEMj1k7sLliUIUk6l0tecOBW8xRpGBh0SpkJAAHuZjnPy+iyiCPa+RSP6gfSIFA9TGlnOYTs5ZWXJAjltQ1wyeebPHcVI2egI6enMbaszmp+bNddLqgcIS3Y1Nv4MwIUPXAy+K8Xr8k31PF1ERZWMX8XLn6MseLn4+3pBjBPEOSU2jTDvoohYmH9IQLwqs/9PK3vCxVZZi/QQ== ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=microsoft.com;  s=arcselector10001;  h=From:Date:Subject:Message-ID:Content-Type:MIME-Version:X-MS-Exchange-AntiSpam-MessageData-ChunkCount:X-MS-Exchange-AntiSpam-MessageData-0:X-MS-Exchange-AntiSpam-MessageData-1;  bh=MRuAMo8aNgpbjqMgYeor6reLbQ1eB9ekMyOgDqAJ6iM=;  b=hu7G8p/tN6Hxntk5HRptoyXqG/taT9mG0++yUFp3+r6je1Y6w65CNNxsvqDNZ+tLrt0TVYyCdHkjkUaM4Pr97zAIsmepftcEk6dvNCYOts3KGu7KzatrZHF08XXr1WYtEp4SCYtsYh1GNrbO1ny2AKAJb09+ISguhFNKszzJgGI+GU3TQJWXLrsKbSvthFEmcBBURDDp73pyouQTU9JZO0NYsRKn5UT4HBmFu1SSJjgN3CbX7KxSG0owE0LhhnjWSJp3Nmwp/omo1U6iK0KghMjbPUrT9xzjtDuqblitRDmLc0B29wK49l9GcjZoMdnJzULecOhAo7cXTp4tq+Jhfw== ARC-Authentication-Results: i=1; mx.microsoft.com 1; spf=softfail (sender ip  is 130.85.33.105) smtp.rcpttodomain=utdallas.edu smtp.mailfrom=rt.umbc.edu;  dmarc=pass (p=none sp=none pct=100) action=none header.from=rt.umbc.edu;  dkim=test (signature was verified) header.d=umbc.edu; arc=none (0) Received: from BN9P220CA0027.NAMP220.PROD.OUTLOOK.COM (2603:10b6:408:13e::32)  by BN0PR01MB6879.prod.exchangelabs.com (2603:10b6:408:165::7) with Microsoft  SMTP Server (version=TLS1_2, cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id  15.20.9203.9; Tue, 7 Oct 2025 19:22:46 +0000 Received: from MN1PEPF0000ECD5.namprd02.prod.outlook.com  (2603:10b6:408:13e:cafe::15) by BN9P220CA0027.outlook.office365.com  (2603:10b6:408:13e::32) with Microsoft SMTP Server (version=TLS1_3,  cipher=TLS_AES_256_GCM_SHA384) id 15.20.9182.20 via Frontend Transport; Tue,  7 Oct 2025 19:22:46 +0000 Authentication-Results: spf=softfail (sender IP is 130.85.33.105)  smtp.mailfrom=rt.umbc.edu; dkim=test (signature was verified)  header.d=umbc.edu;dmarc=pass action=none  header.from=rt.umbc.edu;compauth=pass reason=100 Received-SPF: SoftFail (protection.outlook.com: domain of transitioning  rt.umbc.edu discourages use of 130.85.33.105 as permitted sender) Received: from mx.mail.umbc.edu (130.85.33.105) by  MN1PEPF0000ECD5.mail.protection.outlook.com (10.167.242.133) with Microsoft  SMTP Server (version=TLS1_2, cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id  15.20.9203.9 via Frontend Transport; Tue, 7 Oct 2025 19:22:45 +0000 Received: from campus-relay.mail.umbc.edu (campus-relay1-v1.mail.umbc.edu [130.85.33.100])         by mx.mail.umbc.edu (Postfix) with ESMTPS id 6831218123E2         for <zweck@umbc.edu>; Tue,  7 Oct 2025 15:22:44 -0400 (EDT) Received: from rt.umbc.edu (rt-web1-v1.umbc.edu [172.24.2.4])         by campus-relay.mail.umbc.edu (Postfix) with ESMTPS id 45E67180605A         for <zweck@umbc.edu>; Tue,  7 Oct 2025 15:22:44 -0400 (EDT) DKIM-Filter: OpenDKIM Filter v2.11.0 campus-relay.mail.umbc.edu 45E67180605A DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=umbc.edu;         s=campusrelay; t=1759864964;         bh=qxRidtKvC+Pt41W7AhRw0TdIc55Br//2gZtQCE6zBNo=;         h=Subject:From:Reply-To:In-Reply-To:References:To:Date:From;         b=d4b2RdN++9Xqkn9Jyc0DJENxh4DKZPAHat3ZPm3ke43fCp8DjDiytTo5CP2hoq6ac          wTU6wPbbOJQahIZore7G9yNKrwFQGyAW9nljbXEENkET1AS+swsfQtccGF7yKAmjwR          tEIzQuQ/FNNcCNb8acp+mIqOJTeUEi+62c6JfdkA= Received: from umbc-rt-app-prod (_gateway [172.19.0.1])         by rt.umbc.edu (8.16.1/8.16.1) with SMTP id 597JMg2P014631         for <zweck@umbc.edu>; Tue, 7 Oct 2025 19:22:42 GMT Received: by umbc-rt-app-prod (sSMTP sendmail emulation); Tue, 07 Oct 2025 19:22:42 +0000 Subject: [Research Computing #3287969] Migrating Research Storage Volume to Ceph Cluster - pi_zweck  [AutoReply] From: "" via RT"" <UMBCHelp@rt.umbc.edu> Reply-To: UMBCHelp@rt.umbc.edu In-Reply-To: References: <RT-Ticket-3287969@rt.umbc.edu> Message-ID: <rt-5.0.5-29288-1759864962-374.3287969-3-0@rt.umbc.edu> X-RT-Loop-Prevention: rt.umbc.edu X-RT-Ticket: rt.umbc.edu #3287969 X-Managed-BY: RT 5.0.5 (http://www.bestpractical.com/rt/) X-RT-Originator: elliotg2@umbc.edu Auto-Submitted: auto-replied To: <zweck@umbc.edu> Content-Type: text/plain; charset=""utf-8"" Content-Transfer-Encoding: quoted-printable X-RT-Original-Encoding: utf-8 Precedence: bulk Date: Tue, 7 Oct 2025 15:22:42 -0400 MIME-Version: 1.0 Return-Path: UMBCHelp@rt.umbc.edu X-EOPAttributedMessage: 0 X-EOPTenantAttributedMessage: 8d281d1d-9c4d-4bf7-b16e-032d15de9f6c:0 X-MS-PublicTrafficType: Email X-MS-TrafficTypeDiagnostic: MN1PEPF0000ECD5:EE_|BN0PR01MB6879:EE_ X-MS-Office365-Filtering-Correlation-Id: 0a0a176e-64bb-4a65-07eb-08de05d6e4c3 X-MS-Exchange-AtpMessageProperties: SA|SL X-Microsoft-Antispam: BCL:0;ARA:13230040|27102699006|12012899012|13003099007; X-Microsoft-Antispam-Message-Info: =?utf-8?B?ZzdlNkNBWjBYMlJlWVBoN0hWSk93L2h6Qmx6emVZQ2FhVG5RRnI3VDB2aHBN?=  =?utf-8?B?YThjcXJjK2lsWTY1L3NiRzZDc2U4R0Y3cGV2WEt5VjFwaGZHY2pVNDViUG5K?=  =?utf-8?B?dDRzVGx6cnZRQ21xUmQ0Z3lpS1ZGdG9FL1hZMm5xRjdtdkQ4bDdJWXptcmtH?=  =?utf-8?B?M2tneG1GVk1IWENKMTJqSFBhT05zVE9HVTg0N3hwRVVPcEFEOUtxNSt3cWcx?=  =?utf-8?B?VFIvejhKc2xKV0NtWEZRTCtlSUZkTTJBMStYSXp5Ym1hTlRlVWc2cHFTS3NT?=  =?utf-8?B?K0M3KzlJVXRwODB4aG16RTM1OXoyUmJCbUxyVC91Q0ZEei9HTmdvVitPMER6?=  =?utf-8?B?Q3MwOHhFMXBSY3hmQlFFMkZqVGZFQlhoOGw0d1pVTTFPdGtaT2IrMjNoakVx?=  =?utf-8?B?Z1ZzY3dOZDVKb3ByOW4xc2dhNnQrUkVpdWlnWnRxM3ZlazBjUkhhbFBXMDR5?=  =?utf-8?B?aiswNE1Mb3hVY05tK0JBVThMZTQyTjJjdnN5SnRvbHM1Q2cxSzB6QXhxT05z?=  =?utf-8?B?WWloK1dhdjdpZEJXc0tnZmdmWlRFTm9HMkZZbFZiamR1V2NHbjl5d3g4Qkpu?=  =?utf-8?B?eEs0ZEZnM2hSZ0ZSK2RIaDdJd1VRSU4xRnFLdUlYaU5rMzNOTDBhZ2d5N0xU?=  =?utf-8?B?Y1FlRGp0QkV3ZXNKbW1OSytZZWVQOUZpUjloam1LRXVod1B6MDJrNUd6THBv?=  =?utf-8?B?WWQyRWZTTlpNeHVyMlgxT0orbXNrR3k5bXl3REE2ZWpYbDNsSHB5S1V6V1B6?=  =?utf-8?B?ZWNUL1VOSXpaajFUSmNsTmpUc0NBUmV4RWRWYk9iS0ZweEFwVzJjTEVLaEpZ?=  =?utf-8?B?ODdPZG9EcElBY0ZpMFpENnJ2alJDdklsR0NjNml3SWVrREMzTDhpVXlKWjht?=  =?utf-8?B?K1RVeTZibGUwWEorYy9DSVlObWZLZjJid0RNVVNqdzJpYWlBT1lPZ0U4Z3hj?=  =?utf-8?B?c0NIa0l0VGhZU2g4SlpibStQVmhxTjhhQ29naUJVenRoWUtJQjVxR2ZUM1gv?=  =?utf-8?B?djN2N0tiZ3FyM1pHTnp1UTRJMGMzY1dMbkVuWE1VTUVmMjFvOGpPOWxxRG9j?=  =?utf-8?B?dlBSMWNLNEpJOGtTR2NzMUFlMTIyRzNDdGZYL09RNzVNbmNqMEdLOVlCZzZy?=  =?utf-8?B?eHJLc3RqZitQRWNJamQyRzJEVGxDZitUYWRXdHVBN1JrL3RsSGk3T3FZN0ZI?=  =?utf-8?B?eFNMcXZXUkpQT01pc1d1YjFhV09TenR4bnNpYitndUZLcFlGZjhaWVMzdktG?=  =?utf-8?B?NGtZVFRDN1NkdlFOU1hlb2ZVUTV2ZEg3VVNLcWRZL1JRN3dMNm05VitFS3RN?=  =?utf-8?B?VjRqb0F0UUFvRVJzcXRVNCt4ZWVyaVRIckxQMUFsb2xIcVVsTUdIQWpua0tp?=  =?utf-8?B?WTZJZkNWSTA2R2VpbFppSVhTMkw5WmVFOFpQOXI3YmFOUVhTQnlhc3MzdVJL?=  =?utf-8?B?cVc0RHhNMXVRcXRFQVVSamh4M043RXNFZ0RQcHZHcnR4V3FHQzRQRlIyR3BO?=  =?utf-8?B?QTVIZ2lSaHhNaXlCb1Uvdm5WVkV4ZW80Y2tGdTVkVUZkQ3VxK1dnSmRZYUF1?=  =?utf-8?B?NVQ3MGdYd05XaUdnVmpoZ2puNFY3bGdBQnRwNG1KMk05M3FGMkx4VGdwZTh4?=  =?utf-8?B?U25HZTR0QWRRMmxWWHY5NnQyNmh5cERLOUZBcEg4MUN1c2lWdGpYWERweFhu?=  =?utf-8?B?MlpSTTVYRzlLSkRTa3RPa3BQYWxuY3lhQ3dYVnV5WHJoUDhtYnYzTXorV2Rs?=  =?utf-8?B?MXdwNXh5dDV3K3dYYlpXL3lBcWxrQTgvU1h1WFZ2SUFBNDFPUTJRVGpsNXIv?=  =?utf-8?B?UlVONUJFZ3dhRDZBdWQwVXZSbmZCVklab1VjR2I2bEttVnFVdmk2NE15a0Jo?=  =?utf-8?B?UEhNZy8vTklGT3RMVEZEMEFDVVUzMVhhRmpyb2FJejBuMFVnUk9MN2VqbG84?=  =?utf-8?B?MmoxYVZvdERKazRoSXhsTXdYU0RjUVZWL242M2VyUC9QNnQveEpadTB1QXFa?=  =?utf-8?B?eGxBc0N5UTdyTlhZa2dzYmUwbGpvUDhyVjNiNlp6Q1crUkd2aHB0eDVJVi94?=  =?utf-8?B?a0NwWklZWCtuUWNBMFlXaFh0TXBwalJwNmJnMFFveFF2eXdJMGloYm9SRDM0?=  =?utf-8?B?ZVd3VEJvSnd2MU1vU1l3NVY3UGw4N05zREpmQ0hYdFhZWC9KNGZNOVFjZlh6?=  =?utf-8?B?dkFSWEZjVENjS1Q3Q3Z6dW43Q1VCcDIrS2RkWWh2VWZyaVU1T3BtcEpPNUJz?=  =?utf-8?B?N2hZYXBnWnFXSTF0SU9aV2p6Q0Nqa1dSeFpEMHJNcXpzcU9HYnc2UVp3NG5s?=  =?utf-8?B?b2ljbENGK2pCbllHZXBnRlhTRFdxdHozN1FNM1lUNGtBeUFYMHplaVh0OWcv?=  =?utf-8?B?aUlGNjBkWUdDUzVHajBlQUd6SDRSSHN0NDg0aG5QZjg2TlJzQUdJcCtTczNP?=  =?utf-8?B?OW1aV1dTN0E2K29ld2M1MzREOUdRKytpUEVwNFJXcVBTVDJFbi9mdlBqQWc3?=  =?utf-8?B?MTJ0TnVyK2hYR1RaL1E0dXhvYVh6WmVSMzRoaHMyaFArT054R2s4VU1MdmpH?=  =?utf-8?B?R0hyeVhYWlRnZ0psY2pZeGR1ZW1oU0NRaDNSQmM3dkVCYldqZWdXWUoxNzRW?=  =?utf-8?B?TDRQZHlBOXJxampwSm03b25SbmVMamxyaWd3NHpmRWFOdzdqQUJ1SEJQcUtU?=  =?utf-8?B?QnJVRnJXaWFudi9uTDBYRzA5Qjk5SkwrMjhYS3IwaDRHMzdwbkpOQnFDa2tx?=  =?utf-8?B?L2FnMVBvSFZXRXBDZFVQaVdrQ3ZpRWhOSDJEd2pHNURTYjhSOEVmZW40cnZs?=  =?utf-8?B?UlJXMGc3R3pjVURibXlDTWQ2Tmx6N2hkeWhGRS94Z2h6d1Q4K2h3Z1pUR2cx?=  =?utf-8?B?NEJVVTJTTFBiU0ZaR3k2T2hEb2Y0Z0t1OTBTRGwrRy93RlJVVjFaQWlmMnV0?=  =?utf-8?Q?hHpDu+cPrlHX8vkq?= X-Forefront-Antispam-Report: CIP:130.85.33.105;CTRY:US;LANG:en;SCL:1;SRV:;IPV:NLI;SFV:NSPM;H:mx.mail.umbc.edu;PTR:mx2-v3.mail.umbc.edu;CAT:NONE;SFS:(13230040)(27102699006)(12012899012)(13003099007);DIR:INB; X-MS-Exchange-ATPSafeLinks-Stat: 0 X-MS-Exchange-ATPSafeLinks-BitVector: 40000:0x0|0x0|0x0|0x0|0x0|0x40000; X-MS-Exchange-CrossTenant-OriginalArrivalTime: 07 Oct 2025 19:22:45.1833  (UTC) X-MS-Exchange-CrossTenant-Network-Message-Id: 0a0a176e-64bb-4a65-07eb-08de05d6e4c3 X-MS-Exchange-CrossTenant-Id: 8d281d1d-9c4d-4bf7-b16e-032d15de9f6c X-MS-Exchange-CrossTenant-AuthSource: MN1PEPF0000ECD5.namprd02.prod.outlook.com X-MS-Exchange-CrossTenant-AuthAs: Anonymous X-MS-Exchange-CrossTenant-FromEntityHeader: Internet X-MS-Exchange-Transport-CrossTenantHeadersStamped: BN0PR01MB6879 X-OrganizationHeadersPreserved: BN0PR01MB6879.prod.exchangelabs.com X-CrossPremisesHeadersPromoted: UTDEX03.campus.ad.utdallas.edu X-CrossPremisesHeadersFiltered: UTDEX03.campus.ad.utdallas.edu X-OriginatorOrg: utdallas.edu  "
3287969,72147958,Correspond,DoIT-Research-Computing,2025-10-07 19:23:47.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zweck,stalled,Greg Ballantine,gballan1,John Zweck,zweck,zweck@umbc.edu,NULL,exchangeadmins@utdallas.edu,"<html> <Head></head><body> <p><b><font color=""#000066"" size=""3"" face=""Arial"">Delivery has failed to these recipients or groups:</font></b></p> <font color=""#000000"" size=""2"" face=""Tahoma""><p><a href=""mailto:zweck@umbc.edu"">zweck@umbc.edu</a><br> </font> <font color=""#000000"" size=""3"" face=""Arial"">The email address you entered couldn't be found. Please check the recipient's email address and try to resend the message. If the problem continues, please contact your email admin.<br> </font> <font color=""#000000"" size=""2"" face=""Tahoma""><br> </p> </font> <br><br><br><br><br><br> <font color=""#808080"" size=""2"" face=""Tahoma""><p><b>Diagnostic information for administrators:</b></p> <p>Generating server: UTDEX04.campus.ad.utdallas.edu<br> </p> <p>zweck@umbc.edu<br> Remote Server  returned '550 5.1.10 RESOLVER.ADR.RecipientNotFound; Recipient not found by SMTP address lookup'<br> </p> <p>Original message headers:</p> <pre>Received: from UTDEX03.campus.ad.utdallas.edu (10.182.72.101) by  UTDEX04.campus.ad.utdallas.edu (10.182.72.102) with Microsoft SMTP Server  (version=TLS1_2, cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id  15.2.1748.26; Tue, 7 Oct 2025 14:23:30 -0500 Received: from DS2PR08CU001.outbound.protection.outlook.com (10.44.46.82) by  UTDEX03.campus.ad.utdallas.edu (10.182.72.101) with Microsoft SMTP Server  (version=TLS1_2, cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id  15.2.1748.26 via Frontend Transport; Tue, 7 Oct 2025 14:23:30 -0500 ARC-Seal: i=1; a=rsa-sha256; s=arcselector10001; d=microsoft.com; cv=none;  b=earlPCQ+h2uTDovbozVcitWG42O2+rv175kQcYTlHkVYvmBR/Wu50/uMPTyVIhOnGsZh3+c+u0C+2+ea2HA/7hprNKVhEemaKG3Vdvrs1LmNun7XDb23rIjECkBnXCq2QENIrUmz/vEMj1k7sLliUIUk6l0tecOBW8xRpGBh0SpkJAAHuZjnPy+iyiCPa+RSP6gfSIFA9TGlnOYTs5ZWXJAjltQ1wyeebPHcVI2egI6enMbaszmp+bNddLqgcIS3Y1Nv4MwIUPXAy+K8Xr8k31PF1ERZWMX8XLn6MseLn4+3pBjBPEOSU2jTDvoohYmH9IQLwqs/9PK3vCxVZZi/QQ== ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=microsoft.com;  s=arcselector10001;  h=From:Date:Subject:Message-ID:Content-Type:MIME-Version:X-MS-Exchange-AntiSpam-MessageData-ChunkCount:X-MS-Exchange-AntiSpam-MessageData-0:X-MS-Exchange-AntiSpam-MessageData-1;  bh=MRuAMo8aNgpbjqMgYeor6reLbQ1eB9ekMyOgDqAJ6iM=;  b=hu7G8p/tN6Hxntk5HRptoyXqG/taT9mG0++yUFp3+r6je1Y6w65CNNxsvqDNZ+tLrt0TVYyCdHkjkUaM4Pr97zAIsmepftcEk6dvNCYOts3KGu7KzatrZHF08XXr1WYtEp4SCYtsYh1GNrbO1ny2AKAJb09+ISguhFNKszzJgGI+GU3TQJWXLrsKbSvthFEmcBBURDDp73pyouQTU9JZO0NYsRKn5UT4HBmFu1SSJjgN3CbX7KxSG0owE0LhhnjWSJp3Nmwp/omo1U6iK0KghMjbPUrT9xzjtDuqblitRDmLc0B29wK49l9GcjZoMdnJzULecOhAo7cXTp4tq+Jhfw== ARC-Authentication-Results: i=1; mx.microsoft.com 1; spf=softfail (sender ip  is 130.85.33.105) smtp.rcpttodomain=utdallas.edu smtp.mailfrom=rt.umbc.edu;  dmarc=pass (p=none sp=none pct=100) action=none header.from=rt.umbc.edu;  dkim=test (signature was verified) header.d=umbc.edu; arc=none (0) Received: from BN9P220CA0027.NAMP220.PROD.OUTLOOK.COM (2603:10b6:408:13e::32)  by BN0PR01MB6879.prod.exchangelabs.com (2603:10b6:408:165::7) with Microsoft  SMTP Server (version=TLS1_2, cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id  15.20.9203.9; Tue, 7 Oct 2025 19:22:46 +0000 Received: from MN1PEPF0000ECD5.namprd02.prod.outlook.com  (2603:10b6:408:13e:cafe::15) by BN9P220CA0027.outlook.office365.com  (2603:10b6:408:13e::32) with Microsoft SMTP Server (version=TLS1_3,  cipher=TLS_AES_256_GCM_SHA384) id 15.20.9182.20 via Frontend Transport; Tue,  7 Oct 2025 19:22:46 +0000 Authentication-Results: spf=softfail (sender IP is 130.85.33.105)  smtp.mailfrom=rt.umbc.edu; dkim=test (signature was verified)  header.d=umbc.edu;dmarc=pass action=none  header.from=rt.umbc.edu;compauth=pass reason=100 Received-SPF: SoftFail (protection.outlook.com: domain of transitioning  rt.umbc.edu discourages use of 130.85.33.105 as permitted sender) Received: from mx.mail.umbc.edu (130.85.33.105) by  MN1PEPF0000ECD5.mail.protection.outlook.com (10.167.242.133) with Microsoft  SMTP Server (version=TLS1_2, cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id  15.20.9203.9 via Frontend Transport; Tue, 7 Oct 2025 19:22:45 +0000 Received: from campus-relay.mail.umbc.edu (campus-relay1-v1.mail.umbc.edu [130.85.33.100]) 	by mx.mail.umbc.edu (Postfix) with ESMTPS id 6831218123E2 	for &lt;zweck@umbc.edu&gt;; Tue,  7 Oct 2025 15:22:44 -0400 (EDT) Received: from rt.umbc.edu (rt-web1-v1.umbc.edu [172.24.2.4]) 	by campus-relay.mail.umbc.edu (Postfix) with ESMTPS id 45E67180605A 	for &lt;zweck@umbc.edu&gt;; Tue,  7 Oct 2025 15:22:44 -0400 (EDT) DKIM-Filter: OpenDKIM Filter v2.11.0 campus-relay.mail.umbc.edu 45E67180605A DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=umbc.edu; 	s=campusrelay; t=1759864964; 	bh=qxRidtKvC+Pt41W7AhRw0TdIc55Br//2gZtQCE6zBNo=; 	h=Subject:From:Reply-To:In-Reply-To:References:To:Date:From; 	b=d4b2RdN++9Xqkn9Jyc0DJENxh4DKZPAHat3ZPm3ke43fCp8DjDiytTo5CP2hoq6ac 	 wTU6wPbbOJQahIZore7G9yNKrwFQGyAW9nljbXEENkET1AS+swsfQtccGF7yKAmjwR 	 tEIzQuQ/FNNcCNb8acp+mIqOJTeUEi+62c6JfdkA= Received: from umbc-rt-app-prod (_gateway [172.19.0.1]) 	by rt.umbc.edu (8.16.1/8.16.1) with SMTP id 597JMg2P014631 	for &lt;zweck@umbc.edu&gt;; Tue, 7 Oct 2025 19:22:42 GMT Received: by umbc-rt-app-prod (sSMTP sendmail emulation); Tue, 07 Oct 2025 19:22:42 +0000 Subject: [Research Computing #3287969] Migrating Research Storage Volume to Ceph Cluster - pi_zweck  [AutoReply] From: &quot; via RT&quot; &lt;UMBCHelp@rt.umbc.edu&gt; Reply-To: UMBCHelp@rt.umbc.edu In-Reply-To: References: &lt;RT-Ticket-3287969@rt.umbc.edu&gt; Message-ID: &lt;rt-5.0.5-29288-1759864962-374.3287969-3-0@rt.umbc.edu&gt; X-RT-Loop-Prevention: rt.umbc.edu X-RT-Ticket: rt.umbc.edu #3287969 X-Managed-BY: RT 5.0.5 (http://www.bestpractical.com/rt/) X-RT-Originator: elliotg2@umbc.edu Auto-Submitted: auto-replied To: &lt;zweck@umbc.edu&gt; Content-Type: text/plain; charset=&quot;utf-8&quot; Content-Transfer-Encoding: quoted-printable X-RT-Original-Encoding: utf-8 Precedence: bulk Date: Tue, 7 Oct 2025 15:22:42 -0400 MIME-Version: 1.0 Return-Path: UMBCHelp@rt.umbc.edu X-EOPAttributedMessage: 0 X-EOPTenantAttributedMessage: 8d281d1d-9c4d-4bf7-b16e-032d15de9f6c:0 X-MS-PublicTrafficType: Email X-MS-TrafficTypeDiagnostic: MN1PEPF0000ECD5:EE_|BN0PR01MB6879:EE_ X-MS-Office365-Filtering-Correlation-Id: 0a0a176e-64bb-4a65-07eb-08de05d6e4c3 X-MS-Exchange-AtpMessageProperties: SA|SL X-Microsoft-Antispam: BCL:0;ARA:13230040|27102699006|12012899012|13003099007; X-Microsoft-Antispam-Message-Info: =?utf-8?B?ZzdlNkNBWjBYMlJlWVBoN0hWSk93L2h6Qmx6emVZQ2FhVG5RRnI3VDB2aHBN?=  =?utf-8?B?YThjcXJjK2lsWTY1L3NiRzZDc2U4R0Y3cGV2WEt5VjFwaGZHY2pVNDViUG5K?=  =?utf-8?B?dDRzVGx6cnZRQ21xUmQ0Z3lpS1ZGdG9FL1hZMm5xRjdtdkQ4bDdJWXptcmtH?=  =?utf-8?B?M2tneG1GVk1IWENKMTJqSFBhT05zVE9HVTg0N3hwRVVPcEFEOUtxNSt3cWcx?=  =?utf-8?B?VFIvejhKc2xKV0NtWEZRTCtlSUZkTTJBMStYSXp5Ym1hTlRlVWc2cHFTS3NT?=  =?utf-8?B?K0M3KzlJVXRwODB4aG16RTM1OXoyUmJCbUxyVC91Q0ZEei9HTmdvVitPMER6?=  =?utf-8?B?Q3MwOHhFMXBSY3hmQlFFMkZqVGZFQlhoOGw0d1pVTTFPdGtaT2IrMjNoakVx?=  =?utf-8?B?Z1ZzY3dOZDVKb3ByOW4xc2dhNnQrUkVpdWlnWnRxM3ZlazBjUkhhbFBXMDR5?=  =?utf-8?B?aiswNE1Mb3hVY05tK0JBVThMZTQyTjJjdnN5SnRvbHM1Q2cxSzB6QXhxT05z?=  =?utf-8?B?WWloK1dhdjdpZEJXc0tnZmdmWlRFTm9HMkZZbFZiamR1V2NHbjl5d3g4Qkpu?=  =?utf-8?B?eEs0ZEZnM2hSZ0ZSK2RIaDdJd1VRSU4xRnFLdUlYaU5rMzNOTDBhZ2d5N0xU?=  =?utf-8?B?Y1FlRGp0QkV3ZXNKbW1OSytZZWVQOUZpUjloam1LRXVod1B6MDJrNUd6THBv?=  =?utf-8?B?WWQyRWZTTlpNeHVyMlgxT0orbXNrR3k5bXl3REE2ZWpYbDNsSHB5S1V6V1B6?=  =?utf-8?B?ZWNUL1VOSXpaajFUSmNsTmpUc0NBUmV4RWRWYk9iS0ZweEFwVzJjTEVLaEpZ?=  =?utf-8?B?ODdPZG9EcElBY0ZpMFpENnJ2alJDdklsR0NjNml3SWVrREMzTDhpVXlKWjht?=  =?utf-8?B?K1RVeTZibGUwWEorYy9DSVlObWZLZjJid0RNVVNqdzJpYWlBT1lPZ0U4Z3hj?=  =?utf-8?B?c0NIa0l0VGhZU2g4SlpibStQVmhxTjhhQ29naUJVenRoWUtJQjVxR2ZUM1gv?=  =?utf-8?B?djN2N0tiZ3FyM1pHTnp1UTRJMGMzY1dMbkVuWE1VTUVmMjFvOGpPOWxxRG9j?=  =?utf-8?B?dlBSMWNLNEpJOGtTR2NzMUFlMTIyRzNDdGZYL09RNzVNbmNqMEdLOVlCZzZy?=  =?utf-8?B?eHJLc3RqZitQRWNJamQyRzJEVGxDZitUYWRXdHVBN1JrL3RsSGk3T3FZN0ZI?=  =?utf-8?B?eFNMcXZXUkpQT01pc1d1YjFhV09TenR4bnNpYitndUZLcFlGZjhaWVMzdktG?=  =?utf-8?B?NGtZVFRDN1NkdlFOU1hlb2ZVUTV2ZEg3VVNLcWRZL1JRN3dMNm05VitFS3RN?=  =?utf-8?B?VjRqb0F0UUFvRVJzcXRVNCt4ZWVyaVRIckxQMUFsb2xIcVVsTUdIQWpua0tp?=  =?utf-8?B?WTZJZkNWSTA2R2VpbFppSVhTMkw5WmVFOFpQOXI3YmFOUVhTQnlhc3MzdVJL?=  =?utf-8?B?cVc0RHhNMXVRcXRFQVVSamh4M043RXNFZ0RQcHZHcnR4V3FHQzRQRlIyR3BO?=  =?utf-8?B?QTVIZ2lSaHhNaXlCb1Uvdm5WVkV4ZW80Y2tGdTVkVUZkQ3VxK1dnSmRZYUF1?=  =?utf-8?B?NVQ3MGdYd05XaUdnVmpoZ2puNFY3bGdBQnRwNG1KMk05M3FGMkx4VGdwZTh4?=  =?utf-8?B?U25HZTR0QWRRMmxWWHY5NnQyNmh5cERLOUZBcEg4MUN1c2lWdGpYWERweFhu?=  =?utf-8?B?MlpSTTVYRzlLSkRTa3RPa3BQYWxuY3lhQ3dYVnV5WHJoUDhtYnYzTXorV2Rs?=  =?utf-8?B?MXdwNXh5dDV3K3dYYlpXL3lBcWxrQTgvU1h1WFZ2SUFBNDFPUTJRVGpsNXIv?=  =?utf-8?B?UlVONUJFZ3dhRDZBdWQwVXZSbmZCVklab1VjR2I2bEttVnFVdmk2NE15a0Jo?=  =?utf-8?B?UEhNZy8vTklGT3RMVEZEMEFDVVUzMVhhRmpyb2FJejBuMFVnUk9MN2VqbG84?=  =?utf-8?B?MmoxYVZvdERKazRoSXhsTXdYU0RjUVZWL242M2VyUC9QNnQveEpadTB1QXFa?=  =?utf-8?B?eGxBc0N5UTdyTlhZa2dzYmUwbGpvUDhyVjNiNlp6Q1crUkd2aHB0eDVJVi94?=  =?utf-8?B?a0NwWklZWCtuUWNBMFlXaFh0TXBwalJwNmJnMFFveFF2eXdJMGloYm9SRDM0?=  =?utf-8?B?ZVd3VEJvSnd2MU1vU1l3NVY3UGw4N05zREpmQ0hYdFhZWC9KNGZNOVFjZlh6?=  =?utf-8?B?dkFSWEZjVENjS1Q3Q3Z6dW43Q1VCcDIrS2RkWWh2VWZyaVU1T3BtcEpPNUJz?=  =?utf-8?B?N2hZYXBnWnFXSTF0SU9aV2p6Q0Nqa1dSeFpEMHJNcXpzcU9HYnc2UVp3NG5s?=  =?utf-8?B?b2ljbENGK2pCbllHZXBnRlhTRFdxdHozN1FNM1lUNGtBeUFYMHplaVh0OWcv?=  =?utf-8?B?aUlGNjBkWUdDUzVHajBlQUd6SDRSSHN0NDg0aG5QZjg2TlJzQUdJcCtTczNP?=  =?utf-8?B?OW1aV1dTN0E2K29ld2M1MzREOUdRKytpUEVwNFJXcVBTVDJFbi9mdlBqQWc3?=  =?utf-8?B?MTJ0TnVyK2hYR1RaL1E0dXhvYVh6WmVSMzRoaHMyaFArT054R2s4VU1MdmpH?=  =?utf-8?B?R0hyeVhYWlRnZ0psY2pZeGR1ZW1oU0NRaDNSQmM3dkVCYldqZWdXWUoxNzRW?=  =?utf-8?B?TDRQZHlBOXJxampwSm03b25SbmVMamxyaWd3NHpmRWFOdzdqQUJ1SEJQcUtU?=  =?utf-8?B?QnJVRnJXaWFudi9uTDBYRzA5Qjk5SkwrMjhYS3IwaDRHMzdwbkpOQnFDa2tx?=  =?utf-8?B?L2FnMVBvSFZXRXBDZFVQaVdrQ3ZpRWhOSDJEd2pHNURTYjhSOEVmZW40cnZs?=  =?utf-8?B?UlJXMGc3R3pjVURibXlDTWQ2Tmx6N2hkeWhGRS94Z2h6d1Q4K2h3Z1pUR2cx?=  =?utf-8?B?NEJVVTJTTFBiU0ZaR3k2T2hEb2Y0Z0t1OTBTRGwrRy93RlJVVjFaQWlmMnV0?=  =?utf-8?Q?hHpDu+cPrlHX8vkq?= X-Forefront-Antispam-Report: CIP:130.85.33.105;CTRY:US;LANG:en;SCL:1;SRV:;IPV:NLI;SFV:NSPM;H:mx.mail.umbc.edu;PTR:mx2-v3.mail.umbc.edu;CAT:NONE;SFS:(13230040)(27102699006)(12012899012)(13003099007);DIR:INB; X-MS-Exchange-ATPSafeLinks-Stat: 0 X-MS-Exchange-ATPSafeLinks-BitVector: 40000:0x0|0x0|0x0|0x0|0x0|0x40000; X-MS-Exchange-CrossTenant-OriginalArrivalTime: 07 Oct 2025 19:22:45.1833  (UTC) X-MS-Exchange-CrossTenant-Network-Message-Id: 0a0a176e-64bb-4a65-07eb-08de05d6e4c3 X-MS-Exchange-CrossTenant-Id: 8d281d1d-9c4d-4bf7-b16e-032d15de9f6c X-MS-Exchange-CrossTenant-AuthSource: MN1PEPF0000ECD5.namprd02.prod.outlook.com X-MS-Exchange-CrossTenant-AuthAs: Anonymous X-MS-Exchange-CrossTenant-FromEntityHeader: Internet X-MS-Exchange-Transport-CrossTenantHeadersStamped: BN0PR01MB6879 X-OrganizationHeadersPreserved: BN0PR01MB6879.prod.exchangelabs.com X-CrossPremisesHeadersPromoted: UTDEX03.campus.ad.utdallas.edu X-CrossPremisesHeadersFiltered: UTDEX03.campus.ad.utdallas.edu X-OriginatorOrg: utdallas.edu </pre> </font> </body> </html>"
3287969,72147958,Correspond,DoIT-Research-Computing,2025-10-07 19:23:47.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zweck,stalled,Greg Ballantine,gballan1,John Zweck,zweck,zweck@umbc.edu,NULL,exchangeadmins@utdallas.edu,"Greetings,  This message has been automatically generated in response to the creation of a ticket regarding:  ------------------------------------------------------------------------- Subject: ""Migrating Research Storage Volume to Ceph Cluster - pi_zweck""  Message:=20  Dear John,  As per the communication via myUMBC earlier this summer (June HPCF Newslett= er ), DoIT is in the process of migrating data off of an older storage server = to our new RRStor Ceph storage cluster. Your group is using 36.2 GB of a 97.7 = GB quota on the old storage server.  To perform these migrations, we need to take individual storage volumes off= line while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule a date where we can migrate your volume located at =E2=80=9C/umbc/rs/zweck= =E2=80=9D. During the migration, we will take your volume offline and will terminate any jobs run= ning on the chip compute cluster that are accessing this volume.  Below we=E2=80=99ve listed two options for handling this data migration - p= lease let us know which of these you=E2=80=99d prefer.  Option 1: Schedule a group-wide downtime date during standard business hour= s, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate yo= ur volume to the Ceph storage cluster. DoIT staff will send an email alert on = this email thread when the migration has begun and when it has completed. For mo= st storage volumes, this process should take less than a business day. Option 2: If you don=E2=80=99t respond to this email by October 15th, DoIT = staff will assign a day over the following month (October 16th through November 15th) = to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.  Note: After this process has completed, the new storage volume will have a = new name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its data under=  =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, or in your group=E2=80=99s case you will find your volume under =E2=80=9C/u= mbc/rs/pi_zweck=E2=80=9D.  Thank you, Elliot   ------------------------------------------------------------------------- =20 There is no need to reply to this message right now.=20=20  Your ticket has been assigned an ID of [Research Computing #3287969] or you=  can go there directly by clicking the link below.  Ticket <URL: https://nam02.safelinks.protection.outlook.com/?url=3Dhttps%3A= %2F%2Frt.umbc.edu%2FTicket%2FDisplay.html%3Fid%3D3287969&data=3D05%7C02%7Cj= wz120030%40utdallas.edu%7C0a0a176e64bb4a6507eb08de05d6e4c3%7C8d281d1d9c4d4b= f7b16e032d15de9f6c%7C0%7C0%7C638954618100335587%7CUnknown%7CTWFpbGZsb3d8eyJ= FbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIl= dUIjoyfQ%3D%3D%7C40000%7C%7C%7C&sdata=3DAyovA6TarqXV99sinj2dV%2BTaDZmPixk3k= IRcbMRcozQ%3D&reserved=3D0 >  You can login to view your open tickets at any time by visiting https://nam= 02.safelinks.protection.outlook.com/?url=3Dhttp%3A%2F%2Fmy.umbc.edu%2F&data= =3D05%7C02%7Cjwz120030%40utdallas.edu%7C0a0a176e64bb4a6507eb08de05d6e4c3%7C= 8d281d1d9c4d4bf7b16e032d15de9f6c%7C0%7C0%7C638954618100353503%7CUnknown%7CT= WFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFO= IjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C40000%7C%7C%7C&sdata=3D6MP2ezib2RXq04KnsALhs= ruLXxuOesWlgkGbkJEJdd4%3D&reserved=3D0 and clicking on ""Help"" and ""Request = Help"".=20  Alternately you can click on https://nam02.safelinks.protection.outlook.com= /?url=3Dhttp%3A%2F%2Fmy.umbc.edu%2Fhelp&data=3D05%7C02%7Cjwz120030%40utdall= as.edu%7C0a0a176e64bb4a6507eb08de05d6e4c3%7C8d281d1d9c4d4bf7b16e032d15de9f6= c%7C0%7C0%7C638954618100368346%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRy= dWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C= 40000%7C%7C%7C&sdata=3DcIK0j4XZQJ0C3CX9LEAD07wi1d48k2MqMUYF1aU2wmo%3D&reser= ved=3D0                          Thank you  "
3287969,72207456,Comment,DoIT-Research-Computing,2025-10-10 15:31:11.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zweck,stalled,Greg Ballantine,gballan1,John Zweck,zweck,zweck@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>I&#39;ll look into this and try and resend the email. Cc&#39;ing myself to this ticket for updates.</p> "
3287969,72207457,CommentEmailRecord,DoIT-Research-Computing,2025-10-10 15:31:13.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zweck,stalled,Greg Ballantine,gballan1,John Zweck,zweck,zweck@umbc.edu,The RT System itself,NULL,"Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3287969 >  Comment just added.    I'll look into this and try and resend the email. Cc'ing myself to this ticket for updates.  "
3287971,72148009,Create,DoIT-Research-Computing,2025-10-07 19:25:29.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_kturpie,resolved,Greg Ballantine,gballan1,Kevin Turpie,kturpie,kturpie@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Kevin,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB&nbsp;of a 48.8 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/kturpie&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_kturpie&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3287971,72239014,Correspond,DoIT-Research-Computing,2025-10-13 15:42:33.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_kturpie,resolved,Greg Ballantine,gballan1,Kevin Turpie,kturpie,kturpie@umbc.edu,Kevin Turpie,kturpie@umbc.edu,"Dear Elliot,  I will be away Tues and Wed, so that would be the optimal time to execute t= he=20 change. However, I do not think that the move would affect any of my ongoin= g=20 projects at any time over the next month. So, I leave it to your discretion=  to=20 schedule. Please keep me apprised.  Kevin  On 10/7/25 3:25 PM, via RT wrote: > Greetings, > > This message has been automatically generated in response to the > creation of a ticket regarding: > > ------------------------------------------------------------------------- > Subject: ""Migrating Research Storage Volume to Ceph Cluster - pi_kturpie"" > > Message: > > Dear Kevin, > > As per the communication via myUMBC earlier this summer (June HPCF Newsle= tter > ), DoIT is in the process of migrating data off of an older storage serve= r to > our new RRStor Ceph storage cluster. Your group is using 0 GB of a 48.8 GB > quota on the old storage server. > > To perform these migrations, we need to take individual storage volumes o= ffline > while we migrate them to the Ceph cluster. Thus we are reaching out to sc= hedule > a date where we can migrate your volume located at =E2=80=9C/umbc/rs/ktur= pie=E2=80=9D. During > the migration, we will take your volume offline and will terminate any jo= bs > running on the chip compute cluster that are accessing this volume. > > Below we=E2=80=99ve listed two options for handling this data migration -=  please let us > know which of these you=E2=80=99d prefer. > > Option 1: Schedule a group-wide downtime date during standard business ho= urs, > which can be done by responding to this email with your preferred date(s)=  to > perform the migration. During this time, DoIT staff will work to migrate = your > volume to the Ceph storage cluster. DoIT staff will send an email alert o= n this > email thread when the migration has begun and when it has completed. For = most > storage volumes, this process should take less than a business day. > Option 2: If you don=E2=80=99t respond to this email by October 15th, DoI= T staff will > assign a day over the following month (October 16th through November 15th= ) to > migrate your volume. The day chosen will be random and will occur during > business hours. You will be notified of the date chosen to perform the > migration, and will be notified when the migration begins and completes. > > Note: After this process has completed, the new storage volume will have = a new > name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its data und= er =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, > or in your group=E2=80=99s case you will find your volume under =E2=80=9C= /umbc/rs/pi_kturpie=E2=80=9D. > > Thank you, > Elliot > > > ------------------------------------------------------------------------- >=20=20=20 > There is no need to reply to this message right now. > > Your ticket has been assigned an ID of [Research Computing #3287971] or y= ou can go there directly by clicking the link below. > > Ticket <URL:https://rt.umbc.edu/Ticket/Display.html?id=3D3287971 > > > You can login to view your open tickets at any time by visitinghttp://my.= umbc.edu and clicking on ""Help"" and ""Request Help"". > > Alternately you can click onhttp://my.umbc.edu/help > >                          Thank you > "
3287971,72239014,Correspond,DoIT-Research-Computing,2025-10-13 15:42:33.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_kturpie,resolved,Greg Ballantine,gballan1,Kevin Turpie,kturpie,kturpie@umbc.edu,Kevin Turpie,kturpie@umbc.edu,"<!DOCTYPE html> <html>   <head>     <meta http-equiv=3D""Content-Type"" content=3D""text/html; charset=3DUTF-8= "">   </head>   <body>     <font face=3D""Monaco"">Dear Elliot,<br>       <br>       I will be away Tues and Wed, so that would be the optimal time to       execute the change. However, I do not think that the move would       affect any of my ongoing projects at any time over the next month.       So, I leave it to your discretion to schedule. Please keep me       apprised.<br>       <br>       Kevin<br>     </font><br>     <div class=3D""moz-cite-prefix"">On 10/7/25 3:25 PM, via RT wrote:<br>     </div>     <blockquote type=3D""cite""       cite=3D""mid:rt-5.0.5-29286-1759865131-158.3287971-3-0@rt.umbc.edu"">       <pre wrap=3D"""" class=3D""moz-quote-pre"">Greetings,  This message has been automatically generated in response to the creation of a ticket regarding:  ------------------------------------------------------------------------- Subject: ""Migrating Research Storage Volume to Ceph Cluster - pi_kturpie""  Message:=20  Dear Kevin,  As per the communication via myUMBC earlier this summer (June HPCF Newslett= er ), DoIT is in the process of migrating data off of an older storage server = to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 48.8 GB quota on the old storage server.  To perform these migrations, we need to take individual storage volumes off= line while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule a date where we can migrate your volume located at =E2=80=9C/umbc/rs/kturpi= e=E2=80=9D. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.  Below we=E2=80=99ve listed two options for handling this data migration - p= lease let us know which of these you=E2=80=99d prefer.  Option 1: Schedule a group-wide downtime date during standard business hour= s, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate yo= ur volume to the Ceph storage cluster. DoIT staff will send an email alert on = this email thread when the migration has begun and when it has completed. For mo= st storage volumes, this process should take less than a business day. Option 2: If you don=E2=80=99t respond to this email by October 15th, DoIT = staff will assign a day over the following month (October 16th through November 15th) = to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.  Note: After this process has completed, the new storage volume will have a = new name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its data under=  =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, or in your group=E2=80=99s case you will find your volume under =E2=80=9C/u= mbc/rs/pi_kturpie=E2=80=9D.  Thank you, Elliot   ------------------------------------------------------------------------- =20 There is no need to reply to this message right now.=20=20  Your ticket has been assigned an ID of [Research Computing #3287971] or you=  can go there directly by clicking the link below.  Ticket &lt;URL: <a class=3D""moz-txt-link-freetext"" href=3D""https://rt.umbc.= edu/Ticket/Display.html?id=3D3287971"">https://rt.umbc.edu/Ticket/Display.ht= ml?id=3D3287971</a> &gt;  You can login to view your open tickets at any time by visiting <a class=3D= ""moz-txt-link-freetext"" href=3D""http://my.umbc.edu"">http://my.umbc.edu</a> = and clicking on ""Help"" and ""Request Help"".=20  Alternately you can click on <a class=3D""moz-txt-link-freetext"" href=3D""htt= p://my.umbc.edu/help"">http://my.umbc.edu/help</a>                          Thank you  </pre>     </blockquote>     <br>   </body> </html> "
3287971,72299956,Correspond,DoIT-Research-Computing,2025-10-15 13:44:38.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_kturpie,resolved,Greg Ballantine,gballan1,Kevin Turpie,kturpie,kturpie@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Kevin,<br /> <br /> I was out yesterday, so I will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Mon Oct 13 11:42:33 2025, AS55578 wrote:</p>  <blockquote>Dear Elliot,<br /> <br /> I will be away Tues and Wed, so that would be the optimal time to execute the change. However, I do not think that the move would affect any of my ongoing projects at any time over the next month. So, I leave it to your discretion to schedule. Please keep me apprised.<br /> <br /> Kevin<br /> &nbsp; <div>On 10/7/25 3:25 PM, via RT wrote:</div>  <blockquote> <pre> Greetings,  This message has been automatically generated in response to the creation of a ticket regarding:  ------------------------------------------------------------------------- Subject: &quot;Migrating Research Storage Volume to Ceph Cluster - pi_kturpie&quot;  Message:   Dear Kevin,  As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 48.8 GB quota on the old storage server.  To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/kturpie&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.  Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.  Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day. Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.  Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_kturpie&rdquo;.  Thank you, Elliot   -------------------------------------------------------------------------   There is no need to reply to this message right now.    Your ticket has been assigned an ID of [Research Computing #3287971] or you can go there directly by clicking the link below.  Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3287971 &gt;  You can login to view your open tickets at any time by visiting http://my.umbc.edu and clicking on &quot;Help&quot; and &quot;Request Help&quot;.   Alternately you can click on http://my.umbc.edu/help                          Thank you  </pre> </blockquote> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3287971,72301642,Correspond,DoIT-Research-Computing,2025-10-15 14:16:57.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_kturpie,resolved,Greg Ballantine,gballan1,Kevin Turpie,kturpie,kturpie@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Kevin,<br /> <br /> We have finished migrating your volume to the Ceph cluster! As far as we can tell everything seems to have gone smoothly. There are a few things to note:</p>  <ul> 	<li>The path has changed, and is now available under <strong>/umbc/rs/pi_kturpie</strong>.</li> 	<li>The alias used to reach the volume is now <strong>pi_kturpie_common</strong> and <strong>pi_kturpie_user</strong>.</li> 	<li>Your new volume has a quota of <strong>10TB</strong>.</li> </ul>  <p>When you have a chance, could you try running some jobs on Chip using the new volume to verify everything looks good?<br /> <br /> Thank you,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Wed Oct 15 09:44:38 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Hello Kevin,<br /> <br /> I was out yesterday, so I will be performing your group&#39;s migration to the Ceph storage cluster today.&nbsp;During this time, please ensure there are not any jobs being run in your research group, otherwise these may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Mon Oct 13 11:42:33 2025, AS55578 wrote:</p>  <blockquote>Dear Elliot,<br /> <br /> I will be away Tues and Wed, so that would be the optimal time to execute the change. However, I do not think that the move would affect any of my ongoing projects at any time over the next month. So, I leave it to your discretion to schedule. Please keep me apprised.<br /> <br /> Kevin<br /> &nbsp; <div>On 10/7/25 3:25 PM, via RT wrote:</div>  <blockquote> <pre> Greetings,  This message has been automatically generated in response to the creation of a ticket regarding:  ------------------------------------------------------------------------- Subject: &quot;Migrating Research Storage Volume to Ceph Cluster - pi_kturpie&quot;  Message:   Dear Kevin,  As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 48.8 GB quota on the old storage server.  To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/kturpie&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.  Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.  Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day. Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.  Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_kturpie&rdquo;.  Thank you, Elliot   -------------------------------------------------------------------------   There is no need to reply to this message right now.    Your ticket has been assigned an ID of [Research Computing #3287971] or you can go there directly by clicking the link below.  Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3287971 &gt;  You can login to view your open tickets at any time by visiting http://my.umbc.edu and clicking on &quot;Help&quot; and &quot;Request Help&quot;.   Alternately you can click on http://my.umbc.edu/help                          Thank you  </pre> </blockquote> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=""color:#888888"">Gregory Ballantine</span></div>  <div><span style=""color:#888888"">System Administrator for Research and Enterprise Computing</span></div>  <div><span style=""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3287973,72148076,Create,DoIT-Research-Computing,2025-10-07 19:27:40.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zt,stalled,Greg Ballantine,gballan1,Ting Zhu,zt,zt@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear <strong>Ting</strong>,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using <strong>0 GB</strong> of a 48.8 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/<strong>zt</strong>&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/<strong>pi_zt</strong>&rdquo;.<br /> <br /> Thank you,<br /> <strong>Elliot</strong></p> "
3287973,72452396,Correspond,DoIT-Research-Computing,2025-10-23 14:20:25.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zt,stalled,Greg Ballantine,gballan1,Ting Zhu,zt,zt@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Ting,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3287979,72148221,Create,DoIT-Research-Computing,2025-10-07 19:30:26.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zhoul,stalled,Greg Ballantine,gballan1,Lina Zhou,zhoul,zhoul@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Lina,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB&nbsp;of a 48.8 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/zhoul&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_zhoul&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3287979,72452299,Correspond,DoIT-Research-Computing,2025-10-23 14:18:38.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zhoul,stalled,Greg Ballantine,gballan1,Lina Zhou,zhoul,zhoul@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Lina,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3288361,72160712,Create,DoIT-Research-Computing,2025-10-08 15:01:26.0000000,HPC Other Issue: International student access to chip?,resolved,Max Breitmeyer,mb17,Mercedes Burns,burnsm,burnsm@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Mercedes Last Name:                 Burns Email:                     burnsm@umbc.edu Campus ID:                 OF19978  Request Type:              High Performance Cluster  Hello,  I have a Brazilian graduate student on fellowship, Lais Grossel, who generated a large genomic dataset while working in my lab, and who now is trying to analyze this dataset using the program Stacks. She has submitted jobs to servers at her home institution of U Sao Paolo and the UK school where her fellowship continued, but they haven't been running due to timeouts and/or job backlogs. She would like to access chip via my lab's login, but we have a couple of questions about this. 1) Is this allowed? My lab has tried to access chip while abroad and was unsuccessful, but this may have been because the hotel internet was slow. I know the GlobalProtect VPN would probably need to be downloaded too. 2) Can I create a log-in for her? She was assigned a UMBC account while working here in 2023, but I assume that has lapsed.   Thank you for the help!  "
3288361,72186998,Correspond,DoIT-Research-Computing,2025-10-09 15:55:21.0000000,HPC Other Issue: International student access to chip?,resolved,Max Breitmeyer,mb17,Mercedes Burns,burnsm,burnsm@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Mercedes,&nbsp;</p>  <p>Typically we don&#39;t allow for users into UMBC services if they are not in North America (even with VPN). That being said, we can make some exceptions to this rule. For now start by sponsoring an account for the student (information for that can be found here:&nbsp;https://umbc.atlassian.net/wiki/spaces/faq/pages/30739140/How+do+I+request+myUMBC+accounts+for+non-UMBC+users+Create+a+Sponsored+Account), and we&#39;ll go from there.&nbsp;</p>  <p>On Wed Oct 08 11:01:26 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Mercedes Last Name:                 Burns Email:                     burnsm@umbc.edu Campus ID:                 OF19978  Request Type:              High Performance Cluster  Hello,  I have a Brazilian graduate student on fellowship, Lais Grossel, who generated a large genomic dataset while working in my lab, and who now is trying to analyze this dataset using the program Stacks. She has submitted jobs to servers at her home institution of U Sao Paolo and the UK school where her fellowship continued, but they haven&#39;t been running due to timeouts and/or job backlogs. She would like to access chip via my lab&#39;s login, but we have a couple of questions about this. 1) Is this allowed? My lab has tried to access chip while abroad and was unsuccessful, but this may have been because the hotel internet was slow. I know the GlobalProtect VPN would probably need to be downloaded too. 2) Can I create a log-in for her? She was assigned a UMBC account while working here in 2023, but I assume that has lapsed.   Thank you for the help!  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3288361,72196274,Correspond,DoIT-Research-Computing,2025-10-09 20:26:28.0000000,HPC Other Issue: International student access to chip?,resolved,Max Breitmeyer,mb17,Mercedes Burns,burnsm,burnsm@umbc.edu,Mercedes Burns,burnsm@umbc.edu,"<div dir=3D""ltr""><div>OK! I&#39;ve just received notice that the student&#3= 9;s affiliation has been reinstated; her name is Lais Grossel and her ID# i= s VE32332 (laisg1). She should have access through 6/30/2026.</div><div>Wha= t would be the next step for allowing her to access my lab&#39;s volume on = chip? Should=C2=A0I direct her to download the VPN too?</div></div><br><div=  class=3D""gmail_quote""><div dir=3D""ltr"" class=3D""gmail_attr"">On Thu, Oct 9,=  2025 at 11:55=E2=80=AFAM Max Breitmeyer via RT &lt;<a href=3D""mailto:UMBCH= elp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br><= /div><blockquote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;bo= rder-left:1px solid rgb(204,204,204);padding-left:1ex"">Ticket &lt;URL: <a h= ref=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D3288361"" rel=3D""norefer= rer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3288361= </a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Mercedes,<br> <br> Typically we don&#39;t allow for users into UMBC services if they are not i= n North<br> America (even with VPN). That being said, we can make some exceptions to th= is<br> rule. For now start by sponsoring an account for the student (information f= or<br> that can be found here:<br> <a href=3D""https://umbc.atlassian.net/wiki/spaces/faq/pages/30739140/How+do= +I+request+myUMBC+accounts+for+non-UMBC+users+Create+a+Sponsored+Account"" r= el=3D""noreferrer"" target=3D""_blank"">https://umbc.atlassian.net/wiki/spaces/= faq/pages/30739140/How+do+I+request+myUMBC+accounts+for+non-UMBC+users+Crea= te+a+Sponsored+Account</a>),<br> and we&#39;ll go from there.<br> <br> On Wed Oct 08 11:01:26 2025, ZZ99999 wrote:<br> <br> &gt; First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Mer= cedes<br> &gt; Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0Burns<br> &gt; Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 =C2=A0<a href=3D""mailto:burnsm@umbc.edu"" target=3D""_blank"">burnsm@um= bc.edu</a><br> &gt; Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0OF19978<br> &gt; <br> &gt; Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Per= formance Cluster<br> &gt; <br> &gt; Hello,<br> &gt; <br> &gt; I have a Brazilian graduate student on fellowship, Lais Grossel, who g= enerated a large genomic dataset while working in my lab, and who now is tr= ying to analyze this dataset using the program Stacks. She has submitted jo= bs to servers at her home institution of U Sao Paolo and the UK school wher= e her fellowship continued, but they haven&#39;t been running due to timeou= ts and/or job backlogs. She would like to access chip via my lab&#39;s logi= n, but we have a couple of questions about this.<br> &gt; 1) Is this allowed? My lab has tried to access chip while abroad and w= as unsuccessful, but this may have been because the hotel internet was slow= . I know the GlobalProtect VPN would probably need to be downloaded too.<br> &gt; 2) Can I create a log-in for her? She was assigned a UMBC account whil= e working here in 2023, but I assume that has lapsed. <br> &gt; <br> &gt; Thank you for the help!<br> <br> <br> --<br> <br> Best,<br> Max Breitmeyer<br> DOIT HPC System Administrator<br> <br> </blockquote></div><div><br clear=3D""all""></div><br><span class=3D""gmail_si= gnature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_signature""><d= iv dir=3D""ltr""><div><div dir=3D""ltr""><div><div dir=3D""ltr""><div dir=3D""ltr""= ><div dir=3D""ltr"">Dr. Mercedes Burns (she/her/hers)</div><div dir=3D""ltr"">A= ssociate Professor</div><div>Department of Biological Sciences</div><div di= r=3D""ltr"">University of Maryland, Baltimore County</div><div dir=3D""ltr"">10= 00 Hilltop Circle</div><div dir=3D""ltr"">Baltimore, MD 21250</div><div>Offic= e: +1 (410) 455-2147</div><div><a href=3D""http://burnslab.umbc.edu"" target= =3D""_blank"">burnslab.umbc.edu</a></div></div></div></div></div></div></div>= </div> "
3288361,72196274,Correspond,DoIT-Research-Computing,2025-10-09 20:26:28.0000000,HPC Other Issue: International student access to chip?,resolved,Max Breitmeyer,mb17,Mercedes Burns,burnsm,burnsm@umbc.edu,Mercedes Burns,burnsm@umbc.edu,"OK! I've just received notice that the student's affiliation has been reinstated; her name is Lais Grossel and her ID# is VE32332 (laisg1). She should have access through 6/30/2026. What would be the next step for allowing her to access my lab's volume on chip? Should I direct her to download the VPN too?  On Thu, Oct 9, 2025 at 11:55=E2=80=AFAM Max Breitmeyer via RT <UMBCHelp@rt.= umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3288361 > > > Last Update From Ticket: > > Hi Mercedes, > > Typically we don't allow for users into UMBC services if they are not in > North > America (even with VPN). That being said, we can make some exceptions to > this > rule. For now start by sponsoring an account for the student (information > for > that can be found here: > > https://umbc.atlassian.net/wiki/spaces/faq/pages/30739140/How+do+I+reques= t+myUMBC+accounts+for+non-UMBC+users+Create+a+Sponsored+Account > ), > and we'll go from there. > > On Wed Oct 08 11:01:26 2025, ZZ99999 wrote: > > > First Name:                Mercedes > > Last Name:                 Burns > > Email:                     burnsm@umbc.edu > > Campus ID:                 OF19978 > > > > Request Type:              High Performance Cluster > > > > Hello, > > > > I have a Brazilian graduate student on fellowship, Lais Grossel, who > generated a large genomic dataset while working in my lab, and who now is > trying to analyze this dataset using the program Stacks. She has submitted > jobs to servers at her home institution of U Sao Paolo and the UK school > where her fellowship continued, but they haven't been running due to > timeouts and/or job backlogs. She would like to access chip via my lab's > login, but we have a couple of questions about this. > > 1) Is this allowed? My lab has tried to access chip while abroad and was > unsuccessful, but this may have been because the hotel internet was slow.=  I > know the GlobalProtect VPN would probably need to be downloaded too. > > 2) Can I create a log-in for her? She was assigned a UMBC account while > working here in 2023, but I assume that has lapsed. > > > > Thank you for the help! > > > -- > > Best, > Max Breitmeyer > DOIT HPC System Administrator > >  --=20 Dr. Mercedes Burns (she/her/hers) Associate Professor Department of Biological Sciences University of Maryland, Baltimore County 1000 Hilltop Circle Baltimore, MD 21250 Office: +1 (410) 455-2147 burnslab.umbc.edu "
3288361,72208024,Correspond,DoIT-Research-Computing,2025-10-10 15:46:59.0000000,HPC Other Issue: International student access to chip?,resolved,Max Breitmeyer,mb17,Mercedes Burns,burnsm,burnsm@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Please do! I&#39;ll also ask that you submit an RT ticket to formally ad= d the student to your group (https://rtforms.umbc.edu/rt_authenticated/doit= /DoIT-support.php?auto=3DResearch%20Computing). I looked into the VPN issue= , and it seems like the student should be fine as long as they install the = vpn. Once they&#39;re added we&#39;ll test it out and see what happens from=  there.&nbsp;</p>  <p>On Thu Oct 09 16:26:28 2025, OF19978 wrote:</p>  <blockquote> <div> <div>OK! I&#39;ve just received notice that the student&#39;s affiliation h= as been reinstated; her name is Lais Grossel and her ID# is VE32332 (laisg1= ). She should have access through 6/30/2026.</div>  <div>What would be the next step for allowing her to access my lab&#39;s vo= lume on chip? Should&nbsp;I direct her to download the VPN too?</div> </div> &nbsp;  <div> <div>On Thu, Oct 9, 2025 at 11:55=E2=80=AFAM Max Breitmeyer via RT &lt;UMBC= Help@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 88361 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hi Mercedes,<br /> <br /> Typically we don&#39;t allow for users into UMBC services if they are not i= n North<br /> America (even with VPN). That being said, we can make some exceptions to th= is<br /> rule. For now start by sponsoring an account for the student (information f= or<br /> that can be found here:<br /> https://umbc.atlassian.net/wiki/spaces/faq/pages/30739140/How+do+I+request+= myUMBC+accounts+for+non-UMBC+users+Create+a+Sponsored+Account),<br /> and we&#39;ll go from there.<br /> <br /> On Wed Oct 08 11:01:26 2025, ZZ99999 wrote:<br /> <br /> &gt; First Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Mer= cedes<br /> &gt; Last Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p;Burns<br /> &gt; Email:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &= nbsp; &nbsp;burnsm@umbc.edu<br /> &gt; Campus ID:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p;OF19978<br /> &gt;<br /> &gt; Request Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; High Per= formance Cluster<br /> &gt;<br /> &gt; Hello,<br /> &gt;<br /> &gt; I have a Brazilian graduate student on fellowship, Lais Grossel, who g= enerated a large genomic dataset while working in my lab, and who now is tr= ying to analyze this dataset using the program Stacks. She has submitted jo= bs to servers at her home institution of U Sao Paolo and the UK school wher= e her fellowship continued, but they haven&#39;t been running due to timeou= ts and/or job backlogs. She would like to access chip via my lab&#39;s logi= n, but we have a couple of questions about this.<br /> &gt; 1) Is this allowed? My lab has tried to access chip while abroad and w= as unsuccessful, but this may have been because the hotel internet was slow= . I know the GlobalProtect VPN would probably need to be downloaded too.<br=  /> &gt; 2) Can I create a log-in for her? She was assigned a UMBC account whil= e working here in 2023, but I assume that has lapsed.<br /> &gt;<br /> &gt; Thank you for the help!<br /> <br /> <br /> --<br /> <br /> Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator<br /> &nbsp;</blockquote> </div>  <div>&nbsp;</div> <br /> -- <div> <div> <div> <div> <div> <div> <div> <div>Dr. Mercedes Burns (she/her/hers)</div>  <div>Associate Professor</div>  <div>Department of Biological Sciences</div>  <div>University of Maryland, Baltimore County</div>  <div>1000 Hilltop Circle</div>  <div>Baltimore, MD 21250</div>  <div>Office: +1 (410) 455-2147</div>  <div>burnslab.umbc.edu</div> </div> </div> </div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3288361,72308931,Correspond,DoIT-Research-Computing,2025-10-15 17:08:02.0000000,HPC Other Issue: International student access to chip?,resolved,Max Breitmeyer,mb17,Mercedes Burns,burnsm,burnsm@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Mercedes,</p>  <p>Just checking in, was your student able to access the cluster?&nbsp;</p>  <p>On Fri Oct 10 11:46:59 2025, OL73413 wrote:</p>  <blockquote> <div> <p>Please do! I&#39;ll also ask that you submit an RT ticket to formally ad= d the student to your group (https://rtforms.umbc.edu/rt_authenticated/doit= /DoIT-support.php?auto=3DResearch%20Computing). I looked into the VPN issue= , and it seems like the student should be fine as long as they install the = vpn. Once they&#39;re added we&#39;ll test it out and see what happens from=  there.&nbsp;</p>  <p>On Thu Oct 09 16:26:28 2025, OF19978 wrote:</p>  <blockquote> <div> <div>OK! I&#39;ve just received notice that the student&#39;s affiliation h= as been reinstated; her name is Lais Grossel and her ID# is VE32332 (laisg1= ). She should have access through 6/30/2026.</div>  <div>What would be the next step for allowing her to access my lab&#39;s vo= lume on chip? Should&nbsp;I direct her to download the VPN too?</div> </div> &nbsp;  <div> <div>On Thu, Oct 9, 2025 at 11:55=E2=80=AFAM Max Breitmeyer via RT &lt;UMBC= Help@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 88361 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hi Mercedes,<br /> <br /> Typically we don&#39;t allow for users into UMBC services if they are not i= n North<br /> America (even with VPN). That being said, we can make some exceptions to th= is<br /> rule. For now start by sponsoring an account for the student (information f= or<br /> that can be found here:<br /> https://umbc.atlassian.net/wiki/spaces/faq/pages/30739140/How+do+I+request+= myUMBC+accounts+for+non-UMBC+users+Create+a+Sponsored+Account),<br /> and we&#39;ll go from there.<br /> <br /> On Wed Oct 08 11:01:26 2025, ZZ99999 wrote:<br /> <br /> &gt; First Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Mer= cedes<br /> &gt; Last Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p;Burns<br /> &gt; Email:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &= nbsp; &nbsp;burnsm@umbc.edu<br /> &gt; Campus ID:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p;OF19978<br /> &gt;<br /> &gt; Request Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; High Per= formance Cluster<br /> &gt;<br /> &gt; Hello,<br /> &gt;<br /> &gt; I have a Brazilian graduate student on fellowship, Lais Grossel, who g= enerated a large genomic dataset while working in my lab, and who now is tr= ying to analyze this dataset using the program Stacks. She has submitted jo= bs to servers at her home institution of U Sao Paolo and the UK school wher= e her fellowship continued, but they haven&#39;t been running due to timeou= ts and/or job backlogs. She would like to access chip via my lab&#39;s logi= n, but we have a couple of questions about this.<br /> &gt; 1) Is this allowed? My lab has tried to access chip while abroad and w= as unsuccessful, but this may have been because the hotel internet was slow= . I know the GlobalProtect VPN would probably need to be downloaded too.<br=  /> &gt; 2) Can I create a log-in for her? She was assigned a UMBC account whil= e working here in 2023, but I assume that has lapsed.<br /> &gt;<br /> &gt; Thank you for the help!<br /> <br /> <br /> --<br /> <br /> Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator<br /> &nbsp;</blockquote> </div>  <div>&nbsp;</div> <br /> -- <div> <div> <div> <div> <div> <div> <div> <div>Dr. Mercedes Burns (she/her/hers)</div>  <div>Associate Professor</div>  <div>Department of Biological Sciences</div>  <div>University of Maryland, Baltimore County</div>  <div>1000 Hilltop Circle</div>  <div>Baltimore, MD 21250</div>  <div>Office: +1 (410) 455-2147</div>  <div>burnslab.umbc.edu</div> </div> </div> </div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3288361,72318376,Correspond,DoIT-Research-Computing,2025-10-16 01:37:55.0000000,HPC Other Issue: International student access to chip?,resolved,Max Breitmeyer,mb17,Mercedes Burns,burnsm,burnsm@umbc.edu,Mercedes Burns,burnsm@umbc.edu,"Yes, she has! Thanks for checking!  Dr. Mercedes Burns (she/her/hers) Associate Professor Department of Biological Sciences University of Maryland, Baltimore County 1000 Hilltop Circle Baltimore, MD 21250 Office: +1 (410) 455-2147 burnslab.umbc.edu  On Wed, Oct 15, 2025, 1:08=E2=80=AFPM Max Breitmeyer via RT <UMBCHelp@rt.um= bc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3288361 > > > Last Update From Ticket: > > Hi Mercedes, > > Just checking in, was your student able to access the cluster? > > On Fri Oct 10 11:46:59 2025, OL73413 wrote: > > > Please do! I'll also ask that you submit an RT ticket to formally add t= he > > student to your group > > ( > https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=3DRe= search%20Computing > ). > > I looked into the VPN issue, and it seems like the student should be fi= ne > > as long as they install the vpn. Once they're added we'll test it out a= nd > > see what happens from there. > > > On Thu Oct 09 16:26:28 2025, OF19978 wrote: > > >> OK! I've just received notice that the student's affiliation has been > >> reinstated; her name is Lais Grossel and her ID# is VE32332 (laisg1). > >> She should have access through 6/30/2026.What would be the next step > >> for allowing her to access my lab's volume on chip? Should I direct her > >> to download the VPN too? On Thu, Oct 9, 2025 at 11:55 AM Max Breitmeyer > >> via RT <UMBCHelp@rt.umbc.edu> wrote: > > >>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3288361 > > > >>> Last Update From Ticket: > > >>> Hi Mercedes, > > >>> Typically we don't allow for users into UMBC services if they are > >>> not in North > >>> America (even with VPN). That being said, we can make some > >>> exceptions to this > >>> rule. For now start by sponsoring an account for the student > >>> (information for > >>> that can be found here: > >>> > https://umbc.atlassian.net/wiki/spaces/faq/pages/30739140/How+do+I+reques= t+myUMBC+accounts+for+non-UMBC+users+Create+a+Sponsored+Account > ), > >>> and we'll go from there. > > >>> On Wed Oct 08 11:01:26 2025, ZZ99999 wrote: > > >>> > First Name: Mercedes > >>> > Last Name: Burns > >>> > Email: burnsm@umbc.edu > >>> > Campus ID: OF19978 > >>> > > >>> > Request Type: High Performance Cluster > >>> > > >>> > Hello, > >>> > > >>> > I have a Brazilian graduate student on fellowship, Lais Grossel, > >>> who generated a large genomic dataset while working in my lab, and > >>> who now is trying to analyze this dataset using the program Stacks. > >>> She has submitted jobs to servers at her home institution of U Sao > >>> Paolo and the UK school where her fellowship continued, but they > >>> haven't been running due to timeouts and/or job backlogs. She would > >>> like to access chip via my lab's login, but we have a couple of > >>> questions about this. > >>> > 1) Is this allowed? My lab has tried to access chip while abroad > >>> and was unsuccessful, but this may have been because the hotel > >>> internet was slow. I know the GlobalProtect VPN would probably need > >>> to be downloaded too. > >>> > 2) Can I create a log-in for her? She was assigned a UMBC account > >>> while working here in 2023, but I assume that has lapsed. > >>> > > >>> > Thank you for the help! > > > >>> -- > > >>> Best, > >>> Max Breitmeyer > >>> DOIT HPC System Administrator > > > >> -- Dr. Mercedes Burns (she/her/hers)Associate ProfessorDepartment of > >> Biological SciencesUniversity of Maryland, Baltimore County1000 Hilltop > >> CircleBaltimore, MD 21250Office: +1 (410) 455-2147burnslab.umbc.edu > > > -- > > > Best, > > Max Breitmeyer > > DOIT HPC System Administrator > > -- > > Best, > Max Breitmeyer > DOIT HPC System Administrator > > "
3288361,72318376,Correspond,DoIT-Research-Computing,2025-10-16 01:37:55.0000000,HPC Other Issue: International student access to chip?,resolved,Max Breitmeyer,mb17,Mercedes Burns,burnsm,burnsm@umbc.edu,Mercedes Burns,burnsm@umbc.edu,"<div dir=3D""auto""><div>Yes, she has! Thanks for checking!</div><div><br></d= iv><div data-smartmail=3D""gmail_signature""><div dir=3D""ltr""><div><div dir= =3D""ltr""><div><div dir=3D""ltr""><div dir=3D""ltr""><div dir=3D""ltr"">Dr. Merced= es Burns (she/her/hers)</div><div dir=3D""ltr"">Associate Professor</div><div= >Department of Biological Sciences</div><div dir=3D""ltr"">University of Mary= land, Baltimore County</div><div dir=3D""ltr"">1000 Hilltop Circle</div><div = dir=3D""ltr"">Baltimore, MD 21250</div><div>Office: +1 (410) 455-2147</div><d= iv><a href=3D""http://burnslab.umbc.edu"" target=3D""_blank"">burnslab.umbc.edu= </a></div></div></div></div></div></div></div></div></div><br><div class=3D= ""gmail_quote gmail_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">O= n Wed, Oct 15, 2025, 1:08=E2=80=AFPM Max Breitmeyer via RT &lt;<a href=3D""m= ailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><b= lockquote class=3D""gmail_quote"" style=3D""margin:0 0 0 .8ex;border-left:1px = #ccc solid;padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu= /Ticket/Display.html?id=3D3288361"" rel=3D""noreferrer noreferrer"" target=3D""= _blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3288361</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Mercedes,<br> <br> Just checking in, was your student able to access the cluster?<br> <br> On Fri Oct 10 11:46:59 2025, OL73413 wrote:<br> <br> &gt; Please do! I&#39;ll also ask that you submit an RT ticket to formally = add the<br> &gt; student to your group<br> &gt; (<a href=3D""https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-suppor= t.php?auto=3DResearch%20Computing"" rel=3D""noreferrer noreferrer"" target=3D""= _blank"">https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?aut= o=3DResearch%20Computing</a>).<br> &gt; I looked into the VPN issue, and it seems like the student should be f= ine<br> &gt; as long as they install the vpn. Once they&#39;re added we&#39;ll test=  it out and<br> &gt; see what happens from there.<br> <br> &gt; On Thu Oct 09 16:26:28 2025, OF19978 wrote:<br> <br> &gt;&gt; OK! I&#39;ve just received notice that the student&#39;s affiliati= on has been<br> &gt;&gt; reinstated; her name is Lais Grossel and her ID# is VE32332 (laisg= 1).<br> &gt;&gt; She should have access through 6/30/2026.What would be the next st= ep<br> &gt;&gt; for allowing her to access my lab&#39;s volume on chip? Should I d= irect her<br> &gt;&gt; to download the VPN too? On Thu, Oct 9, 2025 at 11:55 AM Max Breit= meyer<br> &gt;&gt; via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blan= k"" rel=3D""noreferrer"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.= html?id=3D3288361"" rel=3D""noreferrer noreferrer"" target=3D""_blank"">https://= rt.umbc.edu/Ticket/Display.html?id=3D3288361</a> &gt;<br> <br> &gt;&gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt;&gt; Hi Mercedes,<br> <br> &gt;&gt;&gt; Typically we don&#39;t allow for users into UMBC services if t= hey are<br> &gt;&gt;&gt; not in North<br> &gt;&gt;&gt; America (even with VPN). That being said, we can make some<br> &gt;&gt;&gt; exceptions to this<br> &gt;&gt;&gt; rule. For now start by sponsoring an account for the student<b= r> &gt;&gt;&gt; (information for<br> &gt;&gt;&gt; that can be found here:<br> &gt;&gt;&gt; <a href=3D""https://umbc.atlassian.net/wiki/spaces/faq/pages/30= 739140/How+do+I+request+myUMBC+accounts+for+non-UMBC+users+Create+a+Sponsor= ed+Account"" rel=3D""noreferrer noreferrer"" target=3D""_blank"">https://umbc.at= lassian.net/wiki/spaces/faq/pages/30739140/How+do+I+request+myUMBC+accounts= +for+non-UMBC+users+Create+a+Sponsored+Account</a>),<br> &gt;&gt;&gt; and we&#39;ll go from there.<br> <br> &gt;&gt;&gt; On Wed Oct 08 11:01:26 2025, ZZ99999 wrote:<br> <br> &gt;&gt;&gt; &gt; First Name: Mercedes<br> &gt;&gt;&gt; &gt; Last Name: Burns<br> &gt;&gt;&gt; &gt; Email: <a href=3D""mailto:burnsm@umbc.edu"" target=3D""_blan= k"" rel=3D""noreferrer"">burnsm@umbc.edu</a><br> &gt;&gt;&gt; &gt; Campus ID: OF19978<br> &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt; Request Type: High Performance Cluster<br> &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt; Hello,<br> &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt; I have a Brazilian graduate student on fellowship, Lais G= rossel,<br> &gt;&gt;&gt; who generated a large genomic dataset while working in my lab,=  and<br> &gt;&gt;&gt; who now is trying to analyze this dataset using the program St= acks.<br> &gt;&gt;&gt; She has submitted jobs to servers at her home institution of U=  Sao<br> &gt;&gt;&gt; Paolo and the UK school where her fellowship continued, but th= ey<br> &gt;&gt;&gt; haven&#39;t been running due to timeouts and/or job backlogs. = She would<br> &gt;&gt;&gt; like to access chip via my lab&#39;s login, but we have a coup= le of<br> &gt;&gt;&gt; questions about this.<br> &gt;&gt;&gt; &gt; 1) Is this allowed? My lab has tried to access chip while=  abroad<br> &gt;&gt;&gt; and was unsuccessful, but this may have been because the hotel= <br> &gt;&gt;&gt; internet was slow. I know the GlobalProtect VPN would probably=  need<br> &gt;&gt;&gt; to be downloaded too.<br> &gt;&gt;&gt; &gt; 2) Can I create a log-in for her? She was assigned a UMBC=  account<br> &gt;&gt;&gt; while working here in 2023, but I assume that has lapsed.<br> &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt; Thank you for the help!<br> <br> <br> &gt;&gt;&gt; --<br> <br> &gt;&gt;&gt; Best,<br> &gt;&gt;&gt; Max Breitmeyer<br> &gt;&gt;&gt; DOIT HPC System Administrator<br> <br> <br> &gt;&gt; -- Dr. Mercedes Burns (she/her/hers)Associate ProfessorDepartment = of<br> &gt;&gt; Biological SciencesUniversity of Maryland, Baltimore County1000 Hi= lltop<br> &gt;&gt; CircleBaltimore, MD 21250Office: +1 (410) <a href=3D""http://455-21= 47burnslab.umbc.edu"" rel=3D""noreferrer noreferrer"" target=3D""_blank"">455-21= 47burnslab.umbc.edu</a><br> <br> &gt; --<br> <br> &gt; Best,<br> &gt; Max Breitmeyer<br> &gt; DOIT HPC System Administrator<br> <br> --<br> <br> Best,<br> Max Breitmeyer<br> DOIT HPC System Administrator<br> <br> </blockquote></div> "
3288390,72161732,Create,DoIT-Research-Computing,2025-10-08 15:26:08.0000000,Utilizing matched nodes on chip-cpu,resolved,Beamlak Bekele,bbekele1,Henrique de Melo Jorge Barbosa,hbarbosa,hbarbosa@umbc.edu,Roy Prouty,proutyr1@umbc.edu,"<p>From Dr. Barbosa:</p>  <p>&quot;We need to help to understand how the PI partitions are working in Chip. When I look at the pi_hbarbosa, I can see that we have nodes 14 to 19 (6 in total) assigned to it. But I should have access also to the +6 additional nodes from the matching funding, right? How can we use these additional nodes? We will have to run some very large simulations, which will require using all 12 nodes at once. We don&#39;t know how to queue such a job, since my partition only shows 6 nodes.&quot;</p>  <p>--&nbsp;</p>  <p>Roy Prouty<br /> DoIT Research Computing Team</p> "
3288390,72207384,Correspond,DoIT-Research-Computing,2025-10-10 15:28:16.0000000,Utilizing matched nodes on chip-cpu,resolved,Beamlak Bekele,bbekele1,Henrique de Melo Jorge Barbosa,hbarbosa,hbarbosa@umbc.edu,Beamlak Bekele,bbekele1@umbc.edu,"<div> <p>Hello&nbsp;</p>  <p>To run a job across 12 nodes, you can submit a job to the contrib&nbsp;partition and request 12 nodes. SLURM will allocate your 6 dedicated nodes first (--nodelist=c24-[14-19]) and then fill the remaining slots from available match nodes.</p>  <p>Here&#39;s an example sbatch script you can use:</p>  <p><br /> #SBATCH --cluster=chip-cpu<br /> #SBATCH --mem=5000M<br /> #SBATCH --time=01:00:00<br /> #SBATCH --account=pi_hbarbosa<br /> #SBATCH --partition=contrib<br /> #SBATCH --nodes=12<br /> #SBATCH --qos=shared<br /> #SBATCH --nodelist=c24-[14-19]</p>  <p>Let us know if that doesn&#39;t work.&nbsp;</p>  <p>On Wed Oct 08 11:26:08 2025, WH39335 wrote:</p>  <blockquote> <p>From Dr. Barbosa:</p>  <p>&quot;We need to help to understand how the PI partitions are working in Chip. When I look at the pi_hbarbosa, I can see that we have nodes 14 to 19 (6 in total) assigned to it. But I should have access also to the +6 additional nodes from the matching funding, right? How can we use these additional nodes? We will have to run some very large simulations, which will require using all 12 nodes at once. We don&#39;t know how to queue such a job, since my partition only shows 6 nodes.&quot;</p>  <p>--&nbsp;</p>  <p>Roy Prouty<br /> DoIT Research Computing Team</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> "
3288602,72170508,Create,DoIT-Research-Computing,2025-10-08 18:43:59.0000000,HPC Other Issue: cannot log into chip,resolved,Hakim Fessuh,hfessuh1,Sergio De souza-machad,sergio,sergio@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Sergio Last Name:                 De souza-machad Email:                     sergio@umbc.edu Campus ID:                 VR64161  Request Type:              High Performance Cluster  Completely Hangs after Last login : Wed Oct 8 ......  "
3288602,72173745,Correspond,DoIT-Research-Computing,2025-10-08 20:04:58.0000000,HPC Other Issue: cannot log into chip,resolved,Hakim Fessuh,hfessuh1,Sergio De souza-machad,sergio,sergio@umbc.edu,Hakim Fessuh,hfessuh1@umbc.edu,"<p>Hello Sergio,</p>  <p>Our team is aware of the issue, and we are working on resolving this at the moment. We understand the inconvenience this may cause you, but we will send an update once this issue has been fixed.</p>  <p>--&nbsp;</p>  <p>Best regards,</p>  <p>Hakim Fessuh</p> "
3288602,72189897,Correspond,DoIT-Research-Computing,2025-10-09 17:20:23.0000000,HPC Other Issue: cannot log into chip,resolved,Hakim Fessuh,hfessuh1,Sergio De souza-machad,sergio,sergio@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Sergio,</p>  <p>I just wanted to give you an update regarding the status of chip. Access to the cluster was restored yesterday afternoon, and access to ada volumes have also been restored. For more information, please check out the myUMBC post:&nbsp;https://my3.my.umbc.edu/groups/hpcf/posts/153425</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Wed Oct 08 14:43:59 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Sergio Last Name:                 De souza-machad Email:                     sergio@umbc.edu Campus ID:                 VR64161  Request Type:              High Performance Cluster  Completely Hangs after Last login : Wed Oct 8 ......  </pre> </blockquote> </div> "
3288614,72170886,Create,DoIT-Research-Computing,2025-10-08 18:53:18.0000000,HPC Other Issue: Unable to connect to chip,resolved,Hakim Fessuh,hfessuh1,Nilanjana Das,ndas2,ndas2@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Nilanjana Last Name:                 Das Email:                     ndas2@umbc.edu Campus ID:                 CJ99898  Request Type:              High Performance Cluster  Hello, please help. I am unable to connect to chip.  "
3288614,72173553,Correspond,DoIT-Research-Computing,2025-10-08 20:00:52.0000000,HPC Other Issue: Unable to connect to chip,resolved,Hakim Fessuh,hfessuh1,Nilanjana Das,ndas2,ndas2@umbc.edu,Hakim Fessuh,hfessuh1@umbc.edu,"<p>Hello Nilanjana,</p>  <p>Our team is aware of the issue, and we are working on resolving this at the moment. We understand the inconvenience this may cause you, but we will send an update once this issue has been fixed.&nbsp;</p>  <p>--&nbsp;</p>  <p>Best regards,</p>  <p>Hakim Fessuh</p> "
3288614,72189864,Correspond,DoIT-Research-Computing,2025-10-09 17:19:20.0000000,HPC Other Issue: Unable to connect to chip,resolved,Hakim Fessuh,hfessuh1,Nilanjana Das,ndas2,ndas2@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi&nbsp;Nilanjana,</p>  <p>I just wanted to give you an update regarding the status of chip. Access to the cluster was restored yesterday afternoon, and access to ada volumes have also been restored. For more information, please check out the myUMBC post:&nbsp;https://my3.my.umbc.edu/groups/hpcf/posts/153425</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Wed Oct 08 16:00:52 2025, KX07455 wrote: <blockquote>Nilanjana</blockquote> </div> "
3288634,72171627,Create,DoIT-Research-Computing,2025-10-08 19:10:27.0000000,HPC Other Issue: Unable to access login shell after connecting to chip.rs.umbc.edu,resolved,Hakim Fessuh,hfessuh1,Chaoqian Yuan,c285,c285@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Chaoqian<br /> Last Name:                 Yuan<br /> Email:                     c285@umbc.edu<br /> Campus ID:                 NO26412<br /> <br /> Request Type:              High Performance Cluster<br /> <br /> Hello HPCF Support Team,=0D<br /> =0D<br /> I=E2=80=99m having trouble accessing my account on the UMBC HPC cluster. Wh= en I connect to chip.rs.umbc.edu via SSH (using ssh c285@chip.rs.umbc.edu),=  the login process stops after displaying the system update message (the lo= ng MOTD about GPU node issues). The terminal never reaches the normal shell=  prompt, and I cannot execute any commands.=0D<br /> =0D<br /> I=E2=80=99ve already tried reconnecting, pressing Enter, and even logging i= n with bash --noprofile --norc, but the session still seems to hang.=0D<br = /> =0D<br /> Could you please check if there=E2=80=99s any issue with my user account or=  the login node configuration?=0D<br /> =0D<br /> Thank you very much for your help!=0D<br /> =0D<br /> Best regards,=0D<br /> Chaoqian Yuan<br /> <br /> Attachment 1: <a href=3D""https://umbc.box.com/s/zsflvrgu6zyxi3yxmhfk3he4ibz= zte6s"" target=3D""_blank"">touble.png</a><br /> "
3288634,72173695,Correspond,DoIT-Research-Computing,2025-10-08 20:03:51.0000000,HPC Other Issue: Unable to access login shell after connecting to chip.rs.umbc.edu,resolved,Hakim Fessuh,hfessuh1,Chaoqian Yuan,c285,c285@umbc.edu,Hakim Fessuh,hfessuh1@umbc.edu,"<p>Hello Chaoqian,</p>  <p>Our team is aware of the issue, and we are working on resolving this at the moment. We understand the inconvenience this may cause you, but we will send an update once this issue has been fixed.&nbsp;</p>  <p>Best regards,</p>  <p>Hakim Fessuh</p> "
3288634,72187656,Correspond,DoIT-Research-Computing,2025-10-09 16:13:04.0000000,HPC Other Issue: Unable to access login shell after connecting to chip.rs.umbc.edu,resolved,Hakim Fessuh,hfessuh1,Chaoqian Yuan,c285,c285@umbc.edu,Chaoqian Yuan,c285@umbc.edu,"<div> <p>I appreciate your clarification~</p>  <p>On Wed Oct 08 16:03:51 2025, KX07455 wrote:</p>  <blockquote> <p>Hello Chaoqian,</p>  <p>Our team is aware of the issue, and we are working on resolving this at the moment. We understand the inconvenience this may cause you, but we will send an update once this issue has been fixed.&nbsp;</p>  <p>Best regards,</p>  <p>Hakim Fessuh</p> </blockquote> </div> "
3288634,72189966,Correspond,DoIT-Research-Computing,2025-10-09 17:21:27.0000000,HPC Other Issue: Unable to access login shell after connecting to chip.rs.umbc.edu,resolved,Hakim Fessuh,hfessuh1,Chaoqian Yuan,c285,c285@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Chaoqian,</p>  <p>I just wanted to give you an update regarding the status of chip. Access to the cluster was restored yesterday afternoon, and access to ada volumes have also been restored. For more information, please check out the myUMBC post:&nbsp;https://my3.my.umbc.edu/groups/hpcf/posts/153425</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Thu Oct 09 12:13:04 2025, NO26412 wrote: <blockquote> <div> <p>I appreciate your clarification~</p>  <p>On Wed Oct 08 16:03:51 2025, KX07455 wrote:</p>  <blockquote> <p>Hello Chaoqian,</p>  <p>Our team is aware of the issue, and we are working on resolving this at the moment. We understand the inconvenience this may cause you, but we will send an update once this issue has been fixed.&nbsp;</p>  <p>Best regards,</p>  <p>Hakim Fessuh</p> </blockquote> </div> </blockquote> </div> "
3288676,72173349,Create,DoIT-Research-Computing,2025-10-08 19:55:51.0000000,HPC Slurm/Software Issue: SSH Connection Hanging,resolved,Hakim Fessuh,hfessuh1,Madeline Rippin,mrippin1,mrippin1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Madeline Last Name:                 Rippin Email:                     mrippin1@umbc.edu Campus ID:                 SV92175  Request Type:              High Performance Cluster   Good afternoon,  When I ssh into chip.rs.umbc.edu, I am able to be successfully logged in. However, the connection just hangs there without presenting a shell prompt afterwards. Is there any way I could resolve this issue? This happens when I try to use putty and my windows powershell.  "
3288676,72173624,Correspond,DoIT-Research-Computing,2025-10-08 20:02:21.0000000,HPC Slurm/Software Issue: SSH Connection Hanging,resolved,Hakim Fessuh,hfessuh1,Madeline Rippin,mrippin1,mrippin1@umbc.edu,Hakim Fessuh,hfessuh1@umbc.edu,"<p>Hello Madeline,</p>  <p>Our team is aware of the issue, and we are working on resolving this at the moment. We understand the inconvenience this may cause you, but we will send an update once this issue has been fixed.&nbsp;</p>  <p>--&nbsp;</p>  <p>Best regards,</p>  <p>Hakim Fessuh</p> "
3288676,72189880,Correspond,DoIT-Research-Computing,2025-10-09 17:19:49.0000000,HPC Slurm/Software Issue: SSH Connection Hanging,resolved,Hakim Fessuh,hfessuh1,Madeline Rippin,mrippin1,mrippin1@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Madeline,</p>  <p>I just wanted to give you an update regarding the status of chip. Access to the cluster was restored yesterday afternoon, and access to ada volumes have also been restored. For more information, please check out the myUMBC post:&nbsp;https://my3.my.umbc.edu/groups/hpcf/posts/153425</p>  <p>Have a nice day!</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Wed Oct 08 15:55:51 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Madeline Last Name:                 Rippin Email:                     mrippin1@umbc.edu Campus ID:                 SV92175  Request Type:              High Performance Cluster   Good afternoon,  When I ssh into chip.rs.umbc.edu, I am able to be successfully logged in. However, the connection just hangs there without presenting a shell prompt afterwards. Is there any way I could resolve this issue? This happens when I try to use putty and my windows powershell.  </pre> </blockquote> </div> "
3288709,72174285,Create,DoIT-Research-Computing,2025-10-08 20:26:02.0000000,HPC Slurm/Software Issue: Cant log in,resolved,Hakim Fessuh,hfessuh1,Shubhashis Roy Dipta,sroydip1,sroydip1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Shubhashis Last Name:                 Roy Dipta Email:                     sroydip1@umbc.edu Campus ID:                 JS93659  Request Type:              High Performance Cluster   I cant log in to chip  "
3288709,72174393,Correspond,DoIT-Research-Computing,2025-10-08 20:28:45.0000000,HPC Slurm/Software Issue: Cant log in,resolved,Hakim Fessuh,hfessuh1,Shubhashis Roy Dipta,sroydip1,sroydip1@umbc.edu,Hakim Fessuh,hfessuh1@umbc.edu,"<p>Hello Shubhashis,</p>  <p>Our team is aware of the issue, and we are working on resolving this at the moment. We understand the inconvenience this may cause you, but we will send an update once this issue has been fixed.</p>  <p>--&nbsp;</p>  <p>Best regards,</p>  <p>Hakim Fessuh</p> "
3288709,72189922,Correspond,DoIT-Research-Computing,2025-10-09 17:20:47.0000000,HPC Slurm/Software Issue: Cant log in,resolved,Hakim Fessuh,hfessuh1,Shubhashis Roy Dipta,sroydip1,sroydip1@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Shubhashis,</p>  <p>I just wanted to give you an update regarding the status of chip. Access to the cluster was restored yesterday afternoon, and access to ada volumes have also been restored. For more information, please check out the myUMBC post:&nbsp;https://my3.my.umbc.edu/groups/hpcf/posts/153425</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Wed Oct 08 16:26:02 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Shubhashis Last Name:                 Roy Dipta Email:                     sroydip1@umbc.edu Campus ID:                 JS93659  Request Type:              High Performance Cluster   I cant log in to chip  </pre> </blockquote> </div> "
3288849,72176641,Create,DoIT-Research-Computing,2025-10-09 03:32:33.0000000,HPC Slurm/Software Issue: ADA storage,resolved,Max Breitmeyer,mb17,Shubhashis Roy Dipta,sroydip1,sroydip1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Shubhashis Last Name:                 Roy Dipta Email:                     sroydip1@umbc.edu Campus ID:                 JS93659  Request Type:              High Performance Cluster   I know the ada storage is not working and its under maintenance. But do you have any timeframe on how much time it will take? My all experiment data is stored in ada, so i cant work on rs without access to that data. Thanks for your understanding.  "
3288849,72187131,Correspond,DoIT-Research-Computing,2025-10-09 15:58:20.0000000,HPC Slurm/Software Issue: ADA storage,resolved,Max Breitmeyer,mb17,Shubhashis Roy Dipta,sroydip1,sroydip1@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Shubhashis,</p>  <p>Thanks for you patience and understanding. The Ada volume are back!&nbsp;https://my3.my.umbc.edu/groups/hpcf/posts/153425</p>  <p>On Wed Oct 08 23:32:33 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Shubhashis Last Name:                 Roy Dipta Email:                     sroydip1@umbc.edu Campus ID:                 JS93659  Request Type:              High Performance Cluster   I know the ada storage is not working and its under maintenance. But do you have any timeframe on how much time it will take? My all experiment data is stored in ada, so i cant work on rs without access to that data. Thanks for your understanding.  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3288952,72181236,Create,DoIT-Research-Computing,2025-10-09 13:33:31.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ljmei,stalled,Greg Ballantine,gballan1,Lanju Mei,ljmei,ljmei@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Lanju,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 97.7 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/ljmei&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_ljmei&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3288952,72398956,Correspond,DoIT-Research-Computing,2025-10-21 16:10:21.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ljmei,stalled,Greg Ballantine,gballan1,Lanju Mei,ljmei,ljmei@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Lanju,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 11th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3288957,72181396,Create,DoIT-Research-Computing,2025-10-09 13:36:28.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_mfagan,stalled,Greg Ballantine,gballan1,Matthew Fagan,mfagan,mfagan@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Matthew,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 97.7 GB&nbsp;quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/mfagan&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_mfagan&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3288957,72450993,Correspond,DoIT-Research-Computing,2025-10-23 13:51:15.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_mfagan,stalled,Greg Ballantine,gballan1,Matthew Fagan,mfagan,mfagan@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Matthew,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3288960,72181489,Create,DoIT-Research-Computing,2025-10-09 13:38:02.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ughosh,stalled,Greg Ballantine,gballan1,Upal Ghosh,ughosh,ughosh@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Upal,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB&nbsp;of a 97.7 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/ughosh&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_ughosh&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3288960,72452058,Correspond,DoIT-Research-Computing,2025-10-23 14:12:36.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ughosh,stalled,Greg Ballantine,gballan1,Upal Ghosh,ughosh,ughosh@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Upal,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3288964,72181577,Create,DoIT-Research-Computing,2025-10-09 13:39:40.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_shenj,stalled,Greg Ballantine,gballan1,Jinglai Shen,shenj,shenj@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Jinglai,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB&nbsp;of a 97.7 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/shenj&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_shenj&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3288964,72451669,Correspond,DoIT-Research-Computing,2025-10-23 14:06:28.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_shenj,stalled,Greg Ballantine,gballan1,Jinglai Shen,shenj,shenj@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Jinglai,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3288965,72181623,Create,DoIT-Research-Computing,2025-10-09 13:41:18.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_mathew,stalled,Greg Ballantine,gballan1,Thomas Mathew,mathew,mathew@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Thomas,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB&nbsp;of a 97.7 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/mathew&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_mathew&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3288965,72450932,Correspond,DoIT-Research-Computing,2025-10-23 13:50:09.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_mathew,stalled,Greg Ballantine,gballan1,Thomas Mathew,mathew,mathew@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Thomas,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3288967,72181713,Create,DoIT-Research-Computing,2025-10-09 13:43:43.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ordopa1,stalled,Greg Ballantine,gballan1,Unknown," ",ordopa1@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Patricia,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB&nbsp;of a 244.1 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/ordopa1&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_ordopa1&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3288967,72451075,Correspond,DoIT-Research-Computing,2025-10-23 13:52:29.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ordopa1,stalled,Greg Ballantine,gballan1,Unknown," ",ordopa1@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Patricia,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3288967,72458334,Correspond,DoIT-Research-Computing,2025-10-23 16:34:03.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ordopa1,stalled,Greg Ballantine,gballan1,Unknown," ",ordopa1@umbc.edu,Patricia Ordóñez,patti.ordonez@umbc.edu,"Dear Elliott,  I am not sure what I have to migrate.  I would appreciate knowing what needs to be migrated? I think I missed the first email.  Thanks, Patti  Patricia Ord=C3=B3=C3=B1ez, PhD PI Arecibo C3 STEM Education Center 2025 Hrabowski Innovation Award Associate Professor Department of Information Systems University of Maryland, Baltimore County 1000 Hilltop Circle, Baltimore, MD 21250   (410) 455-8843 Link to my calendar <https://calendar.google.com/calendar/u/0?cid=3Db3Jkb3BhMUB1bWJjLmVkdQ> if you are from UMBC.   On Thu, Oct 23, 2025 at 9:52=E2=80=AFAM Elliot Gobbert via RT <UMBCHelp@rt.= umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3288967 > > > Last Update From Ticket: > > Dear Patricia, > > As per our previous communications, since you did not schedule a date by > October 15, we will be going with Option 2, randomly assigning a date > between > October 16 and November 15 for your migration. > > We have assigned the date of November 14th for your migration. Let us know > if > there is a better day for you, and within reason, we can reschedule that > date. > > You will be notified when the migration begins and completes. > > Thank you, > > Elliot > > "
3288967,72458334,Correspond,DoIT-Research-Computing,2025-10-23 16:34:03.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ordopa1,stalled,Greg Ballantine,gballan1,Unknown," ",ordopa1@umbc.edu,Patricia Ordóñez,patti.ordonez@umbc.edu,"<div dir=3D""ltr""><div>Dear Elliott,</div><div><br></div><div>I am not sure = what I have to migrate.=C2=A0 I would appreciate knowing what needs to be m= igrated? I think I missed the first email.</div><div><br></div><div>Thanks,= <br>Patti</div><div><br></div><div><div dir=3D""ltr"" class=3D""gmail_signatur= e"" data-smartmail=3D""gmail_signature""><div dir=3D""ltr""><div><font color=3D""= #000000"">Patricia Ord=C3=B3=C3=B1ez, PhD</font></div><div><font color=3D""#0= 00000"">PI Arecibo C3 STEM Education Center</font></div><div><font color=3D""= #000000"">2025 Hrabowski Innovation Award</font></div><div><font color=3D""#0= 00000"">Associate Professor</font></div><div><font color=3D""#000000"">Departm= ent of Information Systems</font></div><div>University of Maryland, Baltimo= re County</div><div>1000 Hilltop Circle, Baltimore, MD 21250</div><div><br>= </div><div><br></div><div>(410) 455-8843</div><div><a href=3D""https://calen= dar.google.com/calendar/u/0?cid=3Db3Jkb3BhMUB1bWJjLmVkdQ"" target=3D""_blank""= >Link to my calendar</a>=C2=A0if you are from UMBC.<br></div></div></div></= div><br></div><br><div class=3D""gmail_quote gmail_quote_container""><div dir= =3D""ltr"" class=3D""gmail_attr"">On Thu, Oct 23, 2025 at 9:52=E2=80=AFAM Ellio= t Gobbert via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.um= bc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""m= argin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left= :1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id= =3D3288967"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket= /Display.html?id=3D3288967</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Dear Patricia,<br> <br> As per our previous communications, since you did not schedule a date by<br> October 15, we will be going with Option 2, randomly assigning a date betwe= en<br> October 16 and November 15 for your migration.<br> <br> We have assigned the date of November 14th for your migration. Let us know = if<br> there is a better day for you, and within reason, we can reschedule that da= te.<br> <br> You will be notified when the migration begins and completes.<br> <br> Thank you,<br> <br> Elliot<br> <br> </blockquote></div> "
3288967,72458897,Correspond,DoIT-Research-Computing,2025-10-23 16:45:18.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ordopa1,stalled,Greg Ballantine,gballan1,Unknown," ",ordopa1@umbc.edu,Patricia Ordóñez,patti.ordonez@umbc.edu,"To be honest, I was not even aware that I had a research cluster.  Any assistance you can give me would be greatly appreciated.  Patti   Patricia Ord=C3=B3=C3=B1ez, PhD PI Arecibo C3 STEM Education Center 2025 Hrabowski Innovation Award Associate Professor Department of Information Systems University of Maryland, Baltimore County 1000 Hilltop Circle, Baltimore, MD 21250   (410) 455-8843 Link to my calendar <https://calendar.google.com/calendar/u/0?cid=3Db3Jkb3BhMUB1bWJjLmVkdQ> if you are from UMBC.   On Thu, Oct 23, 2025 at 12:33=E2=80=AFPM Patricia Ord=C3=B3=C3=B1ez <patti.= ordonez@umbc.edu> wrote:  > Dear Elliott, > > I am not sure what I have to migrate.  I would appreciate knowing what > needs to be migrated? I think I missed the first email. > > Thanks, > Patti > > Patricia Ord=C3=B3=C3=B1ez, PhD > PI Arecibo C3 STEM Education Center > 2025 Hrabowski Innovation Award > Associate Professor > Department of Information Systems > University of Maryland, Baltimore County > 1000 Hilltop Circle, Baltimore, MD 21250 > > > (410) 455-8843 > Link to my calendar > <https://calendar.google.com/calendar/u/0?cid=3Db3Jkb3BhMUB1bWJjLmVkdQ> if > you are from UMBC. > > > On Thu, Oct 23, 2025 at 9:52=E2=80=AFAM Elliot Gobbert via RT < > UMBCHelp@rt.umbc.edu> wrote: > >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3288967 > >> >> Last Update From Ticket: >> >> Dear Patricia, >> >> As per our previous communications, since you did not schedule a date by >> October 15, we will be going with Option 2, randomly assigning a date >> between >> October 16 and November 15 for your migration. >> >> We have assigned the date of November 14th for your migration. Let us >> know if >> there is a better day for you, and within reason, we can reschedule that >> date. >> >> You will be notified when the migration begins and completes. >> >> Thank you, >> >> Elliot >> >> "
3288967,72458897,Correspond,DoIT-Research-Computing,2025-10-23 16:45:18.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ordopa1,stalled,Greg Ballantine,gballan1,Unknown," ",ordopa1@umbc.edu,Patricia Ordóñez,patti.ordonez@umbc.edu,"<div dir=3D""ltr""><div>To be honest, I was not even aware that I had a resea= rch cluster.=C2=A0 Any assistance you can give me would be greatly apprecia= ted.</div><div><br></div><div>Patti</div><div><br></div><div><br></div><div= ><div dir=3D""ltr"" class=3D""gmail_signature"" data-smartmail=3D""gmail_signatu= re""><div dir=3D""ltr""><div><font color=3D""#000000"">Patricia Ord=C3=B3=C3=B1e= z, PhD</font></div><div><font color=3D""#000000"">PI Arecibo C3 STEM Educatio= n Center</font></div><div><font color=3D""#000000"">2025 Hrabowski Innovation=  Award</font></div><div><font color=3D""#000000"">Associate Professor</font><= /div><div><font color=3D""#000000"">Department of Information Systems</font><= /div><div>University of Maryland, Baltimore County</div><div>1000 Hilltop C= ircle, Baltimore, MD 21250</div><div><br></div><div><br></div><div>(410) 45= 5-8843</div><div><a href=3D""https://calendar.google.com/calendar/u/0?cid=3D= b3Jkb3BhMUB1bWJjLmVkdQ"" target=3D""_blank"">Link to my calendar</a>=C2=A0if y= ou are from UMBC.<br></div></div></div></div><br></div><br><div class=3D""gm= ail_quote gmail_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On T= hu, Oct 23, 2025 at 12:33=E2=80=AFPM Patricia Ord=C3=B3=C3=B1ez &lt;<a href= =3D""mailto:patti.ordonez@umbc.edu"">patti.ordonez@umbc.edu</a>&gt; wrote:<br= ></div><blockquote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;= border-left:1px solid rgb(204,204,204);padding-left:1ex""><div dir=3D""ltr""><= div>Dear Elliott,</div><div><br></div><div>I am not sure what I have to mig= rate.=C2=A0 I would appreciate knowing what needs to be migrated? I think I=  missed the first email.</div><div><br></div><div>Thanks,<br>Patti</div><di= v><br></div><div><div dir=3D""ltr"" class=3D""gmail_signature""><div dir=3D""ltr= ""><div><font color=3D""#000000"">Patricia Ord=C3=B3=C3=B1ez, PhD</font></div>= <div><font color=3D""#000000"">PI Arecibo C3 STEM Education Center</font></di= v><div><font color=3D""#000000"">2025 Hrabowski Innovation Award</font></div>= <div><font color=3D""#000000"">Associate Professor</font></div><div><font col= or=3D""#000000"">Department of Information Systems</font></div><div>Universit= y of Maryland, Baltimore County</div><div>1000 Hilltop Circle, Baltimore, M= D 21250</div><div><br></div><div><br></div><div>(410) 455-8843</div><div><a=  href=3D""https://calendar.google.com/calendar/u/0?cid=3Db3Jkb3BhMUB1bWJjLmV= kdQ"" target=3D""_blank"">Link to my calendar</a>=C2=A0if you are from UMBC.<b= r></div></div></div></div><br></div><br><div class=3D""gmail_quote""><div dir= =3D""ltr"" class=3D""gmail_attr"">On Thu, Oct 23, 2025 at 9:52=E2=80=AFAM Ellio= t Gobbert via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_bla= nk"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail= _quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204= ,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Tick= et/Display.html?id=3D3288967"" rel=3D""noreferrer"" target=3D""_blank"">https://= rt.umbc.edu/Ticket/Display.html?id=3D3288967</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Dear Patricia,<br> <br> As per our previous communications, since you did not schedule a date by<br> October 15, we will be going with Option 2, randomly assigning a date betwe= en<br> October 16 and November 15 for your migration.<br> <br> We have assigned the date of November 14th for your migration. Let us know = if<br> there is a better day for you, and within reason, we can reschedule that da= te.<br> <br> You will be notified when the migration begins and completes.<br> <br> Thank you,<br> <br> Elliot<br> <br> </blockquote></div> </blockquote></div> "
3288967,72527247,Comment,DoIT-Research-Computing,2025-10-28 14:40:04.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ordopa1,stalled,Greg Ballantine,gballan1,Unknown," ",ordopa1@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hi,</p>  <p>No worries, you don&#39;t have to migrate anything; this is all on our end. We&#39;re just letting you know what day we&#39;re doing it, so you don&#39;t use the cluster that day.</p>  <p>Yes, you have almost no data on the cluster. I expect the migration to take almost no time at all.</p>  <p>The only thing you have to do is not use the cluster&nbsp;on November 14th, and you&#39;ll be all set.</p>  <p>&nbsp;</p>  <p>Best,&nbsp;</p>  <p>Elliot</p> "
3288967,72527248,CommentEmailRecord,DoIT-Research-Computing,2025-10-28 14:40:05.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ordopa1,stalled,Greg Ballantine,gballan1,Unknown," ",ordopa1@umbc.edu,The RT System itself,NULL,"Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3288967 >  Comment just added.    Hi,  No worries, you don't have to migrate anything; this is all on our end. We're just letting you know what day we're doing it, so you don't use the cluster that day.  Yes, you have almost no data on the cluster. I expect the migration to take almost no time at all.  The only thing you have to do is not use the cluster on November 14th, and you'll be all set.  Best,  Elliot  "
3288968,72181793,Create,DoIT-Research-Computing,2025-10-09 13:45:16.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_thunguye,stalled,Greg Ballantine,gballan1,Thu Nguyen,thunguye,thunguye@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Thu,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 244.1 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/thunguye&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_thunguye&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3288968,72452016,Correspond,DoIT-Research-Computing,2025-10-23 14:11:53.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_thunguye,stalled,Greg Ballantine,gballan1,Thu Nguyen,thunguye,thunguye@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Thu,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3288973,72181935,Create,DoIT-Research-Computing,2025-10-09 13:47:46.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_reynoter,stalled,Greg Ballantine,gballan1,Tera Reynolds,reynoter,reynoter@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Tera,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB&nbsp;of a 244.1 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/reynoter&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_reynoter&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3288973,72451242,Correspond,DoIT-Research-Computing,2025-10-23 13:55:50.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_reynoter,stalled,Greg Ballantine,gballan1,Tera Reynolds,reynoter,reynoter@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Tera,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3288975,72182034,Create,DoIT-Research-Computing,2025-10-09 13:49:49.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_lindahl,stalled,Greg Ballantine,gballan1,Lasse Lindahl,lindahl,lindahl@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Lasse,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 488.3 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/lindahl&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_lindahl&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3288975,72398908,Correspond,DoIT-Research-Computing,2025-10-21 16:09:26.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_lindahl,stalled,Greg Ballantine,gballan1,Lasse Lindahl,lindahl,lindahl@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Lasse,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 11th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3288979,72182133,Create,DoIT-Research-Computing,2025-10-09 13:51:36.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_lpollio1,stalled,Greg Ballantine,gballan1,Luigi Pollio,lpollio1,lpollio1@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Luigi,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 488.3 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/lpollio1&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_lpollio1&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3288979,72399083,Correspond,DoIT-Research-Computing,2025-10-21 16:12:43.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_lpollio1,stalled,Greg Ballantine,gballan1,Luigi Pollio,lpollio1,lpollio1@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Luigi,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 12th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3288980,72182206,Create,DoIT-Research-Computing,2025-10-09 13:52:57.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_motteler,stalled,Greg Ballantine,gballan1,Howard Motteler,motteler,motteler@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Howard,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 488.3 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/motteler&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_motteler&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3288980,72399594,Correspond,DoIT-Research-Computing,2025-10-21 16:20:21.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_motteler,stalled,Greg Ballantine,gballan1,Howard Motteler,motteler,motteler@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Howard,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3288982,72182281,Create,DoIT-Research-Computing,2025-10-09 13:54:35.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ncastro,stalled,Greg Ballantine,gballan1,Nicolas Castro Cienfuegos,ncastro,ncastro@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Nicolas,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 488.3 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/ncastro&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_ncastro&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3288982,72399670,Correspond,DoIT-Research-Computing,2025-10-21 16:21:27.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_ncastro,stalled,Greg Ballantine,gballan1,Nicolas Castro Cienfuegos,ncastro,ncastro@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Nicolas,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3288983,72182331,Create,DoIT-Research-Computing,2025-10-09 13:56:08.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_nroy,stalled,Greg Ballantine,gballan1,Nirmalya Roy,nroy,nroy@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Nirmalya,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 488.3 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/nroy&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_nroy&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3288983,72399720,Correspond,DoIT-Research-Computing,2025-10-21 16:22:17.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_nroy,stalled,Greg Ballantine,gballan1,Nirmalya Roy,nroy,nroy@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Nirmalya,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3288987,72182408,Create,DoIT-Research-Computing,2025-10-09 13:57:37.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_pyoung,stalled,Greg Ballantine,gballan1,Patricia Young,pyoung,pyoung@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Patricia,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 488.3 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/pyoung&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_pyoung&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3288987,72451147,Correspond,DoIT-Research-Computing,2025-10-23 13:54:18.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_pyoung,stalled,Greg Ballantine,gballan1,Patricia Young,pyoung,pyoung@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Patricia,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3288988,72182479,Create,DoIT-Research-Computing,2025-10-09 13:59:03.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_remer,stalled,Greg Ballantine,gballan1,Lorraine Remer,remer,remer@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Lorraine,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 488.3 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/remer&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_remer&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3288988,72451198,Correspond,DoIT-Research-Computing,2025-10-23 13:55:08.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_remer,stalled,Greg Ballantine,gballan1,Lorraine Remer,remer,remer@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Lorraine,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3288991,72182574,Create,DoIT-Research-Computing,2025-10-09 14:00:47.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_sahuja1,stalled,Greg Ballantine,gballan1,Sanjeev Ahuja,sahuja1,sahuja1@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Sanjeev,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 488.3 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/sahuja1&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_sahuja1&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3288991,72451583,Correspond,DoIT-Research-Computing,2025-10-23 14:04:39.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_sahuja1,stalled,Greg Ballantine,gballan1,Sanjeev Ahuja,sahuja1,sahuja1@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Sanjeev,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3288993,72182652,Create,DoIT-Research-Computing,2025-10-09 14:02:26.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_shimei,stalled,Greg Ballantine,gballan1,Shimei Pan,shimei,shimei@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Shimei,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 488.3 GB&nbsp;quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/shimei&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_shimei&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3288993,72451766,Correspond,DoIT-Research-Computing,2025-10-23 14:07:42.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_shimei,stalled,Greg Ballantine,gballan1,Shimei Pan,shimei,shimei@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Shimei,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3288993,72464521,Correspond,DoIT-Research-Computing,2025-10-23 19:01:22.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_shimei,stalled,Greg Ballantine,gballan1,Shimei Pan,shimei,shimei@umbc.edu,Shimei Pan,shimeij6@gmail.com,"That is fine. Thank you for letting me know.  Best Regards,  Shimei.   On Thu, Oct 23, 2025 at 10:07=E2=80=AFAM Elliot Gobbert via RT <UMBCHelp@rt= .umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3288993 > > > Last Update From Ticket: > > Dear Shimei, > > As per our previous communications, since you did not schedule a date by > October 15, we will be going with Option 2, randomly assigning a date > between > October 16 and November 15 for your migration. > > We have assigned the date of November 14th for your migration. Let us know > if > there is a better day for you, and within reason, we can reschedule that > date. > > You will be notified when the migration begins and completes. > > Thank you, > > Elliot > > "
3288993,72464521,Correspond,DoIT-Research-Computing,2025-10-23 19:01:22.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_shimei,stalled,Greg Ballantine,gballan1,Shimei Pan,shimei,shimei@umbc.edu,Shimei Pan,shimeij6@gmail.com,"<div dir=3D""ltr""><div>That is fine. Thank you for letting me know.</div><di= v><br></div><div><div dir=3D""ltr"" class=3D""gmail_signature"" data-smartmail= =3D""gmail_signature""><div dir=3D""ltr"">Best Regards,<div><br></div><div>Shim= ei.</div></div></div></div><br></div><br><div class=3D""gmail_quote gmail_qu= ote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Thu, Oct 23, 2025 a= t 10:07=E2=80=AFAM Elliot Gobbert via RT &lt;<a href=3D""mailto:UMBCHelp@rt.= umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D= ""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(2= 04,204,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.ed= u/Ticket/Display.html?id=3D3288993"" rel=3D""noreferrer"" target=3D""_blank"">ht= tps://rt.umbc.edu/Ticket/Display.html?id=3D3288993</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Dear Shimei,<br> <br> As per our previous communications, since you did not schedule a date by<br> October 15, we will be going with Option 2, randomly assigning a date betwe= en<br> October 16 and November 15 for your migration.<br> <br> We have assigned the date of November 14th for your migration. Let us know = if<br> there is a better day for you, and within reason, we can reschedule that da= te.<br> <br> You will be notified when the migration begins and completes.<br> <br> Thank you,<br> <br> Elliot<br> <br> </blockquote></div> "
3288998,72182763,Create,DoIT-Research-Computing,2025-10-09 14:03:57.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_yamataka,stalled,Greg Ballantine,gballan1,Takashi Yamashita,yamataka,yamataka@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Takashi,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB&nbsp; of a 488.3 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/yamataka&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_yamataka&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3288998,72452211,Correspond,DoIT-Research-Computing,2025-10-23 14:17:11.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_yamataka,stalled,Greg Ballantine,gballan1,Takashi Yamashita,yamataka,yamataka@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Takashi,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3289003,72182868,Create,DoIT-Research-Computing,2025-10-09 14:05:37.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_yurganov,stalled,Greg Ballantine,gballan1,Leonid Iourganov,yurganov,yurganov@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Leonid,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB&nbsp;of a 488.3 GB&nbsp;quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/yurganov&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_yurganov&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3289003,72195514,Comment,DoIT-Research-Computing,2025-10-09 19:55:55.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_yurganov,stalled,Greg Ballantine,gballan1,Leonid Iourganov,yurganov,yurganov@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Greg, somehow, he responded to me from his personal email. Here&#39;s wh= at he said:</p>  <div> <div>&nbsp;</div> </div>  <div> <div> <table cellpadding=3D""0""> 	<tbody> 		<tr> 			<td> 			<table cellpadding=3D""0""> 				<tbody> 					<tr> 						<td> 						<h3>Leonid Yurganov &lt;leonid.yurganov@gmail.com&gt;</h3> 						</td> 					</tr> 				</tbody> 			</table> 			</td> 			<td> 			<div>3:38=E2=80=AFPM (16 minutes ago)</div> 			</td> 		</tr> 	</tbody> </table> </div> </div>  <table cellpadding=3D""0""> 	<tbody> 		<tr> 			<td>&nbsp;</td> 			<td rowspan=3D""2"">&nbsp;</td> 		</tr> 	</tbody> </table>  <table cellpadding=3D""0""> 	<tbody> 		<tr> 		</tr> 	</tbody> </table>  <table cellpadding=3D""0""> 	<tbody> 		<tr> 			<td colspan=3D""3""> 			<table cellpadding=3D""0""> 				<tbody> 					<tr> 						<td> 						<div>to me&nbsp;</div> 						</td> 					</tr> 				</tbody> 			</table> 			</td> 		</tr> 	</tbody> </table>  <div> <div> <div> <div>Dear Elliot,&nbsp; <div>Please cancel my account and e-mail yurganov@umbc.edu.&nbsp;</div>  <div>Thank you.&nbsp;</div>  <div>Leonid</div> </div> </div> </div> </div> "
3289003,72195515,CommentEmailRecord,DoIT-Research-Computing,2025-10-09 19:55:56.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_yurganov,stalled,Greg Ballantine,gballan1,Leonid Iourganov,yurganov,yurganov@umbc.edu,The RT System itself,NULL,"Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3289003 >  Comment just added.    Greg, somehow, he responded to me from his personal email. Here's what he said:   Leonid Yurganov <leonid.yurganov@gmail.com>  Leonid Yurganov <leonid.yurganov@gmail.com>  3:38 PM (16 minutes ago)    to me  to me  Dear Elliot, Please cancel my account and e-mail yurganov@umbc.edu. Thank you. Leonid  "
3289003,72207415,Comment,DoIT-Research-Computing,2025-10-10 15:29:15.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_yurganov,stalled,Greg Ballantine,gballan1,Leonid Iourganov,yurganov,yurganov@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>I&#39;ll also mark him as &quot;Special Case&quot; in the spreadsheet</p> "
3289003,72207416,CommentEmailRecord,DoIT-Research-Computing,2025-10-10 15:29:16.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_yurganov,stalled,Greg Ballantine,gballan1,Leonid Iourganov,yurganov,yurganov@umbc.edu,The RT System itself,NULL,"Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3289003 >  Comment just added.    I'll also mark him as ""Special Case"" in the spreadsheet  "
3289011,72183057,Create,DoIT-Research-Computing,2025-10-09 14:07:39.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zliang,stalled,Greg Ballantine,gballan1,Liang Zhu,zliang,zliang@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Liang,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB&nbsp;of a 488.3 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/zliang&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_zliang&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3289011,72452365,Correspond,DoIT-Research-Computing,2025-10-23 14:19:39.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zliang,stalled,Greg Ballantine,gballan1,Liang Zhu,zliang,zliang@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Liang,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3289011,72455044,Correspond,DoIT-Research-Computing,2025-10-23 15:23:56.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zliang,stalled,Greg Ballantine,gballan1,Liang Zhu,zliang,zliang@umbc.edu,Liang Zhu,zliang@umbc.edu,"Sorry, I probably missed that email.  Nov 14, Friday is fine with me.  Is this an in-person meeting or online meeting?  So far my calendar is open on that day, although morning is better for me.  On Thu, Oct 23, 2025 at 10:19=E2=80=AFAM Elliot Gobbert via RT <UMBCHelp@rt= .umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3289011 > > > Last Update From Ticket: > > Dear Liang, > > As per our previous communications, since you did not schedule a date by > October 15, we will be going with Option 2, randomly assigning a date > between > October 16 and November 15 for your migration. > > We have assigned the date of November 14th for your migration. Let us know > if > there is a better day for you, and within reason, we can reschedule that > date. > > You will be notified when the migration begins and completes. > > Thank you, > > Elliot > >  --=20 Liang Zhu, Ph.D. Professor of Mechanical Engineering University of Maryland Baltimore County 1000 Hilltop Circle Baltimore, MD 21250 Tel: 410-455-3332 Email: zliang@umbc.edu Lab website: bioheat@umbc.edu S-STEM website: me-stem.umbc.edu "
3289011,72455044,Correspond,DoIT-Research-Computing,2025-10-23 15:23:56.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zliang,stalled,Greg Ballantine,gballan1,Liang Zhu,zliang,zliang@umbc.edu,Liang Zhu,zliang@umbc.edu,"<div dir=3D""ltr"">Sorry, I probably missed that email.=C2=A0 Nov 14, Friday = is fine with me.=C2=A0 Is this an in-person meeting or online meeting?=C2= =A0 So far my calendar is open on that day, although morning is better for = me.</div><br><div class=3D""gmail_quote gmail_quote_container""><div dir=3D""l= tr"" class=3D""gmail_attr"">On Thu, Oct 23, 2025 at 10:19=E2=80=AFAM Elliot Go= bbert via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.e= du</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""margi= n:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex= "">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D3= 289011"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Dis= play.html?id=3D3289011</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Dear Liang,<br> <br> As per our previous communications, since you did not schedule a date by<br> October 15, we will be going with Option 2, randomly assigning a date betwe= en<br> October 16 and November 15 for your migration.<br> <br> We have assigned the date of November 14th for your migration. Let us know = if<br> there is a better day for you, and within reason, we can reschedule that da= te.<br> <br> You will be notified when the migration begins and completes.<br> <br> Thank you,<br> <br> Elliot<br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr"">Liang Zhu, Ph.D.<div>Professor of Mechanical Eng= ineering</div><div>University of Maryland Baltimore County</div><div>1000 H= illtop Circle</div><div>Baltimore, MD 21250</div><div>Tel: 410-455-3332</di= v><div>Email: <a href=3D""mailto:zliang@umbc.edu"" target=3D""_blank"">zliang@u= mbc.edu</a></div><div>Lab website: <a href=3D""mailto:bioheat@umbc.edu"" targ= et=3D""_blank"">bioheat@umbc.edu</a></div><div>S-STEM website: <a href=3D""htt= p://me-stem.umbc.edu"" target=3D""_blank"">me-stem.umbc.edu</a></div></div></d= iv> "
3289011,72527474,Comment,DoIT-Research-Computing,2025-10-28 14:42:58.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zliang,stalled,Greg Ballantine,gballan1,Liang Zhu,zliang,zliang@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hi,</p>  <p>No worries, no meeting of any kind. The only thing you have to do is not use the cluster on Nov. 14, and we&#39;ll do the migration for you.</p>  <p>Best,</p>  <p>Elliot</p> "
3289011,72527475,CommentEmailRecord,DoIT-Research-Computing,2025-10-28 14:42:59.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zliang,stalled,Greg Ballantine,gballan1,Liang Zhu,zliang,zliang@umbc.edu,The RT System itself,NULL,"Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3289011 >  Comment just added.    Hi,  No worries, no meeting of any kind. The only thing you have to do is not use the cluster on Nov. 14, and we'll do the migration for you.  Best,  Elliot  "
3289012,72183111,Create,DoIT-Research-Computing,2025-10-09 14:09:19.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_riaduli,stalled,Greg Ballantine,gballan1,Riadul Islam,riaduli,riaduli@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Riadul,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 488.3 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/riaduli&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_riaduli&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3289012,72451285,Correspond,DoIT-Research-Computing,2025-10-23 13:56:44.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_riaduli,stalled,Greg Ballantine,gballan1,Riadul Islam,riaduli,riaduli@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Riadul,</p>  <p>As per our previous communications, since you did not schedule a date by October 15, we will be going with Option 2, randomly assigning a date between October 16 and November 15 for your migration.&nbsp;</p>  <p>We have assigned the date of November 14th for your migration. Let us know if there is a better day for you, and within reason, we can reschedule that date.</p>  <p>You will be notified when the migration begins and completes.</p>  <p>Thank you,</p>  <p>Elliot</p> "
3289018,72183265,Create,DoIT-Research-Computing,2025-10-09 14:12:43.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_sergio,resolved,Greg Ballantine,gballan1,Sergio De souza-machad,sergio,sergio@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Sergio,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 0 GB of a 488.3 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/sergio&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_sergio&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3289018,72190902,Correspond,DoIT-Research-Computing,2025-10-09 17:45:18.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_sergio,resolved,Greg Ballantine,gballan1,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"Hello,  The needed migration can be done sometime next week (Oct 15, 16,17); any or some of those would work fine with me.  Thanks  Sergio  On Thu, Oct 9, 2025 at 10:12=E2=80=AFAM via RT <UMBCHelp@rt.umbc.edu> wrote:  > Greetings, > > This message has been automatically generated in response to the > creation of a ticket regarding: > > ------------------------------------------------------------------------- > Subject: ""Migrating Research Storage Volume to Ceph Cluster - pi_sergio"" > > Message: > > Dear Sergio, > > As per the communication via myUMBC earlier this summer (June HPCF > Newsletter > ), DoIT is in the process of migrating data off of an older storage server > to > our new RRStor Ceph storage cluster. Your group is using 0 GB of a 488.3 = GB > quota on the old storage server. > > To perform these migrations, we need to take individual storage volumes > offline > while we migrate them to the Ceph cluster. Thus we are reaching out to > schedule > a date where we can migrate your volume located at =E2=80=9C/umbc/rs/serg= io=E2=80=9D. > During > the migration, we will take your volume offline and will terminate any jo= bs > running on the chip compute cluster that are accessing this volume. > > Below we=E2=80=99ve listed two options for handling this data migration -=  please > let us > know which of these you=E2=80=99d prefer. > > Option 1: Schedule a group-wide downtime date during standard business > hours, > which can be done by responding to this email with your preferred date(s) > to > perform the migration. During this time, DoIT staff will work to migrate > your > volume to the Ceph storage cluster. DoIT staff will send an email alert on > this > email thread when the migration has begun and when it has completed. For > most > storage volumes, this process should take less than a business day. > Option 2: If you don=E2=80=99t respond to this email by October 15th, DoI= T staff > will > assign a day over the following month (October 16th through November 15th) > to > migrate your volume. The day chosen will be random and will occur during > business hours. You will be notified of the date chosen to perform the > migration, and will be notified when the migration begins and completes. > > Note: After this process has completed, the new storage volume will have a > new > name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its data und= er > =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, > or in your group=E2=80=99s case you will find your volume under > =E2=80=9C/umbc/rs/pi_sergio=E2=80=9D. > > Thank you, > Elliot > > > ------------------------------------------------------------------------- > > There is no need to reply to this message right now. > > Your ticket has been assigned an ID of [Research Computing #3289018] or > you can go there directly by clicking the link below. > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3289018 > > > You can login to view your open tickets at any time by visiting > http://my.umbc.edu and clicking on ""Help"" and ""Request Help"". > > Alternately you can click on http://my.umbc.edu/help > >                         Thank you > >  --=20 ---------------------------------------------------------------------------= ------------------------------------------ Sergio DeSouza-Machado sergio@umbc.edu Research Assoc. Professor,                                              (W) 410-455-1944 JCET/Dept of Physics (F) 410-455-1072 UMBC, Baltimore MD 21250 "
3289018,72190902,Correspond,DoIT-Research-Computing,2025-10-09 17:45:18.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_sergio,resolved,Greg Ballantine,gballan1,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"<div dir=3D""ltr"">Hello,<div><br></div><div>The needed migration can be done=  sometime next week (Oct 15, 16,17); any or some of those would work fine w= ith me.</div><div><br></div><div>Thanks</div><div><br></div><div>Sergio</di= v></div><br><div class=3D""gmail_quote gmail_quote_container""><div dir=3D""lt= r"" class=3D""gmail_attr"">On Thu, Oct 9, 2025 at 10:12=E2=80=AFAM via RT &lt;= <a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:= <br></div><blockquote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8= ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"">Greetings,<br> <br> This message has been automatically generated in response to the<br> creation of a ticket regarding:<br> <br> -------------------------------------------------------------------------<b= r> Subject: &quot;Migrating Research Storage Volume to Ceph Cluster - pi_sergi= o&quot;<br> <br> Message: <br> <br> Dear Sergio,<br> <br> As per the communication via myUMBC earlier this summer (June HPCF Newslett= er<br> ), DoIT is in the process of migrating data off of an older storage server = to<br> our new RRStor Ceph storage cluster. Your group is using 0 GB of a 488.3 GB= <br> quota on the old storage server.<br> <br> To perform these migrations, we need to take individual storage volumes off= line<br> while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule<br> a date where we can migrate your volume located at =E2=80=9C/umbc/rs/sergio= =E2=80=9D. During<br> the migration, we will take your volume offline and will terminate any jobs= <br> running on the chip compute cluster that are accessing this volume.<br> <br> Below we=E2=80=99ve listed two options for handling this data migration - p= lease let us<br> know which of these you=E2=80=99d prefer.<br> <br> Option 1: Schedule a group-wide downtime date during standard business hour= s,<br> which can be done by responding to this email with your preferred date(s) t= o<br> perform the migration. During this time, DoIT staff will work to migrate yo= ur<br> volume to the Ceph storage cluster. DoIT staff will send an email alert on = this<br> email thread when the migration has begun and when it has completed. For mo= st<br> storage volumes, this process should take less than a business day.<br> Option 2: If you don=E2=80=99t respond to this email by October 15th, DoIT = staff will<br> assign a day over the following month (October 16th through November 15th) = to<br> migrate your volume. The day chosen will be random and will occur during<br> business hours. You will be notified of the date chosen to perform the<br> migration, and will be notified when the migration begins and completes.<br> <br> Note: After this process has completed, the new storage volume will have a = new<br> name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its data under=  =E2=80=9C/umbc/rs/pi_doit=E2=80=9D,<br> or in your group=E2=80=99s case you will find your volume under =E2=80=9C/u= mbc/rs/pi_sergio=E2=80=9D.<br> <br> Thank you,<br> Elliot<br> <br> <br> -------------------------------------------------------------------------<b= r> <br> There is no need to reply to this message right now.=C2=A0 <br> <br> Your ticket has been assigned an ID of [Research Computing #3289018] or you=  can go there directly by clicking the link below.<br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D328= 9018"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3289018</a> &gt;<br> <br> You can login to view your open tickets at any time by visiting <a href=3D""= http://my.umbc.edu"" rel=3D""noreferrer"" target=3D""_blank"">http://my.umbc.edu= </a> and clicking on &quot;Help&quot; and &quot;Request Help&quot;. <br> <br> Alternately you can click on <a href=3D""http://my.umbc.edu/help"" rel=3D""nor= eferrer"" target=3D""_blank"">http://my.umbc.edu/help</a><br> <br> =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 Thank you<br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><div><div dir=3D""ltr""><div><div dir=3D""ltr""><div= >--------------------------------------------------------------------------= -------------------------------------------<br>Sergio DeSouza-Machado=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 <a href=3D""mailto:sergio@umbc.edu"" target= =3D""_blank"">sergio@umbc.edu</a><br>Research Assoc. Professor,=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 (W) 410-455-1944<br>JCET/Dept of Physics= =C2=A0=C2=A0 =C2=A0=C2=A0 =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0 (F) 410-455-1072<br></div><div>UMBC, Baltimore MD 21250<br>= </div></div></div></div></div></div></div> "
3289018,72191839,Correspond,DoIT-Research-Computing,2025-10-09 18:14:50.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_sergio,resolved,Greg Ballantine,gballan1,Sergio De souza-machad,sergio,sergio@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Sergio,<br /> <br /> Sounds good, I&#39;ve put your group&#39;s migration on our schedule for We= dnesday October 15th. We will send you an email alert via this RT ticket wh= en we begin the migration, and again once it has completed.<br /> <br /> Please let me know if you have any questions or concerns.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Thu Oct 09 13:45:18 2025, VR64161 wrote:</p>  <blockquote> <div>Hello, <div>&nbsp;</div>  <div>The needed migration can be done sometime next week (Oct 15, 16,17); a= ny or some of those would work fine with me.</div>  <div>&nbsp;</div>  <div>Thanks</div>  <div>&nbsp;</div>  <div>Sergio</div> </div> &nbsp;  <div> <div>On Thu, Oct 9, 2025 at 10:12=E2=80=AFAM via RT &lt;UMBCHelp@rt.umbc.ed= u&gt; wrote:</div>  <blockquote>Greetings,<br /> <br /> This message has been automatically generated in response to the<br /> creation of a ticket regarding:<br /> <br /> -------------------------------------------------------------------------<b= r /> Subject: &quot;Migrating Research Storage Volume to Ceph Cluster - pi_sergi= o&quot;<br /> <br /> Message:<br /> <br /> Dear Sergio,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newslett= er<br /> ), DoIT is in the process of migrating data off of an older storage server = to<br /> our new RRStor Ceph storage cluster. Your group is using 0 GB of a 488.3 GB= <br /> quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes off= line<br /> while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule<br /> a date where we can migrate your volume located at &ldquo;/umbc/rs/sergio&r= dquo;. During<br /> the migration, we will take your volume offline and will terminate any jobs= <br /> running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - ple= ase let us<br /> know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hour= s,<br /> which can be done by responding to this email with your preferred date(s) t= o<br /> perform the migration. During this time, DoIT staff will work to migrate yo= ur<br /> volume to the Ceph storage cluster. DoIT staff will send an email alert on = this<br /> email thread when the migration has begun and when it has completed. For mo= st<br /> storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT st= aff will<br /> assign a day over the following month (October 16th through November 15th) = to<br /> migrate your volume. The day chosen will be random and will occur during<br=  /> business hours. You will be notified of the date chosen to perform the<br /> migration, and will be notified when the migration begins and completes.<br=  /> <br /> Note: After this process has completed, the new storage volume will have a = new<br /> name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ld= quo;/umbc/rs/pi_doit&rdquo;,<br /> or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/= rs/pi_sergio&rdquo;.<br /> <br /> Thank you,<br /> Elliot<br /> <br /> <br /> -------------------------------------------------------------------------<b= r /> <br /> There is no need to reply to this message right now.&nbsp;<br /> <br /> Your ticket has been assigned an ID of [Research Computing #3289018] or you=  can go there directly by clicking the link below.<br /> <br /> Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3289018 &gt;<b= r /> <br /> You can login to view your open tickets at any time by visiting http://my.u= mbc.edu and clicking on &quot;Help&quot; and &quot;Request Help&quot;.<br /> <br /> Alternately you can click on http://my.umbc.edu/help<br /> <br /> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp= ; &nbsp; Thank you<br /> &nbsp;</blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --  <div> <div> <div> <div> <div> <div> <div>----------------------------------------------------------------------= -----------------------------------------------<br /> Sergio DeSouza-Machado&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sergio@umbc.e= du<br /> Research Assoc. Professor,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&= nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbs= p;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&= nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (W)=  410-455-1944<br /> JCET/Dept of Physics&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (F) 410-455-1072</div>  <div>UMBC, Baltimore MD 21250</div> </div> </div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3289018,72206820,Comment,DoIT-Research-Computing,2025-10-10 15:14:16.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_sergio,resolved,Greg Ballantine,gballan1,Sergio De souza-machad,sergio,sergio@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Marked this as &quot;scheduled&quot; in the spreadsheet</p> "
3289018,72206821,CommentEmailRecord,DoIT-Research-Computing,2025-10-10 15:14:18.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_sergio,resolved,Greg Ballantine,gballan1,Sergio De souza-machad,sergio,sergio@umbc.edu,The RT System itself,NULL,"Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3289018 >  Comment just added.    Marked this as ""scheduled"" in the spreadsheet  "
3289018,72300423,Correspond,DoIT-Research-Computing,2025-10-15 13:53:44.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_sergio,resolved,Greg Ballantine,gballan1,Sergio De souza-machad,sergio,sergio@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Good morning Sergio,<br /> <br /> This is a reminder that we will be migrating your group&#39;s research stor= age volume to the Ceph storage cluster today. During this time, please ensu= re there are not any jobs being run in your research group, otherwise these=  may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Thu Oct 09 14:14:50 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Hello Sergio,<br /> <br /> Sounds good, I&#39;ve put your group&#39;s migration on our schedule for We= dnesday October 15th. We will send you an email alert via this RT ticket wh= en we begin the migration, and again once it has completed.<br /> <br /> Please let me know if you have any questions or concerns.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Thu Oct 09 13:45:18 2025, VR64161 wrote:</p>  <blockquote> <div>Hello, <div>&nbsp;</div>  <div>The needed migration can be done sometime next week (Oct 15, 16,17); a= ny or some of those would work fine with me.</div>  <div>&nbsp;</div>  <div>Thanks</div>  <div>&nbsp;</div>  <div>Sergio</div> </div> &nbsp;  <div> <div>On Thu, Oct 9, 2025 at 10:12=E2=80=AFAM via RT &lt;UMBCHelp@rt.umbc.ed= u&gt; wrote:</div>  <blockquote>Greetings,<br /> <br /> This message has been automatically generated in response to the<br /> creation of a ticket regarding:<br /> <br /> -------------------------------------------------------------------------<b= r /> Subject: &quot;Migrating Research Storage Volume to Ceph Cluster - pi_sergi= o&quot;<br /> <br /> Message:<br /> <br /> Dear Sergio,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newslett= er<br /> ), DoIT is in the process of migrating data off of an older storage server = to<br /> our new RRStor Ceph storage cluster. Your group is using 0 GB of a 488.3 GB= <br /> quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes off= line<br /> while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule<br /> a date where we can migrate your volume located at &ldquo;/umbc/rs/sergio&r= dquo;. During<br /> the migration, we will take your volume offline and will terminate any jobs= <br /> running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - ple= ase let us<br /> know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hour= s,<br /> which can be done by responding to this email with your preferred date(s) t= o<br /> perform the migration. During this time, DoIT staff will work to migrate yo= ur<br /> volume to the Ceph storage cluster. DoIT staff will send an email alert on = this<br /> email thread when the migration has begun and when it has completed. For mo= st<br /> storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT st= aff will<br /> assign a day over the following month (October 16th through November 15th) = to<br /> migrate your volume. The day chosen will be random and will occur during<br=  /> business hours. You will be notified of the date chosen to perform the<br /> migration, and will be notified when the migration begins and completes.<br=  /> <br /> Note: After this process has completed, the new storage volume will have a = new<br /> name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ld= quo;/umbc/rs/pi_doit&rdquo;,<br /> or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/= rs/pi_sergio&rdquo;.<br /> <br /> Thank you,<br /> Elliot<br /> <br /> <br /> -------------------------------------------------------------------------<b= r /> <br /> There is no need to reply to this message right now.&nbsp;<br /> <br /> Your ticket has been assigned an ID of [Research Computing #3289018] or you=  can go there directly by clicking the link below.<br /> <br /> Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3289018 &gt;<b= r /> <br /> You can login to view your open tickets at any time by visiting http://my.u= mbc.edu and clicking on &quot;Help&quot; and &quot;Request Help&quot;.<br /> <br /> Alternately you can click on http://my.umbc.edu/help<br /> <br /> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp= ; &nbsp; Thank you<br /> &nbsp;</blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --  <div> <div> <div> <div> <div> <div> <div>----------------------------------------------------------------------= -----------------------------------------------<br /> Sergio DeSouza-Machado&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sergio@umbc.e= du<br /> Research Assoc. Professor,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&= nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbs= p;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&= nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (W)=  410-455-1944<br /> JCET/Dept of Physics&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (F) 410-455-1072</div>  <div>UMBC, Baltimore MD 21250</div> </div> </div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3289018,72301608,Correspond,DoIT-Research-Computing,2025-10-15 14:16:24.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_sergio,resolved,Greg Ballantine,gballan1,Sergio De souza-machad,sergio,sergio@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Sergio,<br /> <br /> We have finished migrating your volume to the Ceph cluster! As far as we ca= n tell everything seems to have gone smoothly. There are a few things to no= te:</p>  <ul> 	<li>The path has changed, and is now available under <strong>/umbc/rs/pi_s= ergio</strong>.</li> 	<li>The alias used to reach the volume is now <strong>pi_sergio_common</st= rong> and <strong>pi_sergio_user</strong>.</li> 	<li>Your new volume has a quota of <strong>10TB</strong>.</li> </ul>  <p>When you have a chance, could you try running some jobs on Chip using th= e new volume to verify everything looks good?<br /> <br /> Thank you,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Wed Oct 15 09:53:44 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Good morning Sergio,<br /> <br /> This is a reminder that we will be migrating your group&#39;s research stor= age volume to the Ceph storage cluster today. During this time, please ensu= re there are not any jobs being run in your research group, otherwise these=  may be terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Thu Oct 09 14:14:50 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Hello Sergio,<br /> <br /> Sounds good, I&#39;ve put your group&#39;s migration on our schedule for We= dnesday October 15th. We will send you an email alert via this RT ticket wh= en we begin the migration, and again once it has completed.<br /> <br /> Please let me know if you have any questions or concerns.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Thu Oct 09 13:45:18 2025, VR64161 wrote:</p>  <blockquote> <div>Hello, <div>&nbsp;</div>  <div>The needed migration can be done sometime next week (Oct 15, 16,17); a= ny or some of those would work fine with me.</div>  <div>&nbsp;</div>  <div>Thanks</div>  <div>&nbsp;</div>  <div>Sergio</div> </div> &nbsp;  <div> <div>On Thu, Oct 9, 2025 at 10:12=E2=80=AFAM via RT &lt;UMBCHelp@rt.umbc.ed= u&gt; wrote:</div>  <blockquote>Greetings,<br /> <br /> This message has been automatically generated in response to the<br /> creation of a ticket regarding:<br /> <br /> -------------------------------------------------------------------------<b= r /> Subject: &quot;Migrating Research Storage Volume to Ceph Cluster - pi_sergi= o&quot;<br /> <br /> Message:<br /> <br /> Dear Sergio,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newslett= er<br /> ), DoIT is in the process of migrating data off of an older storage server = to<br /> our new RRStor Ceph storage cluster. Your group is using 0 GB of a 488.3 GB= <br /> quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes off= line<br /> while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule<br /> a date where we can migrate your volume located at &ldquo;/umbc/rs/sergio&r= dquo;. During<br /> the migration, we will take your volume offline and will terminate any jobs= <br /> running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - ple= ase let us<br /> know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hour= s,<br /> which can be done by responding to this email with your preferred date(s) t= o<br /> perform the migration. During this time, DoIT staff will work to migrate yo= ur<br /> volume to the Ceph storage cluster. DoIT staff will send an email alert on = this<br /> email thread when the migration has begun and when it has completed. For mo= st<br /> storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT st= aff will<br /> assign a day over the following month (October 16th through November 15th) = to<br /> migrate your volume. The day chosen will be random and will occur during<br=  /> business hours. You will be notified of the date chosen to perform the<br /> migration, and will be notified when the migration begins and completes.<br=  /> <br /> Note: After this process has completed, the new storage volume will have a = new<br /> name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ld= quo;/umbc/rs/pi_doit&rdquo;,<br /> or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/= rs/pi_sergio&rdquo;.<br /> <br /> Thank you,<br /> Elliot<br /> <br /> <br /> -------------------------------------------------------------------------<b= r /> <br /> There is no need to reply to this message right now.&nbsp;<br /> <br /> Your ticket has been assigned an ID of [Research Computing #3289018] or you=  can go there directly by clicking the link below.<br /> <br /> Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3289018 &gt;<b= r /> <br /> You can login to view your open tickets at any time by visiting http://my.u= mbc.edu and clicking on &quot;Help&quot; and &quot;Request Help&quot;.<br /> <br /> Alternately you can click on http://my.umbc.edu/help<br /> <br /> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp= ; &nbsp; Thank you<br /> &nbsp;</blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --  <div> <div> <div> <div> <div> <div> <div>----------------------------------------------------------------------= -----------------------------------------------<br /> Sergio DeSouza-Machado&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sergio@umbc.e= du<br /> Research Assoc. Professor,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&= nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbs= p;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&= nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (W)=  410-455-1944<br /> JCET/Dept of Physics&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (F) 410-455-1072</div>  <div>UMBC, Baltimore MD 21250</div> </div> </div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3289018,72336234,Correspond,DoIT-Research-Computing,2025-10-16 19:05:22.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_sergio,resolved,Greg Ballantine,gballan1,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"<div dir=3D""ltr"">Hi Greg,<div><br></div><div>Thanks for the message. I&#39;= ll do that next week as I am unwell right now.</div><div><br></div><div>Ser= gio</div><div><br></div></div><br><div class=3D""gmail_quote gmail_quote_con= tainer""><div dir=3D""ltr"" class=3D""gmail_attr"">On Wed, Oct 15, 2025 at 10:16= =E2=80=AFAM Greg Ballantine via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.e= du"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail= _quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204= ,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Tick= et/Display.html?id=3D3289018"" rel=3D""noreferrer"" target=3D""_blank"">https://= rt.umbc.edu/Ticket/Display.html?id=3D3289018</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hello Sergio,<br> <br> We have finished migrating your volume to the Ceph cluster! As far as we ca= n<br> tell everything seems to have gone smoothly. There are a few things to note= :<br> <br> &gt; * The path has changed, and is now available under /umbc/rs/pi_sergio.= <br> <br> &gt; * The alias used to reach the volume is now pi_sergio_common and<br> &gt;&gt; pi_sergio_user.<br> <br> &gt; * Your new volume has a quota of 10TB.<br> <br> When you have a chance, could you try running some jobs on Chip using the n= ew<br> volume to verify everything looks good?<br> <br> Thank you,<br> Greg<br> <br> On Wed Oct 15 09:53:44 2025, OJ87090 wrote:<br> <br> &gt; Good morning Sergio,<br> <br> &gt; This is a reminder that we will be migrating your group&#39;s research=  storage<br> &gt; volume to the Ceph storage cluster today. During this time, please ens= ure<br> &gt; there are not any jobs being run in your research group, otherwise the= se<br> &gt; may be terminated.<br> <br> &gt; We will provide an update once completed.<br> <br> &gt; Best,<br> &gt; Greg<br> <br> &gt; On Thu Oct 09 14:14:50 2025, OJ87090 wrote:<br> <br> &gt;&gt; Hello Sergio,<br> <br> &gt;&gt; Sounds good, I&#39;ve put your group&#39;s migration on our schedu= le for<br> &gt;&gt; Wednesday October 15th. We will send you an email alert via this R= T<br> &gt;&gt; ticket when we begin the migration, and again once it has complete= d.<br> <br> &gt;&gt; Please let me know if you have any questions or concerns.<br> <br> &gt;&gt; Best,<br> &gt;&gt; Greg<br> <br> &gt;&gt; On Thu Oct 09 13:45:18 2025, VR64161 wrote:<br> <br> &gt;&gt;&gt; Hello, The needed migration can be done sometime next week (Oc= t 15,<br> &gt;&gt;&gt; 16,17); any or some of those would work fine with me. Thanks S= ergio<br> &gt;&gt;&gt; On Thu, Oct 9, 2025 at 10:12 AM via RT &lt;<a href=3D""mailto:U= MBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt;<br> &gt;&gt;&gt; wrote:<br> <br> &gt;&gt;&gt;&gt; Greetings,<br> <br> &gt;&gt;&gt;&gt; This message has been automatically generated in response = to<br> &gt;&gt;&gt;&gt; the<br> &gt;&gt;&gt;&gt; creation of a ticket regarding:<br> <br> &gt;&gt;&gt;&gt; ----------------------------------------------------------= ---------------<br> &gt;&gt;&gt;&gt; Subject: &quot;Migrating Research Storage Volume to Ceph C= luster -<br> &gt;&gt;&gt;&gt; pi_sergio&quot;<br> <br> &gt;&gt;&gt;&gt; Message:<br> <br> &gt;&gt;&gt;&gt; Dear Sergio,<br> <br> &gt;&gt;&gt;&gt; As per the communication via myUMBC earlier this summer (J= une<br> &gt;&gt;&gt;&gt; HPCF Newsletter<br> &gt;&gt;&gt;&gt; ), DoIT is in the process of migrating data off of an olde= r<br> &gt;&gt;&gt;&gt; storage server to<br> &gt;&gt;&gt;&gt; our new RRStor Ceph storage cluster. Your group is using 0=  GB<br> &gt;&gt;&gt;&gt; of a 488.3 GB<br> &gt;&gt;&gt;&gt; quota on the old storage server.<br> <br> &gt;&gt;&gt;&gt; To perform these migrations, we need to take individual st= orage<br> &gt;&gt;&gt;&gt; volumes offline<br> &gt;&gt;&gt;&gt; while we migrate them to the Ceph cluster. Thus we are rea= ching<br> &gt;&gt;&gt;&gt; out to schedule<br> &gt;&gt;&gt;&gt; a date where we can migrate your volume located at<br> &gt;&gt;&gt;&gt; =E2=80=9C/umbc/rs/sergio=E2=80=9D. During<br> &gt;&gt;&gt;&gt; the migration, we will take your volume offline and will<b= r> &gt;&gt;&gt;&gt; terminate any jobs<br> &gt;&gt;&gt;&gt; running on the chip compute cluster that are accessing thi= s<br> &gt;&gt;&gt;&gt; volume.<br> <br> &gt;&gt;&gt;&gt; Below we=E2=80=99ve listed two options for handling this d= ata migration<br> &gt;&gt;&gt;&gt; - please let us<br> &gt;&gt;&gt;&gt; know which of these you=E2=80=99d prefer.<br> <br> &gt;&gt;&gt;&gt; Option 1: Schedule a group-wide downtime date during stand= ard<br> &gt;&gt;&gt;&gt; business hours,<br> &gt;&gt;&gt;&gt; which can be done by responding to this email with your<br> &gt;&gt;&gt;&gt; preferred date(s) to<br> &gt;&gt;&gt;&gt; perform the migration. During this time, DoIT staff will w= ork<br> &gt;&gt;&gt;&gt; to migrate your<br> &gt;&gt;&gt;&gt; volume to the Ceph storage cluster. DoIT staff will send a= n<br> &gt;&gt;&gt;&gt; email alert on this<br> &gt;&gt;&gt;&gt; email thread when the migration has begun and when it has<= br> &gt;&gt;&gt;&gt; completed. For most<br> &gt;&gt;&gt;&gt; storage volumes, this process should take less than a busi= ness<br> &gt;&gt;&gt;&gt; day.<br> &gt;&gt;&gt;&gt; Option 2: If you don=E2=80=99t respond to this email by Oc= tober 15th,<br> &gt;&gt;&gt;&gt; DoIT staff will<br> &gt;&gt;&gt;&gt; assign a day over the following month (October 16th throug= h<br> &gt;&gt;&gt;&gt; November 15th) to<br> &gt;&gt;&gt;&gt; migrate your volume. The day chosen will be random and wil= l<br> &gt;&gt;&gt;&gt; occur during<br> &gt;&gt;&gt;&gt; business hours. You will be notified of the date chosen to= <br> &gt;&gt;&gt;&gt; perform the<br> &gt;&gt;&gt;&gt; migration, and will be notified when the migration begins = and<br> &gt;&gt;&gt;&gt; completes.<br> <br> &gt;&gt;&gt;&gt; Note: After this process has completed, the new storage vo= lume<br> &gt;&gt;&gt;&gt; will have a new<br> &gt;&gt;&gt;&gt; name. For example, group =E2=80=9Cpi_doit=E2=80=9D will fi= nd its data under<br> &gt;&gt;&gt;&gt; =E2=80=9C/umbc/rs/pi_doit=E2=80=9D,<br> &gt;&gt;&gt;&gt; or in your group=E2=80=99s case you will find your volume = under<br> &gt;&gt;&gt;&gt; =E2=80=9C/umbc/rs/pi_sergio=E2=80=9D.<br> <br> &gt;&gt;&gt;&gt; Thank you,<br> &gt;&gt;&gt;&gt; Elliot<br> <br> <br> &gt;&gt;&gt;&gt; ----------------------------------------------------------= ---------------<br> <br> &gt;&gt;&gt;&gt; There is no need to reply to this message right now.<br> <br> &gt;&gt;&gt;&gt; Your ticket has been assigned an ID of [Research Computing= <br> &gt;&gt;&gt;&gt; #3289018] or you can go there directly by clicking the lin= k<br> &gt;&gt;&gt;&gt; below.<br> <br> &gt;&gt;&gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Disp= lay.html?id=3D3289018"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc= .edu/Ticket/Display.html?id=3D3289018</a><br> &gt;&gt;&gt;&gt; &gt;<br> <br> &gt;&gt;&gt;&gt; You can login to view your open tickets at any time by vis= iting<br> &gt;&gt;&gt;&gt; <a href=3D""http://my.umbc.edu"" rel=3D""noreferrer"" target= =3D""_blank"">http://my.umbc.edu</a> and clicking on &quot;Help&quot; and &qu= ot;Request Help&quot;.<br> <br> &gt;&gt;&gt;&gt; Alternately you can click on <a href=3D""http://my.umbc.edu= /help"" rel=3D""noreferrer"" target=3D""_blank"">http://my.umbc.edu/help</a><br> <br> &gt;&gt;&gt;&gt; Thank you<br> <br> &gt;&gt;&gt; --<br> &gt;&gt;&gt; --------------------------------------------------------------= -------------------------------------------------------<br> &gt;&gt;&gt; Sergio DeSouza-Machado <a href=3D""mailto:sergio@umbc.edu"" targ= et=3D""_blank"">sergio@umbc.edu</a><br> &gt;&gt;&gt; Research Assoc. Professor, (W) 410-455-1944<br> &gt;&gt;&gt; JCET/Dept of Physics (F) 410-455-1072UMBC, Baltimore MD 21250<= br> <br> &gt;&gt; --<br> <br> &gt;&gt; Gregory BallantineSystem Administrator for Research and Enterprise= <br> &gt;&gt; ComputingUMBC - DoIT<br> <br> &gt; --<br> <br> &gt; Gregory BallantineSystem Administrator for Research and Enterprise<br> &gt; ComputingUMBC - DoIT<br> <br> --<br> <br> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br> - DoIT<br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><div><div dir=3D""ltr""><div><div dir=3D""ltr""><div= >--------------------------------------------------------------------------= -------------------------------------------<br>Sergio DeSouza-Machado=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 <a href=3D""mailto:sergio@umbc.edu"" target= =3D""_blank"">sergio@umbc.edu</a><br>Research Assoc. Professor,=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 (W) 410-455-1944<br>JCET/Dept of Physics= =C2=A0=C2=A0 =C2=A0=C2=A0 =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0 (F) 410-455-1072<br></div><div>UMBC, Baltimore MD 21250<br>= </div></div></div></div></div></div></div> "
3289018,72336234,Correspond,DoIT-Research-Computing,2025-10-16 19:05:22.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_sergio,resolved,Greg Ballantine,gballan1,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"Hi Greg,  Thanks for the message. I'll do that next week as I am unwell right now.  Sergio   On Wed, Oct 15, 2025 at 10:16=E2=80=AFAM Greg Ballantine via RT < UMBCHelp@rt.umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3289018 > > > Last Update From Ticket: > > Hello Sergio, > > We have finished migrating your volume to the Ceph cluster! As far as we > can > tell everything seems to have gone smoothly. There are a few things to > note: > > > * The path has changed, and is now available under /umbc/rs/pi_sergio. > > > * The alias used to reach the volume is now pi_sergio_common and > >> pi_sergio_user. > > > * Your new volume has a quota of 10TB. > > When you have a chance, could you try running some jobs on Chip using the > new > volume to verify everything looks good? > > Thank you, > Greg > > On Wed Oct 15 09:53:44 2025, OJ87090 wrote: > > > Good morning Sergio, > > > This is a reminder that we will be migrating your group's research > storage > > volume to the Ceph storage cluster today. During this time, please ensu= re > > there are not any jobs being run in your research group, otherwise these > > may be terminated. > > > We will provide an update once completed. > > > Best, > > Greg > > > On Thu Oct 09 14:14:50 2025, OJ87090 wrote: > > >> Hello Sergio, > > >> Sounds good, I've put your group's migration on our schedule for > >> Wednesday October 15th. We will send you an email alert via this RT > >> ticket when we begin the migration, and again once it has completed. > > >> Please let me know if you have any questions or concerns. > > >> Best, > >> Greg > > >> On Thu Oct 09 13:45:18 2025, VR64161 wrote: > > >>> Hello, The needed migration can be done sometime next week (Oct 15, > >>> 16,17); any or some of those would work fine with me. Thanks Sergio > >>> On Thu, Oct 9, 2025 at 10:12 AM via RT <UMBCHelp@rt.umbc.edu> > >>> wrote: > > >>>> Greetings, > > >>>> This message has been automatically generated in response to > >>>> the > >>>> creation of a ticket regarding: > > >>>> > ------------------------------------------------------------------------- > >>>> Subject: ""Migrating Research Storage Volume to Ceph Cluster - > >>>> pi_sergio"" > > >>>> Message: > > >>>> Dear Sergio, > > >>>> As per the communication via myUMBC earlier this summer (June > >>>> HPCF Newsletter > >>>> ), DoIT is in the process of migrating data off of an older > >>>> storage server to > >>>> our new RRStor Ceph storage cluster. Your group is using 0 GB > >>>> of a 488.3 GB > >>>> quota on the old storage server. > > >>>> To perform these migrations, we need to take individual storage > >>>> volumes offline > >>>> while we migrate them to the Ceph cluster. Thus we are reaching > >>>> out to schedule > >>>> a date where we can migrate your volume located at > >>>> =E2=80=9C/umbc/rs/sergio=E2=80=9D. During > >>>> the migration, we will take your volume offline and will > >>>> terminate any jobs > >>>> running on the chip compute cluster that are accessing this > >>>> volume. > > >>>> Below we=E2=80=99ve listed two options for handling this data migrat= ion > >>>> - please let us > >>>> know which of these you=E2=80=99d prefer. > > >>>> Option 1: Schedule a group-wide downtime date during standard > >>>> business hours, > >>>> which can be done by responding to this email with your > >>>> preferred date(s) to > >>>> perform the migration. During this time, DoIT staff will work > >>>> to migrate your > >>>> volume to the Ceph storage cluster. DoIT staff will send an > >>>> email alert on this > >>>> email thread when the migration has begun and when it has > >>>> completed. For most > >>>> storage volumes, this process should take less than a business > >>>> day. > >>>> Option 2: If you don=E2=80=99t respond to this email by October 15th, > >>>> DoIT staff will > >>>> assign a day over the following month (October 16th through > >>>> November 15th) to > >>>> migrate your volume. The day chosen will be random and will > >>>> occur during > >>>> business hours. You will be notified of the date chosen to > >>>> perform the > >>>> migration, and will be notified when the migration begins and > >>>> completes. > > >>>> Note: After this process has completed, the new storage volume > >>>> will have a new > >>>> name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its dat= a under > >>>> =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, > >>>> or in your group=E2=80=99s case you will find your volume under > >>>> =E2=80=9C/umbc/rs/pi_sergio=E2=80=9D. > > >>>> Thank you, > >>>> Elliot > > > >>>> > ------------------------------------------------------------------------- > > >>>> There is no need to reply to this message right now. > > >>>> Your ticket has been assigned an ID of [Research Computing > >>>> #3289018] or you can go there directly by clicking the link > >>>> below. > > >>>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3289018 > >>>> > > > >>>> You can login to view your open tickets at any time by visiting > >>>> http://my.umbc.edu and clicking on ""Help"" and ""Request Help"". > > >>>> Alternately you can click on http://my.umbc.edu/help > > >>>> Thank you > > >>> -- > >>> > -------------------------------------------------------------------------= -------------------------------------------- > >>> Sergio DeSouza-Machado sergio@umbc.edu > >>> Research Assoc. Professor, (W) 410-455-1944 > >>> JCET/Dept of Physics (F) 410-455-1072UMBC, Baltimore MD 21250 > > >> -- > > >> Gregory BallantineSystem Administrator for Research and Enterprise > >> ComputingUMBC - DoIT > > > -- > > > Gregory BallantineSystem Administrator for Research and Enterprise > > ComputingUMBC - DoIT > > -- > > Gregory BallantineSystem Administrator for Research and Enterprise > ComputingUMBC > - DoIT > >  --=20 ---------------------------------------------------------------------------= ------------------------------------------ Sergio DeSouza-Machado sergio@umbc.edu Research Assoc. Professor,                                              (W) 410-455-1944 JCET/Dept of Physics (F) 410-455-1072 UMBC, Baltimore MD 21250 "
3289018,72336371,Correspond,DoIT-Research-Computing,2025-10-16 19:09:44.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_sergio,resolved,Greg Ballantine,gballan1,Sergio De souza-machad,sergio,sergio@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Sergio,<br /> <br /> That&#39;s okay, take your time with checking out your volume. I hope you g= et well soon!<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Thu Oct 16 15:05:22 2025, VR64161 wrote:</p>  <blockquote> <div>Hi Greg, <div>&nbsp;</div>  <div>Thanks for the message. I&#39;ll do that next week as I am unwell righ= t now.</div>  <div>&nbsp;</div>  <div>Sergio</div>  <div>&nbsp;</div> </div> &nbsp;  <div> <div>On Wed, Oct 15, 2025 at 10:16=E2=80=AFAM Greg Ballantine via RT &lt;UM= BCHelp@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 89018 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hello Sergio,<br /> <br /> We have finished migrating your volume to the Ceph cluster! As far as we ca= n<br /> tell everything seems to have gone smoothly. There are a few things to note= :<br /> <br /> &gt; * The path has changed, and is now available under /umbc/rs/pi_sergio.= <br /> <br /> &gt; * The alias used to reach the volume is now pi_sergio_common and<br /> &gt;&gt; pi_sergio_user.<br /> <br /> &gt; * Your new volume has a quota of 10TB.<br /> <br /> When you have a chance, could you try running some jobs on Chip using the n= ew<br /> volume to verify everything looks good?<br /> <br /> Thank you,<br /> Greg<br /> <br /> On Wed Oct 15 09:53:44 2025, OJ87090 wrote:<br /> <br /> &gt; Good morning Sergio,<br /> <br /> &gt; This is a reminder that we will be migrating your group&#39;s research=  storage<br /> &gt; volume to the Ceph storage cluster today. During this time, please ens= ure<br /> &gt; there are not any jobs being run in your research group, otherwise the= se<br /> &gt; may be terminated.<br /> <br /> &gt; We will provide an update once completed.<br /> <br /> &gt; Best,<br /> &gt; Greg<br /> <br /> &gt; On Thu Oct 09 14:14:50 2025, OJ87090 wrote:<br /> <br /> &gt;&gt; Hello Sergio,<br /> <br /> &gt;&gt; Sounds good, I&#39;ve put your group&#39;s migration on our schedu= le for<br /> &gt;&gt; Wednesday October 15th. We will send you an email alert via this R= T<br /> &gt;&gt; ticket when we begin the migration, and again once it has complete= d.<br /> <br /> &gt;&gt; Please let me know if you have any questions or concerns.<br /> <br /> &gt;&gt; Best,<br /> &gt;&gt; Greg<br /> <br /> &gt;&gt; On Thu Oct 09 13:45:18 2025, VR64161 wrote:<br /> <br /> &gt;&gt;&gt; Hello, The needed migration can be done sometime next week (Oc= t 15,<br /> &gt;&gt;&gt; 16,17); any or some of those would work fine with me. Thanks S= ergio<br /> &gt;&gt;&gt; On Thu, Oct 9, 2025 at 10:12 AM via RT &lt;UMBCHelp@rt.umbc.ed= u&gt;<br /> &gt;&gt;&gt; wrote:<br /> <br /> &gt;&gt;&gt;&gt; Greetings,<br /> <br /> &gt;&gt;&gt;&gt; This message has been automatically generated in response = to<br /> &gt;&gt;&gt;&gt; the<br /> &gt;&gt;&gt;&gt; creation of a ticket regarding:<br /> <br /> &gt;&gt;&gt;&gt; ----------------------------------------------------------= ---------------<br /> &gt;&gt;&gt;&gt; Subject: &quot;Migrating Research Storage Volume to Ceph C= luster -<br /> &gt;&gt;&gt;&gt; pi_sergio&quot;<br /> <br /> &gt;&gt;&gt;&gt; Message:<br /> <br /> &gt;&gt;&gt;&gt; Dear Sergio,<br /> <br /> &gt;&gt;&gt;&gt; As per the communication via myUMBC earlier this summer (J= une<br /> &gt;&gt;&gt;&gt; HPCF Newsletter<br /> &gt;&gt;&gt;&gt; ), DoIT is in the process of migrating data off of an olde= r<br /> &gt;&gt;&gt;&gt; storage server to<br /> &gt;&gt;&gt;&gt; our new RRStor Ceph storage cluster. Your group is using 0=  GB<br /> &gt;&gt;&gt;&gt; of a 488.3 GB<br /> &gt;&gt;&gt;&gt; quota on the old storage server.<br /> <br /> &gt;&gt;&gt;&gt; To perform these migrations, we need to take individual st= orage<br /> &gt;&gt;&gt;&gt; volumes offline<br /> &gt;&gt;&gt;&gt; while we migrate them to the Ceph cluster. Thus we are rea= ching<br /> &gt;&gt;&gt;&gt; out to schedule<br /> &gt;&gt;&gt;&gt; a date where we can migrate your volume located at<br /> &gt;&gt;&gt;&gt; &ldquo;/umbc/rs/sergio&rdquo;. During<br /> &gt;&gt;&gt;&gt; the migration, we will take your volume offline and will<b= r /> &gt;&gt;&gt;&gt; terminate any jobs<br /> &gt;&gt;&gt;&gt; running on the chip compute cluster that are accessing thi= s<br /> &gt;&gt;&gt;&gt; volume.<br /> <br /> &gt;&gt;&gt;&gt; Below we&rsquo;ve listed two options for handling this dat= a migration<br /> &gt;&gt;&gt;&gt; - please let us<br /> &gt;&gt;&gt;&gt; know which of these you&rsquo;d prefer.<br /> <br /> &gt;&gt;&gt;&gt; Option 1: Schedule a group-wide downtime date during stand= ard<br /> &gt;&gt;&gt;&gt; business hours,<br /> &gt;&gt;&gt;&gt; which can be done by responding to this email with your<br=  /> &gt;&gt;&gt;&gt; preferred date(s) to<br /> &gt;&gt;&gt;&gt; perform the migration. During this time, DoIT staff will w= ork<br /> &gt;&gt;&gt;&gt; to migrate your<br /> &gt;&gt;&gt;&gt; volume to the Ceph storage cluster. DoIT staff will send a= n<br /> &gt;&gt;&gt;&gt; email alert on this<br /> &gt;&gt;&gt;&gt; email thread when the migration has begun and when it has<= br /> &gt;&gt;&gt;&gt; completed. For most<br /> &gt;&gt;&gt;&gt; storage volumes, this process should take less than a busi= ness<br /> &gt;&gt;&gt;&gt; day.<br /> &gt;&gt;&gt;&gt; Option 2: If you don&rsquo;t respond to this email by Octo= ber 15th,<br /> &gt;&gt;&gt;&gt; DoIT staff will<br /> &gt;&gt;&gt;&gt; assign a day over the following month (October 16th throug= h<br /> &gt;&gt;&gt;&gt; November 15th) to<br /> &gt;&gt;&gt;&gt; migrate your volume. The day chosen will be random and wil= l<br /> &gt;&gt;&gt;&gt; occur during<br /> &gt;&gt;&gt;&gt; business hours. You will be notified of the date chosen to= <br /> &gt;&gt;&gt;&gt; perform the<br /> &gt;&gt;&gt;&gt; migration, and will be notified when the migration begins = and<br /> &gt;&gt;&gt;&gt; completes.<br /> <br /> &gt;&gt;&gt;&gt; Note: After this process has completed, the new storage vo= lume<br /> &gt;&gt;&gt;&gt; will have a new<br /> &gt;&gt;&gt;&gt; name. For example, group &ldquo;pi_doit&rdquo; will find i= ts data under<br /> &gt;&gt;&gt;&gt; &ldquo;/umbc/rs/pi_doit&rdquo;,<br /> &gt;&gt;&gt;&gt; or in your group&rsquo;s case you will find your volume un= der<br /> &gt;&gt;&gt;&gt; &ldquo;/umbc/rs/pi_sergio&rdquo;.<br /> <br /> &gt;&gt;&gt;&gt; Thank you,<br /> &gt;&gt;&gt;&gt; Elliot<br /> <br /> <br /> &gt;&gt;&gt;&gt; ----------------------------------------------------------= ---------------<br /> <br /> &gt;&gt;&gt;&gt; There is no need to reply to this message right now.<br /> <br /> &gt;&gt;&gt;&gt; Your ticket has been assigned an ID of [Research Computing= <br /> &gt;&gt;&gt;&gt; #3289018] or you can go there directly by clicking the lin= k<br /> &gt;&gt;&gt;&gt; below.<br /> <br /> &gt;&gt;&gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id= =3D3289018<br /> &gt;&gt;&gt;&gt; &gt;<br /> <br /> &gt;&gt;&gt;&gt; You can login to view your open tickets at any time by vis= iting<br /> &gt;&gt;&gt;&gt; http://my.umbc.edu and clicking on &quot;Help&quot; and &q= uot;Request Help&quot;.<br /> <br /> &gt;&gt;&gt;&gt; Alternately you can click on http://my.umbc.edu/help<br /> <br /> &gt;&gt;&gt;&gt; Thank you<br /> <br /> &gt;&gt;&gt; --<br /> &gt;&gt;&gt; --------------------------------------------------------------= -------------------------------------------------------<br /> &gt;&gt;&gt; Sergio DeSouza-Machado sergio@umbc.edu<br /> &gt;&gt;&gt; Research Assoc. Professor, (W) 410-455-1944<br /> &gt;&gt;&gt; JCET/Dept of Physics (F) 410-455-1072UMBC, Baltimore MD 21250<= br /> <br /> &gt;&gt; --<br /> <br /> &gt;&gt; Gregory BallantineSystem Administrator for Research and Enterprise= <br /> &gt;&gt; ComputingUMBC - DoIT<br /> <br /> &gt; --<br /> <br /> &gt; Gregory BallantineSystem Administrator for Research and Enterprise<br = /> &gt; ComputingUMBC - DoIT<br /> <br /> --<br /> <br /> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br /> - DoIT<br /> &nbsp;</blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --  <div> <div> <div> <div> <div> <div> <div>----------------------------------------------------------------------= -----------------------------------------------<br /> Sergio DeSouza-Machado&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sergio@umbc.e= du<br /> Research Assoc. Professor,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&= nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbs= p;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&= nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (W)=  410-455-1944<br /> JCET/Dept of Physics&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (F) 410-455-1072</div>  <div>UMBC, Baltimore MD 21250</div> </div> </div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3289025,72183448,Create,DoIT-Research-Computing,2025-10-09 14:15:07.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zmclaren,resolved,Greg Ballantine,gballan1,Zoe McLaren,zmclaren,zmclaren@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear Zoe,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 502.9 GB of a 488.3 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/zmclaren&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_zmclaren&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3289025,72185978,Correspond,DoIT-Research-Computing,2025-10-09 15:31:27.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zmclaren,resolved,Greg Ballantine,gballan1,Zoe McLaren,zmclaren,zmclaren@umbc.edu,Zoe McLaren,zmclaren@umbc.edu,"October 28 would be great. Or Oct 29 if that isn't possible. Thanks!  On Thu, Oct 9, 2025 at 10:15=E2=80=AFAM via RT <UMBCHelp@rt.umbc.edu> wrote:  > Greetings, > > This message has been automatically generated in response to the > creation of a ticket regarding: > > ------------------------------------------------------------------------- > Subject: ""Migrating Research Storage Volume to Ceph Cluster - pi_zmclaren"" > > Message: > > Dear Zoe, > > As per the communication via myUMBC earlier this summer (June HPCF > Newsletter > ), DoIT is in the process of migrating data off of an older storage server > to > our new RRStor Ceph storage cluster. Your group is using 502.9 GB of a > 488.3 GB > quota on the old storage server. > > To perform these migrations, we need to take individual storage volumes > offline > while we migrate them to the Ceph cluster. Thus we are reaching out to > schedule > a date where we can migrate your volume located at =E2=80=9C/umbc/rs/zmcl= aren=E2=80=9D. > During > the migration, we will take your volume offline and will terminate any jo= bs > running on the chip compute cluster that are accessing this volume. > > Below we=E2=80=99ve listed two options for handling this data migration -=  please > let us > know which of these you=E2=80=99d prefer. > > Option 1: Schedule a group-wide downtime date during standard business > hours, > which can be done by responding to this email with your preferred date(s) > to > perform the migration. During this time, DoIT staff will work to migrate > your > volume to the Ceph storage cluster. DoIT staff will send an email alert on > this > email thread when the migration has begun and when it has completed. For > most > storage volumes, this process should take less than a business day. > Option 2: If you don=E2=80=99t respond to this email by October 15th, DoI= T staff > will > assign a day over the following month (October 16th through November 15th) > to > migrate your volume. The day chosen will be random and will occur during > business hours. You will be notified of the date chosen to perform the > migration, and will be notified when the migration begins and completes. > > Note: After this process has completed, the new storage volume will have a > new > name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its data und= er > =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, > or in your group=E2=80=99s case you will find your volume under > =E2=80=9C/umbc/rs/pi_zmclaren=E2=80=9D. > > Thank you, > Elliot > > > ------------------------------------------------------------------------- > > There is no need to reply to this message right now. > > Your ticket has been assigned an ID of [Research Computing #3289025] or > you can go there directly by clicking the link below. > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3289025 > > > You can login to view your open tickets at any time by visiting > http://my.umbc.edu and clicking on ""Help"" and ""Request Help"". > > Alternately you can click on http://my.umbc.edu/help > >                         Thank you > >  --=20 Zo=C3=AB M. McLaren Associate Professor School of Public Policy University of Maryland Baltimore County Baltimore, MD CV and paper downloads: http://zoemclaren.com Faculty website: https://publicpolicy.umbc.edu/zoe-m-mclaren/ Twitter: https://twitter.com/zoemclaren "
3289025,72185978,Correspond,DoIT-Research-Computing,2025-10-09 15:31:27.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zmclaren,resolved,Greg Ballantine,gballan1,Zoe McLaren,zmclaren,zmclaren@umbc.edu,Zoe McLaren,zmclaren@umbc.edu,"<div dir=3D""ltr""><div class=3D""gmail_default"" style=3D""font-family:tahoma,s= ans-serif"">October 28 would be great. Or Oct 29 if that isn&#39;t possible.=  Thanks!</div></div><br><div class=3D""gmail_quote gmail_quote_container""><d= iv dir=3D""ltr"" class=3D""gmail_attr"">On Thu, Oct 9, 2025 at 10:15=E2=80=AFAM=  via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a= >&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""margin:0px=  0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"">Gre= etings,<br> <br> This message has been automatically generated in response to the<br> creation of a ticket regarding:<br> <br> -------------------------------------------------------------------------<b= r> Subject: &quot;Migrating Research Storage Volume to Ceph Cluster - pi_zmcla= ren&quot;<br> <br> Message: <br> <br> Dear Zoe,<br> <br> As per the communication via myUMBC earlier this summer (June HPCF Newslett= er<br> ), DoIT is in the process of migrating data off of an older storage server = to<br> our new RRStor Ceph storage cluster. Your group is using 502.9 GB of a 488.= 3 GB<br> quota on the old storage server.<br> <br> To perform these migrations, we need to take individual storage volumes off= line<br> while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule<br> a date where we can migrate your volume located at =E2=80=9C/umbc/rs/zmclar= en=E2=80=9D. During<br> the migration, we will take your volume offline and will terminate any jobs= <br> running on the chip compute cluster that are accessing this volume.<br> <br> Below we=E2=80=99ve listed two options for handling this data migration - p= lease let us<br> know which of these you=E2=80=99d prefer.<br> <br> Option 1: Schedule a group-wide downtime date during standard business hour= s,<br> which can be done by responding to this email with your preferred date(s) t= o<br> perform the migration. During this time, DoIT staff will work to migrate yo= ur<br> volume to the Ceph storage cluster. DoIT staff will send an email alert on = this<br> email thread when the migration has begun and when it has completed. For mo= st<br> storage volumes, this process should take less than a business day.<br> Option 2: If you don=E2=80=99t respond to this email by October 15th, DoIT = staff will<br> assign a day over the following month (October 16th through November 15th) = to<br> migrate your volume. The day chosen will be random and will occur during<br> business hours. You will be notified of the date chosen to perform the<br> migration, and will be notified when the migration begins and completes.<br> <br> Note: After this process has completed, the new storage volume will have a = new<br> name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its data under=  =E2=80=9C/umbc/rs/pi_doit=E2=80=9D,<br> or in your group=E2=80=99s case you will find your volume under =E2=80=9C/u= mbc/rs/pi_zmclaren=E2=80=9D.<br> <br> Thank you,<br> Elliot<br> <br> <br> -------------------------------------------------------------------------<b= r> <br> There is no need to reply to this message right now.=C2=A0 <br> <br> Your ticket has been assigned an ID of [Research Computing #3289025] or you=  can go there directly by clicking the link below.<br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D328= 9025"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3289025</a> &gt;<br> <br> You can login to view your open tickets at any time by visiting <a href=3D""= http://my.umbc.edu"" rel=3D""noreferrer"" target=3D""_blank"">http://my.umbc.edu= </a> and clicking on &quot;Help&quot; and &quot;Request Help&quot;. <br> <br> Alternately you can click on <a href=3D""http://my.umbc.edu/help"" rel=3D""nor= eferrer"" target=3D""_blank"">http://my.umbc.edu/help</a><br> <br> =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 Thank you<br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr"">Zo=C3=AB M. McLaren<br>Associate Professor<br>Sc= hool of Public Policy<br>University of Maryland Baltimore County<br>Baltimo= re, MD<br>CV and paper downloads: <a href=3D""http://zoemclaren.com"" target= =3D""_blank"">http://zoemclaren.com</a><br>Faculty website: <a href=3D""https:= //publicpolicy.umbc.edu/zoe-m-mclaren/"" target=3D""_blank"">https://publicpol= icy.umbc.edu/zoe-m-mclaren/</a><div>Twitter:=C2=A0<a href=3D""https://twitte= r.com/zoemclaren"" target=3D""_blank"">https://twitter.com/zoemclaren</a></div= ></div></div> "
3289025,72186951,Correspond,DoIT-Research-Computing,2025-10-09 15:54:13.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zmclaren,resolved,Greg Ballantine,gballan1,Zoe McLaren,zmclaren,zmclaren@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Zoe,<br /> <br /> Sounds good, I&#39;ve put your group&#39;s migration on our schedule for Oc= tober 28th. We will send you an email alert via this RT ticket when we begi= n the migration, and again once it has completed.<br /> <br /> Please let me know if you have any questions or concerns.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Thu Oct 09 11:31:27 2025, OE75135 wrote:</p>  <blockquote> <div> <div>October 28 would be great. Or Oct 29 if that isn&#39;t possible. Thank= s!</div> </div> &nbsp;  <div> <div>On Thu, Oct 9, 2025 at 10:15=E2=80=AFAM via RT &lt;UMBCHelp@rt.umbc.ed= u&gt; wrote:</div>  <blockquote>Greetings,<br /> <br /> This message has been automatically generated in response to the<br /> creation of a ticket regarding:<br /> <br /> -------------------------------------------------------------------------<b= r /> Subject: &quot;Migrating Research Storage Volume to Ceph Cluster - pi_zmcla= ren&quot;<br /> <br /> Message:<br /> <br /> Dear Zoe,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newslett= er<br /> ), DoIT is in the process of migrating data off of an older storage server = to<br /> our new RRStor Ceph storage cluster. Your group is using 502.9 GB of a 488.= 3 GB<br /> quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes off= line<br /> while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule<br /> a date where we can migrate your volume located at &ldquo;/umbc/rs/zmclaren= &rdquo;. During<br /> the migration, we will take your volume offline and will terminate any jobs= <br /> running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - ple= ase let us<br /> know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hour= s,<br /> which can be done by responding to this email with your preferred date(s) t= o<br /> perform the migration. During this time, DoIT staff will work to migrate yo= ur<br /> volume to the Ceph storage cluster. DoIT staff will send an email alert on = this<br /> email thread when the migration has begun and when it has completed. For mo= st<br /> storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT st= aff will<br /> assign a day over the following month (October 16th through November 15th) = to<br /> migrate your volume. The day chosen will be random and will occur during<br=  /> business hours. You will be notified of the date chosen to perform the<br /> migration, and will be notified when the migration begins and completes.<br=  /> <br /> Note: After this process has completed, the new storage volume will have a = new<br /> name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ld= quo;/umbc/rs/pi_doit&rdquo;,<br /> or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/= rs/pi_zmclaren&rdquo;.<br /> <br /> Thank you,<br /> Elliot<br /> <br /> <br /> -------------------------------------------------------------------------<b= r /> <br /> There is no need to reply to this message right now.&nbsp;<br /> <br /> Your ticket has been assigned an ID of [Research Computing #3289025] or you=  can go there directly by clicking the link below.<br /> <br /> Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3289025 &gt;<b= r /> <br /> You can login to view your open tickets at any time by visiting http://my.u= mbc.edu and clicking on &quot;Help&quot; and &quot;Request Help&quot;.<br /> <br /> Alternately you can click on http://my.umbc.edu/help<br /> <br /> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp= ; &nbsp; Thank you<br /> &nbsp;</blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --  <div> <div>Zo&euml; M. McLaren<br /> Associate Professor<br /> School of Public Policy<br /> University of Maryland Baltimore County<br /> Baltimore, MD<br /> CV and paper downloads: http://zoemclaren.com<br /> Faculty website: https://publicpolicy.umbc.edu/zoe-m-mclaren/ <div>Twitter:&nbsp;https://twitter.com/zoemclaren</div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3289025,72207440,Comment,DoIT-Research-Computing,2025-10-10 15:30:03.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zmclaren,resolved,Greg Ballantine,gballan1,Zoe McLaren,zmclaren,zmclaren@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Marked as &quot;scheduled&quot; in the spreadsheet</p> "
3289025,72207441,CommentEmailRecord,DoIT-Research-Computing,2025-10-10 15:30:05.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zmclaren,resolved,Greg Ballantine,gballan1,Zoe McLaren,zmclaren,zmclaren@umbc.edu,The RT System itself,NULL,"Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3289025 >  Comment just added.    Marked as ""scheduled"" in the spreadsheet  "
3289025,72527869,Correspond,DoIT-Research-Computing,2025-10-28 14:49:40.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zmclaren,resolved,Greg Ballantine,gballan1,Zoe McLaren,zmclaren,zmclaren@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Zoe,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to=  the Ceph storage cluster today.&nbsp;During this time, please ensure there=  are not any jobs being run in your research group, otherwise these may be = terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Thu Oct 09 11:54:13 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Hello Zoe,<br /> <br /> Sounds good, I&#39;ve put your group&#39;s migration on our schedule for Oc= tober 28th. We will send you an email alert via this RT ticket when we begi= n the migration, and again once it has completed.<br /> <br /> Please let me know if you have any questions or concerns.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Thu Oct 09 11:31:27 2025, OE75135 wrote:</p>  <blockquote> <div> <div>October 28 would be great. Or Oct 29 if that isn&#39;t possible. Thank= s!</div> </div> &nbsp;  <div> <div>On Thu, Oct 9, 2025 at 10:15=E2=80=AFAM via RT &lt;UMBCHelp@rt.umbc.ed= u&gt; wrote:</div>  <blockquote>Greetings,<br /> <br /> This message has been automatically generated in response to the<br /> creation of a ticket regarding:<br /> <br /> -------------------------------------------------------------------------<b= r /> Subject: &quot;Migrating Research Storage Volume to Ceph Cluster - pi_zmcla= ren&quot;<br /> <br /> Message:<br /> <br /> Dear Zoe,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newslett= er<br /> ), DoIT is in the process of migrating data off of an older storage server = to<br /> our new RRStor Ceph storage cluster. Your group is using 502.9 GB of a 488.= 3 GB<br /> quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes off= line<br /> while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule<br /> a date where we can migrate your volume located at &ldquo;/umbc/rs/zmclaren= &rdquo;. During<br /> the migration, we will take your volume offline and will terminate any jobs= <br /> running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - ple= ase let us<br /> know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hour= s,<br /> which can be done by responding to this email with your preferred date(s) t= o<br /> perform the migration. During this time, DoIT staff will work to migrate yo= ur<br /> volume to the Ceph storage cluster. DoIT staff will send an email alert on = this<br /> email thread when the migration has begun and when it has completed. For mo= st<br /> storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT st= aff will<br /> assign a day over the following month (October 16th through November 15th) = to<br /> migrate your volume. The day chosen will be random and will occur during<br=  /> business hours. You will be notified of the date chosen to perform the<br /> migration, and will be notified when the migration begins and completes.<br=  /> <br /> Note: After this process has completed, the new storage volume will have a = new<br /> name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ld= quo;/umbc/rs/pi_doit&rdquo;,<br /> or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/= rs/pi_zmclaren&rdquo;.<br /> <br /> Thank you,<br /> Elliot<br /> <br /> <br /> -------------------------------------------------------------------------<b= r /> <br /> There is no need to reply to this message right now.&nbsp;<br /> <br /> Your ticket has been assigned an ID of [Research Computing #3289025] or you=  can go there directly by clicking the link below.<br /> <br /> Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3289025 &gt;<b= r /> <br /> You can login to view your open tickets at any time by visiting http://my.u= mbc.edu and clicking on &quot;Help&quot; and &quot;Request Help&quot;.<br /> <br /> Alternately you can click on http://my.umbc.edu/help<br /> <br /> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp= ; &nbsp; Thank you<br /> &nbsp;</blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --  <div> <div>Zo&euml; M. McLaren<br /> Associate Professor<br /> School of Public Policy<br /> University of Maryland Baltimore County<br /> Baltimore, MD<br /> CV and paper downloads: http://zoemclaren.com<br /> Faculty website: https://publicpolicy.umbc.edu/zoe-m-mclaren/ <div>Twitter:&nbsp;https://twitter.com/zoemclaren</div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3289025,72539097,Correspond,DoIT-Research-Computing,2025-10-28 19:37:01.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zmclaren,resolved,Greg Ballantine,gballan1,Zoe McLaren,zmclaren,zmclaren@umbc.edu,Greg Ballantine,gballan1@umbc.edu,"<div> <p>Hello Zoe,<br /> <br /> We have finished migrating your volume to the Ceph cluster! As far as we ca= n tell everything seems to have gone smoothly. There are a few things to no= te:</p>  <ul> 	<li>The path has changed, and is now available under <strong>/umbc/rs/pi_z= mclaren</strong>.</li> 	<li>The alias used to reach the volume is now <strong>pi_zmclaren_common</= strong> and <strong>pi_zmclaren_user</strong>.</li> 	<li>Your new volume has a quota of <strong>10TB</strong>.</li> </ul>  <p>When you have a chance, could you try running some jobs on Chip using th= e new volume to verify everything looks good?<br /> <br /> Thank you,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Tue Oct 28 10:49:40 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Hello Zoe,<br /> <br /> This is a reminder that we will be performing your group&#39;s migration to=  the Ceph storage cluster today.&nbsp;During this time, please ensure there=  are not any jobs being run in your research group, otherwise these may be = terminated.<br /> <br /> We will provide an update once completed.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Thu Oct 09 11:54:13 2025, OJ87090 wrote:</p>  <blockquote> <div> <p>Hello Zoe,<br /> <br /> Sounds good, I&#39;ve put your group&#39;s migration on our schedule for Oc= tober 28th. We will send you an email alert via this RT ticket when we begi= n the migration, and again once it has completed.<br /> <br /> Please let me know if you have any questions or concerns.<br /> <br /> Best,<br /> Greg</p>  <p>&nbsp;</p>  <p>On Thu Oct 09 11:31:27 2025, OE75135 wrote:</p>  <blockquote> <div> <div>October 28 would be great. Or Oct 29 if that isn&#39;t possible. Thank= s!</div> </div> &nbsp;  <div> <div>On Thu, Oct 9, 2025 at 10:15=E2=80=AFAM via RT &lt;UMBCHelp@rt.umbc.ed= u&gt; wrote:</div>  <blockquote>Greetings,<br /> <br /> This message has been automatically generated in response to the<br /> creation of a ticket regarding:<br /> <br /> -------------------------------------------------------------------------<b= r /> Subject: &quot;Migrating Research Storage Volume to Ceph Cluster - pi_zmcla= ren&quot;<br /> <br /> Message:<br /> <br /> Dear Zoe,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newslett= er<br /> ), DoIT is in the process of migrating data off of an older storage server = to<br /> our new RRStor Ceph storage cluster. Your group is using 502.9 GB of a 488.= 3 GB<br /> quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes off= line<br /> while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule<br /> a date where we can migrate your volume located at &ldquo;/umbc/rs/zmclaren= &rdquo;. During<br /> the migration, we will take your volume offline and will terminate any jobs= <br /> running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - ple= ase let us<br /> know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hour= s,<br /> which can be done by responding to this email with your preferred date(s) t= o<br /> perform the migration. During this time, DoIT staff will work to migrate yo= ur<br /> volume to the Ceph storage cluster. DoIT staff will send an email alert on = this<br /> email thread when the migration has begun and when it has completed. For mo= st<br /> storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT st= aff will<br /> assign a day over the following month (October 16th through November 15th) = to<br /> migrate your volume. The day chosen will be random and will occur during<br=  /> business hours. You will be notified of the date chosen to perform the<br /> migration, and will be notified when the migration begins and completes.<br=  /> <br /> Note: After this process has completed, the new storage volume will have a = new<br /> name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ld= quo;/umbc/rs/pi_doit&rdquo;,<br /> or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/= rs/pi_zmclaren&rdquo;.<br /> <br /> Thank you,<br /> Elliot<br /> <br /> <br /> -------------------------------------------------------------------------<b= r /> <br /> There is no need to reply to this message right now.&nbsp;<br /> <br /> Your ticket has been assigned an ID of [Research Computing #3289025] or you=  can go there directly by clicking the link below.<br /> <br /> Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3289025 &gt;<b= r /> <br /> You can login to view your open tickets at any time by visiting http://my.u= mbc.edu and clicking on &quot;Help&quot; and &quot;Request Help&quot;.<br /> <br /> Alternately you can click on http://my.umbc.edu/help<br /> <br /> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp= ; &nbsp; Thank you<br /> &nbsp;</blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --  <div> <div>Zo&euml; M. McLaren<br /> Associate Professor<br /> School of Public Policy<br /> University of Maryland Baltimore County<br /> Baltimore, MD<br /> CV and paper downloads: http://zoemclaren.com<br /> Faculty website: https://publicpolicy.umbc.edu/zoe-m-mclaren/ <div>Twitter:&nbsp;https://twitter.com/zoemclaren</div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <div> <div> <div> <div> <div><span style=3D""color:#888888"">Gregory Ballantine</span></div>  <div><span style=3D""color:#888888"">System Administrator for Research and En= terprise Computing</span></div>  <div><span style=3D""color:#888888"">UMBC - DoIT</span></div> </div> </div> </div> </div> "
3289025,72549278,Correspond,DoIT-Research-Computing,2025-10-29 13:46:59.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zmclaren,resolved,Greg Ballantine,gballan1,Zoe McLaren,zmclaren,zmclaren@umbc.edu,Zoe McLaren,zmclaren@umbc.edu,"I've updated my files with the new file path and run a couple of them. Things seem to be working fine as far as I can tell.  Thanks and good luck with the rest of the migration process! Zoe  On Tue, Oct 28, 2025 at 3:37=E2=80=AFPM Greg Ballantine via RT <UMBCHelp@rt= .umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3289025 > > > Last Update From Ticket: > > Hello Zoe, > > We have finished migrating your volume to the Ceph cluster! As far as we > can > tell everything seems to have gone smoothly. There are a few things to > note: > > > * The path has changed, and is now available under /umbc/rs/pi_zmclaren. > > > * The alias used to reach the volume is now pi_zmclaren_common and > >> pi_zmclaren_user. > > > * Your new volume has a quota of 10TB. > > When you have a chance, could you try running some jobs on Chip using the > new > volume to verify everything looks good? > > Thank you, > Greg > > On Tue Oct 28 10:49:40 2025, OJ87090 wrote: > > > Hello Zoe, > > > This is a reminder that we will be performing your group's migration to > the > > Ceph storage cluster today. During this time, please ensure there are n= ot > > any jobs being run in your research group, otherwise these may be > > terminated. > > > We will provide an update once completed. > > > Best, > > Greg > > > On Thu Oct 09 11:54:13 2025, OJ87090 wrote: > > >> Hello Zoe, > > >> Sounds good, I've put your group's migration on our schedule for > >> October 28th. We will send you an email alert via this RT ticket when > >> we begin the migration, and again once it has completed. > > >> Please let me know if you have any questions or concerns. > > >> Best, > >> Greg > > >> On Thu Oct 09 11:31:27 2025, OE75135 wrote: > > >>> October 28 would be great. Or Oct 29 if that isn't possible. > >>> Thanks! On Thu, Oct 9, 2025 at 10:15 AM via RT > >>> <UMBCHelp@rt.umbc.edu> wrote: > > >>>> Greetings, > > >>>> This message has been automatically generated in response to > >>>> the > >>>> creation of a ticket regarding: > > >>>> > ------------------------------------------------------------------------- > >>>> Subject: ""Migrating Research Storage Volume to Ceph Cluster - > >>>> pi_zmclaren"" > > >>>> Message: > > >>>> Dear Zoe, > > >>>> As per the communication via myUMBC earlier this summer (June > >>>> HPCF Newsletter > >>>> ), DoIT is in the process of migrating data off of an older > >>>> storage server to > >>>> our new RRStor Ceph storage cluster. Your group is using 502.9 > >>>> GB of a 488.3 GB > >>>> quota on the old storage server. > > >>>> To perform these migrations, we need to take individual storage > >>>> volumes offline > >>>> while we migrate them to the Ceph cluster. Thus we are reaching > >>>> out to schedule > >>>> a date where we can migrate your volume located at > >>>> =E2=80=9C/umbc/rs/zmclaren=E2=80=9D. During > >>>> the migration, we will take your volume offline and will > >>>> terminate any jobs > >>>> running on the chip compute cluster that are accessing this > >>>> volume. > > >>>> Below we=E2=80=99ve listed two options for handling this data migrat= ion > >>>> - please let us > >>>> know which of these you=E2=80=99d prefer. > > >>>> Option 1: Schedule a group-wide downtime date during standard > >>>> business hours, > >>>> which can be done by responding to this email with your > >>>> preferred date(s) to > >>>> perform the migration. During this time, DoIT staff will work > >>>> to migrate your > >>>> volume to the Ceph storage cluster. DoIT staff will send an > >>>> email alert on this > >>>> email thread when the migration has begun and when it has > >>>> completed. For most > >>>> storage volumes, this process should take less than a business > >>>> day. > >>>> Option 2: If you don=E2=80=99t respond to this email by October 15th, > >>>> DoIT staff will > >>>> assign a day over the following month (October 16th through > >>>> November 15th) to > >>>> migrate your volume. The day chosen will be random and will > >>>> occur during > >>>> business hours. You will be notified of the date chosen to > >>>> perform the > >>>> migration, and will be notified when the migration begins and > >>>> completes. > > >>>> Note: After this process has completed, the new storage volume > >>>> will have a new > >>>> name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its dat= a under > >>>> =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, > >>>> or in your group=E2=80=99s case you will find your volume under > >>>> =E2=80=9C/umbc/rs/pi_zmclaren=E2=80=9D. > > >>>> Thank you, > >>>> Elliot > > > >>>> > ------------------------------------------------------------------------- > > >>>> There is no need to reply to this message right now. > > >>>> Your ticket has been assigned an ID of [Research Computing > >>>> #3289025] or you can go there directly by clicking the link > >>>> below. > > >>>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3289025 > >>>> > > > >>>> You can login to view your open tickets at any time by visiting > >>>> http://my.umbc.edu and clicking on ""Help"" and ""Request Help"". > > >>>> Alternately you can click on http://my.umbc.edu/help > > >>>> Thank you > > >>> -- Zo=C3=AB M. McLaren > >>> Associate Professor > >>> School of Public Policy > >>> University of Maryland Baltimore County > >>> Baltimore, MD > >>> CV and paper downloads: http://zoemclaren.com > >>> Faculty website: https://publicpolicy.umbc.edu/zoe-m-mclaren/ > >>> Twitter: https://twitter.com/zoemclaren > > >> -- > > >> Gregory BallantineSystem Administrator for Research and Enterprise > >> ComputingUMBC - DoIT > > > -- > > > Gregory BallantineSystem Administrator for Research and Enterprise > > ComputingUMBC - DoIT > > -- > > Gregory BallantineSystem Administrator for Research and Enterprise > ComputingUMBC > - DoIT > >  --=20 Zo=C3=AB M. McLaren Associate Professor School of Public Policy University of Maryland Baltimore County Baltimore, MD CV and paper downloads: http://zoemclaren.com Faculty website: https://publicpolicy.umbc.edu/zoe-m-mclaren/ Twitter: https://twitter.com/zoemclaren "
3289025,72549278,Correspond,DoIT-Research-Computing,2025-10-29 13:46:59.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zmclaren,resolved,Greg Ballantine,gballan1,Zoe McLaren,zmclaren,zmclaren@umbc.edu,Zoe McLaren,zmclaren@umbc.edu,"<div dir=3D""ltr""><div class=3D""gmail_default"" style=3D""font-family:tahoma,s= ans-serif"">I&#39;ve updated my files with the new file path and run a coupl= e of them. Things seem to be working fine as far as I can tell.</div><div c= lass=3D""gmail_default"" style=3D""font-family:tahoma,sans-serif""><br></div><d= iv class=3D""gmail_default"" style=3D""font-family:tahoma,sans-serif"">Thanks a= nd good luck with the=C2=A0rest of the migration process!</div><div class= =3D""gmail_default"" style=3D""font-family:tahoma,sans-serif"">Zoe</div></div><= br><div class=3D""gmail_quote gmail_quote_container""><div dir=3D""ltr"" class= =3D""gmail_attr"">On Tue, Oct 28, 2025 at 3:37=E2=80=AFPM Greg Ballantine via=  RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt= ; wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""margin:0px 0px=  0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"">Ticket = &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D3289025"" r= el=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html= ?id=3D3289025</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hello Zoe,<br> <br> We have finished migrating your volume to the Ceph cluster! As far as we ca= n<br> tell everything seems to have gone smoothly. There are a few things to note= :<br> <br> &gt; * The path has changed, and is now available under /umbc/rs/pi_zmclare= n.<br> <br> &gt; * The alias used to reach the volume is now pi_zmclaren_common and<br> &gt;&gt; pi_zmclaren_user.<br> <br> &gt; * Your new volume has a quota of 10TB.<br> <br> When you have a chance, could you try running some jobs on Chip using the n= ew<br> volume to verify everything looks good?<br> <br> Thank you,<br> Greg<br> <br> On Tue Oct 28 10:49:40 2025, OJ87090 wrote:<br> <br> &gt; Hello Zoe,<br> <br> &gt; This is a reminder that we will be performing your group&#39;s migrati= on to the<br> &gt; Ceph storage cluster today. During this time, please ensure there are = not<br> &gt; any jobs being run in your research group, otherwise these may be<br> &gt; terminated.<br> <br> &gt; We will provide an update once completed.<br> <br> &gt; Best,<br> &gt; Greg<br> <br> &gt; On Thu Oct 09 11:54:13 2025, OJ87090 wrote:<br> <br> &gt;&gt; Hello Zoe,<br> <br> &gt;&gt; Sounds good, I&#39;ve put your group&#39;s migration on our schedu= le for<br> &gt;&gt; October 28th. We will send you an email alert via this RT ticket w= hen<br> &gt;&gt; we begin the migration, and again once it has completed.<br> <br> &gt;&gt; Please let me know if you have any questions or concerns.<br> <br> &gt;&gt; Best,<br> &gt;&gt; Greg<br> <br> &gt;&gt; On Thu Oct 09 11:31:27 2025, OE75135 wrote:<br> <br> &gt;&gt;&gt; October 28 would be great. Or Oct 29 if that isn&#39;t possibl= e.<br> &gt;&gt;&gt; Thanks! On Thu, Oct 9, 2025 at 10:15 AM via RT<br> &gt;&gt;&gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">= UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt;&gt;&gt; Greetings,<br> <br> &gt;&gt;&gt;&gt; This message has been automatically generated in response = to<br> &gt;&gt;&gt;&gt; the<br> &gt;&gt;&gt;&gt; creation of a ticket regarding:<br> <br> &gt;&gt;&gt;&gt; ----------------------------------------------------------= ---------------<br> &gt;&gt;&gt;&gt; Subject: &quot;Migrating Research Storage Volume to Ceph C= luster -<br> &gt;&gt;&gt;&gt; pi_zmclaren&quot;<br> <br> &gt;&gt;&gt;&gt; Message:<br> <br> &gt;&gt;&gt;&gt; Dear Zoe,<br> <br> &gt;&gt;&gt;&gt; As per the communication via myUMBC earlier this summer (J= une<br> &gt;&gt;&gt;&gt; HPCF Newsletter<br> &gt;&gt;&gt;&gt; ), DoIT is in the process of migrating data off of an olde= r<br> &gt;&gt;&gt;&gt; storage server to<br> &gt;&gt;&gt;&gt; our new RRStor Ceph storage cluster. Your group is using 5= 02.9<br> &gt;&gt;&gt;&gt; GB of a 488.3 GB<br> &gt;&gt;&gt;&gt; quota on the old storage server.<br> <br> &gt;&gt;&gt;&gt; To perform these migrations, we need to take individual st= orage<br> &gt;&gt;&gt;&gt; volumes offline<br> &gt;&gt;&gt;&gt; while we migrate them to the Ceph cluster. Thus we are rea= ching<br> &gt;&gt;&gt;&gt; out to schedule<br> &gt;&gt;&gt;&gt; a date where we can migrate your volume located at<br> &gt;&gt;&gt;&gt; =E2=80=9C/umbc/rs/zmclaren=E2=80=9D. During<br> &gt;&gt;&gt;&gt; the migration, we will take your volume offline and will<b= r> &gt;&gt;&gt;&gt; terminate any jobs<br> &gt;&gt;&gt;&gt; running on the chip compute cluster that are accessing thi= s<br> &gt;&gt;&gt;&gt; volume.<br> <br> &gt;&gt;&gt;&gt; Below we=E2=80=99ve listed two options for handling this d= ata migration<br> &gt;&gt;&gt;&gt; - please let us<br> &gt;&gt;&gt;&gt; know which of these you=E2=80=99d prefer.<br> <br> &gt;&gt;&gt;&gt; Option 1: Schedule a group-wide downtime date during stand= ard<br> &gt;&gt;&gt;&gt; business hours,<br> &gt;&gt;&gt;&gt; which can be done by responding to this email with your<br> &gt;&gt;&gt;&gt; preferred date(s) to<br> &gt;&gt;&gt;&gt; perform the migration. During this time, DoIT staff will w= ork<br> &gt;&gt;&gt;&gt; to migrate your<br> &gt;&gt;&gt;&gt; volume to the Ceph storage cluster. DoIT staff will send a= n<br> &gt;&gt;&gt;&gt; email alert on this<br> &gt;&gt;&gt;&gt; email thread when the migration has begun and when it has<= br> &gt;&gt;&gt;&gt; completed. For most<br> &gt;&gt;&gt;&gt; storage volumes, this process should take less than a busi= ness<br> &gt;&gt;&gt;&gt; day.<br> &gt;&gt;&gt;&gt; Option 2: If you don=E2=80=99t respond to this email by Oc= tober 15th,<br> &gt;&gt;&gt;&gt; DoIT staff will<br> &gt;&gt;&gt;&gt; assign a day over the following month (October 16th throug= h<br> &gt;&gt;&gt;&gt; November 15th) to<br> &gt;&gt;&gt;&gt; migrate your volume. The day chosen will be random and wil= l<br> &gt;&gt;&gt;&gt; occur during<br> &gt;&gt;&gt;&gt; business hours. You will be notified of the date chosen to= <br> &gt;&gt;&gt;&gt; perform the<br> &gt;&gt;&gt;&gt; migration, and will be notified when the migration begins = and<br> &gt;&gt;&gt;&gt; completes.<br> <br> &gt;&gt;&gt;&gt; Note: After this process has completed, the new storage vo= lume<br> &gt;&gt;&gt;&gt; will have a new<br> &gt;&gt;&gt;&gt; name. For example, group =E2=80=9Cpi_doit=E2=80=9D will fi= nd its data under<br> &gt;&gt;&gt;&gt; =E2=80=9C/umbc/rs/pi_doit=E2=80=9D,<br> &gt;&gt;&gt;&gt; or in your group=E2=80=99s case you will find your volume = under<br> &gt;&gt;&gt;&gt; =E2=80=9C/umbc/rs/pi_zmclaren=E2=80=9D.<br> <br> &gt;&gt;&gt;&gt; Thank you,<br> &gt;&gt;&gt;&gt; Elliot<br> <br> <br> &gt;&gt;&gt;&gt; ----------------------------------------------------------= ---------------<br> <br> &gt;&gt;&gt;&gt; There is no need to reply to this message right now.<br> <br> &gt;&gt;&gt;&gt; Your ticket has been assigned an ID of [Research Computing= <br> &gt;&gt;&gt;&gt; #3289025] or you can go there directly by clicking the lin= k<br> &gt;&gt;&gt;&gt; below.<br> <br> &gt;&gt;&gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Disp= lay.html?id=3D3289025"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc= .edu/Ticket/Display.html?id=3D3289025</a><br> &gt;&gt;&gt;&gt; &gt;<br> <br> &gt;&gt;&gt;&gt; You can login to view your open tickets at any time by vis= iting<br> &gt;&gt;&gt;&gt; <a href=3D""http://my.umbc.edu"" rel=3D""noreferrer"" target= =3D""_blank"">http://my.umbc.edu</a> and clicking on &quot;Help&quot; and &qu= ot;Request Help&quot;.<br> <br> &gt;&gt;&gt;&gt; Alternately you can click on <a href=3D""http://my.umbc.edu= /help"" rel=3D""noreferrer"" target=3D""_blank"">http://my.umbc.edu/help</a><br> <br> &gt;&gt;&gt;&gt; Thank you<br> <br> &gt;&gt;&gt; -- Zo=C3=AB M. McLaren<br> &gt;&gt;&gt; Associate Professor<br> &gt;&gt;&gt; School of Public Policy<br> &gt;&gt;&gt; University of Maryland Baltimore County<br> &gt;&gt;&gt; Baltimore, MD<br> &gt;&gt;&gt; CV and paper downloads: <a href=3D""http://zoemclaren.com"" rel= =3D""noreferrer"" target=3D""_blank"">http://zoemclaren.com</a><br> &gt;&gt;&gt; Faculty website: <a href=3D""https://publicpolicy.umbc.edu/zoe-= m-mclaren/"" rel=3D""noreferrer"" target=3D""_blank"">https://publicpolicy.umbc.= edu/zoe-m-mclaren/</a><br> &gt;&gt;&gt; Twitter: <a href=3D""https://twitter.com/zoemclaren"" rel=3D""nor= eferrer"" target=3D""_blank"">https://twitter.com/zoemclaren</a><br> <br> &gt;&gt; --<br> <br> &gt;&gt; Gregory BallantineSystem Administrator for Research and Enterprise= <br> &gt;&gt; ComputingUMBC - DoIT<br> <br> &gt; --<br> <br> &gt; Gregory BallantineSystem Administrator for Research and Enterprise<br> &gt; ComputingUMBC - DoIT<br> <br> --<br> <br> Gregory BallantineSystem Administrator for Research and Enterprise Computin= gUMBC<br> - DoIT<br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr"">Zo=C3=AB M. McLaren<br>Associate Professor<br>Sc= hool of Public Policy<br>University of Maryland Baltimore County<br>Baltimo= re, MD<br>CV and paper downloads: <a href=3D""http://zoemclaren.com"" target= =3D""_blank"">http://zoemclaren.com</a><br>Faculty website: <a href=3D""https:= //publicpolicy.umbc.edu/zoe-m-mclaren/"" target=3D""_blank"">https://publicpol= icy.umbc.edu/zoe-m-mclaren/</a><div>Twitter:=C2=A0<a href=3D""https://twitte= r.com/zoemclaren"" target=3D""_blank"">https://twitter.com/zoemclaren</a></div= ></div></div> "
3289155,72187992,Create,DoIT-Research-Computing,2025-10-09 16:29:08.0000000,HPC New Group: pi_rmwillia,resolved,Elliot Gobbert,elliotg2,Rebecca Williams,rmwillia,rmwillia@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Rebecca Last Name:                 Williams Email:                     rmwillia@umbc.edu Campus ID:                 KC16614  Request Type:              High Performance Cluster  Group Type:            Project Title:        Misc Project Abstract:     Group for lab  there are no notes/comments  "
3289155,72203302,Correspond,DoIT-Research-Computing,2025-10-10 13:50:36.0000000,HPC New Group: pi_rmwillia,resolved,Elliot Gobbert,elliotg2,Rebecca Williams,rmwillia,rmwillia@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hi there,</p>  <p>Unfortunately, I can&#39;t create a group with this information, cited from the wiki, I&#39;ll need:</p>  <p><strong>Group Account / PI Group Request:</strong><br /> Chip user accounts are assigned a primary group (e.g., <code>pi_groupname</code>). If a new PI group is needed:</p>  <ul> 	<li> 	<p>The faculty PI should contact DoIT via RT ticket.</p> 	</li> 	<li> 	<p>They should provide details about the research project, list of users, and any storage/resource needs.</p> 	</li> </ul>  <p>Here is a link to the page documenting how to create a group on the HPC:</p>  <p>https://umbc.atlassian.net/wiki/spaces/faq/pages/1327431728/How+to+request+a+user+group+account+on+chip</p>  <p>&nbsp;</p>  <p>Best,</p>  <p>Elliot Gobbert</p> "
3289155,72209830,Correspond,DoIT-Research-Computing,2025-10-10 16:35:41.0000000,HPC New Group: pi_rmwillia,resolved,Elliot Gobbert,elliotg2,Rebecca Williams,rmwillia,rmwillia@umbc.edu,Rebecca Williams,rmwillia@umbc.edu,"just submitted a new ticket for ""create new/existing group"", following the wiki, but there wasn't a place to submit the list of users.  On Fri, Oct 10, 2025 at 9:50=E2=80=AFAM Elliot Gobbert via RT <UMBCHelp@rt.= umbc.edu> wrote:  > If you agree your issue is resolved, please give us feedback on your > experience by completing a brief satisfaction survey: > > > https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&customeremail= =3Drmwillia%40umbc.edu&groupid=3DEIS&ticketid=3D3289155&ticketsubject=3DHPC= %20New%20Group%3A%20pi_rmwillia > > If you believe your issue has not been resolved, please respond to this > message, which will reopen your ticket. Note: A full record of your reque= st > can be found at: > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3289155 > > > Thank You > > _________________________________________ > > R e s o l u t i o n: > =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D = =3D > > Hi there, > > Unfortunately, I can't create a group with this information, cited from t= he > wiki, I'll need: > > Group Account / PI Group Request: > Chip user accounts are assigned a primary group (e.g., pi_groupname). If a > new > PI group is needed: > > > * > > >> The faculty PI should contact DoIT via RT ticket. > > > * > > >> They should provide details about the research project, list of users, > and > >> any storage/resource needs. > > Here is a link to the page documenting how to create a group on the HPC: > > > https://umbc.atlassian.net/wiki/spaces/faq/pages/1327431728/How+to+reques= t+a+user+group+account+on+chip > > Best, > > Elliot Gobbert > > > > ______________________________________ > > Original Request: > > Requestors: Rebecca Williams > > First Name:                Rebecca > Last Name:                 Williams > Email:                     rmwillia@umbc.edu > Campus ID:                 KC16614 > > Request Type:              High Performance Cluster > > Group Type: > Project Title:        Misc > Project Abstract:     Group for lab > > there are no notes/comments > > >  --=20 Office: ITE 335 Book an appointment: https://calendar.app.google/k9FqyKJzuhNW2Tfd8 site: https://sites.google.com/umbc.edu/prof-rebecca-williams/ "
3289155,72209830,Correspond,DoIT-Research-Computing,2025-10-10 16:35:41.0000000,HPC New Group: pi_rmwillia,resolved,Elliot Gobbert,elliotg2,Rebecca Williams,rmwillia,rmwillia@umbc.edu,Rebecca Williams,rmwillia@umbc.edu,"<div dir=3D""ltr"">just submitted a new ticket for &quot;create new/existing = group&quot;, following the wiki, but there wasn&#39;t a place to submit the=  list of users.</div><br><div class=3D""gmail_quote gmail_quote_container""><= div dir=3D""ltr"" class=3D""gmail_attr"">On Fri, Oct 10, 2025 at 9:50=E2=80=AFA= M Elliot Gobbert via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHel= p@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" sty= le=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);paddi= ng-left:1ex"">If you agree your issue is resolved, please give us feedback o= n your experience by completing a brief satisfaction survey: <br> <br> <a href=3D""https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&amp;= customeremail=3Drmwillia%40umbc.edu&amp;groupid=3DEIS&amp;ticketid=3D328915= 5&amp;ticketsubject=3DHPC%20New%20Group%3A%20pi_rmwillia"" rel=3D""noreferrer= "" target=3D""_blank"">https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTIS= F6Ly&amp;customeremail=3Drmwillia%40umbc.edu&amp;groupid=3DEIS&amp;ticketid= =3D3289155&amp;ticketsubject=3DHPC%20New%20Group%3A%20pi_rmwillia</a><br> <br> If you believe your issue has not been resolved, please respond to this mes= sage, which will reopen your ticket. Note: A full record of your request ca= n be found at:=C2=A0 <br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D328= 9155"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3289155</a> &gt;<br> <br> Thank You<br> <br> _________________________________________<br> <br> R e s o l u t i o n:<br> =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D= =C2=A0 <br> <br> Hi there,<br> <br> Unfortunately, I can&#39;t create a group with this information, cited from=  the<br> wiki, I&#39;ll need:<br> <br> Group Account / PI Group Request:<br> Chip user accounts are assigned a primary group (e.g., pi_groupname). If a = new<br> PI group is needed:<br> <br> &gt; * <br> <br> &gt;&gt; The faculty PI should contact DoIT via RT ticket.<br> <br> &gt; * <br> <br> &gt;&gt; They should provide details about the research project, list of us= ers, and<br> &gt;&gt; any storage/resource needs.<br> <br> Here is a link to the page documenting how to create a group on the HPC:<br> <br> <a href=3D""https://umbc.atlassian.net/wiki/spaces/faq/pages/1327431728/How+= to+request+a+user+group+account+on+chip"" rel=3D""noreferrer"" target=3D""_blan= k"">https://umbc.atlassian.net/wiki/spaces/faq/pages/1327431728/How+to+reque= st+a+user+group+account+on+chip</a><br> <br> Best,<br> <br> Elliot Gobbert<br> <br> <br> <br> ______________________________________<br> <br> Original Request:<br> <br> Requestors: Rebecca Williams<br> <br> First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Rebecca<= br> Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0Wil= liams<br> Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=  =C2=A0<a href=3D""mailto:rmwillia@umbc.edu"" target=3D""_blank"">rmwillia@umbc= .edu</a><br> Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0KC1= 6614<br> <br> Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Performa= nce Cluster<br> <br> Group Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0<br> Project Title:=C2=A0 =C2=A0 =C2=A0 =C2=A0 Misc<br> Project Abstract:=C2=A0 =C2=A0 =C2=A0Group for lab<br> <br> there are no notes/comments<br> <br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><div>Office: ITE 335</div>Book an appointment:= =C2=A0<a href=3D""https://calendar.app.google/k9FqyKJzuhNW2Tfd8"" target=3D""_= blank"">https://calendar.app.google/k9FqyKJzuhNW2Tfd8</a><div>site:=C2=A0<a = href=3D""https://sites.google.com/umbc.edu/prof-rebecca-williams/"" target=3D= ""_blank"">https://sites.google.com/umbc.edu/prof-rebecca-williams/</a></div>= </div></div> "
3289155,72210034,Correspond,DoIT-Research-Computing,2025-10-10 16:42:25.0000000,HPC New Group: pi_rmwillia,resolved,Elliot Gobbert,elliotg2,Rebecca Williams,rmwillia,rmwillia@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Feel free to just give me the list of users you&#39;d like to add in this email chain.</p>  <p>However, you&#39;re going to need to be a bit more descriptive, especially in regards to the project abstract. Here&#39;s an example:</p>  <h1>Dr. Stephen Miller, Department of Biological Sciences.</h1>  <div> <div> <div> <div> <p><strong>Miller Lab RNAseq Analysis</strong></p> </div> </div>  <div> <div> <p>Dr. Stephen Miller, Department of Biological Sciences.</p>  <p>The Volvox carteri somatic regenerator (regA) gene encodes a putative transcription factor that is expressed only in somatic cells and is essential for maintenance of the somatic cell fate. rlsA is one of the closest paralogs of regA and its spatial and temporal expression patterns are very similar to that of regA, suggesting that this gene might also function in somatic cell development. To test this idea, we used CRISPR to generate an early frameshift mutation in rlsA that eliminates rlsA function. We are conducting RNA-seq analysis of the mutant to investigate the possibility of genetic compensation, and to better understand the role of rlsA in regulating the expression of other genes. Sequence reads have been obtained for the wild type and rlsA mutant and an RNAseq pipeline will be used to map the reads to an assembled V. carteri genome then determine the statistical significance of DEGs in different gene functional classes.</p>  <p>&nbsp;</p>  <p>&nbsp;</p>  <p>Best,</p>  <p>Elliot</p> </div> </div> </div> </div> "
3289155,72389472,Correspond,DoIT-Research-Computing,2025-10-21 13:20:32.0000000,HPC New Group: pi_rmwillia,resolved,Elliot Gobbert,elliotg2,Rebecca Williams,rmwillia,rmwillia@umbc.edu,Rebecca Williams,rmwillia@umbc.edu,"Ok thanks! list of users is: rmwillia@umbc.edu and samn2@umbc.edu  Description: From Refusal to Prevention: Anticipatory Safety Mechanisms for Conversational AI  Rebecca Williams, PhD, University of Maryland Baltimore County,USA  Cynthia Matuszek, PhD, University of Maryland Baltimore County, USA  We propose to use an interdisciplinary approach to design an alignment-focused conversational guardrails and explanation system that predicts potentially risky conversation trajectories before the conversation reaches a harmful request, building up up an explanation over multiple turns that is culturally relevant, aligned to impartial principles, and convincingly justified to the user. We will use techniques from Human-Robot Interaction, NLP, Explainable AI, and Qualitative Methods in Cognitive Science to design control protocols and red-team a guardrail + explanation system.  Relevance to Alignment Project Objectives  Our objective is to create, test, and red-team a LLM-based conversation control system that characterizes and predicts when dialogue is on a trajectory toward a problematic user request, and intervenes early with justified, culturally-aware conversational redirection, rather than waiting for a problematic trigger to shut the request down with boilerplate refusal. Our project fits with The Alignment Project=E2=80=99s goal of desi= gning AI systems that do not attempt risky actions in the first place, in the Empirical Investigations Into AI Monitoring and Red Teaming Research Agenda, situated in the domain Human-Robot Interaction domain, as embodied interactions are increasingly employing LLMs and multi-turn conversations [1].   On Fri, Oct 10, 2025 at 12:42=E2=80=AFPM Elliot Gobbert via RT <UMBCHelp@rt= .umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3289155 > > > Last Update From Ticket: > > Feel free to just give me the list of users you'd like to add in this ema= il > chain. > > However, you're going to need to be a bit more descriptive, especially in > regards to the project abstract. Here's an example: > > > > Dr. Stephen Miller, Department of Biological Sciences. > =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D= =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D= =3D=3D=3D=3D=3D > > Miller Lab RNAseq Analysis > > Dr. Stephen Miller, Department of Biological Sciences. > > The Volvox carteri somatic regenerator (regA) gene encodes a putative > transcription factor that is expressed only in somatic cells and is > essential > for maintenance of the somatic cell fate. rlsA is one of the closest > paralogs > of regA and its spatial and temporal expression patterns are very similar > to > that of regA, suggesting that this gene might also function in somatic ce= ll > development. To test this idea, we used CRISPR to generate an early > frameshift > mutation in rlsA that eliminates rlsA function. We are conducting RNA-seq > analysis of the mutant to investigate the possibility of genetic > compensation, > and to better understand the role of rlsA in regulating the expression of > other > genes. Sequence reads have been obtained for the wild type and rlsA mutant > and > an RNAseq pipeline will be used to map the reads to an assembled V. carte= ri > genome then determine the statistical significance of DEGs in different > gene > functional classes. > > Best, > > Elliot > >  --=20 Office: ITE 335 Book an appointment: https://calendar.app.google/k9FqyKJzuhNW2Tfd8 site: https://sites.google.com/umbc.edu/prof-rebecca-williams/ "
3289155,72389472,Correspond,DoIT-Research-Computing,2025-10-21 13:20:32.0000000,HPC New Group: pi_rmwillia,resolved,Elliot Gobbert,elliotg2,Rebecca Williams,rmwillia,rmwillia@umbc.edu,Rebecca Williams,rmwillia@umbc.edu,"<div dir=3D""ltr"">Ok thanks! list of users is: <a href=3D""mailto:rmwillia@um= bc.edu"">rmwillia@umbc.edu</a> and <a href=3D""mailto:samn2@umbc.edu"">samn2@u= mbc.edu</a><div><br></div><div>Description:</div><div><span id=3D""gmail-doc= s-internal-guid-0f1a71ef-7fff-8a50-ae21-d3cb4a77c44c""><h1 dir=3D""ltr"" style= =3D""line-height:1.38;background-color:rgb(254,254,254);margin-top:20pt;marg= in-bottom:0pt;padding:0pt 0pt 6pt""><span style=3D""font-size:20pt;font-famil= y:Arial,sans-serif;color:rgb(0,0,0);background-color:transparent;font-weigh= t:400;font-variant-numeric:normal;font-variant-east-asian:normal;font-varia= nt-alternates:normal;vertical-align:baseline"">From Refusal to Prevention: A= nticipatory Safety Mechanisms for Conversational AI</span></h1><p dir=3D""lt= r"" style=3D""line-height:1.38;margin-top:0pt;margin-bottom:0pt""><span style= =3D""font-size:11pt;font-family:Arial,sans-serif;color:rgb(0,0,0);background= -color:transparent;font-variant-numeric:normal;font-variant-east-asian:norm= al;font-variant-alternates:normal;vertical-align:baseline"">Rebecca Williams= , PhD, University of Maryland Baltimore County,USA</span></p><p dir=3D""ltr""=  style=3D""line-height:1.38;margin-top:0pt;margin-bottom:0pt""><span style=3D= ""font-size:11pt;font-family:Arial,sans-serif;color:rgb(0,0,0);background-co= lor:transparent;font-variant-numeric:normal;font-variant-east-asian:normal;= font-variant-alternates:normal;vertical-align:baseline"">Cynthia Matuszek, P= hD, University of Maryland Baltimore County, USA</span></p><br><p dir=3D""lt= r"" style=3D""line-height:1.92;margin-top:0pt;margin-bottom:0pt;padding:0pt 0= pt 12pt""><span style=3D""font-size:12pt;font-family:Arial,sans-serif;color:r= gb(34,40,48);background-color:rgb(254,254,254);font-variant-numeric:normal;= font-variant-east-asian:normal;font-variant-alternates:normal;vertical-alig= n:baseline"">We propose to use an interdisciplinary approach to design an al= ignment-focused conversational guardrails </span><span style=3D""font-size:1= 2pt;font-family:Arial,sans-serif;color:rgb(34,40,48);background-color:rgb(2= 54,254,254);font-variant-numeric:normal;font-variant-east-asian:normal;font= -variant-alternates:normal;text-decoration-line:underline;vertical-align:ba= seline"">and</span><span style=3D""font-size:12pt;font-family:Arial,sans-seri= f;color:rgb(34,40,48);background-color:rgb(254,254,254);font-variant-numeri= c:normal;font-variant-east-asian:normal;font-variant-alternates:normal;vert= ical-align:baseline""> explanation system that predicts potentially risky co= nversation trajectories before the conversation reaches a harmful request, = building up up an explanation over multiple turns that is culturally releva= nt, aligned to impartial principles, and convincingly justified to the user= . We will use techniques from Human-Robot Interaction, NLP, Explainable AI,=  and Qualitative Methods in Cognitive Science to design control protocols a= nd red-team a guardrail + explanation system.</span></p><p dir=3D""ltr"" styl= e=3D""line-height:1.92;margin-top:0pt;margin-bottom:0pt;padding:0pt 0pt 12pt= ""><span style=3D""font-size:12pt;font-family:Arial,sans-serif;color:rgb(34,4= 0,48);background-color:rgb(254,254,254);font-weight:700;font-variant-numeri= c:normal;font-variant-east-asian:normal;font-variant-alternates:normal;vert= ical-align:baseline"">Relevance to Alignment Project Objectives=C2=A0</span>= </p><p dir=3D""ltr"" style=3D""line-height:1.92;margin-top:0pt;margin-bottom:1= 2pt""><span style=3D""font-size:12pt;font-family:Arial,sans-serif;color:rgb(3= 4,40,48);background-color:rgb(254,254,254);font-variant-numeric:normal;font= -variant-east-asian:normal;font-variant-alternates:normal;vertical-align:ba= seline"">Our objective is to create, test, and red-team a LLM-based conversa= tion control system that characterizes and predicts when dialogue is on a t= rajectory toward a problematic user request, and intervenes early with just= ified, culturally-aware conversational redirection, rather than waiting for=  a problematic trigger to shut the request down with boilerplate refusal. O= ur project fits with The Alignment Project=E2=80=99s goal of designing AI s= ystems that do not attempt risky actions in the first place, in the Empiric= al Investigations Into AI Monitoring and Red Teaming Research Agenda, situa= ted in the domain Human-Robot Interaction domain, as embodied interactions = are increasingly employing LLMs and multi-turn conversations [1].</span></p= ></span><br class=3D""gmail-Apple-interchange-newline""></div></div><br><div = class=3D""gmail_quote gmail_quote_container""><div dir=3D""ltr"" class=3D""gmail= _attr"">On Fri, Oct 10, 2025 at 12:42=E2=80=AFPM Elliot Gobbert via RT &lt;<= a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<= br></div><blockquote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8e= x;border-left:1px solid rgb(204,204,204);padding-left:1ex"">Ticket &lt;URL: = <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D3289155"" rel=3D""nor= eferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D328= 9155</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Feel free to just give me the list of users you&#39;d like to add in this e= mail<br> chain.<br> <br> However, you&#39;re going to need to be a bit more descriptive, especially = in<br> regards to the project abstract. Here&#39;s an example:<br> <br> <br> <br> Dr. Stephen Miller, Department of Biological Sciences.<br> =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D= =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D= =3D=3D=3D=3D<br> <br> Miller Lab RNAseq Analysis<br> <br> Dr. Stephen Miller, Department of Biological Sciences.<br> <br> The Volvox carteri somatic regenerator (regA) gene encodes a putative<br> transcription factor that is expressed only in somatic cells and is essenti= al<br> for maintenance of the somatic cell fate. rlsA is one of the closest paralo= gs<br> of regA and its spatial and temporal expression patterns are very similar t= o<br> that of regA, suggesting that this gene might also function in somatic cell= <br> development. To test this idea, we used CRISPR to generate an early framesh= ift<br> mutation in rlsA that eliminates rlsA function. We are conducting RNA-seq<b= r> analysis of the mutant to investigate the possibility of genetic compensati= on,<br> and to better understand the role of rlsA in regulating the expression of o= ther<br> genes. Sequence reads have been obtained for the wild type and rlsA mutant = and<br> an RNAseq pipeline will be used to map the reads to an assembled V. carteri= <br> genome then determine the statistical significance of DEGs in different gen= e<br> functional classes.<br> <br> Best,<br> <br> Elliot<br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><div>Office: ITE 335</div>Book an appointment:= =C2=A0<a href=3D""https://calendar.app.google/k9FqyKJzuhNW2Tfd8"" target=3D""_= blank"">https://calendar.app.google/k9FqyKJzuhNW2Tfd8</a><div>site:=C2=A0<a = href=3D""https://sites.google.com/umbc.edu/prof-rebecca-williams/"" target=3D= ""_blank"">https://sites.google.com/umbc.edu/prof-rebecca-williams/</a></div>= </div></div> "
3289155,72396333,Correspond,DoIT-Research-Computing,2025-10-21 15:27:18.0000000,HPC New Group: pi_rmwillia,resolved,Elliot Gobbert,elliotg2,Rebecca Williams,rmwillia,rmwillia@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hi Rebecca,</p>  <p>Your account (rmwillia) has been created on chip.rs.umbc.edu.<br /> Your primary group is pi_rmwillia.<br /> Your home directory has 500M of storage.<br /> Please read through the documentation found at hpcf.umbc.edu &gt; User Support.<br /> All available modules can be viewed using the command &#39;module avail&#39;.<br /> Please submit additional questions or issues as separate tickets via the following link.<br /> (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/)</p>  <p><br /> &nbsp;Welcome to chip, UMBC&#39;s High Performance Computing Cluster!</p>  <p>&nbsp; &nbsp; The group, pi_rmwillia, now exists on the chip cluster. Members of this group can access and contribute to the research storage space allocated to the group.</p>  <p>&nbsp; &nbsp; This storage space is located at /umbc/rs/pi_rmwillia, and currently has a quota of 25T.</p>  <p>&nbsp; &nbsp; For information on accessing the cluster, adding accounts to your group, and getting started using the cluster, check out the tutorial on our wiki: https://umbc.atlassian.net/wiki/x/R4BPQg</p>  <p>&nbsp; &nbsp; Additional documentation is also available here: https://umbc.atlassian.net/wiki/x/FwCHQ</p>  <p>&nbsp; &nbsp; If you have any questions or issues, please submit a new RT ticket at: https://doit.umbc.edu/request-tracker-rt/doit-research-computing/</p>  <p>&nbsp;</p>  <p>&nbsp;</p>  <p>Account creation for users:</p>  <p>Hi Sam,</p>  <p>Your account (samn2) has been created on chip.rs.umbc.edu.<br /> Your primary group is pi_rmwillia.<br /> Your home directory has 500M of storage.<br /> Please read through the documentation found at hpcf.umbc.edu &gt; User Support.<br /> All available modules can be viewed using the command &#39;module avail&#39;.<br /> Please submit additional questions or issues as separate tickets via the following link.<br /> (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/)</p>  <p>&nbsp;</p>  <p>&nbsp;</p>  <p>Let me know if you have any more questions!</p>  <p>Elliot Gobbert</p> "
3289186,72188998,Create,DoIT-Research-Computing,2025-10-09 16:54:34.0000000,Start a new group under a PI,new,Nobody in particular," ",Paris von Lockette,pvonlock,pvonlock@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Paris Last Name:                 von Lockette Email:                     pvonlock@umbc.edu Campus ID:                 FA15338  Request Type:              Help with something else  I=E2=80=99d like to start two new groups.=20  1)  for a class =E2=80=9CENME444 - Mechanical  Engineering Capstone Design= =E2=80=9D  2) my research group =E2=80=9CeMACS Lab=E2=80=9D  Thank you   "
3289187,72189012,Create,DoIT-Research-Computing,2025-10-09 16:54:38.0000000,Start a new group under a PI,resolved,Danielle Esposito,desposi1,Paris von Lockette,pvonlock,pvonlock@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Paris Last Name:                 von Lockette Email:                     pvonlock@umbc.edu Campus ID:                 FA15338  Request Type:              Help with something else  I=E2=80=99d like to start two new groups.=20  1)  for a class =E2=80=9CENME444 - Mechanical  Engineering Capstone Design= =E2=80=9D  2) my research group =E2=80=9CeMACS Lab=E2=80=9D  Thank you   "
3289187,72191668,Correspond,DoIT-Research-Computing,2025-10-09 18:11:12.0000000,Start a new group under a PI,resolved,Danielle Esposito,desposi1,Paris von Lockette,pvonlock,pvonlock@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Paris,</p>  <p>I created two groups for you, along with an account for your user.</p>  <p>First,&nbsp;Your account (pvonlock) has been created on chip.rs.umbc.edu.<br /> Your primary group is pi_pvonlock.<br /> Your home directory has 500M of storage.<br /> You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip<br /> Please read through the documentation found at hpcf.umbc.edu &gt; User Support.<br /> All available modules can be viewed using the command &#39;module avail&#39;.</p>  <p>Secondly,&nbsp;</p>  <p>The group pi_pvonlock now exists on the chip cluster.</p>  <p>Members of this group can therefore access and contribute to the research storage space allocated to the group.</p>  <p>This storage is located at /umbc/rs/pi_pvonlock and currently has a quota of 25T.</p>  <p>And lastly,</p>  <p>The group enme444fa25 now exists on the chip cluster.</p>  <p>Members of this group can therefore access and contribute to the research storage space allocated to the group.</p>  <p>This storage is located at /umbc/class/enme444fa25 and currently has a quota of 5T.<br /> Please review documentation on the hpcf.umbc.edu website.</p>  <p>Submit any questions or issues as separate RT Tickets at:<br /> https://doit.umbc.edu/request-tracker-rt/doit-research-computing/.</p>  <p>Feel free to let us know if you encounter any issues with either of your groups! Have a good day!</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Thu Oct 09 12:54:38 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Paris Last Name:                 von Lockette Email:                     pvonlock@umbc.edu Campus ID:                 FA15338  Request Type:              Help with something else  I&rsquo;d like to start two new groups.   1)  for a class &ldquo;ENME444 - Mechanical  Engineering Capstone Design&rdquo;  2) my research group &ldquo;eMACS Lab&rdquo;  Thank you   </pre> </blockquote> </div> "
3289431,72197356,Create,DoIT-Research-Computing,2025-10-09 22:22:47.0000000,HPC Other Issue: Students lack global access in the common directory in pi_mkann lab space on CHIP,resolved,Danielle Esposito,desposi1,Petra Tembei,ptembei1,ptembei1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Petra Last Name:                 Tembei Email:                     ptembei1@umbc.edu Campus ID:                 WF57450  Request Type:              High Performance Cluster  I work in Dr. Kann=E2=80=99s lab, where we collaborate using shared directo= ries. Without global rwx access to the common directory, we=E2=80=99re unab= le to move files, create subdirectories, or delete files that are no longer=  needed in directories created by other users. We would really appreciate y= our help with this.  "
3289431,72202880,Correspond,DoIT-Research-Computing,2025-10-10 13:37:16.0000000,HPC Other Issue: Students lack global access in the common directory in pi_mkann lab space on CHIP,resolved,Danielle Esposito,desposi1,Petra Tembei,ptembei1,ptembei1@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Petra,</p>  <p>Students in your group&nbsp;<em>do</em>&nbsp;have access to the common directory of the research volume. I have tested permissions with multiple student accounts, and there are no issues with the common directory. Please share exactly what you are attempting to do, and what directory you are in.&nbsp;</p>  <p>Please see below:</p>  <p>[ptembei1@chip-login2 ~]$ pi_mkann_common<br /> [ptembei1@chip-login2 common]$ pwd<br /> /umbc/rs/pi_mkann/common<br /> [ptembei1@chip-login2 common]$ touch test<br /> [ptembei1@chip-login2 common]$ ls<br /> dbraw&nbsp; downloaded_data&nbsp; Projects&nbsp; test<br /> [ptembei1@chip-login2 common]$</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Thu Oct 09 18:22:47 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Petra Last Name:                 Tembei Email:                     ptembei1@umbc.edu Campus ID:                 WF57450  Request Type:              High Performance Cluster  I work in Dr. Kann&rsquo;s lab, where we collaborate using shared directories. Without global rwx access to the common directory, we&rsquo;re unable to move files, create subdirectories, or delete files that are no longer needed in directories created by other users. We would really appreciate your help with this.  </pre> </blockquote> </div> "
3289431,72207546,Correspond,DoIT-Research-Computing,2025-10-10 15:33:35.0000000,HPC Other Issue: Students lack global access in the common directory in pi_mkann lab space on CHIP,resolved,Danielle Esposito,desposi1,Petra Tembei,ptembei1,ptembei1@umbc.edu,Petra Tembei,ptembei1@umbc.edu,"Hi Danielle,  I just tried again and attached a screenshot of my attempt. I=E2=80=99m abl= e to create and work freely within folders that I created inside the *common* directory. However, I can=E2=80=99t perform any operations in subfolders wi= thin *common* that I didn=E2=80=99t create.  For example, the folder *downloaded_data* was created by my PI, Dr. Kann. To illustrate the issue, I tried creating a test folder within *downloaded_data*, but I was denied permission. I also tried moving some data into a subfolder within *downloaded_data* and received the same error.  Everything works fine when I=E2=80=99m working within folders I created mys= elf in *common*. This is what I mean by a global access issue in the *common* directory.  Best, Petra   On Fri, Oct 10, 2025 at 9:37=E2=80=AFAM Danielle Esposito via RT < UMBCHelp@rt.umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3289431 > > > Last Update From Ticket: > > Hi Petra, > > Students in your group do have access to the common directory of the > research > volume. I have tested permissions with multiple student accounts, and > there are > no issues with the common directory. Please share exactly what you are > attempting to do, and what directory you are in. > > Please see below: > > [ptembei1@chip-login2 ~]$ pi_mkann_common > [ptembei1@chip-login2 common]$ pwd > /umbc/rs/pi_mkann/common > [ptembei1@chip-login2 common]$ touch test > [ptembei1@chip-login2 common]$ ls > dbraw downloaded_data Projects test > [ptembei1@chip-login2 common]$ > > -- > > Kind regards, > Danielle Esposito (she/her/hers) > DoIT Unix Infra Student Worker > > On Thu Oct 09 18:22:47 2025, ZZ99999 wrote: > > > First Name:                Petra > > Last Name:                 Tembei > > Email:                     ptembei1@umbc.edu > > Campus ID:                 WF57450 > > > > Request Type:              High Performance Cluster > > > > I work in Dr. Kann=E2=80=99s lab, where we collaborate using shared dir= ectories. > Without global rwx access to the common directory, we=E2=80=99re unable t= o move > files, create subdirectories, or delete files that are no longer needed in > directories created by other users. We would really appreciate your help > with this. > > > "
3289431,72207546,Correspond,DoIT-Research-Computing,2025-10-10 15:33:35.0000000,HPC Other Issue: Students lack global access in the common directory in pi_mkann lab space on CHIP,resolved,Danielle Esposito,desposi1,Petra Tembei,ptembei1,ptembei1@umbc.edu,Petra Tembei,ptembei1@umbc.edu,"<div dir=3D""ltr""><div><p>Hi Danielle,</p> <p>I just tried again and attached a screenshot of my attempt. I=E2=80=99m = able to create and work freely within folders that I created inside the <em= >common</em> directory. However, I can=E2=80=99t perform any operations in = subfolders within <em>common</em> that I didn=E2=80=99t create.</p> <p>For example, the folder <em>downloaded_data</em> was created by my PI, D= r. Kann. To illustrate the issue, I tried creating a test folder within <em= >downloaded_data</em>, but I was denied permission. I also tried moving som= e data into a subfolder within <em>downloaded_data</em> and received the sa= me error.</p> <p>Everything works fine when I=E2=80=99m working within folders I created = myself in <em>common</em>. This is what I mean by a global access issue in = the <em>common</em> directory.</p> <p>Best,<br> Petra</p></div><div><br></div></div><br><div class=3D""gmail_quote gmail_quo= te_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Fri, Oct 10, 2025 at=  9:37=E2=80=AFAM Danielle Esposito via RT &lt;<a href=3D""mailto:UMBCHelp@rt= .umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class= =3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rg= b(204,204,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc= .edu/Ticket/Display.html?id=3D3289431"" rel=3D""noreferrer"" target=3D""_blank""= >https://rt.umbc.edu/Ticket/Display.html?id=3D3289431</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Petra,<br> <br> Students in your group do have access to the common directory of the resear= ch<br> volume. I have tested permissions with multiple student accounts, and there=  are<br> no issues with the common directory. Please share exactly what you are<br> attempting to do, and what directory you are in.<br> <br> Please see below:<br> <br> [ptembei1@chip-login2 ~]$ pi_mkann_common<br> [ptembei1@chip-login2 common]$ pwd<br> /umbc/rs/pi_mkann/common<br> [ptembei1@chip-login2 common]$ touch test<br> [ptembei1@chip-login2 common]$ ls<br> dbraw downloaded_data Projects test<br> [ptembei1@chip-login2 common]$<br> <br> --<br> <br> Kind regards,<br> Danielle Esposito (she/her/hers)<br> DoIT Unix Infra Student Worker<br> <br> On Thu Oct 09 18:22:47 2025, ZZ99999 wrote:<br> <br> &gt; First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Pet= ra<br> &gt; Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0Tembei<br> &gt; Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 =C2=A0<a href=3D""mailto:ptembei1@umbc.edu"" target=3D""_blank"">ptembei= 1@umbc.edu</a><br> &gt; Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0WF57450<br> &gt; <br> &gt; Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Per= formance Cluster<br> &gt; <br> &gt; I work in Dr. Kann=E2=80=99s lab, where we collaborate using shared di= rectories. Without global rwx access to the common directory, we=E2=80=99re=  unable to move files, create subdirectories, or delete files that are no l= onger needed in directories created by other users. We would really appreci= ate your help with this.<br> <br> <br> </blockquote></div> "
3289431,72209847,Correspond,DoIT-Research-Computing,2025-10-10 16:36:14.0000000,HPC Other Issue: Students lack global access in the common directory in pi_mkann lab space on CHIP,resolved,Danielle Esposito,desposi1,Petra Tembei,ptembei1,ptembei1@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Petra,</p>  <p>When you have a chance, try again. It should all be working now. Let me = know if it works as expected!</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Fri Oct 10 11:33:35 2025, WF57450 wrote: <blockquote> <div> <div> <p>Hi Danielle,</p>  <p>I just tried again and attached a screenshot of my attempt. I&rsquo;m ab= le to create and work freely within folders that I created inside the <em>c= ommon</em> directory. However, I can&rsquo;t perform any operations in subf= olders within <em>common</em> that I didn&rsquo;t create.</p>  <p>For example, the folder <em>downloaded_data</em> was created by my PI, D= r. Kann. To illustrate the issue, I tried creating a test folder within <em= >downloaded_data</em>, but I was denied permission. I also tried moving som= e data into a subfolder within <em>downloaded_data</em> and received the sa= me error.</p>  <p>Everything works fine when I&rsquo;m working within folders I created my= self in <em>common</em>. This is what I mean by a global access issue in th= e <em>common</em> directory.</p>  <p>Best,<br /> Petra</p> </div>  <div>&nbsp;</div> </div> &nbsp;  <div> <div>On Fri, Oct 10, 2025 at 9:37=E2=80=AFAM Danielle Esposito via RT &lt;U= MBCHelp@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 89431 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hi Petra,<br /> <br /> Students in your group do have access to the common directory of the resear= ch<br /> volume. I have tested permissions with multiple student accounts, and there=  are<br /> no issues with the common directory. Please share exactly what you are<br /> attempting to do, and what directory you are in.<br /> <br /> Please see below:<br /> <br /> [ptembei1@chip-login2 ~]$ pi_mkann_common<br /> [ptembei1@chip-login2 common]$ pwd<br /> /umbc/rs/pi_mkann/common<br /> [ptembei1@chip-login2 common]$ touch test<br /> [ptembei1@chip-login2 common]$ ls<br /> dbraw downloaded_data Projects test<br /> [ptembei1@chip-login2 common]$<br /> <br /> --<br /> <br /> Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker<br /> <br /> On Thu Oct 09 18:22:47 2025, ZZ99999 wrote:<br /> <br /> &gt; First Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Pet= ra<br /> &gt; Last Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p;Tembei<br /> &gt; Email:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &= nbsp; &nbsp;ptembei1@umbc.edu<br /> &gt; Campus ID:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p;WF57450<br /> &gt;<br /> &gt; Request Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; High Per= formance Cluster<br /> &gt;<br /> &gt; I work in Dr. Kann&rsquo;s lab, where we collaborate using shared dire= ctories. Without global rwx access to the common directory, we&rsquo;re una= ble to move files, create subdirectories, or delete files that are no longe= r needed in directories created by other users. We would really appreciate = your help with this.<br /> <br /> &nbsp;</blockquote> </div> </blockquote> </div> "
3289431,72210700,Correspond,DoIT-Research-Computing,2025-10-10 17:08:15.0000000,HPC Other Issue: Students lack global access in the common directory in pi_mkann lab space on CHIP,resolved,Danielle Esposito,desposi1,Petra Tembei,ptembei1,ptembei1@umbc.edu,Petra Tembei,ptembei1@umbc.edu,"Hello Danielle,  It works for me now. Thank you so much for your help.  Best, Petra Tembei  On Fri, Oct 10, 2025 at 12:36=E2=80=AFPM Danielle Esposito via RT < UMBCHelp@rt.umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3289431 > > > Last Update From Ticket: > > Hi Petra, > > When you have a chance, try again. It should all be working now. Let me > know if > it works as expected! > > -- > > Kind regards, > Danielle Esposito (she/her/hers) > DoIT Unix Infra Student Worker > > On Fri Oct 10 11:33:35 2025, WF57450 wrote: > > > Hi Danielle, > > > I just tried again and attached a screenshot of my attempt. I=E2=80=99m=  able to > > create and work freely within folders that I created inside the common > > directory. However, I can=E2=80=99t perform any operations in subfolder= s within > > common that I didn=E2=80=99t create. > > > For example, the folder downloaded_data was created by my PI, Dr. Kann. > To > > illustrate the issue, I tried creating a test folder within > downloaded_data, > > but I was denied permission. I also tried moving some data into a > subfolder > > within downloaded_data and received the same error. > > > Everything works fine when I=E2=80=99m working within folders I created=  myself in > > common. This is what I mean by a global access issue in the common > > directory. > > > Best, > > Petra > > > On Fri, Oct 10, 2025 at 9:37 AM Danielle Esposito via RT > > <UMBCHelp@rt.umbc.edu> wrote: > > >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3289431 > > > >> Last Update From Ticket: > > >> Hi Petra, > > >> Students in your group do have access to the common directory of the > >> research > >> volume. I have tested permissions with multiple student accounts, and > >> there are > >> no issues with the common directory. Please share exactly what you are > >> attempting to do, and what directory you are in. > > >> Please see below: > > >> [ptembei1@chip-login2 ~]$ pi_mkann_common > >> [ptembei1@chip-login2 common]$ pwd > >> /umbc/rs/pi_mkann/common > >> [ptembei1@chip-login2 common]$ touch test > >> [ptembei1@chip-login2 common]$ ls > >> dbraw downloaded_data Projects test > >> [ptembei1@chip-login2 common]$ > > >> -- > > >> Kind regards, > >> Danielle Esposito (she/her/hers) > >> DoIT Unix Infra Student Worker > > >> On Thu Oct 09 18:22:47 2025, ZZ99999 wrote: > > >> > First Name: Petra > >> > Last Name: Tembei > >> > Email: ptembei1@umbc.edu > >> > Campus ID: WF57450 > >> > > >> > Request Type: High Performance Cluster > >> > > >> > I work in Dr. Kann=E2=80=99s lab, where we collaborate using shared > >> directories. Without global rwx access to the common directory, we=E2= =80=99re > >> unable to move files, create subdirectories, or delete files that are > >> no longer needed in directories created by other users. We would really > >> appreciate your help with this. > > "
3289431,72210700,Correspond,DoIT-Research-Computing,2025-10-10 17:08:15.0000000,HPC Other Issue: Students lack global access in the common directory in pi_mkann lab space on CHIP,resolved,Danielle Esposito,desposi1,Petra Tembei,ptembei1,ptembei1@umbc.edu,Petra Tembei,ptembei1@umbc.edu,"<div dir=3D""ltr"">Hello Danielle,<div><br></div><div>It works for me now. Th= ank you so much for your help.</div><div><br></div><div>Best,</div><div>Pet= ra Tembei</div></div><br><div class=3D""gmail_quote gmail_quote_container""><= div dir=3D""ltr"" class=3D""gmail_attr"">On Fri, Oct 10, 2025 at 12:36=E2=80=AF= PM Danielle Esposito via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMB= CHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote""=  style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);p= adding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Disp= lay.html?id=3D3289431"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc= .edu/Ticket/Display.html?id=3D3289431</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Petra,<br> <br> When you have a chance, try again. It should all be working now. Let me kno= w if<br> it works as expected!<br> <br> --<br> <br> Kind regards,<br> Danielle Esposito (she/her/hers)<br> DoIT Unix Infra Student Worker<br> <br> On Fri Oct 10 11:33:35 2025, WF57450 wrote:<br> <br> &gt; Hi Danielle,<br> <br> &gt; I just tried again and attached a screenshot of my attempt. I=E2=80=99= m able to<br> &gt; create and work freely within folders that I created inside the common= <br> &gt; directory. However, I can=E2=80=99t perform any operations in subfolde= rs within<br> &gt; common that I didn=E2=80=99t create.<br> <br> &gt; For example, the folder downloaded_data was created by my PI, Dr. Kann= . To<br> &gt; illustrate the issue, I tried creating a test folder within downloaded= _data,<br> &gt; but I was denied permission. I also tried moving some data into a subf= older<br> &gt; within downloaded_data and received the same error.<br> <br> &gt; Everything works fine when I=E2=80=99m working within folders I create= d myself in<br> &gt; common. This is what I mean by a global access issue in the common<br> &gt; directory.<br> <br> &gt; Best,<br> &gt; Petra<br> <br> &gt; On Fri, Oct 10, 2025 at 9:37 AM Danielle Esposito via RT<br> &gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp= @rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html= ?id=3D3289431"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tic= ket/Display.html?id=3D3289431</a> &gt;<br> <br> &gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt; Hi Petra,<br> <br> &gt;&gt; Students in your group do have access to the common directory of t= he<br> &gt;&gt; research<br> &gt;&gt; volume. I have tested permissions with multiple student accounts, = and<br> &gt;&gt; there are<br> &gt;&gt; no issues with the common directory. Please share exactly what you=  are<br> &gt;&gt; attempting to do, and what directory you are in.<br> <br> &gt;&gt; Please see below:<br> <br> &gt;&gt; [ptembei1@chip-login2 ~]$ pi_mkann_common<br> &gt;&gt; [ptembei1@chip-login2 common]$ pwd<br> &gt;&gt; /umbc/rs/pi_mkann/common<br> &gt;&gt; [ptembei1@chip-login2 common]$ touch test<br> &gt;&gt; [ptembei1@chip-login2 common]$ ls<br> &gt;&gt; dbraw downloaded_data Projects test<br> &gt;&gt; [ptembei1@chip-login2 common]$<br> <br> &gt;&gt; --<br> <br> &gt;&gt; Kind regards,<br> &gt;&gt; Danielle Esposito (she/her/hers)<br> &gt;&gt; DoIT Unix Infra Student Worker<br> <br> &gt;&gt; On Thu Oct 09 18:22:47 2025, ZZ99999 wrote:<br> <br> &gt;&gt; &gt; First Name: Petra<br> &gt;&gt; &gt; Last Name: Tembei<br> &gt;&gt; &gt; Email: <a href=3D""mailto:ptembei1@umbc.edu"" target=3D""_blank""= >ptembei1@umbc.edu</a><br> &gt;&gt; &gt; Campus ID: WF57450<br> &gt;&gt; &gt;<br> &gt;&gt; &gt; Request Type: High Performance Cluster<br> &gt;&gt; &gt;<br> &gt;&gt; &gt; I work in Dr. Kann=E2=80=99s lab, where we collaborate using = shared<br> &gt;&gt; directories. Without global rwx access to the common directory, we= =E2=80=99re<br> &gt;&gt; unable to move files, create subdirectories, or delete files that = are<br> &gt;&gt; no longer needed in directories created by other users. We would r= eally<br> &gt;&gt; appreciate your help with this.<br> <br> </blockquote></div> "
3289431,72211306,Correspond,DoIT-Research-Computing,2025-10-10 17:33:55.0000000,HPC Other Issue: Students lack global access in the common directory in pi_mkann lab space on CHIP,resolved,Danielle Esposito,desposi1,Petra Tembei,ptembei1,ptembei1@umbc.edu,Maricel Kann,mkann@umbc.edu,"<html><head><meta http-equiv=3D""content-type"" content=3D""text/html; charset= =3Dutf-8""></head><body dir=3D""auto"">Thank you all for troubleshooting&nbsp;= <div>M<br id=3D""lineBreakAtBeginningOfSignature""><div dir=3D""ltr"">Sent from=  my iPhone</div><div dir=3D""ltr""><br><blockquote type=3D""cite"">On Oct 10, 2= 025, at 1:08=E2=80=AFPM, Petra Tembei &lt;ptembei1@umbc.edu&gt; wrote:<br><= br></blockquote></div><blockquote type=3D""cite""><div dir=3D""ltr"">=EF=BB=BF<= div dir=3D""ltr"">Hello Danielle,<div><br></div><div>It works for me now. Tha= nk you so much for your help.</div><div><br></div><div>Best,</div><div>Petr= a Tembei</div></div><br><div class=3D""gmail_quote gmail_quote_container""><d= iv dir=3D""ltr"" class=3D""gmail_attr"">On Fri, Oct 10, 2025 at 12:36=E2=80=AFP= M Danielle Esposito via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBC= Help@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" = style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);pa= dding-left:1ex"">Ticket &lt;URL: <a href=3D""https://www.google.com/url?q=3Dh= ttps://rt.umbc.edu/Ticket/Display.html?id%3D3289431&amp;source=3Dgmail-imap= &amp;ust=3D1760720897000000&amp;usg=3DAOvVaw3DIZGsaazTTSruQm0v_0s7"" rel=3D""= noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D= 3289431</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Petra,<br> <br> When you have a chance, try again. It should all be working now. Let me kno= w if<br> it works as expected!<br> <br> --<br> <br> Kind regards,<br> Danielle Esposito (she/her/hers)<br> DoIT Unix Infra Student Worker<br> <br> On Fri Oct 10 11:33:35 2025, WF57450 wrote:<br> <br> &gt; Hi Danielle,<br> <br> &gt; I just tried again and attached a screenshot of my attempt. I=E2=80=99= m able to<br> &gt; create and work freely within folders that I created inside the common= <br> &gt; directory. However, I can=E2=80=99t perform any operations in subfolde= rs within<br> &gt; common that I didn=E2=80=99t create.<br> <br> &gt; For example, the folder downloaded_data was created by my PI, Dr. Kann= . To<br> &gt; illustrate the issue, I tried creating a test folder within downloaded= _data,<br> &gt; but I was denied permission. I also tried moving some data into a subf= older<br> &gt; within downloaded_data and received the same error.<br> <br> &gt; Everything works fine when I=E2=80=99m working within folders I create= d myself in<br> &gt; common. This is what I mean by a global access issue in the common<br> &gt; directory.<br> <br> &gt; Best,<br> &gt; Petra<br> <br> &gt; On Fri, Oct 10, 2025 at 9:37 AM Danielle Esposito via RT<br> &gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp= @rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; Ticket &lt;URL: <a href=3D""https://www.google.com/url?q=3Dhttps://= rt.umbc.edu/Ticket/Display.html?id%3D3289431&amp;source=3Dgmail-imap&amp;us= t=3D1760720897000000&amp;usg=3DAOvVaw3DIZGsaazTTSruQm0v_0s7"" rel=3D""norefer= rer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3289431= </a> &gt;<br> <br> &gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt; Hi Petra,<br> <br> &gt;&gt; Students in your group do have access to the common directory of t= he<br> &gt;&gt; research<br> &gt;&gt; volume. I have tested permissions with multiple student accounts, = and<br> &gt;&gt; there are<br> &gt;&gt; no issues with the common directory. Please share exactly what you=  are<br> &gt;&gt; attempting to do, and what directory you are in.<br> <br> &gt;&gt; Please see below:<br> <br> &gt;&gt; [ptembei1@chip-login2 ~]$ pi_mkann_common<br> &gt;&gt; [ptembei1@chip-login2 common]$ pwd<br> &gt;&gt; /umbc/rs/pi_mkann/common<br> &gt;&gt; [ptembei1@chip-login2 common]$ touch test<br> &gt;&gt; [ptembei1@chip-login2 common]$ ls<br> &gt;&gt; dbraw downloaded_data Projects test<br> &gt;&gt; [ptembei1@chip-login2 common]$<br> <br> &gt;&gt; --<br> <br> &gt;&gt; Kind regards,<br> &gt;&gt; Danielle Esposito (she/her/hers)<br> &gt;&gt; DoIT Unix Infra Student Worker<br> <br> &gt;&gt; On Thu Oct 09 18:22:47 2025, ZZ99999 wrote:<br> <br> &gt;&gt; &gt; First Name: Petra<br> &gt;&gt; &gt; Last Name: Tembei<br> &gt;&gt; &gt; Email: <a href=3D""mailto:ptembei1@umbc.edu"" target=3D""_blank""= >ptembei1@umbc.edu</a><br> &gt;&gt; &gt; Campus ID: WF57450<br> &gt;&gt; &gt;<br> &gt;&gt; &gt; Request Type: High Performance Cluster<br> &gt;&gt; &gt;<br> &gt;&gt; &gt; I work in Dr. Kann=E2=80=99s lab, where we collaborate using = shared<br> &gt;&gt; directories. Without global rwx access to the common directory, we= =E2=80=99re<br> &gt;&gt; unable to move files, create subdirectories, or delete files that = are<br> &gt;&gt; no longer needed in directories created by other users. We would r= eally<br> &gt;&gt; appreciate your help with this.<br> <br> </blockquote></div> </div></blockquote></div></body></html>= "
3289503,72198525,Create,DoIT-Research-Computing,2025-10-10 04:37:13.0000000,"HPC User Account: ansukhin in Center for Navigation, Timing & Frequency Research",resolved,Elliot Gobbert,elliotg2,Alexey Sukhinin,ansukhin,ansukhin@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Alexey Last Name:                 Sukhinin Email:                     ansukhin@umbc.edu Campus ID:                 WL72131  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    menuk@umbc.edu Existing Group:       Center for Navigation, Timing & Frequency Research Project Title:        Generation of Frequency Combs by coupled resonators Project Abstract:     Numerical simulation of coupled Lugiato=E2=80=93Lefev= er equations to produce soliton propagation for over 100K roundtrips in ord= er to generate two color frequency combs.  Please consider this request ASAP. Thank you so much. Alexey.  "
3289503,72204275,Correspond,DoIT-Research-Computing,2025-10-10 14:13:59.0000000,"HPC User Account: ansukhin in Center for Navigation, Timing & Frequency Research",resolved,Elliot Gobbert,elliotg2,Alexey Sukhinin,ansukhin,ansukhin@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hi there,</p>  <p>Sounds good, a couple of notes though:</p>  <p>1. I assumed you misspelled &quot;menyuk&quot;, so I assume you&#39;re talking about&nbsp;Curtis Menyuk? Hope that&#39;s correct.</p>  <p>2. We need the PI&#39;s permission first, so cc&#39;ing Curtis to this, all he has to do is reply to this with an &quot;I approve&quot; or something similar, and we can get rolling.</p>  <p>&nbsp;</p>  <p>Let me know if my assumption about &quot;menyuk&quot; was incorrect, and I&#39;ll correct the pi name.</p>  <p>Best,</p>  <p>Elliot Gobbert</p> "
3289503,72207420,Correspond,DoIT-Research-Computing,2025-10-10 15:29:27.0000000,"HPC User Account: ansukhin in Center for Navigation, Timing & Frequency Research",resolved,Elliot Gobbert,elliotg2,Alexey Sukhinin,ansukhin,ansukhin@umbc.edu,Alexey Sukhinin,ansukhin@umbc.edu,"Dear Elliot,  Yes, the PI is Professor Curtis Menyuk.=20  Alexey  > On Oct 10, 2025, at 10:14=E2=80=AFAM, Elliot Gobbert via RT <UMBCHelp@rt.= umbc.edu> wrote: >=20 > Ticket <URL: https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Di= splay.html?id%3D3289503&source=3Dgmail-imap&ust=3D1760710443000000&usg=3DAO= vVaw0FuOStRmhFBQ9Ey9ctYaHQ > >=20 > Last Update From Ticket: >=20 > Hi there, >=20 > Sounds good, a couple of notes though: >=20 > 1. I assumed you misspelled ""menyuk"", so I assume you're talking about Cu= rtis > Menyuk? Hope that's correct. >=20 > 2. We need the PI's permission first, so cc'ing Curtis to this, all he ha= s to > do is reply to this with an ""I approve"" or something similar, and we can = get > rolling. >=20 > Let me know if my assumption about ""menyuk"" was incorrect, and I'll corre= ct the > pi name. >=20 > Best, >=20 > Elliot Gobbert >=20   "
3289503,72210983,Correspond,DoIT-Research-Computing,2025-10-10 17:20:17.0000000,"HPC User Account: ansukhin in Center for Navigation, Timing & Frequency Research",resolved,Elliot Gobbert,elliotg2,Alexey Sukhinin,ansukhin,ansukhin@umbc.edu,Curtis Menyuk,menyuk@umbc.edu,"<div dir=3D""ltr""><div>I approve this request.=C2=A0 ---CRM</div><div><br></= div><div><div dir=3D""ltr"" class=3D""gmail_signature"" data-smartmail=3D""gmail= _signature""><div dir=3D""ltr"">Curtis R. Menyuk, Professor and Director<div>C= omputer Science and Electrical Engineering Department</div><div>Center for = Navigation, Time and Frequency Research (Centavr)</div><div>University of M= aryland Baltimore County</div><div>Baltimore, MD 21250</div><div>Email: =C2= =A0<a href=3D""mailto:menyuk@umbc.edu"" target=3D""_blank"">menyuk@umbc.edu</a>= ; =C2=A0URL: =C2=A0<a href=3D""http://www.umbc.edu/photonics"" target=3D""_bla= nk"">www.umbc.edu/photonics</a></div><div>Phone: =C2=A0410-455-3501; =C2=A0F= ax: =C2=A0410-455-6500</div><div>Pronouns:=C2=A0 <i>he, him, his</i></div><= /div></div></div><br></div><br><div class=3D""gmail_quote gmail_quote_contai= ner""><div dir=3D""ltr"" class=3D""gmail_attr"">On Fri, Oct 10, 2025 at 11:29=E2= =80=AFAM Alexey Sukhinin via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu""= >UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_qu= ote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,20= 4);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/= Display.html?id=3D3289503"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.= umbc.edu/Ticket/Display.html?id=3D3289503</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Dear Elliot,<br> <br> Yes, the PI is Professor Curtis Menyuk. <br> <br> Alexey<br> <br> &gt; On Oct 10, 2025, at 10:14=E2=80=AFAM, Elliot Gobbert via RT &lt;<a hre= f=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a= >&gt; wrote:<br> &gt; <br> &gt; Ticket &lt;URL: <a href=3D""https://www.google.com/url?q=3Dhttps://rt.u= mbc.edu/Ticket/Display.html?id%3D3289503&amp;source=3Dgmail-imap&amp;ust=3D= 1760710443000000&amp;usg=3DAOvVaw0FuOStRmhFBQ9Ey9ctYaHQ"" rel=3D""noreferrer""=  target=3D""_blank"">https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticke= t/Display.html?id%3D3289503&amp;source=3Dgmail-imap&amp;ust=3D1760710443000= 000&amp;usg=3DAOvVaw0FuOStRmhFBQ9Ey9ctYaHQ</a> &gt;<br> &gt; <br> &gt; Last Update From Ticket:<br> &gt; <br> &gt; Hi there,<br> &gt; <br> &gt; Sounds good, a couple of notes though:<br> &gt; <br> &gt; 1. I assumed you misspelled &quot;menyuk&quot;, so I assume you&#39;re=  talking about Curtis<br> &gt; Menyuk? Hope that&#39;s correct.<br> &gt; <br> &gt; 2. We need the PI&#39;s permission first, so cc&#39;ing Curtis to this= , all he has to<br> &gt; do is reply to this with an &quot;I approve&quot; or something similar= , and we can get<br> &gt; rolling.<br> &gt; <br> &gt; Let me know if my assumption about &quot;menyuk&quot; was incorrect, a= nd I&#39;ll correct the<br> &gt; pi name.<br> &gt; <br> &gt; Best,<br> &gt; <br> &gt; Elliot Gobbert<br> &gt; <br> <br> <br> <br> </blockquote></div> "
3289503,72210983,Correspond,DoIT-Research-Computing,2025-10-10 17:20:17.0000000,"HPC User Account: ansukhin in Center for Navigation, Timing & Frequency Research",resolved,Elliot Gobbert,elliotg2,Alexey Sukhinin,ansukhin,ansukhin@umbc.edu,Curtis Menyuk,menyuk@umbc.edu,"I approve this request.  ---CRM  Curtis R. Menyuk, Professor and Director Computer Science and Electrical Engineering Department Center for Navigation, Time and Frequency Research (Centavr) University of Maryland Baltimore County Baltimore, MD 21250 Email:  menyuk@umbc.edu;  URL:  www.umbc.edu/photonics Phone:  410-455-3501;  Fax:  410-455-6500 Pronouns:  *he, him, his*   On Fri, Oct 10, 2025 at 11:29=E2=80=AFAM Alexey Sukhinin via RT < UMBCHelp@rt.umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3289503 > > > Last Update From Ticket: > > Dear Elliot, > > Yes, the PI is Professor Curtis Menyuk. > > Alexey > > > On Oct 10, 2025, at 10:14=E2=80=AFAM, Elliot Gobbert via RT < > UMBCHelp@rt.umbc.edu> wrote: > > > > Ticket <URL: > https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id= %3D3289503&source=3Dgmail-imap&ust=3D1760710443000000&usg=3DAOvVaw0FuOStRmh= FBQ9Ey9ctYaHQ > > > > > > Last Update From Ticket: > > > > Hi there, > > > > Sounds good, a couple of notes though: > > > > 1. I assumed you misspelled ""menyuk"", so I assume you're talking about > Curtis > > Menyuk? Hope that's correct. > > > > 2. We need the PI's permission first, so cc'ing Curtis to this, all he > has to > > do is reply to this with an ""I approve"" or something similar, and we can > get > > rolling. > > > > Let me know if my assumption about ""menyuk"" was incorrect, and I'll > correct the > > pi name. > > > > Best, > > > > Elliot Gobbert > > > > > > "
3289503,72211007,Correspond,DoIT-Research-Computing,2025-10-10 17:21:30.0000000,"HPC User Account: ansukhin in Center for Navigation, Timing & Frequency Research",resolved,Elliot Gobbert,elliotg2,Alexey Sukhinin,ansukhin,ansukhin@umbc.edu,Curtis Menyuk,menyuk@umbc.edu,"I approve.  ---CRM  Curtis R. Menyuk, Professor and Director Computer Science and Electrical Engineering Department Center for Navigation, Time and Frequency Research (Centavr) University of Maryland Baltimore County Baltimore, MD 21250 Email:  menyuk@umbc.edu;  URL:  www.umbc.edu/photonics Phone:  410-455-3501;  Fax:  410-455-6500 Pronouns:  *he, him, his*   On Fri, Oct 10, 2025 at 10:14=E2=80=AFAM Elliot Gobbert via RT <UMBCHelp@rt= .umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3289503 > > > Last Update From Ticket: > > Hi there, > > Sounds good, a couple of notes though: > > 1. I assumed you misspelled ""menyuk"", so I assume you're talking about > Curtis > Menyuk? Hope that's correct. > > 2. We need the PI's permission first, so cc'ing Curtis to this, all he has > to > do is reply to this with an ""I approve"" or something similar, and we can > get > rolling. > > Let me know if my assumption about ""menyuk"" was incorrect, and I'll > correct the > pi name. > > Best, > > Elliot Gobbert > > > "
3289503,72211007,Correspond,DoIT-Research-Computing,2025-10-10 17:21:30.0000000,"HPC User Account: ansukhin in Center for Navigation, Timing & Frequency Research",resolved,Elliot Gobbert,elliotg2,Alexey Sukhinin,ansukhin,ansukhin@umbc.edu,Curtis Menyuk,menyuk@umbc.edu,"<div dir=3D""ltr""><div>I approve.=C2=A0 ---CRM</div><div><br></div><div><div=  dir=3D""ltr"" class=3D""gmail_signature"" data-smartmail=3D""gmail_signature""><= div dir=3D""ltr"">Curtis R. Menyuk, Professor and Director<div>Computer Scien= ce and Electrical Engineering Department</div><div>Center for Navigation, T= ime and Frequency Research (Centavr)</div><div>University of Maryland Balti= more County</div><div>Baltimore, MD 21250</div><div>Email: =C2=A0<a href=3D= ""mailto:menyuk@umbc.edu"" target=3D""_blank"">menyuk@umbc.edu</a>; =C2=A0URL: = =C2=A0<a href=3D""http://www.umbc.edu/photonics"" target=3D""_blank"">www.umbc.= edu/photonics</a></div><div>Phone: =C2=A0410-455-3501; =C2=A0Fax: =C2=A0410= -455-6500</div><div>Pronouns:=C2=A0 <i>he, him, his</i></div></div></div></= div><br></div><br><div class=3D""gmail_quote gmail_quote_container""><div dir= =3D""ltr"" class=3D""gmail_attr"">On Fri, Oct 10, 2025 at 10:14=E2=80=AFAM Elli= ot Gobbert via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.u= mbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""= margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-lef= t:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?i= d=3D3289503"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticke= t/Display.html?id=3D3289503</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi there,<br> <br> Sounds good, a couple of notes though:<br> <br> 1. I assumed you misspelled &quot;menyuk&quot;, so I assume you&#39;re talk= ing about Curtis<br> Menyuk? Hope that&#39;s correct.<br> <br> 2. We need the PI&#39;s permission first, so cc&#39;ing Curtis to this, all=  he has to<br> do is reply to this with an &quot;I approve&quot; or something similar, and=  we can get<br> rolling.<br> <br> Let me know if my assumption about &quot;menyuk&quot; was incorrect, and I&= #39;ll correct the<br> pi name.<br> <br> Best,<br> <br> Elliot Gobbert<br> <br> <br> </blockquote></div> "
3289503,72214706,Correspond,DoIT-Research-Computing,2025-10-10 19:14:02.0000000,"HPC User Account: ansukhin in Center for Navigation, Timing & Frequency Research",resolved,Elliot Gobbert,elliotg2,Alexey Sukhinin,ansukhin,ansukhin@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hi Alexey,</p>  <p>Your account (ansukhin) has been created on chip.rs.umbc.edu.<br /> Your primary group is pi_menyuk.<br /> Your home directory has 500M of storage.<br /> You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip<br /> Please read through the documentation found at hpcf.umbc.edu &gt; User Support.<br /> All available modules can be viewed using the command &#39;module avail&#39;.<br /> Please submit additional questions or issues as separate tickets via the following link.<br /> (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/)<br /> <br /> Let me know if you have any more questions,</p>  <p>Elliot Gobbert</p> "
3289659,72204453,Create,DoIT-Research-Computing,2025-10-10 14:18:16.0000000,RCD Consult: Deep Lab Cut,resolved,Max Breitmeyer,mb17,Kathleen Hoffman,khoffman,khoffman@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Kathleen Last Name:                 Hoffman Email:                     khoffman@umbc.edu Campus ID:                 AT36888  Request Type:              Research Computing & Data Consultation  Good Morning,  I am a Prof is the Math/Stat department. My area of research is not high performance computing, but I am working on a project that may need access to a machine with GPUs.  We need to use software called Deep Lab Cut https://deeplabcut.github.io/DeepLabCut/docs/installation.html to analyze experimental data. As I understand it, the first step is to use Deep Lab Cut on a laptop to recreate an *.eml file. That file would then be used within a batch job on a GPU computer to generate other files. I have two undergraduate students who will be working on this project. I am also collaborating with a postdoc at JHU. I am not sure where to begin, which is why I am submitting this request.   Thanks,  Kathleen Hoffman      "
3289659,72210707,Correspond,DoIT-Research-Computing,2025-10-10 17:08:43.0000000,RCD Consult: Deep Lab Cut,resolved,Max Breitmeyer,mb17,Kathleen Hoffman,khoffman,khoffman@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Kathleen,</p>  <p>First we&#39;ll have you start by putting a group request. Documentation on how to do that can be found here:&nbsp;https://umbc.atlassian.net/wiki/spaces/faq/pages/1219461147/Connecting#Requesting-a-User-Account-on-chip, with a link to the RT form to request a group being found here:&nbsp;https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research%20Computing</p>  <p>Additionally, once you&#39;ve been set up on the cluster, there is a getting started tutorial that can be found here:&nbsp;https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip</p>  <p>If you have trouble with any of the above let me know and we can walk through it.&nbsp;</p>  <p>On Fri Oct 10 10:18:16 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Kathleen Last Name:                 Hoffman Email:                     khoffman@umbc.edu Campus ID:                 AT36888  Request Type:              Research Computing &amp; Data Consultation  Good Morning,  I am a Prof is the Math/Stat department. My area of research is not high performance computing, but I am working on a project that may need access to a machine with GPUs.  We need to use software called Deep Lab Cut https://deeplabcut.github.io/DeepLabCut/docs/installation.html to analyze experimental data. As I understand it, the first step is to use Deep Lab Cut on a laptop to recreate an *.eml file. That file would then be used within a batch job on a GPU computer to generate other files. I have two undergraduate students who will be working on this project. I am also collaborating with a postdoc at JHU. I am not sure where to begin, which is why I am submitting this request.   Thanks,  Kathleen Hoffman      </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3289659,72223269,Correspond,DoIT-Research-Computing,2025-10-12 20:10:29.0000000,RCD Consult: Deep Lab Cut,resolved,Max Breitmeyer,mb17,Kathleen Hoffman,khoffman,khoffman@umbc.edu,Kathleen Hoffman,khoffman@umbc.edu,"Thanks Max! I have submitted a group request. Please let me know if you need additional information.  Thanks,  Kathleen Hoffman  On Fri, Oct 10, 2025 at 1:08=E2=80=AFPM Max Breitmeyer via RT <UMBCHelp@rt.= umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3289659 > > > Last Update From Ticket: > > Hi Kathleen, > > First we'll have you start by putting a group request. Documentation on > how to > do that can be found here: > > https://umbc.atlassian.net/wiki/spaces/faq/pages/1219461147/Connecting#Re= questing-a-User-Account-on-chip > , > with a link to the RT form to request a group being found here: > > https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=3DRe= search%20Computing > > Additionally, once you've been set up on the cluster, there is a getting > started tutorial that can be found here: > > https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Start= ed+on+chip > > If you have trouble with any of the above let me know and we can walk > through > it. > > On Fri Oct 10 10:18:16 2025, ZZ99999 wrote: > > > First Name:                Kathleen > > Last Name:                 Hoffman > > Email:                     khoffman@umbc.edu > > Campus ID:                 AT36888 > > > > Request Type:              Research Computing & Data Consultation > > > > Good Morning, > >  I am a Prof is the Math/Stat department. My area of research is not > high performance computing, but I am working on a project that may need > access to a machine with GPUs.  We need to use software called Deep Lab C= ut > https://deeplabcut.github.io/DeepLabCut/docs/installation.html > > to analyze experimental data. As I understand it, the first step is to > use Deep Lab Cut on a laptop to recreate an *.eml file. That file would > then be used within a batch job on a GPU computer to generate other files. > I have two undergraduate students who will be working on this project. I = am > also collaborating with a postdoc at JHU. I am not sure where to begin, > which is why I am submitting this request. > > > > Thanks, > > > > Kathleen Hoffman > > > > > > > > > > > -- > > Best, > Max Breitmeyer > DOIT HPC System Administrator > >  --=20 Kathleen Hoffman Associate Dean, College of Natural and Mathematical Sciences Professor of Mathematics and Statistics, UMBC Affiliate Professor of Biological Sciences,UMBC Visiting Professor of Anatomy and Neurophysiology, UMB "
3289659,72223269,Correspond,DoIT-Research-Computing,2025-10-12 20:10:29.0000000,RCD Consult: Deep Lab Cut,resolved,Max Breitmeyer,mb17,Kathleen Hoffman,khoffman,khoffman@umbc.edu,Kathleen Hoffman,khoffman@umbc.edu,"<div dir=3D""ltr"">Thanks Max! I have submitted a group request. Please let m= e know if you need additional information.<div><br></div><div>Thanks,</div>= <div><br></div><div>Kathleen Hoffman</div></div><br><div class=3D""gmail_quo= te gmail_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Fri, Oct=  10, 2025 at 1:08=E2=80=AFPM Max Breitmeyer via RT &lt;<a href=3D""mailto:UM= BCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquot= e class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px s= olid rgb(204,204,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://= rt.umbc.edu/Ticket/Display.html?id=3D3289659"" rel=3D""noreferrer"" target=3D""= _blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3289659</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Kathleen,<br> <br> First we&#39;ll have you start by putting a group request. Documentation on=  how to<br> do that can be found here:<br> <a href=3D""https://umbc.atlassian.net/wiki/spaces/faq/pages/1219461147/Conn= ecting#Requesting-a-User-Account-on-chip"" rel=3D""noreferrer"" target=3D""_bla= nk"">https://umbc.atlassian.net/wiki/spaces/faq/pages/1219461147/Connecting#= Requesting-a-User-Account-on-chip</a>,<br> with a link to the RT form to request a group being found here:<br> <a href=3D""https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?= auto=3DResearch%20Computing"" rel=3D""noreferrer"" target=3D""_blank"">https://r= tforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=3DResearch%20Co= mputing</a><br> <br> Additionally, once you&#39;ve been set up on the cluster, there is a gettin= g<br> started tutorial that can be found here:<br> <a href=3D""https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Gett= ing+Started+on+chip"" rel=3D""noreferrer"" target=3D""_blank"">https://umbc.atla= ssian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip</a><br> <br> If you have trouble with any of the above let me know and we can walk throu= gh<br> it.<br> <br> On Fri Oct 10 10:18:16 2025, ZZ99999 wrote:<br> <br> &gt; First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Kat= hleen<br> &gt; Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0Hoffman<br> &gt; Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 =C2=A0<a href=3D""mailto:khoffman@umbc.edu"" target=3D""_blank"">khoffma= n@umbc.edu</a><br> &gt; Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0AT36888<br> &gt; <br> &gt; Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Research=  Computing &amp; Data Consultation<br> &gt; <br> &gt; Good Morning,<br> &gt;=C2=A0 I am a Prof is the Math/Stat department. My area of research is = not high performance computing, but I am working on a project that may need=  access to a machine with GPUs.=C2=A0 We need to use software called Deep L= ab Cut <a href=3D""https://deeplabcut.github.io/DeepLabCut/docs/installation= .html"" rel=3D""noreferrer"" target=3D""_blank"">https://deeplabcut.github.io/De= epLabCut/docs/installation.html</a><br> &gt; to analyze experimental data. As I understand it, the first step is to=  use Deep Lab Cut on a laptop to recreate an *.eml file. That file would th= en be used within a batch job on a GPU computer to generate other files. I = have two undergraduate students who will be working on this project. I am a= lso collaborating with a postdoc at JHU. I am not sure where to begin, whic= h is why I am submitting this request. <br> &gt; <br> &gt; Thanks,<br> &gt; <br> &gt; Kathleen Hoffman<br> &gt; <br> &gt; <br> &gt; <br> &gt; <br> <br> <br> --<br> <br> Best,<br> Max Breitmeyer<br> DOIT HPC System Administrator<br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><div>Kathleen Hoffman</div><div>Associate Dean, = College of Natural and Mathematical Sciences<br>Professor of Mathematics an= d Statistics, UMBC=C2=A0<br></div><div>Affiliate Professor of Biological Sc= iences,UMBC</div><div>Visiting Professor of Anatomy and Neurophysiology, UM= B</div><div><br></div></div></div> "
3289689,72205705,Create,DoIT-Research-Computing,2025-10-10 14:47:09.0000000,HPC New Group: pi_proutyr1,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Roy Last Name:                 Prouty Email:                     proutyr1@umbc.edu Campus ID:                 WH39335  Request Type:              High Performance Cluster  Group Type:            Project Title:        Sampling strategy and design for Chesapeake Bay habitat assessment Project Abstract:     The Chesapeake Bay has had a large monitoring program for water quality for the last four decades. Given the advancement of continuous monitoring infrastructure, recent studies pointed to the high values of small numbers of in-situ monitoring stations with high temporal frequency to provide a sound basis for water quality assessment. In addition, a coupling of on-shore and off-shore monitoring can further inform the habitat assessment. This study will couple big data from hydrodynamic models to evaluate the monitoring strategies that incorporate novel continuous in-situ monitoring technologies. The computing needs involve storage of large NetCDF files generated from a high-resolution hydrodynamic model covering the Chesapeake Bay, as well as Monte Carlo simulations to evaluate a large collection of potential sampling scenarios.  The newer monitoring, in turn, will enhance the Chesapeake Bay habitat assessment and address stakeholder planning needs. This study will consider agency-specific logistic constraints on site locations while optimizing for habitat assessment.  This group should be named pi_dliang2 .   "
3289689,72210493,Comment,DoIT-Research-Computing,2025-10-10 16:58:29.0000000,HPC New Group: pi_proutyr1,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>When putting abstract on the website please put &quot;(MDREN Participant)&quot; after the abstract title.</p>  <p>On Fri Oct 10 10:47:09 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Roy Last Name:                 Prouty Email:                     proutyr1@umbc.edu Campus ID:                 WH39335  Request Type:              High Performance Cluster  Group Type:            Project Title:        Sampling strategy and design for Chesapeake Bay habitat assessment Project Abstract:     The Chesapeake Bay has had a large monitoring program for water quality for the last four decades. Given the advancement of continuous monitoring infrastructure, recent studies pointed to the high values of small numbers of in-situ monitoring stations with high temporal frequency to provide a sound basis for water quality assessment. In addition, a coupling of on-shore and off-shore monitoring can further inform the habitat assessment. This study will couple big data from hydrodynamic models to evaluate the monitoring strategies that incorporate novel continuous in-situ monitoring technologies. The computing needs involve storage of large NetCDF files generated from a high-resolution hydrodynamic model covering the Chesapeake Bay, as well as Monte Carlo simulations to evaluate a large collection of potential sampling scenarios.  The newer monitoring, in turn, will enhance the Chesapeake Bay habitat assessment and address stakeholder planning needs. This study will consider agency-specific logistic constraints on site locations while optimizing for habitat assessment.  This group should be named pi_dliang2 .   </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3289689,72210494,CommentEmailRecord,DoIT-Research-Computing,2025-10-10 16:58:30.0000000,HPC New Group: pi_proutyr1,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,The RT System itself,NULL,"Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3289689 >  Comment just added.    When putting abstract on the website please put ""(MDREN Participant)"" after the abstract title.  On Fri Oct 10 10:47:09 2025, ZZ99999 wrote:  > First Name:                Roy > Last Name:                 Prouty > Email:                     proutyr1@umbc.edu > Campus ID:                 WH39335 >  > Request Type:              High Performance Cluster >  > Group Type:            > Project Title:        Sampling strategy and design for Chesapeake Bay habitat assessment > Project Abstract:     The Chesapeake Bay has had a large monitoring program for water quality for the last four decades. Given the advancement of continuous monitoring infrastructure, recent studies pointed to the high values of small numbers of in-situ monitoring stations with high temporal frequency to provide a sound basis for water quality assessment. In addition, a coupling of on-shore and off-shore monitoring can further inform the habitat assessment. This study will couple big data from hydrodynamic models to evaluate the monitoring strategies that incorporate novel continuous in-situ monitoring technologies. The computing needs involve storage of large NetCDF files generated from a high-resolution hydrodynamic model covering the Chesapeake Bay, as well as Monte Carlo simulations to evaluate a large collection of potential sampling scenarios.  The newer monitoring, in turn, will enhance the Chesapeake Bay habitat assessment and address stakeholder planning needs. This study will consider agency-specific logistic constraints on site locations while optimizing for habitat assessment. >  > This group should be named pi_dliang2 .    --  Best, Max Breitmeyer DOIT HPC System Administrator  "
3289689,72216003,Correspond,DoIT-Research-Computing,2025-10-10 19:52:19.0000000,HPC New Group: pi_proutyr1,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Dong,</p>  <p>Welcome to chip, UMBC&#39;s High Performance Computing Cluster!</p>  <p>The group, pi_dliang2, now exists on the chip cluster. Members of this group can access and contibute to the research storage space allocated to the group.</p>  <p>This storage space is located at /umbc/rs/pi_dliang2, and currently has a quota of 10TB.</p>  <p>For information on accessing the cluster, adding accounts to your group, and getting started using the cluster, check out the tutorial on our wiki: https://umbc.atlassian.net/wiki/x/R4BPQg</p>  <p>Additional documentation is also available here: https://umbc.atlassian.net/wiki/x/FwCHQ</p>  <p>Submit any questions or issues as separate RT tickets at: https://doit.umbc.edu/request-tracker-rt/doit-research-computing/</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Fri Oct 10 10:47:09 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Roy Last Name:                 Prouty Email:                     proutyr1@umbc.edu Campus ID:                 WH39335  Request Type:              High Performance Cluster  Group Type:            Project Title:        Sampling strategy and design for Chesapeake Bay habitat assessment Project Abstract:     The Chesapeake Bay has had a large monitoring program for water quality for the last four decades. Given the advancement of continuous monitoring infrastructure, recent studies pointed to the high values of small numbers of in-situ monitoring stations with high temporal frequency to provide a sound basis for water quality assessment. In addition, a coupling of on-shore and off-shore monitoring can further inform the habitat assessment. This study will couple big data from hydrodynamic models to evaluate the monitoring strategies that incorporate novel continuous in-situ monitoring technologies. The computing needs involve storage of large NetCDF files generated from a high-resolution hydrodynamic model covering the Chesapeake Bay, as well as Monte Carlo simulations to evaluate a large collection of potential sampling scenarios.  The newer monitoring, in turn, will enhance the Chesapeake Bay habitat assessment and address stakeholder planning needs. This study will consider agency-specific logistic constraints on site locations while optimizing for habitat assessment.  This group should be named pi_dliang2 .   </pre> </blockquote> </div> "
3289693,72205873,Create,DoIT-Research-Computing,2025-10-10 14:51:37.0000000,HPC New Group: pi_bfisher2,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Roy Last Name:                 Prouty Email:                     proutyr1@umbc.edu Campus ID:                 WH39335  Request Type:              High Performance Cluster  Group Type:            Project Title:        Get from PI Project Abstract:     Get from PI  This group should be named pi_bfisher2 . Don't proceed without getting title/abstract from PI.  "
3289693,72210593,Comment,DoIT-Research-Computing,2025-10-10 17:02:59.0000000,HPC New Group: pi_bfisher2,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>When putting abstract on the website please put &quot;(MDREN Participant)&quot; after the abstract title.</p>  <p>On Fri Oct 10 10:51:37 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Roy Last Name:                 Prouty Email:                     proutyr1@umbc.edu Campus ID:                 WH39335  Request Type:              High Performance Cluster  Group Type:            Project Title:        Get from PI Project Abstract:     Get from PI  This group should be named pi_bfisher2 . Don&#39;t proceed without getting title/abstract from PI.  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3289693,72210594,CommentEmailRecord,DoIT-Research-Computing,2025-10-10 17:03:00.0000000,HPC New Group: pi_bfisher2,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,The RT System itself,NULL,"Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3289693 >  Comment just added.    When putting abstract on the website please put ""(MDREN Participant)"" after the abstract title.  On Fri Oct 10 10:51:37 2025, ZZ99999 wrote:  > First Name:                Roy > Last Name:                 Prouty > Email:                     proutyr1@umbc.edu > Campus ID:                 WH39335 >  > Request Type:              High Performance Cluster >  > Group Type:            > Project Title:        Get from PI > Project Abstract:     Get from PI >  > This group should be named pi_bfisher2 . Don't proceed without getting title/abstract from PI.   --  Best, Max Breitmeyer DOIT HPC System Administrator  "
3289693,72268351,Correspond,DoIT-Research-Computing,2025-10-14 17:36:29.0000000,HPC New Group: pi_bfisher2,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Burch,</p>  <p>Once I receive your project title and abstract, I can create an account on chip for you!&nbsp;</p>  <p>Have a nice day!</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Fri Oct 10 10:51:37 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Roy Last Name:                 Prouty Email:                     proutyr1@umbc.edu Campus ID:                 WH39335  Request Type:              High Performance Cluster  Group Type:            Project Title:        Get from PI Project Abstract:     Get from PI  This group should be named pi_bfisher2 . Don&#39;t proceed without getting title/abstract from PI.  </pre> </blockquote> </div> "
3289693,72275080,Correspond,DoIT-Research-Computing,2025-10-14 20:14:47.0000000,HPC New Group: pi_bfisher2,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,NULL,burch.fisher@umces.edu,"Here you go Danielle. Thanks for all your help.  TITLE: Leveraging HPC Resources to Accelerate LiDAR and Remote Sensing Workflows  ABSTRACT: This project will leverage high-performance computing (HPC) resources to de= sign and scale advanced workflows for processing large LiDAR and remote sen= sing datasets. The allocation will support experiments in both multi-core p= arallelization and GPU acceleration to improve the efficiency of computatio= nally intensive tasks such as point-cloud classification, canopy-height mod= el generation, raster mosaicking, and large-area geospatial analysis. By in= tegrating open-source tools within an HPC environment at UMBC, the project = will develop reproducible, high-throughput pipelines optimized for hybrid C= PU=E2=80=93GPU architectures. These workflows will provide a foundation for=  scalable environmental research applications, demonstrating how HPC resour= ces can substantially expand the speed, scope, and analytical capability of=  modern remote sensing workflows.  Let me know if you need something more.  All the best, Burch  G. BURCH FISHER (he/him) Associate Research Scientist SCIPE-CGC Research Facilitator University of Maryland Center for Environmental Science Appalachian Laboratory, 301 Braddock Rd, Frostburg, MD 21532 UMCES <https://www.umces.edu/burch-fisher> | SCIPE <https://scipe.umces.edu= /> | GITHUB <https://github.com/CGC-UMCES>=20   =EF=BF=BC  > On Oct 14, 2025, at 1:36=E2=80=AFPM, Danielle Esposito via RT <UMBCHelp@r= t.umbc.edu> wrote: >=20 > Ticket <URL: https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Di= splay.html?id%3D3289693&source=3Dgmail-imap&ust=3D1761068193000000&usg=3DAO= vVaw3XyhhykucmhC4ZwNkwOqRl > >=20 > Last Update From Ticket: >=20 > Hi Burch, >=20 > Once I receive your project title and abstract, I can create an account o= n chip > for you! >=20 > Have a nice day! >=20 > -- >=20 > Kind regards, > Danielle Esposito (she/her/hers) > DoIT Unix Infra Student Worker >=20 > On Fri Oct 10 10:51:37 2025, ZZ99999 wrote: >=20 >> First Name:                Roy >> Last Name:                 Prouty >> Email:                     proutyr1@umbc.edu >> Campus ID:                 WH39335 >>=20 >> Request Type:              High Performance Cluster >>=20 >> Group Type:=20=20=20=20=20=20=20=20=20=20=20 >> Project Title:        Get from PI >> Project Abstract:     Get from PI >>=20 >> This group should be named pi_bfisher2 . Don't proceed without getting t= itle/abstract from PI. >=20 >=20 >=20  "
3289693,72275080,Correspond,DoIT-Research-Computing,2025-10-14 20:14:47.0000000,HPC New Group: pi_bfisher2,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,NULL,burch.fisher@umces.edu,"<html><head><meta http-equiv=3D""content-type"" content=3D""text/html; charset= =3Dutf-8""></head><body style=3D""overflow-wrap: break-word; -webkit-nbsp-mod= e: space; line-break: after-white-space;"">Here you go Danielle. Thanks for = all your help.<div><br></div><div>TITLE:</div><div>Leveraging HPC Resources=  to Accelerate LiDAR and Remote Sensing Workflows</div><div><br></div><div>= ABSTRACT:</div><div><div>This project will leverage high-performance comput= ing (HPC) resources to design and scale advanced workflows for processing l= arge LiDAR and remote sensing datasets. The allocation will support experim= ents in both multi-core parallelization and GPU acceleration to improve the=  efficiency of computationally intensive tasks such as point-cloud classifi= cation, canopy-height model generation, raster mosaicking, and large-area g= eospatial analysis. By integrating open-source tools within an HPC environm= ent at UMBC, the project will develop reproducible, high-throughput pipelin= es optimized for hybrid CPU=E2=80=93GPU architectures. These workflows will=  provide a foundation for scalable environmental research applications, dem= onstrating how HPC resources can substantially expand the speed, scope, and=  analytical capability of modern remote sensing workflows.</div><div><br></= div><div>Let me know if you need something more.</div><div> <meta charset=3D""UTF-8""><div dir=3D""auto"" style=3D""caret-color: rgb(0, 0, 0= ); letter-spacing: normal; text-align: start; text-indent: 0px; text-transf= orm: none; white-space: normal; word-spacing: 0px; -webkit-text-stroke-widt= h: 0px; text-decoration: none; overflow-wrap: break-word; -webkit-nbsp-mode= : space; line-break: after-white-space;""><div dir=3D""auto"" style=3D""caret-c= olor: rgb(0, 0, 0); font-family: Helvetica; font-size: 14px; font-style: no= rmal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; = orphans: auto; text-align: start; text-indent: 0px; text-transform: none; w= hite-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-wi= dth: 0px; text-decoration: none; color: rgb(0, 0, 0); overflow-wrap: break-= word; -webkit-nbsp-mode: space; line-break: after-white-space;""><div dir=3D= ""auto"" style=3D""color: rgb(0, 0, 0); font-family: Helvetica; font-size: 14p= x; font-style: normal; font-variant-caps: normal; font-weight: 400; letter-= spacing: normal; text-align: start; text-indent: 0px; text-transform: none;=  white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; te= xt-decoration: none; caret-color: rgb(0, 0, 0); overflow-wrap: break-word; = -webkit-nbsp-mode: space; line-break: after-white-space;""><div dir=3D""auto""=  style=3D""caret-color: rgb(0, 0, 0); letter-spacing: normal; text-align: st= art; text-indent: 0px; text-transform: none; white-space: normal; word-spac= ing: 0px; -webkit-text-stroke-width: 0px; text-decoration: none; overflow-w= rap: break-word; -webkit-nbsp-mode: space; line-break: after-white-space;"">= <div dir=3D""auto"" style=3D""caret-color: rgb(0, 0, 0); letter-spacing: norma= l; text-align: start; text-indent: 0px; text-transform: none; white-space: = normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration:=  none; overflow-wrap: break-word; -webkit-nbsp-mode: space; line-break: aft= er-white-space;""><div dir=3D""auto"" style=3D""text-align: start; text-indent:=  0px; overflow-wrap: break-word; -webkit-nbsp-mode: space; line-break: afte= r-white-space;""><div dir=3D""auto"" style=3D""text-align: start; text-indent: = 0px; overflow-wrap: break-word; -webkit-nbsp-mode: space; line-break: after= -white-space;""><div style=3D""color: rgb(0, 0, 0); caret-color: rgb(0, 0, 0)= ; letter-spacing: normal; text-transform: none; white-space: normal; word-s= pacing: 0px; text-decoration: none; -webkit-text-stroke-width: 0px;""><br></= div><div style=3D""color: rgb(0, 0, 0); caret-color: rgb(0, 0, 0); letter-sp= acing: normal; text-transform: none; white-space: normal; word-spacing: 0px= ; text-decoration: none; -webkit-text-stroke-width: 0px;"">All the best,<br>= Burch<br><font><br></font></div></div></div></div></div></div><div style=3D= ""color: rgb(0, 0, 0); caret-color: rgb(0, 0, 0); font-family: Helvetica; fo= nt-size: 14px; font-style: normal; font-variant-caps: normal; font-weight: = 400; letter-spacing: normal; text-align: start; text-indent: 0px; text-tran= sform: none; white-space: normal; word-spacing: 0px; -webkit-text-stroke-wi= dth: 0px; text-decoration: none;""><font><font color=3D""#919191""><b>G. BURCH=  FISHER<span class=3D""Apple-converted-space"">&nbsp;</span></b>(he/him)</fon= t></font></div><div style=3D""color: rgb(0, 0, 0); caret-color: rgb(0, 0, 0)= ; font-family: Helvetica; font-size: 14px; font-style: normal; font-variant= -caps: normal; font-weight: 400; letter-spacing: normal; text-align: start;=  text-indent: 0px; text-transform: none; white-space: normal; word-spacing:=  0px; -webkit-text-stroke-width: 0px; text-decoration: none;""><font color= =3D""#919191"">Associate Research Scientist</font></div><div style=3D""color: = rgb(0, 0, 0); caret-color: rgb(0, 0, 0); font-family: Helvetica; font-size:=  14px; font-style: normal; font-variant-caps: normal; font-weight: 400; let= ter-spacing: normal; text-align: start; text-indent: 0px; text-transform: n= one; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px= ; text-decoration: none;""><font color=3D""#919191"">SCIPE-CGC Research Facili= tator<br>University of Maryland Center for Environmental Science</font></di= v><div style=3D""color: rgb(0, 0, 0); caret-color: rgb(0, 0, 0); font-family= : Helvetica; font-size: 14px; font-style: normal; font-variant-caps: normal= ; font-weight: 400; letter-spacing: normal; text-align: start; text-indent:=  0px; text-transform: none; white-space: normal; word-spacing: 0px; -webkit= -text-stroke-width: 0px; text-decoration: none;""><font color=3D""#919191"">Ap= palachian Laboratory,&nbsp;301 Braddock Rd, Frostburg, MD 21532</font></div= ><div style=3D""caret-color: rgb(0, 0, 0); font-family: Helvetica; font-size= : 14px; font-style: normal; font-variant-caps: normal; letter-spacing: norm= al; text-align: start; text-indent: 0px; text-transform: none; white-space:=  normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration= : none;""><font color=3D""#808080""><font style=3D""font-weight: 400;""><a href= =3D""https://www.umces.edu/burch-fisher"" style=3D""font-weight: 400;"">UMCES</= a></font><b>&nbsp;|&nbsp;</b><a href=3D""https://scipe.umces.edu/"">SCIPE</a>= <b>&nbsp;|</b>&nbsp;<a href=3D""https://github.com/CGC-UMCES"">GITHUB</a></fo= nt>&nbsp;</div><div style=3D""color: rgb(0, 0, 0); caret-color: rgb(0, 0, 0)= ; font-family: Helvetica; font-size: 14px; font-style: normal; font-variant= -caps: normal; letter-spacing: normal; text-align: start; text-indent: 0px;=  text-transform: none; white-space: normal; word-spacing: 0px; -webkit-text= -stroke-width: 0px; text-decoration: none;""><br></div></div></div><br class= =3D""Apple-interchange-newline""><span><img alt=3D""UMCES-100th-Appalachian-Lo= go-Download-3.png"" src=3D""cid:1FF53104-49EE-494F-9199-D11BCDB6870C""></span> </div> <div><br><blockquote type=3D""cite""><div>On Oct 14, 2025, at 1:36=E2=80=AFPM= , Danielle Esposito via RT &lt;UMBCHelp@rt.umbc.edu&gt; wrote:</div><br cla= ss=3D""Apple-interchange-newline""><div><div>Ticket &lt;URL: https://www.goog= le.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3289693&amp;sou= rce=3Dgmail-imap&amp;ust=3D1761068193000000&amp;usg=3DAOvVaw3XyhhykucmhC4Zw= NkwOqRl &gt;<br><br>Last Update From Ticket:<br><br>Hi Burch,<br><br>Once I=  receive your project title and abstract, I can create an account on chip<b= r>for you!<br><br>Have a nice day!<br><br>--<br><br>Kind regards,<br>Daniel= le Esposito (she/her/hers)<br>DoIT Unix Infra Student Worker<br><br>On Fri = Oct 10 10:51:37 2025, ZZ99999 wrote:<br><br><blockquote type=3D""cite"">First=  Name: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;Roy<br>Last Name: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Prouty<br>Emai= l: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;proutyr1@umbc.edu<br>Campus=  ID: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbs= p;&nbsp;&nbsp;&nbsp;&nbsp;WH39335<br><br>Request Type: &nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;High Performance = Cluster<br><br>Group Type: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= &nbsp;&nbsp;<br>Project Title: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ge= t from PI<br>Project Abstract: &nbsp;&nbsp;&nbsp;&nbsp;Get from PI<br><br>T= his group should be named pi_bfisher2 . Don't proceed without getting title= /abstract from PI.<br></blockquote><br><br><br></div></div></blockquote></d= iv><br></div></body></html>= "
3289693,72329932,Correspond,DoIT-Research-Computing,2025-10-16 17:12:44.0000000,HPC New Group: pi_bfisher2,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Burch,</p>  <p>Welcome to chip, UMBC&#39;s High Performance Computing Cluster!</p>  <p>&nbsp; &nbsp; The group, pi_bfisher2, now exists on the chip cluster. Members of this group can access and contribute to the research storage space allocated to the group.</p>  <p>&nbsp; &nbsp; This storage space is located at /umbc/rs/pi_bfisher2, and currently has a quota of 10T.</p>  <p>&nbsp; &nbsp; For information on accessing the cluster, adding accounts to your group, and getting started using the cluster, check out the tutorial on our wiki: https://umbc.atlassian.net/wiki/x/R4BPQg</p>  <p>&nbsp; &nbsp; Additional documentation is also available here: https://umbc.atlassian.net/wiki/x/FwCHQ</p>  <p>&nbsp; &nbsp; If you have any questions or issues, please submit a new RT ticket at: https://doit.umbc.edu/request-tracker-rt/doit-research-computing/</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Tue Oct 14 16:14:47 2025, burch.fisher@umces.edu wrote: <blockquote>Leveraging HPC Resources to Accelerate LiDAR and Remote Sensing Workflows</blockquote> </div> "
3289694,72205919,Create,DoIT-Research-Computing,2025-10-10 14:52:53.0000000,HPC New Group: pi_kcho8,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Roy Last Name:                 Prouty Email:                     proutyr1@umbc.edu Campus ID:                 WH39335  Request Type:              High Performance Cluster  Group Type:            Project Title:        Get from PI Project Abstract:     Get from PI  This group should be named pi_kcho8 . Don't proceed without getting title/abstract from PI.  "
3289694,72210512,Comment,DoIT-Research-Computing,2025-10-10 17:00:03.0000000,HPC New Group: pi_kcho8,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>When putting abstract on the website please put &quot;(MDREN Participant)&quot; after the abstract title.</p>  <p>On Fri Oct 10 10:52:53 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Roy Last Name:                 Prouty Email:                     proutyr1@umbc.edu Campus ID:                 WH39335  Request Type:              High Performance Cluster  Group Type:            Project Title:        Get from PI Project Abstract:     Get from PI  This group should be named pi_kcho8 . Don&#39;t proceed without getting title/abstract from PI.  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3289694,72210513,CommentEmailRecord,DoIT-Research-Computing,2025-10-10 17:00:05.0000000,HPC New Group: pi_kcho8,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,The RT System itself,NULL,"Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3289694 >  Comment just added.    When putting abstract on the website please put ""(MDREN Participant)"" after the abstract title.  On Fri Oct 10 10:52:53 2025, ZZ99999 wrote:  > First Name:                Roy > Last Name:                 Prouty > Email:                     proutyr1@umbc.edu > Campus ID:                 WH39335 >  > Request Type:              High Performance Cluster >  > Group Type:            > Project Title:        Get from PI > Project Abstract:     Get from PI >  > This group should be named pi_kcho8 . Don't proceed without getting title/abstract from PI.   --  Best, Max Breitmeyer DOIT HPC System Administrator  "
3289694,72268513,Correspond,DoIT-Research-Computing,2025-10-14 17:38:55.0000000,HPC New Group: pi_kcho8,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi&nbsp;Kyoung,</p>  <p>Once I receive your project title and abstract, I can create an account on chip for you!&nbsp;</p>  <p>Have a nice day!</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Fri Oct 10 10:52:53 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Roy Last Name:                 Prouty Email:                     proutyr1@umbc.edu Campus ID:                 WH39335  Request Type:              High Performance Cluster  Group Type:            Project Title:        Get from PI Project Abstract:     Get from PI  This group should be named pi_kcho8 . Don&#39;t proceed without getting title/abstract from PI.  </pre> </blockquote> </div> "
3289694,72306023,Correspond,DoIT-Research-Computing,2025-10-15 15:56:11.0000000,HPC New Group: pi_kcho8,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,NULL,KCho@umes.edu,"Good morning Ms. Danielle Esposito,  I send my project title and abstract below:  Project title: Deep Structural Feature Extraction from Maize Proteins and G= ene Sequences Using DNA Language Models  Abstract: Understanding the relationship between the maize (Zea mays) genome and its = corresponding protein structures remains a major challenge in computational=  biology. This project applies advanced machine learning approaches to anal= yze large-scale maize gene and protein sequence data, aiming to uncover nov= el structural and functional insights encoded within DNA. We propose to dev= elop and implement a DNA language model trained on maize genomic sequences = to learn biologically meaningful representations that capture the intrinsic=  syntax and semantics of DNA. These learned embeddings will be integrated w= ith our custom k-mer distance=E2=80=93based model and deep neural architect= ures to predict tertiary structural and physicochemical features directly f= rom gene and protein sequences.  The large-scale data processing, model training, and structural mapping tas= ks require access to GPU-enabled high-performance computing (HPC) resources=  with distributed processing capabilities. This project is conducted in col= laboration with the Maize Genetics and Genomics Database (MaizeGDB), USDA-A= RS, Corn Insects and Crop Genetics Research Unit, Iowa State University, Am= es, IA, leveraging their extensive maize genomic and proteomic datasets. Ex= pected outcomes include (1) novel computational methods for extracting stru= ctural features from biological sequences, (2) deeper understanding of gene= -to-structure relationships in maize, and (3) new machine learning tools to=  support functional genomics and crop improvement research within the Maize= GDB community.   Please let me know if you have any questions. Thank you,  Kyoung Tak Cho, Ph.D. Assistant Professor Department of Computer Science and Engineering Technology University of Maryland Eastern Shore ________________________________ From: Danielle Esposito via RT <UMBCHelp@rt.umbc.edu> Sent: Tuesday, October 14, 2025 1:38 PM To: Cho, Kyoung Tak <KCho@umes.edu>; kcho8@umbc.edu <kcho8@umbc.edu> Cc: Cho, Kyoung Tak <KCho@umes.edu>; kcho8@umbc.edu <kcho8@umbc.edu> Subject: [Research Computing #3289694] CC Correspondence  [You don't often get email from umbchelp@rt.umbc.edu. Learn why this is imp= ortant at https://aka.ms/LearnAboutSenderIdentification ]  CAUTION: This e-mail came from OUTSIDE of UMES and is not an official e-mai= l from the University.  WARNING: DO NOT click on links in message, and do not download attachments = unless you are expecting them and know the sender.  Ticket <URL: https://nam11.safelinks.protection.outlook.com/?url=3Dhttps%3A= %2F%2Frt.umbc.edu%2FTicket%2FDisplay.html%3Fid%3D3289694&data=3D05%7C02%7CK= Cho%40umes.edu%7Ce5006f166f3843ad7e4908de0b488e25%7Cf3b74c6baf13485097f02e9= 7a8a9b7d7%7C0%7C0%7C638960603420350940%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1h= cGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%= 3D%3D%7C80000%7C%7C%7C&sdata=3DVuR%2F08SGgGHYistHD3%2FeoTAYDJi7sPynlU9vBZVa= Ve4%3D&reserved=3D0<https://rt.umbc.edu/Ticket/Display.html?id=3D3289694> >  Last Update From Ticket:  Hi Kyoung,  Once I receive your project title and abstract, I can create an account on = chip for you!  Have a nice day!  --  Kind regards, Danielle Esposito (she/her/hers) DoIT Unix Infra Student Worker  On Fri Oct 10 10:52:53 2025, ZZ99999 wrote:  > First Name:                Roy > Last Name:                 Prouty > Email:                     proutyr1@umbc.edu > Campus ID:                 WH39335 > > Request Type:              High Performance Cluster > > Group Type: > Project Title:        Get from PI > Project Abstract:     Get from PI > > This group should be named pi_kcho8 . Don't proceed without getting title= /abstract from PI.    "
3289694,72306023,Correspond,DoIT-Research-Computing,2025-10-15 15:56:11.0000000,HPC New Group: pi_kcho8,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,NULL,KCho@umes.edu,"<html> <head> <meta http-equiv=3D""Content-Type"" content=3D""text/html; charset=3DWindows-1= 252""> <style type=3D""text/css"" style=3D""display:none;""> P {margin-top:0;margin-bo= ttom:0;} </style> </head> <body dir=3D""ltr""> <div style=3D""font-family: Aptos, Aptos_EmbeddedFont, Aptos_MSFontService, = Calibri, Helvetica, sans-serif; font-size: 12pt; color: rgb(0, 0, 0);"" clas= s=3D""elementToProof""> Good morning Ms. Danielle Esposito,</div> <div style=3D""font-family: Aptos, Aptos_EmbeddedFont, Aptos_MSFontService, = Calibri, Helvetica, sans-serif; font-size: 12pt; color: rgb(0, 0, 0);"" clas= s=3D""elementToProof""> <br> </div> <div style=3D""font-family: Aptos, Aptos_EmbeddedFont, Aptos_MSFontService, = Calibri, Helvetica, sans-serif; font-size: 12pt; color: rgb(0, 0, 0);"" clas= s=3D""elementToProof""> I send my project title and abstract below:</div> <div style=3D""font-family: Aptos, Aptos_EmbeddedFont, Aptos_MSFontService, = Calibri, Helvetica, sans-serif; font-size: 12pt; color: rgb(0, 0, 0);"" clas= s=3D""elementToProof""> <br> </div> <div style=3D""font-family: Aptos, Aptos_EmbeddedFont, Aptos_MSFontService, = Calibri, Helvetica, sans-serif; font-size: 12pt; color: rgb(0, 0, 0);"" clas= s=3D""elementToProof""> Project title: Deep Structural Feature Extraction from Maize Proteins and G= ene Sequences Using DNA Language Models</div> <div style=3D""font-family: Aptos, Aptos_EmbeddedFont, Aptos_MSFontService, = Calibri, Helvetica, sans-serif; font-size: 12pt; color: rgb(0, 0, 0);"" clas= s=3D""elementToProof""> <br> </div> <div style=3D""font-family: Aptos, Aptos_EmbeddedFont, Aptos_MSFontService, = Calibri, Helvetica, sans-serif; font-size: 12pt; color: rgb(0, 0, 0);"" clas= s=3D""elementToProof""> Abstract:</div> <div style=3D""font-family: Aptos, Aptos_EmbeddedFont, Aptos_MSFontService, = Calibri, Helvetica, sans-serif; font-size: 12pt; color: rgb(0, 0, 0);"" clas= s=3D""elementToProof""> Understanding the relationship between the maize (Zea mays) genome and its = corresponding protein structures remains a major challenge in computational=  biology. This project applies advanced machine learning approaches to anal= yze large-scale maize gene and protein  sequence data, aiming to uncover novel structural and functional insights = encoded within DNA. We propose to develop and implement a DNA language mode= l trained on maize genomic sequences to learn biologically meaningful repre= sentations that capture the intrinsic  syntax and semantics of DNA. These learned embeddings will be integrated w= ith our custom k-mer distance=E2=80=93based model and deep neural architect= ures to predict tertiary structural and physicochemical features directly f= rom gene and protein sequences.</div> <div style=3D""font-family: Aptos, Aptos_EmbeddedFont, Aptos_MSFontService, = Calibri, Helvetica, sans-serif; font-size: 12pt; color: rgb(0, 0, 0);"" clas= s=3D""elementToProof""> <br> </div> <div style=3D""font-family: Aptos, Aptos_EmbeddedFont, Aptos_MSFontService, = Calibri, Helvetica, sans-serif; font-size: 12pt; color: rgb(0, 0, 0);"" clas= s=3D""elementToProof""> The large-scale data processing, model training, and structural mapping tas= ks require access to GPU-enabled high-performance computing (HPC) resources=  with distributed processing capabilities. This project is conducted in col= laboration with the Maize Genetics  and Genomics Database (MaizeGDB), USDA-ARS, Corn Insects and Crop Genetics=  Research Unit, Iowa State University, Ames, IA, leveraging their extensive=  maize genomic and proteomic datasets. Expected outcomes include (1) novel = computational methods for extracting  structural features from biological sequences, (2) deeper understanding of=  gene-to-structure relationships in maize, and (3) new machine learning too= ls to support functional genomics and crop improvement research within the = MaizeGDB community.</div> <div style=3D""font-family: Aptos, Aptos_EmbeddedFont, Aptos_MSFontService, = Calibri, Helvetica, sans-serif; font-size: 12pt; color: rgb(0, 0, 0);"" clas= s=3D""elementToProof""> <br> </div> <div style=3D""font-family: Aptos, Aptos_EmbeddedFont, Aptos_MSFontService, = Calibri, Helvetica, sans-serif; font-size: 12pt; color: rgb(0, 0, 0);"" clas= s=3D""elementToProof""> <br> </div> <div style=3D""font-family: Aptos, Aptos_EmbeddedFont, Aptos_MSFontService, = Calibri, Helvetica, sans-serif; font-size: 12pt; color: rgb(0, 0, 0);"" clas= s=3D""elementToProof""> Please let me know if you have any questions.</div> <div style=3D""font-family: Aptos, Aptos_EmbeddedFont, Aptos_MSFontService, = Calibri, Helvetica, sans-serif; font-size: 12pt; color: rgb(0, 0, 0);"" clas= s=3D""elementToProof""> Thank you,</div> <div id=3D""Signature""> <div style=3D""font-family: Aptos, Aptos_EmbeddedFont, Aptos_MSFontService, = Calibri, Helvetica, sans-serif; font-size: 12pt; color: rgb(0, 0, 0);""> <br> </div> <div style=3D""font-family: &quot;Times New Roman&quot;, Times, serif; font-= size: 12pt; color: rgb(0, 0, 0);""> <b>Kyoung Tak Cho, Ph.D</b>.</div> <div style=3D""font-family: &quot;Times New Roman&quot;, Times, serif; font-= size: 12pt; color: rgb(0, 0, 0);""> <b>Assistant Professor</b></div> <div style=3D""font-family: &quot;Times New Roman&quot;, Times, serif; font-= size: 12pt; color: rgb(0, 0, 0);""> <b>Department of Computer Science and Engineering Technology</b></div> <div style=3D""font-family: &quot;Times New Roman&quot;, Times, serif; font-= size: 12pt; color: rgb(0, 0, 0);""> <b>University of Maryland Eastern Shore</b><span style=3D""font-family: Apto= s, Aptos_EmbeddedFont, Aptos_MSFontService, Calibri, Helvetica, sans-serif;= ""><br> </span></div> </div> <div id=3D""appendonsend""></div> <hr style=3D""display:inline-block;width:98%"" tabindex=3D""-1""> <div id=3D""divRplyFwdMsg"" dir=3D""ltr""><font face=3D""Calibri, sans-serif"" st= yle=3D""font-size:11pt"" color=3D""#000000""><b>From:</b> Danielle Esposito via=  RT &lt;UMBCHelp@rt.umbc.edu&gt;<br> <b>Sent:</b> Tuesday, October 14, 2025 1:38 PM<br> <b>To:</b> Cho, Kyoung Tak &lt;KCho@umes.edu&gt;; kcho8@umbc.edu &lt;kcho8@= umbc.edu&gt;<br> <b>Cc:</b> Cho, Kyoung Tak &lt;KCho@umes.edu&gt;; kcho8@umbc.edu &lt;kcho8@= umbc.edu&gt;<br> <b>Subject:</b> [Research Computing #3289694] CC Correspondence</font> <div>&nbsp;</div> </div> <div class=3D""BodyFragment""><font size=3D""2""><span style=3D""font-size:11pt;= ""> <div class=3D""PlainText"">[You don't often get email from umbchelp@rt.umbc.e= du. Learn why this is important at <a href=3D""https://aka.ms/LearnAboutSenderIdentification"">https://aka.ms/Le= arnAboutSenderIdentification</a> ]<br> <br> CAUTION: This e-mail came from OUTSIDE of UMES and is not an official e-mai= l from the University.<br> <br> WARNING: DO NOT click on links in message, and do not download attachments = unless you are expecting them and know the sender.<br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D328= 9694"">https://nam11.safelinks.protection.outlook.com/?url=3Dhttps%3A%2F%2Fr= t.umbc.edu%2FTicket%2FDisplay.html%3Fid%3D3289694&amp;data=3D05%7C02%7CKCho= %40umes.edu%7Ce5006f166f3843ad7e4908de0b488e25%7Cf3b74c6baf13485097f02e97a8= a9b7d7%7C0%7C0%7C638960603420350940%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGk= iOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%= 3D%7C80000%7C%7C%7C&amp;sdata=3DVuR%2F08SGgGHYistHD3%2FeoTAYDJi7sPynlU9vBZV= aVe4%3D&amp;reserved=3D0</a>  &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Kyoung,<br> <br> Once I receive your project title and abstract, I can create an account on = chip<br> for you!<br> <br> Have a nice day!<br> <br> --<br> <br> Kind regards,<br> Danielle Esposito (she/her/hers)<br> DoIT Unix Infra Student Worker<br> <br> On Fri Oct 10 10:52:53 2025, ZZ99999 wrote:<br> <br> &gt; First Name:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Roy<br> &gt; Last Name:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Prouty<br> &gt; Email:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbs= p;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; proutyr1@umbc.edu<= br> &gt; Campus ID:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; WH39335<br> &gt;<br> &gt; Request Type:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nb= sp;&nbsp;&nbsp;&nbsp; High Performance Cluster<br> &gt;<br> &gt; Group Type:<br> &gt; Project Title:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Get from PI<b= r> &gt; Project Abstract:&nbsp;&nbsp;&nbsp;&nbsp; Get from PI<br> &gt;<br> &gt; This group should be named pi_kcho8 . Don't proceed without getting ti= tle/abstract from PI.<br> <br> <br> <br> </div> </span></font></div> </body> </html> "
3289694,72327037,Correspond,DoIT-Research-Computing,2025-10-16 15:32:18.0000000,HPC New Group: pi_kcho8,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Kyoung,</p>  <p>Welcome to chip, UMBC&#39;s High Performance Computing Cluster!</p>  <p>&nbsp; &nbsp; The group, pi_kcho8, now exists on the chip cluster. Members of this group can access and contribute to the research storage space allocated to the group.</p>  <p>&nbsp; &nbsp; This storage space is located at /umbc/rs/pi_kcho8, and currently has a quota of 10T.</p>  <p>&nbsp; &nbsp; For information on accessing the cluster, adding accounts to your group, and getting started using the cluster, check out the tutorial on our wiki: https://umbc.atlassian.net/wiki/x/R4BPQg</p>  <p>&nbsp; &nbsp; Additional documentation is also available here: https://umbc.atlassian.net/wiki/x/FwCHQ</p>  <p>&nbsp; &nbsp; If you have any questions or issues, please submit a new RT ticket at: https://doit.umbc.edu/request-tracker-rt/doit-research-computing/</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Wed Oct 15 11:56:11 2025, KCho@umes.edu wrote: <blockquote> <div>Good morning Ms. Danielle Esposito,</div>  <div>&nbsp;</div>  <div>I send my project title and abstract below:</div>  <div>&nbsp;</div>  <div>Project title: Deep Structural Feature Extraction from Maize Proteins and Gene Sequences Using DNA Language Models</div>  <div>&nbsp;</div>  <div>Abstract:</div>  <div>Understanding the relationship between the maize (Zea mays) genome and its corresponding protein structures remains a major challenge in computational biology. This project applies advanced machine learning approaches to analyze large-scale maize gene and protein sequence data, aiming to uncover novel structural and functional insights encoded within DNA. We propose to develop and implement a DNA language model trained on maize genomic sequences to learn biologically meaningful representations that capture the intrinsic syntax and semantics of DNA. These learned embeddings will be integrated with our custom k-mer distance&ndash;based model and deep neural architectures to predict tertiary structural and physicochemical features directly from gene and protein sequences.</div>  <div>&nbsp;</div>  <div>The large-scale data processing, model training, and structural mapping tasks require access to GPU-enabled high-performance computing (HPC) resources with distributed processing capabilities. This project is conducted in collaboration with the Maize Genetics and Genomics Database (MaizeGDB), USDA-ARS, Corn Insects and Crop Genetics Research Unit, Iowa State University, Ames, IA, leveraging their extensive maize genomic and proteomic datasets. Expected outcomes include (1) novel computational methods for extracting structural features from biological sequences, (2) deeper understanding of gene-to-structure relationships in maize, and (3) new machine learning tools to support functional genomics and crop improvement research within the MaizeGDB community.</div>  <div>&nbsp;</div>  <div>&nbsp;</div>  <div>Please let me know if you have any questions.</div>  <div>Thank you,</div>  <div> <div>&nbsp;</div>  <div><strong>Kyoung Tak Cho, Ph.D</strong>.</div>  <div><strong>Assistant Professor</strong></div>  <div><strong>Department of Computer Science and Engineering Technology</strong></div>  <div><strong>University of Maryland Eastern Shore</strong></div> </div>  <div>&nbsp;</div>  <div><span style=""color:#000000""><strong>From:</strong> Danielle Esposito via RT &lt;UMBCHelp@rt.umbc.edu&gt;<br /> <strong>Sent:</strong> Tuesday, October 14, 2025 1:38 PM<br /> <strong>To:</strong> Cho, Kyoung Tak &lt;KCho@umes.edu&gt;; kcho8@umbc.edu &lt;kcho8@umbc.edu&gt;<br /> <strong>Cc:</strong> Cho, Kyoung Tak &lt;KCho@umes.edu&gt;; kcho8@umbc.edu &lt;kcho8@umbc.edu&gt;<br /> <strong>Subject:</strong> [Research Computing #3289694] CC Correspondence</span>  <div>&nbsp;</div> </div>  <div> <div>[You don&#39;t often get email from umbchelp@rt.umbc.edu. Learn why this is important at https://aka.ms/LearnAboutSenderIdentification ]<br /> <br /> CAUTION: This e-mail came from OUTSIDE of UMES and is not an official e-mail from the University.<br /> <br /> WARNING: DO NOT click on links in message, and do not download attachments unless you are expecting them and know the sender.<br /> <br /> Ticket &lt;URL: https://nam11.safelinks.protection.outlook.com/?url=https%3A%2F%2Frt.umbc.edu%2FTicket%2FDisplay.html%3Fid%3D3289694&amp;data=05%7C02%7CKCho%40umes.edu%7Ce5006f166f3843ad7e4908de0b488e25%7Cf3b74c6baf13485097f02e97a8a9b7d7%7C0%7C0%7C638960603420350940%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C80000%7C%7C%7C&amp;sdata=VuR%2F08SGgGHYistHD3%2FeoTAYDJi7sPynlU9vBZVaVe4%3D&amp;reserved=0 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hi Kyoung,<br /> <br /> Once I receive your project title and abstract, I can create an account on chip<br /> for you!<br /> <br /> Have a nice day!<br /> <br /> --<br /> <br /> Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker<br /> <br /> On Fri Oct 10 10:52:53 2025, ZZ99999 wrote:<br /> <br /> &gt; First Name:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Roy<br /> &gt; Last Name:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Prouty<br /> &gt; Email:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; proutyr1@umbc.edu<br /> &gt; Campus ID:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; WH39335<br /> &gt;<br /> &gt; Request Type:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; High Performance Cluster<br /> &gt;<br /> &gt; Group Type:<br /> &gt; Project Title:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Get from PI<br /> &gt; Project Abstract:&nbsp;&nbsp;&nbsp;&nbsp; Get from PI<br /> &gt;<br /> &gt; This group should be named pi_kcho8 . Don&#39;t proceed without getting title/abstract from PI.<br /> <br /> <br /> &nbsp;</div> </div> </blockquote> </div> "
3289695,72205976,Create,DoIT-Research-Computing,2025-10-10 14:54:30.0000000,HPC New Group: pi_hodae1,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Roy Last Name:                 Prouty Email:                     proutyr1@umbc.edu Campus ID:                 WH39335  Request Type:              High Performance Cluster  Group Type:            Project Title:        Get from PI Project Abstract:     Get from PI  This group should be named pi_hodae1 . Don't proceed without getting title/abstract from PI.   "
3289695,72210536,Comment,DoIT-Research-Computing,2025-10-10 17:01:29.0000000,HPC New Group: pi_hodae1,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>When putting abstract on the website please put &quot;(MDREN Participant)&quot; after the abstract title.</p>  <p>On Fri Oct 10 10:54:30 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Roy Last Name:                 Prouty Email:                     proutyr1@umbc.edu Campus ID:                 WH39335  Request Type:              High Performance Cluster  Group Type:            Project Title:        Get from PI Project Abstract:     Get from PI  This group should be named pi_hodae1 . Don&#39;t proceed without getting title/abstract from PI.   </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3289695,72210537,CommentEmailRecord,DoIT-Research-Computing,2025-10-10 17:01:31.0000000,HPC New Group: pi_hodae1,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,The RT System itself,NULL,"Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3289695 >  Comment just added.    When putting abstract on the website please put ""(MDREN Participant)"" after the abstract title.  On Fri Oct 10 10:54:30 2025, ZZ99999 wrote:  > First Name:                Roy > Last Name:                 Prouty > Email:                     proutyr1@umbc.edu > Campus ID:                 WH39335 >  > Request Type:              High Performance Cluster >  > Group Type:            > Project Title:        Get from PI > Project Abstract:     Get from PI >  > This group should be named pi_hodae1 . Don't proceed without getting title/abstract from PI. >    --  Best, Max Breitmeyer DOIT HPC System Administrator  "
3289695,72268600,Correspond,DoIT-Research-Computing,2025-10-14 17:40:58.0000000,HPC New Group: pi_hodae1,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Hoda,</p>  <p>Once I receive your project title and abstract, I can create an account on chip for you!&nbsp;</p>  <p>Have a nice day!</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Fri Oct 10 10:54:30 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Roy Last Name:                 Prouty Email:                     proutyr1@umbc.edu Campus ID:                 WH39335  Request Type:              High Performance Cluster  Group Type:            Project Title:        Get from PI Project Abstract:     Get from PI  This group should be named pi_hodae1 . Don&#39;t proceed without getting title/abstract from PI.   </pre> </blockquote> </div> "
3289695,72328328,Correspond,DoIT-Research-Computing,2025-10-16 16:16:29.0000000,HPC New Group: pi_hodae1,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,NULL,hoda.elsayed@gmail.com,"Greetings,  Please find attached the projects title and abstract.  On Tue, Oct 14, 2025 at 1:41=E2=80=AFPM Danielle Esposito via RT < UMBCHelp@rt.umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3289695 > > > Last Update From Ticket: > > Hi Hoda, > > Once I receive your project title and abstract, I can create an account on > chip > for you! > > Have a nice day! > > -- > > Kind regards, > Danielle Esposito (she/her/hers) > DoIT Unix Infra Student Worker > > On Fri Oct 10 10:54:30 2025, ZZ99999 wrote: > > > First Name:                Roy > > Last Name:                 Prouty > > Email:                     proutyr1@umbc.edu > > Campus ID:                 WH39335 > > > > Request Type:              High Performance Cluster > > > > Group Type: > > Project Title:        Get from PI > > Project Abstract:     Get from PI > > > > This group should be named pi_hodae1 . Don't proceed without getting > title/abstract from PI. > > > > > > "
3289695,72328328,Correspond,DoIT-Research-Computing,2025-10-16 16:16:29.0000000,HPC New Group: pi_hodae1,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,NULL,hoda.elsayed@gmail.com,"<div dir=3D""ltr"">Greetings,<div><br></div><div>Please find attached the pro= jects title and abstract.</div></div><br><div class=3D""gmail_quote gmail_qu= ote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Tue, Oct 14, 2025 a= t 1:41=E2=80=AFPM Danielle Esposito via RT &lt;<a href=3D""mailto:UMBCHelp@r= t.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class= =3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rg= b(204,204,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc= .edu/Ticket/Display.html?id=3D3289695"" rel=3D""noreferrer"" target=3D""_blank""= >https://rt.umbc.edu/Ticket/Display.html?id=3D3289695</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Hoda,<br> <br> Once I receive your project title and abstract, I can create an account on = chip<br> for you!<br> <br> Have a nice day!<br> <br> --<br> <br> Kind regards,<br> Danielle Esposito (she/her/hers)<br> DoIT Unix Infra Student Worker<br> <br> On Fri Oct 10 10:54:30 2025, ZZ99999 wrote:<br> <br> &gt; First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Roy= <br> &gt; Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0Prouty<br> &gt; Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 =C2=A0<a href=3D""mailto:proutyr1@umbc.edu"" target=3D""_blank"">proutyr= 1@umbc.edu</a><br> &gt; Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0WH39335<br> &gt; <br> &gt; Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Per= formance Cluster<br> &gt; <br> &gt; Group Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0<br> &gt; Project Title:=C2=A0 =C2=A0 =C2=A0 =C2=A0 Get from PI<br> &gt; Project Abstract:=C2=A0 =C2=A0 =C2=A0Get from PI<br> &gt; <br> &gt; This group should be named pi_hodae1 . Don&#39;t proceed without getti= ng title/abstract from PI.<br> &gt; <br> <br> <br> <br> </blockquote></div> "
3289695,72329215,Correspond,DoIT-Research-Computing,2025-10-16 16:49:01.0000000,HPC New Group: pi_hodae1,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Hoda,</p>  <p>Welcome to chip, UMBC&#39;s High Performance Computing Cluster!</p>  <p>&nbsp; &nbsp; The group, pi_hodae1, now exists on the chip cluster. Memb= ers of this group can access and contribute to the research storage space a= llocated to the group.</p>  <p>&nbsp; &nbsp; This storage space is located at /umbc/rs/pi_hodae1, and c= urrently has a quota of 10T.</p>  <p>&nbsp; &nbsp; For information on accessing the cluster, adding accounts = to your group, and getting started using the cluster, check out the tutoria= l on our wiki: https://umbc.atlassian.net/wiki/x/R4BPQg</p>  <p>&nbsp; &nbsp; Additional documentation is also available here: https://u= mbc.atlassian.net/wiki/x/FwCHQ</p>  <p>&nbsp; &nbsp; If you have any questions or issues, please submit a new R= T ticket at: https://doit.umbc.edu/request-tracker-rt/doit-research-computi= ng/</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Thu Oct 16 12:16:29 2025, hoda.elsayed@gmail.com wrote: <blockquote> <div>Greetings, <div>&nbsp;</div>  <div>Please find attached the projects title and abstract.</div> </div> &nbsp;  <div> <div>On Tue, Oct 14, 2025 at 1:41=E2=80=AFPM Danielle Esposito via RT &lt;U= MBCHelp@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 89695 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hi Hoda,<br /> <br /> Once I receive your project title and abstract, I can create an account on = chip<br /> for you!<br /> <br /> Have a nice day!<br /> <br /> --<br /> <br /> Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker<br /> <br /> On Fri Oct 10 10:54:30 2025, ZZ99999 wrote:<br /> <br /> &gt; First Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Roy= <br /> &gt; Last Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p;Prouty<br /> &gt; Email:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &= nbsp; &nbsp;proutyr1@umbc.edu<br /> &gt; Campus ID:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p;WH39335<br /> &gt;<br /> &gt; Request Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; High Per= formance Cluster<br /> &gt;<br /> &gt; Group Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br /> &gt; Project Title:&nbsp; &nbsp; &nbsp; &nbsp; Get from PI<br /> &gt; Project Abstract:&nbsp; &nbsp; &nbsp;Get from PI<br /> &gt;<br /> &gt; This group should be named pi_hodae1 . Don&#39;t proceed without getti= ng title/abstract from PI.<br /> &gt;<br /> <br /> <br /> &nbsp;</blockquote> </div> </blockquote> </div> "
3289695,72467677,Forward Transaction,DoIT-Research-Computing,2025-10-23 20:20:19.0000000,HPC New Group: pi_hodae1,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,Roy Prouty,proutyr1@umbc.edu,"<p>Dr. El-Sayed and Dr. Ogundiran,<br /> <br /> Please see this help ticket response (reproduced below). Let me know if you have issues here. Dr.&nbsp;Ogundiran: you may not be able to open the ticket without a UMBC account. Should we make one for you for troubleshooting issues such as this?.<br /> &quot;&quot;&quot;</p>  <p>Hi Hoda,</p>  <p>Welcome to chip, UMBC&#39;s High Performance Computing Cluster!</p>  <p>&nbsp; &nbsp; The group, pi_hodae1, now exists on the chip cluster. Members of this group can access and contribute to the research storage space allocated to the group.</p>  <p>&nbsp; &nbsp; This storage space is located at /umbc/rs/pi_hodae1, and currently has a quota of 10T.</p>  <p>&nbsp; &nbsp; For information on accessing the cluster, adding accounts to your group, and getting started using the cluster, check out the tutorial on our wiki: https://umbc.atlassian.net/wiki/x/R4BPQg</p>  <p>&nbsp; &nbsp; Additional documentation is also available here: https://umbc.atlassian.net/wiki/x/FwCHQ</p>  <p>&nbsp; &nbsp; If you have any questions or issues, please submit a new RT ticket at: https://doit.umbc.edu/request-tracker-rt/doit-research-computing/</p>  <p>--&nbsp;<br /> &quot;&quot;&quot;</p>  <p>--&nbsp;</p>  <p>Roy Prouty<br /> DoIT Research Computing Team</p> "
3289695,72597674,Correspond,DoIT-Research-Computing,2025-10-30 14:27:39.0000000,HPC New Group: pi_hodae1,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,Ayodeji Ogundiran,AOgundiran@bowiestate.edu,"Good morning,=0D Can you kindly create an account for Dr. El-Sayed for this group? From the = information provided, I see a group was created, but I do not see any accou= nt for her. The link https://doit.umbc.edu/request-tracker-rt/doit-research= -computing/ is broken.=0D =0D Thanks.=0D Sincerely,=0D [A yellow flame in the dark  Description automatically generated]=0D Ayodeji Ogundiran, D.Sc.=0D Systems Administrator=0D Department of Computer Science=0D Bowie State University=0D 14000 Jericho Park Road, Bowie, MD 20715-9465=0D [Speaker phone with solid fill]:(301)-860-3880=0D [Phone Vibration with solid fill]:(240)-906-3468=0D [Email with solid fill]:aogundiran@bowiestate.edu=0D =0D From:=0D Sent: None=0D Subject:=0D =0D =0D Hi Hoda,=0D =0D Welcome to chip, UMBC's High Performance Computing Cluster!=0D =0D     The group, pi_hodae1, now exists on the chip cluster. Members of this g= roup can access and contribute to the research storage space allocated to t= he group.=0D =0D     This storage space is located at /umbc/rs/pi_hodae1, and currently has = a quota of 10T.=0D =0D     For information on accessing the cluster, adding accounts to your group= , and getting started using the cluster, check out the tutorial on our wiki= : https://umbc.atlassian.net/wiki/x/R4BPQg=0D =0D     Additional documentation is also available here: https://umbc.atlassian= .net/wiki/x/FwCHQ=0D =0D     If you have any questions or issues, please submit a new RT ticket at: = https://doit.umbc.edu/request-tracker-rt/doit-research-computing/=0D =0D --=0D =0D Kind regards,=0D Danielle Esposito (she/her/hers)=0D DoIT Unix Infra Student Worker=0D =0D =0D On Thu Oct 16 12:16:29 2025, hoda.elsayed@gmail.com<mailto:hoda.elsayed@gma= il.com> wrote:=0D Greetings,=0D =0D Please find attached the projects title and abstract.=0D =0D On Tue, Oct 14, 2025 at 1:41=E2=80=AFPM Danielle Esposito via RT <UMBCHelp@= rt.umbc.edu<mailto:UMBCHelp@rt.umbc.edu>> wrote:=0D Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3289695 >=0D =0D Last Update From Ticket:=0D =0D Hi Hoda,=0D =0D Once I receive your project title and abstract, I can create an account on = chip=0D for you!=0D =0D Have a nice day!=0D =0D --=0D =0D Kind regards,=0D Danielle Esposito (she/her/hers)=0D DoIT Unix Infra Student Worker=0D =0D On Fri Oct 10 10:54:30 2025, ZZ99999 wrote:=0D =0D > First Name:                Roy=0D > Last Name:                 Prouty=0D > Email:                     proutyr1@umbc.edu<mailto:proutyr1@umbc.edu>=0D > Campus ID:                 WH39335=0D >=0D > Request Type:              High Performance Cluster=0D >=0D > Group Type:=0D > Project Title:        Get from PI=0D > Project Abstract:     Get from PI=0D >=0D > This group should be named pi_hodae1 . Don't proceed without getting titl= e/abstract from PI.=0D >=0D =0D =0D =0D "
3289695,72597674,Correspond,DoIT-Research-Computing,2025-10-30 14:27:39.0000000,HPC New Group: pi_hodae1,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,Ayodeji Ogundiran,AOgundiran@bowiestate.edu,"<html xmlns:v=3D""urn:schemas-microsoft-com:vml"" xmlns:o=3D""urn:schemas-micr= osoft-com:office:office"" xmlns:w=3D""urn:schemas-microsoft-com:office:word"" = xmlns:m=3D""http://schemas.microsoft.com/office/2004/12/omml"" xmlns=3D""http:= //www.w3.org/TR/REC-html40"">=0D <head>=0D <meta http-equiv=3D""Content-Type"" content=3D""text/html; charset=3Dutf-8"">=0D <meta name=3D""Generator"" content=3D""Microsoft Word 15 (filtered medium)"">=0D <!--[if !mso]><style>v\:* {behavior:url(#default#VML);}=0D o\:* {behavior:url(#default#VML);}=0D w\:* {behavior:url(#default#VML);}=0D .shape {behavior:url(#default#VML);}=0D </style><![endif]--><style><!--=0D /* Font Definitions */=0D @font-face=0D 	{font-family:""Cambria Math"";=0D 	panose-1:2 4 5 3 5 4 6 3 2 4;}=0D @font-face=0D 	{font-family:Calibri;=0D 	panose-1:2 15 5 2 2 2 4 3 2 4;}=0D @font-face=0D 	{font-family:Verdana;=0D 	panose-1:2 11 6 4 3 5 4 4 2 4;}=0D @font-face=0D 	{font-family:Aptos;}=0D @font-face=0D 	{font-family:Georgia;=0D 	panose-1:2 4 5 2 5 4 5 2 3 3;}=0D @font-face=0D 	{font-family:""Segoe UI Symbol"";=0D 	panose-1:2 11 5 2 4 2 4 2 2 3;}=0D @font-face=0D 	{font-family:inherit;}=0D /* Style Definitions */=0D p.MsoNormal, li.MsoNormal, div.MsoNormal=0D 	{margin:0in;=0D 	font-size:12.0pt;=0D 	font-family:""Aptos"",sans-serif;}=0D a:link, span.MsoHyperlink=0D 	{mso-style-priority:99;=0D 	color:#467886;=0D 	text-decoration:underline;}=0D span.EmailStyle20=0D 	{mso-style-type:personal-reply;=0D 	font-family:""Aptos"",sans-serif;=0D 	color:#153D63;}=0D .MsoChpDefault=0D 	{mso-style-type:export-only;}=0D @page WordSection1=0D 	{size:8.5in 11.0in;=0D 	margin:1.0in 1.0in 1.0in 1.0in;}=0D div.WordSection1=0D 	{page:WordSection1;}=0D --></style>=0D </head>=0D <body lang=3D""EN-US"" link=3D""#467886"" vlink=3D""#96607D"" style=3D""word-wrap:= break-word"">=0D <div class=3D""WordSection1"">=0D <p class=3D""MsoNormal""><span style=3D""color:#153D63"">Good morning,<o:p></o:= p></span></p>=0D <p class=3D""MsoNormal""><span style=3D""color:#153D63"">Can you kindly create = an account for Dr. El-Sayed for this group? From the information provided, = I see a group was created, but I do not see any account for her. The link=0D </span><a href=3D""https://doit.umbc.edu/request-tracker-rt/doit-research-co= mputing/"">https://doit.umbc.edu/request-tracker-rt/doit-research-computing/= </a> is broken.<span style=3D""color:#153D63""><o:p></o:p></span></p>=0D <p class=3D""MsoNormal""><span style=3D""color:#153D63""><o:p>&nbsp;</o:p></spa= n></p>=0D <p class=3D""MsoNormal"" style=3D""line-height:12.25pt;background:white""><b><s= pan style=3D""font-size:10.0pt;font-family:&quot;Georgia&quot;,serif;color:#= 0E2841;border:none windowtext 1.0pt;padding:0in"">Thanks.</span></b><span st= yle=3D""color:black""><o:p></o:p></span></p>=0D <p class=3D""MsoNormal"" style=3D""line-height:12.25pt;background:white""><b><s= pan style=3D""font-size:10.0pt;font-family:&quot;Georgia&quot;,serif;color:#= 0E2841;border:none windowtext 1.0pt;padding:0in"">Sincerely,</span></b><span=  style=3D""color:black""><o:p></o:p></span></p>=0D <p class=3D""MsoNormal"" style=3D""line-height:12.25pt;background:white""><b><s= pan style=3D""font-size:8.0pt;font-family:&quot;Georgia&quot;,serif;color:bl= ack;border:none windowtext 1.0pt;padding:0in""><img border=3D""0"" width=3D""12= 0"" height=3D""108"" style=3D""width:1.25in;height:1.125in"" id=3D""Picture_x0020= _1"" src=3D""cid:image001.png@01DC4987.C6D654A0"" alt=3D""A yellow flame in the=  dark  Description automatically generated""></span></b><span style=3D""color:black""= ><o:p></o:p></span></p>=0D <p class=3D""MsoNormal"" style=3D""background:white""><b><span style=3D""font-fa= mily:&quot;Georgia&quot;,serif;color:black;border:none windowtext 1.0pt;pad= ding:0in"">Ayodeji Ogundiran, D.Sc.</span></b><b><span style=3D""font-family:= &quot;Georgia&quot;,serif;color:black""><o:p></o:p></span></b></p>=0D <p class=3D""MsoNormal"" style=3D""background:white""><b><span style=3D""font-si= ze:10.0pt;font-family:&quot;Georgia&quot;,serif;color:#3B3838;border:none w= indowtext 1.0pt;padding:0in"">Systems Administrator</span></b><span style=3D= ""font-size:10.0pt;color:black""><o:p></o:p></span></p>=0D <p class=3D""MsoNormal"" style=3D""background:white""><span style=3D""font-size:= 10.0pt;font-family:&quot;Verdana&quot;,sans-serif;color:#3B3838;border:none=  windowtext 1.0pt;padding:0in"">Department of Computer Science</span><span s= tyle=3D""font-size:13.5pt;color:black""><o:p></o:p></span></p>=0D <p class=3D""MsoNormal"" style=3D""background:white""><u><span style=3D""font-si= ze:10.0pt;font-family:&quot;Verdana&quot;,sans-serif;color:#0033CC;border:n= one windowtext 1.0pt;padding:0in"">Bowie State University</span></u><span st= yle=3D""font-size:13.5pt;color:black""><o:p></o:p></span></p>=0D <p class=3D""MsoNormal"" style=3D""background:white""><span style=3D""font-size:= 10.0pt;font-family:inherit;color:#3B3838;border:none windowtext 1.0pt;paddi= ng:0in"">14000 Jericho Park Road, Bowie, MD 20715-9465</span><span style=3D""= font-size:13.5pt;color:black""><o:p></o:p></span></p>=0D <p class=3D""MsoNormal""><span style=3D""color:#153D63""><img border=3D""0"" widt= h=3D""21"" height=3D""21"" style=3D""width:.2166in;height:.2166in"" id=3D""Graphic= _x0020_4"" src=3D""cid:image002.png@01DC4987.C6D654A0"" alt=3D""Speaker phone w= ith solid fill""></span><b><span style=3D""font-size:11.0pt;font-family:inher= it;color:#0A2F41;border:none windowtext 1.0pt;padding:0in"">:(301)-860-3880<= /span></b><span style=3D""font-size:11.0pt;color:#0E2841""><o:p></o:p></span>= </p>=0D <p class=3D""MsoNormal""><span style=3D""color:#153D63""><img border=3D""0"" widt= h=3D""19"" height=3D""19"" style=3D""width:.2in;height:.2in"" id=3D""Graphic_x0020= _5"" src=3D""cid:image003.png@01DC4987.C6D654A0"" alt=3D""Phone Vibration with = solid fill""></span><b><span style=3D""font-size:11.0pt;font-family:inherit;c= olor:#0A2F41;border:none windowtext 1.0pt;padding:0in"">:(240)-906-3468</spa= n></b><span style=3D""color:#153D63""><o:p></o:p></span></p>=0D <p class=3D""MsoNormal""><span style=3D""color:#153D63""><img border=3D""0"" widt= h=3D""15"" height=3D""15"" style=3D""width:.1583in;height:.1583in"" id=3D""Graphic= _x0020_1"" src=3D""cid:image004.png@01DC4987.C6D654A0"" alt=3D""Email with soli= d fill""></span><span style=3D""font-family:&quot;Segoe UI Symbol&quot;,sans-= serif;color:#153D63"">:</span><u><span style=3D""font-size:10.0pt;font-family= :&quot;Segoe UI Symbol&quot;,sans-serif;color:#153D63"">aogundiran@bowiestat= e.edu</span></u><span style=3D""font-family:&quot;Segoe UI Symbol&quot;,sans= -serif;color:#153D63""><o:p></o:p></span></p>=0D <p class=3D""MsoNormal""><span style=3D""color:#153D63""><o:p>&nbsp;</o:p></spa= n></p>=0D <div style=3D""border:none;border-top:solid #E1E1E1 1.0pt;padding:3.0pt 0in = 0in 0in"">=0D <p class=3D""MsoNormal""><b><span style=3D""font-size:11.0pt;font-family:&quot= ;Calibri&quot;,sans-serif"">From:</span></b><span style=3D""font-size:11.0pt;= font-family:&quot;Calibri&quot;,sans-serif"">=0D <br>=0D <b>Sent:</b> None<br>=0D <b>Subject:</b> <o:p></o:p></span></p>=0D </div>=0D <p class=3D""MsoNormal""><o:p>&nbsp;</o:p></p>=0D <p>Hi Hoda,<o:p></o:p></p>=0D <p>Welcome to chip, UMBC's High Performance Computing Cluster!<o:p></o:p></= p>=0D <p>&nbsp; &nbsp; The group, pi_hodae1, now exists on the chip cluster. Memb= ers of this group can access and contribute to the research storage space a= llocated to the group.<o:p></o:p></p>=0D <p>&nbsp; &nbsp; This storage space is located at /umbc/rs/pi_hodae1, and c= urrently has a quota of 10T.<o:p></o:p></p>=0D <p>&nbsp; &nbsp; For information on accessing the cluster, adding accounts = to your group, and getting started using the cluster, check out the tutoria= l on our wiki:=0D <a href=3D""https://umbc.atlassian.net/wiki/x/R4BPQg"">https://umbc.atlassian= .net/wiki/x/R4BPQg</a><o:p></o:p></p>=0D <p>&nbsp; &nbsp; Additional documentation is also available here: <a href= =3D""https://umbc.atlassian.net/wiki/x/FwCHQ"">=0D https://umbc.atlassian.net/wiki/x/FwCHQ</a><o:p></o:p></p>=0D <p>&nbsp; &nbsp; If you have any questions or issues, please submit a new R= T ticket at: <a href=3D""https://doit.umbc.edu/request-tracker-rt/doit-resea= rch-computing/"">=0D https://doit.umbc.edu/request-tracker-rt/doit-research-computing/</a><o:p><= /o:p></p>=0D <p>--&nbsp;<o:p></o:p></p>=0D <p>Kind regards,<br>=0D Danielle Esposito (she/her/hers)<br>=0D DoIT Unix Infra Student Worker<o:p></o:p></p>=0D <p>&nbsp;<o:p></o:p></p>=0D <div>=0D <p class=3D""MsoNormal"">On Thu Oct 16 12:16:29 2025, <a href=3D""mailto:hoda.= elsayed@gmail.com"">=0D hoda.elsayed@gmail.com</a> wrote: <o:p></o:p></p>=0D <blockquote style=3D""margin-top:5.0pt;margin-bottom:5.0pt"">=0D <div>=0D <p class=3D""MsoNormal"">Greetings, <o:p></o:p></p>=0D <div>=0D <p class=3D""MsoNormal"">&nbsp;<o:p></o:p></p>=0D </div>=0D <div>=0D <p class=3D""MsoNormal"">Please find attached the projects title and abstract= .<o:p></o:p></p>=0D </div>=0D </div>=0D <p class=3D""MsoNormal"">&nbsp; <o:p></o:p></p>=0D <div>=0D <div>=0D <p class=3D""MsoNormal"">On Tue, Oct 14, 2025 at 1:41<span style=3D""font-fami= ly:&quot;Arial&quot;,sans-serif"">=E2=80=AF</span>PM Danielle Esposito via R= T &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; = wrote:<o:p></o:p></p>=0D </div>=0D <blockquote style=3D""margin-top:5.0pt;margin-bottom:5.0pt"">=0D <p class=3D""MsoNormal"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticke= t/Display.html?id=3D3289695"">=0D https://rt.umbc.edu/Ticket/Display.html?id=3D3289695</a> &gt;<br>=0D <br>=0D Last Update From Ticket:<br>=0D <br>=0D Hi Hoda,<br>=0D <br>=0D Once I receive your project title and abstract, I can create an account on = chip<br>=0D for you!<br>=0D <br>=0D Have a nice day!<br>=0D <br>=0D --<br>=0D <br>=0D Kind regards,<br>=0D Danielle Esposito (she/her/hers)<br>=0D DoIT Unix Infra Student Worker<br>=0D <br>=0D On Fri Oct 10 10:54:30 2025, ZZ99999 wrote:<br>=0D <br>=0D &gt; First Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Roy= <br>=0D &gt; Last Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p;Prouty<br>=0D &gt; Email:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &= nbsp; &nbsp;<a href=3D""mailto:proutyr1@umbc.edu"">proutyr1@umbc.edu</a><br>= =0D &gt; Campus ID:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p;WH39335<br>=0D &gt;<br>=0D &gt; Request Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; High Per= formance Cluster<br>=0D &gt;<br>=0D &gt; Group Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br>=0D &gt; Project Title:&nbsp; &nbsp; &nbsp; &nbsp; Get from PI<br>=0D &gt; Project Abstract:&nbsp; &nbsp; &nbsp;Get from PI<br>=0D &gt;<br>=0D &gt; This group should be named pi_hodae1 . Don't proceed without getting t= itle/abstract from PI.<br>=0D &gt;<br>=0D <br>=0D <br>=0D &nbsp;<o:p></o:p></p>=0D </blockquote>=0D </div>=0D </blockquote>=0D </div>=0D </div>=0D </body>=0D </html>=0D "
3289695,72638112,Correspond,DoIT-Research-Computing,2025-10-31 17:48:54.0000000,HPC New Group: pi_hodae1,resolved,Danielle Esposito,desposi1,Roy Prouty,proutyr1,proutyr1@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi&nbsp;Ayodeji,</p>  <p>An account was created for Dr. El-Sayed at the time the group was create= d. They should be able to follow the log in instructions located on our wik= i to connect to the cluster. Please find documentation for that at this lin= k:&nbsp;https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting= +Started+on+chip#Accessing-chip</p>  <p>Additionally, the broken link has been fixed, here is the corrected link= :&nbsp;https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto= =3DResearch%20Computing</p>  <p>Have a nice day!</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Thu Oct 30 10:27:39 2025, AOgundiran@bowiestate.edu wrote: <blockquote> <div> <p><span style=3D""color:#153d63"">Good morning,</span></p>  <p><span style=3D""color:#153d63"">Can you kindly create an account for Dr. E= l-Sayed for this group? From the information provided, I see a group was cr= eated, but I do not see any account for her. The link </span>https://doit.u= mbc.edu/request-tracker-rt/doit-research-computing/ is broken.</p>  <p><span style=3D""color:#153d63"">&nbsp;</span></p>  <p><strong><span style=3D""color:#0e2841"">Thanks.</span></strong></p>  <p><strong><span style=3D""color:#0e2841"">Sincerely,</span></strong></p>  <p>&nbsp;</p>  <p><strong><span style=3D""color:black"">Ayodeji Ogundiran, D.Sc.</span></str= ong></p>  <p><strong><span style=3D""color:#3b3838"">Systems Administrator</span></stro= ng></p>  <p><span style=3D""color:#3b3838"">Department of Computer Science</span></p>  <p><span style=3D""color:#0033cc"">Bowie State University</span></p>  <p><span style=3D""color:#3b3838"">14000 Jericho Park Road, Bowie, MD 20715-9= 465</span></p>  <p><strong><span style=3D""color:#0a2f41"">:(301)-860-3880</span></strong></p>  <p><strong><span style=3D""color:#0a2f41"">:(240)-906-3468</span></strong></p>  <p><span style=3D""color:#153d63"">:</span><span style=3D""color:#153d63"">aogu= ndiran@bowiestate.edu</span></p>  <p><span style=3D""color:#153d63"">&nbsp;</span></p>  <div> <p><strong>From:</strong><br /> <strong>Sent:</strong> None<br /> <strong>Subject:</strong></p> </div>  <p>&nbsp;</p>  <p>Hi Hoda,</p>  <p>Welcome to chip, UMBC&#39;s High Performance Computing Cluster!</p>  <p>&nbsp; &nbsp; The group, pi_hodae1, now exists on the chip cluster. Memb= ers of this group can access and contribute to the research storage space a= llocated to the group.</p>  <p>&nbsp; &nbsp; This storage space is located at /umbc/rs/pi_hodae1, and c= urrently has a quota of 10T.</p>  <p>&nbsp; &nbsp; For information on accessing the cluster, adding accounts = to your group, and getting started using the cluster, check out the tutoria= l on our wiki: https://umbc.atlassian.net/wiki/x/R4BPQg</p>  <p>&nbsp; &nbsp; Additional documentation is also available here: https://u= mbc.atlassian.net/wiki/x/FwCHQ</p>  <p>&nbsp; &nbsp; If you have any questions or issues, please submit a new R= T ticket at: https://doit.umbc.edu/request-tracker-rt/doit-research-computi= ng/</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div> <p>On Thu Oct 16 12:16:29 2025, hoda.elsayed@gmail.com wrote:</p>  <blockquote> <div> <p>Greetings,</p>  <div> <p>&nbsp;</p> </div>  <div> <p>Please find attached the projects title and abstract.</p> </div> </div>  <p>&nbsp;</p>  <div> <div> <p>On Tue, Oct 14, 2025 at 1:41=E2=80=AFPM Danielle Esposito via RT &lt;UMB= CHelp@rt.umbc.edu&gt; wrote:</p> </div>  <blockquote> <p>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3289695 &gt= ;<br /> <br /> Last Update From Ticket:<br /> <br /> Hi Hoda,<br /> <br /> Once I receive your project title and abstract, I can create an account on = chip<br /> for you!<br /> <br /> Have a nice day!<br /> <br /> --<br /> <br /> Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker<br /> <br /> On Fri Oct 10 10:54:30 2025, ZZ99999 wrote:<br /> <br /> &gt; First Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Roy= <br /> &gt; Last Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p;Prouty<br /> &gt; Email:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &= nbsp; &nbsp;proutyr1@umbc.edu<br /> &gt; Campus ID:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p;WH39335<br /> &gt;<br /> &gt; Request Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; High Per= formance Cluster<br /> &gt;<br /> &gt; Group Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br /> &gt; Project Title:&nbsp; &nbsp; &nbsp; &nbsp; Get from PI<br /> &gt; Project Abstract:&nbsp; &nbsp; &nbsp;Get from PI<br /> &gt;<br /> &gt; This group should be named pi_hodae1 . Don&#39;t proceed without getti= ng title/abstract from PI.<br /> &gt;<br /> <br /> <br /> &nbsp;</p> </blockquote> </div> </blockquote> </div> </div> </blockquote> </div> "
3289729,72207558,Create,DoIT-Research-Computing,2025-10-10 15:33:37.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zweck,stalled,Greg Ballantine,gballan1,John Zweck,zweck,zweck@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Dear John,<br /> <br /> As per the communication via myUMBC earlier this summer (June HPCF Newsletter ), DoIT is in the process of migrating data off of an older storage server to our new RRStor Ceph storage cluster. Your group is using 36.2 GB of a 97.7 GB quota on the old storage server.<br /> <br /> To perform these migrations, we need to take individual storage volumes offline while we migrate them to the Ceph cluster. Thus we are reaching out to schedule a date where we can migrate your volume located at &ldquo;/umbc/rs/zweck&rdquo;. During the migration, we will take your volume offline and will terminate any jobs running on the chip compute cluster that are accessing this volume.<br /> <br /> Below we&rsquo;ve listed two options for handling this data migration - please let us know which of these you&rsquo;d prefer.<br /> <br /> Option 1: Schedule a group-wide downtime date during standard business hours, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate your volume to the Ceph storage cluster. DoIT staff will send an email alert on this email thread when the migration has begun and when it has completed. For most storage volumes, this process should take less than a business day.<br /> Option 2: If you don&rsquo;t respond to this email by October 15th, DoIT staff will assign a day over the following month (October 16th through November 15th) to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.<br /> <br /> Note: After this process has completed, the new storage volume will have a new name. For example, group &ldquo;pi_doit&rdquo; will find its data under &ldquo;/umbc/rs/pi_doit&rdquo;, or in your group&rsquo;s case you will find your volume under &ldquo;/umbc/rs/pi_zweck&rdquo;.<br /> <br /> Thank you,<br /> Elliot</p> "
3289729,72207567,Correspond,DoIT-Research-Computing,2025-10-10 15:33:50.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zweck,stalled,Greg Ballantine,gballan1,John Zweck,zweck,zweck@umbc.edu,NULL,exchangeadmins@utdallas.edu,"Delivery has failed to these recipients or groups:  zweck@umbc.edu<mailto:zweck@umbc.edu> The email address you entered couldn't be found. Please check the recipient's email address and try to resend the message. If the problem continues, please contact your email admin.         Diagnostic information for administrators:  Generating server: UTDEX03.campus.ad.utdallas.edu  zweck@umbc.edu Remote Server returned '550 5.1.10 RESOLVER.ADR.RecipientNotFound; Recipient not found by SMTP address lookup'  Original message headers:  Received: from UTDEX03.campus.ad.utdallas.edu (10.182.72.101) by  UTDEX03.campus.ad.utdallas.edu (10.182.72.101) with Microsoft SMTP Server  (version=TLS1_2, cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id  15.2.1748.26; Fri, 10 Oct 2025 10:33:45 -0500 Received: from DS2PR08CU001.outbound.protection.outlook.com (10.44.46.82) by  UTDEX03.campus.ad.utdallas.edu (10.182.72.101) with Microsoft SMTP Server  (version=TLS1_2, cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id  15.2.1748.26 via Frontend Transport; Fri, 10 Oct 2025 10:33:45 -0500 ARC-Seal: i=1; a=rsa-sha256; s=arcselector10001; d=microsoft.com; cv=none;  b=ZBsalmW2LMilgC8p2dKQ/WTsEZuIXmebIYW0Fq27gBjyjQmMBFRCr9BPsYjDnZ5yhV3r4MqPVsWWcszqrHA2IQ/dmNxPXO5yZt0Kd4pC7T1mOxjNmxT6MuKKpYE3iFQrFKNyS44kQro1RVcmw9JakzKqPti1KXvGCr6N96bZNvhODJ1A9hA8R0jYjYpwQlODpugokxYYaxoK3QuvCy0xMGfe1lvV/EPiEjTLgS/xLvlzrcWX3WBDjt6oy9qIxsqcM1425Iu0ZmQGFpm1KTnfKO78Ww27P0vCy/TutczPij2Bc03tWtROEtunmanK+cFavzadulX6wGdvEpFvL3d2BA== ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=microsoft.com;  s=arcselector10001;  h=From:Date:Subject:Message-ID:Content-Type:MIME-Version:X-MS-Exchange-AntiSpam-MessageData-ChunkCount:X-MS-Exchange-AntiSpam-MessageData-0:X-MS-Exchange-AntiSpam-MessageData-1;  bh=aqpgg9Hau2ECkC7BK/myqg4Qz1kQhjJTyVkXPe5hE3w=;  b=J8VakD/AZdlje1zpnCfFzGPVEhMGr0TVwgkamqP3/ZA8IE0tPPf3i5QFV46A0NHvE4gfTU/BpwkFy5ZHm+cLk3sNIJAwbO3RTWGjSbsKqqVxET3sj4UMUkqG7IMM40L6yjN5q3HAsu2Hk9kyFeFzimbTxRbR9KP81oamTDhJrgf/PZuFBLC0dfK9iV5VpaqQ9RKFjenFe9WOTtACQJFbmrqTq6ykPq/tEtzUXSU7q0EniMApjN1XDGbd2GUMKbo+iI0GgaD0ktRVwFgj47kwUT6AVBWX3b9k0Pv9RiqGMqrfKZJiVBypf++T2pswrkKZQ6uhjsM9wvem1tAp7FYl1Q== ARC-Authentication-Results: i=1; mx.microsoft.com 1; spf=softfail (sender ip  is 130.85.33.105) smtp.rcpttodomain=utdallas.edu smtp.mailfrom=rt.umbc.edu;  dmarc=pass (p=none sp=none pct=100) action=none header.from=rt.umbc.edu;  dkim=test (signature was verified) header.d=umbc.edu; arc=none (0) Received: from SJ0PR03CA0104.namprd03.prod.outlook.com (2603:10b6:a03:333::19)  by CO1PR01MB6741.prod.exchangelabs.com (2603:10b6:303:f2::16) with Microsoft  SMTP Server (version=TLS1_2, cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id  15.20.9203.10; Fri, 10 Oct 2025 15:33:43 +0000 Received: from SJ1PEPF00001CE6.namprd03.prod.outlook.com  (2603:10b6:a03:333:cafe::65) by SJ0PR03CA0104.outlook.office365.com  (2603:10b6:a03:333::19) with Microsoft SMTP Server (version=TLS1_3,  cipher=TLS_AES_256_GCM_SHA384) id 15.20.9182.20 via Frontend Transport; Fri,  10 Oct 2025 15:33:43 +0000 Authentication-Results: spf=softfail (sender IP is 130.85.33.105)  smtp.mailfrom=rt.umbc.edu; dkim=test (signature was verified)  header.d=umbc.edu;dmarc=pass action=none  header.from=rt.umbc.edu;compauth=pass reason=100 Received-SPF: SoftFail (protection.outlook.com: domain of transitioning  rt.umbc.edu discourages use of 130.85.33.105 as permitted sender) Received: from mx.mail.umbc.edu (130.85.33.105) by  SJ1PEPF00001CE6.mail.protection.outlook.com (10.167.242.22) with Microsoft  SMTP Server (version=TLS1_2, cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id  15.20.9203.9 via Frontend Transport; Fri, 10 Oct 2025 15:33:42 +0000 Received: from campus-relay.mail.umbc.edu (campus-relay1-v1.mail.umbc.edu [130.85.33.100])         by mx.mail.umbc.edu (Postfix) with ESMTPS id 509FA180E80D         for <zweck@umbc.edu>; Fri, 10 Oct 2025 11:33:41 -0400 (EDT) Received: from rt.umbc.edu (rt-web1-v1.umbc.edu [172.24.2.4])         by campus-relay.mail.umbc.edu (Postfix) with ESMTPS id 2E2571811431         for <zweck@umbc.edu>; Fri, 10 Oct 2025 11:33:41 -0400 (EDT) DKIM-Filter: OpenDKIM Filter v2.11.0 campus-relay.mail.umbc.edu 2E2571811431 DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=umbc.edu;         s=campusrelay; t=1760110421;         bh=2BPXUA1UYKt41C++cJVFyU28+4vQVM01myn7+Fz84sg=;         h=Subject:From:Reply-To:In-Reply-To:References:To:Date:From;         b=TFjoD++Fgx9b6MAr7T5bxskWlC3TtQxCyal8ydejRTjUaSncCPolFsj44B33UJ8xR          cc8aHFHC+5NDSm+fDmcua9V0WJbdQHXZTYZKw9vqgx5PK57I2T8jsYhsjCm4eUhdkM          Q0zcRSaKI0/+TP0AMW43nwwxhCn9TT52EKXp/Gio= Received: from umbc-rt-app-prod (_gateway [172.19.0.1])         by rt.umbc.edu (8.16.1/8.16.1) with SMTP id 59AFXd3U002618         for <zweck@umbc.edu>; Fri, 10 Oct 2025 15:33:39 GMT Received: by umbc-rt-app-prod (sSMTP sendmail emulation); Fri, 10 Oct 2025 15:33:39 +0000 Subject: [Research Computing #3289729] Migrating Research Storage Volume to Ceph Cluster - pi_zweck  [AutoReply] From: "" via RT"" <UMBCHelp@rt.umbc.edu> Reply-To: UMBCHelp@rt.umbc.edu In-Reply-To: References: <RT-Ticket-3289729@rt.umbc.edu> Message-ID: <rt-5.0.5-15671-1760110419-901.3289729-3-0@rt.umbc.edu> X-RT-Loop-Prevention: rt.umbc.edu X-RT-Ticket: rt.umbc.edu #3289729 X-Managed-BY: RT 5.0.5 (http://www.bestpractical.com/rt/) X-RT-Originator: elliotg2@umbc.edu Auto-Submitted: auto-replied To: <zweck@umbc.edu> Content-Type: text/plain; charset=""utf-8"" Content-Transfer-Encoding: quoted-printable X-RT-Original-Encoding: utf-8 Precedence: bulk Date: Fri, 10 Oct 2025 11:33:39 -0400 MIME-Version: 1.0 Return-Path: UMBCHelp@rt.umbc.edu X-EOPAttributedMessage: 0 X-EOPTenantAttributedMessage: 8d281d1d-9c4d-4bf7-b16e-032d15de9f6c:0 X-MS-PublicTrafficType: Email X-MS-TrafficTypeDiagnostic: SJ1PEPF00001CE6:EE_|CO1PR01MB6741:EE_ X-MS-Office365-Filtering-Correlation-Id: 4f9a9823-0ec9-479b-4676-08de081264c4 X-MS-Exchange-AtpMessageProperties: SA|SL X-Microsoft-Antispam: BCL:0;ARA:13230040|12012899012; X-Microsoft-Antispam-Message-Info: =?utf-8?B?R2VhUDJyYUMzRTR5eWgwMnZOTWIwNzBpQ1pNcHVrblpLSFl6b21MUVdOc3Fu?=  =?utf-8?B?cW15MDVZNEJGZHhtakFacVBKQ2V2bDJUcXp3YnYrZVByQ3JTMWhjWDVsV1Ji?=  =?utf-8?B?eFQrQkJYV0kyTmpNZndxNVhMYXJjZVlBS1NlbW9NcUsvVU42NW1IVTFaY3ZR?=  =?utf-8?B?dThSY0pkb1QycjloaFEvWXYvWDl5WDN2cDBhNzZrTFhyVUNtc1RFRDBlT2pK?=  =?utf-8?B?dHpLNkozVldVaWpUdmhxT2F3ZVZna3I2empRU2lMU0VXUGdhZVBRUmJoTmtG?=  =?utf-8?B?YUR4OCtMUU1VL0YwRDFOK1l3NFV4eUVYYi84V1ExelduVHdIWmJjL3RyeG1F?=  =?utf-8?B?cTF4Z2FQUXpNNk50eWI4cUt2R0ovdjNJMDc2Z0Z5NGF6Rk4waEhXLzd0WG9k?=  =?utf-8?B?NHBEQ3U4WmJjSGM1azdqSS96VElXUjIzRFhzVHpQMVRGTDNDYWlKWGxDUWNl?=  =?utf-8?B?RFZ6LzA5aDFzdTJaUWJZRXJzc3ZBR25BRFpENGoxVUw5Z2JTbGZUVjRmbWdm?=  =?utf-8?B?d0R4MUxFWXJHZnFEaFRnWmhOUFBDeWI4UWFpcXR4QXBHVFc4cDEvNk8rQ2l5?=  =?utf-8?B?T0xjN20zdmFKeGpkMzhBaFhwcDlEanIrbyt2TjFxK3VBOUZPQUZBSUhTTzMy?=  =?utf-8?B?L3JWRVRSOWVDWmhidUVybGVmc3dXRUhvRklvSllKY004bktEN1pZQzNCcVBE?=  =?utf-8?B?YXoyL0llOHUxM1dhZUYyQmpJclczN2xPTEpwTXFwbnEya29HcGJjamM5djZI?=  =?utf-8?B?N3R0YmJBUFIxMXhTSExLU2xHTFlHblEwZ2VPU3hDeXZjeHpITlBuRmtHSVhj?=  =?utf-8?B?OWN4TkpGK0lOc1kwQjFVTkRrTDFtRzZKU0NpeWFPTUxCZCtBaVJkdDZPYlJr?=  =?utf-8?B?K3YvRGxYYUNQRnBCbUV0ekN5eHVZNG56UUNPL2wvWVM3a0ZRQ2l2OWxvMjdj?=  =?utf-8?B?OFl0MVhTcUl6bitEblRoa2FVTzIrNTBGM1loY0ZGK1ZRY1hWb2pETTBVWHlK?=  =?utf-8?B?SmlXbjR6b01oZzBWQ0lDUVhVeCtZcmUxc2JaekdnN3kremQ4RWJtTDVMQ0s3?=  =?utf-8?B?RC9OWkVTMHRTcGNNNTVuYU1NRG9mMjVQVTNDQ0pnT1ViQnpSZUx2SE5NeDN5?=  =?utf-8?B?MmZkbjBWeGRTM1BhMm51YUZmNXdnM0c4QmxEQi9LQ20wcGwxcjZPK0ZPaWFm?=  =?utf-8?B?RzQ0MzdFbkFFRnRSRlNqTFF4TG1BRUZhdWw2NUR5VlFWTitDNTlSamQxVnBE?=  =?utf-8?B?K0gzTlVubWVST3ovZDJXcnE4UmtRSlBRT0JTOHRvOStIV3o4MzhwQkRqNkx4?=  =?utf-8?B?WHovUGVkbWJTQlJmUUt6QmljcURHYmFoUytPUTlHa2ZUZEtvMDlidWJlbW9G?=  =?utf-8?B?TGprNEVQOTJWd1gwYzlSdWlpd0lOQ0hCMkhBSFpOdFNsMmcxd3VmUjZ2cjdV?=  =?utf-8?B?WStZd0N1Z2NtaWxpckdqcVhnVWh4R2Y4SVE4TWJqNisrRUx4V2FsYXBZMnhx?=  =?utf-8?B?Y29xZTlKSWFldVpqMFlYa3FhMk9qSFUwbnVSZTl2UmNZRlJQdTdORDRXSmg0?=  =?utf-8?B?K2lCamJKRE1pQzQwVmsycVZsV1ZSWGR4T1JmWkRyN0pZNHRZTjdjQzFodjNm?=  =?utf-8?B?VjRIZ2ZqNmxiVlplcVlhKzVrU3E0V3JyOXFlNFVLSllSWGZNemtFZVFESGZQ?=  =?utf-8?B?L1Y3NlRpUFZlNTJSQUo2MEo5VzBKcmZMUC80UjJHNG92QURSQk4zQWo5QkRr?=  =?utf-8?B?cDVMZm1CRFFpZ1dJY1RiY0VRRkcyTnZzQU1kc2lhQytSWGZ0ZWhyVlFzUVUx?=  =?utf-8?B?SloxWU1kSlZiV3FvTWhoMDZTcUdpR09Cb0VmR2xVOUp0eDVVWitJeWhxd1hE?=  =?utf-8?B?N3d6YW5QR0xQQTNBZVc0RVdvUWlZMCtUVXpCZXY3SEN6L2loZkpWQXBPUzVC?=  =?utf-8?B?MVNyS3d0bWk2Q2xKN2ZxYXRDNEVkSXRZVFptVDFxN2sxUGVvSnh1bUNDeUVX?=  =?utf-8?B?V1dnQ01hdVFScUxFQ2xiZ1hxYmdyTTZHZGh3YnVFSmYwaVRPclVnWGFlVFZB?=  =?utf-8?B?M0xPZTNzZXo5MG42dXFtalcrQTdpZThZV1hxeVI3NFV4UUtMZzhpcHFNNCtr?=  =?utf-8?B?ZUVvc29sV05oczJQakhxUHR3eGVzQ1pHblRZV2hkRy9QMzZqWWdCeVIwN2Ux?=  =?utf-8?B?cDJQN3lEYkRXSUhYNkFUeWh0YkJlYzdBQ1dqazNZZEEzZTk0K2g1UGZxMG5W?=  =?utf-8?B?NGVaYlR0Wnk4UXFzUXZrVlFhbWkrUDdZU1AwZW14Nnk0aTV1UFBENVBZaU1L?=  =?utf-8?B?cys4d0hCY1p2Nks4YjBGZnNDenYwV2gxd2krYXEyQUpjdEhZWFZYNWZDNURI?=  =?utf-8?B?WE0rQ1VZUU92RGVRaHZLa0wxTmhQSjNZZU1STzdUbkdWMjI0OE4zU3FrZ0p4?=  =?utf-8?B?cDVUdWpldDBzNEpaeVZRRjZ3L2lxd0xHVGxMS2dyTncxMjkwckNuWHVSeFlU?=  =?utf-8?B?T1FvWTFvcHRSQUVUeGxUR2w1SmRYUUduNG9pNlIrWmZkcUQwa05qbU5ta0hv?=  =?utf-8?B?YW5mMkNvQmJlQTBWd2VXNGh0Mmt1SktnV2xMNThxT3dHSFFKbzZSNmszR0ZZ?=  =?utf-8?B?M3Bjd0NtNGYrbFRWa0t3WjY2V0pUd0k0aDFTTlVMZmtyNHpMRXVsSHVpaTBN?=  =?utf-8?B?MlhWcjVGU2hXYnVCNEI3blVCTVZlb1ExeVdFUi9CUkc0VnUyNGRYbXBhVDYv?=  =?utf-8?B?OUlsMFBraGtDajFBajNyMSttb0pXTTR0Vmg4enRweklWcU9GaENQc0lJL3ZZ?=  =?utf-8?B?RmtTZEdDTEZMK25nMDJUMlN3ZHU3M1NkYlVEamF4YjZYRFhNZzZ4WkF1SGYw?=  =?utf-8?B?UGZuZldiSS9tcnl6cHE5V3k1WHhxWTZsRkhmczNKQUd1WjBzblRIeHpMQzVt?=  =?utf-8?Q?omuEpM/vH9k1Gh89?= X-Forefront-Antispam-Report: CIP:130.85.33.105;CTRY:US;LANG:en;SCL:1;SRV:;IPV:NLI;SFV:NSPM;H:mx.mail.umbc.edu;PTR:mx2-v3.mail.umbc.edu;CAT:NONE;SFS:(13230040)(12012899012);DIR:INB; X-MS-Exchange-ATPSafeLinks-Stat: 0 X-MS-Exchange-ATPSafeLinks-BitVector: 80000:0x0|0x0|0x0|0x0|0x0|0x80000; X-MS-Exchange-CrossTenant-OriginalArrivalTime: 10 Oct 2025 15:33:42.4323  (UTC) X-MS-Exchange-CrossTenant-Network-Message-Id: 4f9a9823-0ec9-479b-4676-08de081264c4 X-MS-Exchange-CrossTenant-Id: 8d281d1d-9c4d-4bf7-b16e-032d15de9f6c X-MS-Exchange-CrossTenant-AuthSource: SJ1PEPF00001CE6.namprd03.prod.outlook.com X-MS-Exchange-CrossTenant-AuthAs: Anonymous X-MS-Exchange-CrossTenant-FromEntityHeader: Internet X-MS-Exchange-Transport-CrossTenantHeadersStamped: CO1PR01MB6741 X-OrganizationHeadersPreserved: CO1PR01MB6741.prod.exchangelabs.com X-CrossPremisesHeadersPromoted: UTDEX03.campus.ad.utdallas.edu X-CrossPremisesHeadersFiltered: UTDEX03.campus.ad.utdallas.edu X-OriginatorOrg: utdallas.edu  "
3289729,72207567,Correspond,DoIT-Research-Computing,2025-10-10 15:33:50.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zweck,stalled,Greg Ballantine,gballan1,John Zweck,zweck,zweck@umbc.edu,NULL,exchangeadmins@utdallas.edu,"<html> <Head></head><body> <p><b><font color=""#000066"" size=""3"" face=""Arial"">Delivery has failed to these recipients or groups:</font></b></p> <font color=""#000000"" size=""2"" face=""Tahoma""><p><a href=""mailto:zweck@umbc.edu"">zweck@umbc.edu</a><br> </font> <font color=""#000000"" size=""3"" face=""Arial"">The email address you entered couldn't be found. Please check the recipient's email address and try to resend the message. If the problem continues, please contact your email admin.<br> </font> <font color=""#000000"" size=""2"" face=""Tahoma""><br> </p> </font> <br><br><br><br><br><br> <font color=""#808080"" size=""2"" face=""Tahoma""><p><b>Diagnostic information for administrators:</b></p> <p>Generating server: UTDEX03.campus.ad.utdallas.edu<br> </p> <p>zweck@umbc.edu<br> Remote Server  returned '550 5.1.10 RESOLVER.ADR.RecipientNotFound; Recipient not found by SMTP address lookup'<br> </p> <p>Original message headers:</p> <pre>Received: from UTDEX03.campus.ad.utdallas.edu (10.182.72.101) by  UTDEX03.campus.ad.utdallas.edu (10.182.72.101) with Microsoft SMTP Server  (version=TLS1_2, cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id  15.2.1748.26; Fri, 10 Oct 2025 10:33:45 -0500 Received: from DS2PR08CU001.outbound.protection.outlook.com (10.44.46.82) by  UTDEX03.campus.ad.utdallas.edu (10.182.72.101) with Microsoft SMTP Server  (version=TLS1_2, cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id  15.2.1748.26 via Frontend Transport; Fri, 10 Oct 2025 10:33:45 -0500 ARC-Seal: i=1; a=rsa-sha256; s=arcselector10001; d=microsoft.com; cv=none;  b=ZBsalmW2LMilgC8p2dKQ/WTsEZuIXmebIYW0Fq27gBjyjQmMBFRCr9BPsYjDnZ5yhV3r4MqPVsWWcszqrHA2IQ/dmNxPXO5yZt0Kd4pC7T1mOxjNmxT6MuKKpYE3iFQrFKNyS44kQro1RVcmw9JakzKqPti1KXvGCr6N96bZNvhODJ1A9hA8R0jYjYpwQlODpugokxYYaxoK3QuvCy0xMGfe1lvV/EPiEjTLgS/xLvlzrcWX3WBDjt6oy9qIxsqcM1425Iu0ZmQGFpm1KTnfKO78Ww27P0vCy/TutczPij2Bc03tWtROEtunmanK+cFavzadulX6wGdvEpFvL3d2BA== ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=microsoft.com;  s=arcselector10001;  h=From:Date:Subject:Message-ID:Content-Type:MIME-Version:X-MS-Exchange-AntiSpam-MessageData-ChunkCount:X-MS-Exchange-AntiSpam-MessageData-0:X-MS-Exchange-AntiSpam-MessageData-1;  bh=aqpgg9Hau2ECkC7BK/myqg4Qz1kQhjJTyVkXPe5hE3w=;  b=J8VakD/AZdlje1zpnCfFzGPVEhMGr0TVwgkamqP3/ZA8IE0tPPf3i5QFV46A0NHvE4gfTU/BpwkFy5ZHm+cLk3sNIJAwbO3RTWGjSbsKqqVxET3sj4UMUkqG7IMM40L6yjN5q3HAsu2Hk9kyFeFzimbTxRbR9KP81oamTDhJrgf/PZuFBLC0dfK9iV5VpaqQ9RKFjenFe9WOTtACQJFbmrqTq6ykPq/tEtzUXSU7q0EniMApjN1XDGbd2GUMKbo+iI0GgaD0ktRVwFgj47kwUT6AVBWX3b9k0Pv9RiqGMqrfKZJiVBypf++T2pswrkKZQ6uhjsM9wvem1tAp7FYl1Q== ARC-Authentication-Results: i=1; mx.microsoft.com 1; spf=softfail (sender ip  is 130.85.33.105) smtp.rcpttodomain=utdallas.edu smtp.mailfrom=rt.umbc.edu;  dmarc=pass (p=none sp=none pct=100) action=none header.from=rt.umbc.edu;  dkim=test (signature was verified) header.d=umbc.edu; arc=none (0) Received: from SJ0PR03CA0104.namprd03.prod.outlook.com (2603:10b6:a03:333::19)  by CO1PR01MB6741.prod.exchangelabs.com (2603:10b6:303:f2::16) with Microsoft  SMTP Server (version=TLS1_2, cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id  15.20.9203.10; Fri, 10 Oct 2025 15:33:43 +0000 Received: from SJ1PEPF00001CE6.namprd03.prod.outlook.com  (2603:10b6:a03:333:cafe::65) by SJ0PR03CA0104.outlook.office365.com  (2603:10b6:a03:333::19) with Microsoft SMTP Server (version=TLS1_3,  cipher=TLS_AES_256_GCM_SHA384) id 15.20.9182.20 via Frontend Transport; Fri,  10 Oct 2025 15:33:43 +0000 Authentication-Results: spf=softfail (sender IP is 130.85.33.105)  smtp.mailfrom=rt.umbc.edu; dkim=test (signature was verified)  header.d=umbc.edu;dmarc=pass action=none  header.from=rt.umbc.edu;compauth=pass reason=100 Received-SPF: SoftFail (protection.outlook.com: domain of transitioning  rt.umbc.edu discourages use of 130.85.33.105 as permitted sender) Received: from mx.mail.umbc.edu (130.85.33.105) by  SJ1PEPF00001CE6.mail.protection.outlook.com (10.167.242.22) with Microsoft  SMTP Server (version=TLS1_2, cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id  15.20.9203.9 via Frontend Transport; Fri, 10 Oct 2025 15:33:42 +0000 Received: from campus-relay.mail.umbc.edu (campus-relay1-v1.mail.umbc.edu [130.85.33.100]) 	by mx.mail.umbc.edu (Postfix) with ESMTPS id 509FA180E80D 	for &lt;zweck@umbc.edu&gt;; Fri, 10 Oct 2025 11:33:41 -0400 (EDT) Received: from rt.umbc.edu (rt-web1-v1.umbc.edu [172.24.2.4]) 	by campus-relay.mail.umbc.edu (Postfix) with ESMTPS id 2E2571811431 	for &lt;zweck@umbc.edu&gt;; Fri, 10 Oct 2025 11:33:41 -0400 (EDT) DKIM-Filter: OpenDKIM Filter v2.11.0 campus-relay.mail.umbc.edu 2E2571811431 DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=umbc.edu; 	s=campusrelay; t=1760110421; 	bh=2BPXUA1UYKt41C++cJVFyU28+4vQVM01myn7+Fz84sg=; 	h=Subject:From:Reply-To:In-Reply-To:References:To:Date:From; 	b=TFjoD++Fgx9b6MAr7T5bxskWlC3TtQxCyal8ydejRTjUaSncCPolFsj44B33UJ8xR 	 cc8aHFHC+5NDSm+fDmcua9V0WJbdQHXZTYZKw9vqgx5PK57I2T8jsYhsjCm4eUhdkM 	 Q0zcRSaKI0/+TP0AMW43nwwxhCn9TT52EKXp/Gio= Received: from umbc-rt-app-prod (_gateway [172.19.0.1]) 	by rt.umbc.edu (8.16.1/8.16.1) with SMTP id 59AFXd3U002618 	for &lt;zweck@umbc.edu&gt;; Fri, 10 Oct 2025 15:33:39 GMT Received: by umbc-rt-app-prod (sSMTP sendmail emulation); Fri, 10 Oct 2025 15:33:39 +0000 Subject: [Research Computing #3289729] Migrating Research Storage Volume to Ceph Cluster - pi_zweck  [AutoReply] From: &quot; via RT&quot; &lt;UMBCHelp@rt.umbc.edu&gt; Reply-To: UMBCHelp@rt.umbc.edu In-Reply-To: References: &lt;RT-Ticket-3289729@rt.umbc.edu&gt; Message-ID: &lt;rt-5.0.5-15671-1760110419-901.3289729-3-0@rt.umbc.edu&gt; X-RT-Loop-Prevention: rt.umbc.edu X-RT-Ticket: rt.umbc.edu #3289729 X-Managed-BY: RT 5.0.5 (http://www.bestpractical.com/rt/) X-RT-Originator: elliotg2@umbc.edu Auto-Submitted: auto-replied To: &lt;zweck@umbc.edu&gt; Content-Type: text/plain; charset=&quot;utf-8&quot; Content-Transfer-Encoding: quoted-printable X-RT-Original-Encoding: utf-8 Precedence: bulk Date: Fri, 10 Oct 2025 11:33:39 -0400 MIME-Version: 1.0 Return-Path: UMBCHelp@rt.umbc.edu X-EOPAttributedMessage: 0 X-EOPTenantAttributedMessage: 8d281d1d-9c4d-4bf7-b16e-032d15de9f6c:0 X-MS-PublicTrafficType: Email X-MS-TrafficTypeDiagnostic: SJ1PEPF00001CE6:EE_|CO1PR01MB6741:EE_ X-MS-Office365-Filtering-Correlation-Id: 4f9a9823-0ec9-479b-4676-08de081264c4 X-MS-Exchange-AtpMessageProperties: SA|SL X-Microsoft-Antispam: BCL:0;ARA:13230040|12012899012; X-Microsoft-Antispam-Message-Info: =?utf-8?B?R2VhUDJyYUMzRTR5eWgwMnZOTWIwNzBpQ1pNcHVrblpLSFl6b21MUVdOc3Fu?=  =?utf-8?B?cW15MDVZNEJGZHhtakFacVBKQ2V2bDJUcXp3YnYrZVByQ3JTMWhjWDVsV1Ji?=  =?utf-8?B?eFQrQkJYV0kyTmpNZndxNVhMYXJjZVlBS1NlbW9NcUsvVU42NW1IVTFaY3ZR?=  =?utf-8?B?dThSY0pkb1QycjloaFEvWXYvWDl5WDN2cDBhNzZrTFhyVUNtc1RFRDBlT2pK?=  =?utf-8?B?dHpLNkozVldVaWpUdmhxT2F3ZVZna3I2empRU2lMU0VXUGdhZVBRUmJoTmtG?=  =?utf-8?B?YUR4OCtMUU1VL0YwRDFOK1l3NFV4eUVYYi84V1ExelduVHdIWmJjL3RyeG1F?=  =?utf-8?B?cTF4Z2FQUXpNNk50eWI4cUt2R0ovdjNJMDc2Z0Z5NGF6Rk4waEhXLzd0WG9k?=  =?utf-8?B?NHBEQ3U4WmJjSGM1azdqSS96VElXUjIzRFhzVHpQMVRGTDNDYWlKWGxDUWNl?=  =?utf-8?B?RFZ6LzA5aDFzdTJaUWJZRXJzc3ZBR25BRFpENGoxVUw5Z2JTbGZUVjRmbWdm?=  =?utf-8?B?d0R4MUxFWXJHZnFEaFRnWmhOUFBDeWI4UWFpcXR4QXBHVFc4cDEvNk8rQ2l5?=  =?utf-8?B?T0xjN20zdmFKeGpkMzhBaFhwcDlEanIrbyt2TjFxK3VBOUZPQUZBSUhTTzMy?=  =?utf-8?B?L3JWRVRSOWVDWmhidUVybGVmc3dXRUhvRklvSllKY004bktEN1pZQzNCcVBE?=  =?utf-8?B?YXoyL0llOHUxM1dhZUYyQmpJclczN2xPTEpwTXFwbnEya29HcGJjamM5djZI?=  =?utf-8?B?N3R0YmJBUFIxMXhTSExLU2xHTFlHblEwZ2VPU3hDeXZjeHpITlBuRmtHSVhj?=  =?utf-8?B?OWN4TkpGK0lOc1kwQjFVTkRrTDFtRzZKU0NpeWFPTUxCZCtBaVJkdDZPYlJr?=  =?utf-8?B?K3YvRGxYYUNQRnBCbUV0ekN5eHVZNG56UUNPL2wvWVM3a0ZRQ2l2OWxvMjdj?=  =?utf-8?B?OFl0MVhTcUl6bitEblRoa2FVTzIrNTBGM1loY0ZGK1ZRY1hWb2pETTBVWHlK?=  =?utf-8?B?SmlXbjR6b01oZzBWQ0lDUVhVeCtZcmUxc2JaekdnN3kremQ4RWJtTDVMQ0s3?=  =?utf-8?B?RC9OWkVTMHRTcGNNNTVuYU1NRG9mMjVQVTNDQ0pnT1ViQnpSZUx2SE5NeDN5?=  =?utf-8?B?MmZkbjBWeGRTM1BhMm51YUZmNXdnM0c4QmxEQi9LQ20wcGwxcjZPK0ZPaWFm?=  =?utf-8?B?RzQ0MzdFbkFFRnRSRlNqTFF4TG1BRUZhdWw2NUR5VlFWTitDNTlSamQxVnBE?=  =?utf-8?B?K0gzTlVubWVST3ovZDJXcnE4UmtRSlBRT0JTOHRvOStIV3o4MzhwQkRqNkx4?=  =?utf-8?B?WHovUGVkbWJTQlJmUUt6QmljcURHYmFoUytPUTlHa2ZUZEtvMDlidWJlbW9G?=  =?utf-8?B?TGprNEVQOTJWd1gwYzlSdWlpd0lOQ0hCMkhBSFpOdFNsMmcxd3VmUjZ2cjdV?=  =?utf-8?B?WStZd0N1Z2NtaWxpckdqcVhnVWh4R2Y4SVE4TWJqNisrRUx4V2FsYXBZMnhx?=  =?utf-8?B?Y29xZTlKSWFldVpqMFlYa3FhMk9qSFUwbnVSZTl2UmNZRlJQdTdORDRXSmg0?=  =?utf-8?B?K2lCamJKRE1pQzQwVmsycVZsV1ZSWGR4T1JmWkRyN0pZNHRZTjdjQzFodjNm?=  =?utf-8?B?VjRIZ2ZqNmxiVlplcVlhKzVrU3E0V3JyOXFlNFVLSllSWGZNemtFZVFESGZQ?=  =?utf-8?B?L1Y3NlRpUFZlNTJSQUo2MEo5VzBKcmZMUC80UjJHNG92QURSQk4zQWo5QkRr?=  =?utf-8?B?cDVMZm1CRFFpZ1dJY1RiY0VRRkcyTnZzQU1kc2lhQytSWGZ0ZWhyVlFzUVUx?=  =?utf-8?B?SloxWU1kSlZiV3FvTWhoMDZTcUdpR09Cb0VmR2xVOUp0eDVVWitJeWhxd1hE?=  =?utf-8?B?N3d6YW5QR0xQQTNBZVc0RVdvUWlZMCtUVXpCZXY3SEN6L2loZkpWQXBPUzVC?=  =?utf-8?B?MVNyS3d0bWk2Q2xKN2ZxYXRDNEVkSXRZVFptVDFxN2sxUGVvSnh1bUNDeUVX?=  =?utf-8?B?V1dnQ01hdVFScUxFQ2xiZ1hxYmdyTTZHZGh3YnVFSmYwaVRPclVnWGFlVFZB?=  =?utf-8?B?M0xPZTNzZXo5MG42dXFtalcrQTdpZThZV1hxeVI3NFV4UUtMZzhpcHFNNCtr?=  =?utf-8?B?ZUVvc29sV05oczJQakhxUHR3eGVzQ1pHblRZV2hkRy9QMzZqWWdCeVIwN2Ux?=  =?utf-8?B?cDJQN3lEYkRXSUhYNkFUeWh0YkJlYzdBQ1dqazNZZEEzZTk0K2g1UGZxMG5W?=  =?utf-8?B?NGVaYlR0Wnk4UXFzUXZrVlFhbWkrUDdZU1AwZW14Nnk0aTV1UFBENVBZaU1L?=  =?utf-8?B?cys4d0hCY1p2Nks4YjBGZnNDenYwV2gxd2krYXEyQUpjdEhZWFZYNWZDNURI?=  =?utf-8?B?WE0rQ1VZUU92RGVRaHZLa0wxTmhQSjNZZU1STzdUbkdWMjI0OE4zU3FrZ0p4?=  =?utf-8?B?cDVUdWpldDBzNEpaeVZRRjZ3L2lxd0xHVGxMS2dyTncxMjkwckNuWHVSeFlU?=  =?utf-8?B?T1FvWTFvcHRSQUVUeGxUR2w1SmRYUUduNG9pNlIrWmZkcUQwa05qbU5ta0hv?=  =?utf-8?B?YW5mMkNvQmJlQTBWd2VXNGh0Mmt1SktnV2xMNThxT3dHSFFKbzZSNmszR0ZZ?=  =?utf-8?B?M3Bjd0NtNGYrbFRWa0t3WjY2V0pUd0k0aDFTTlVMZmtyNHpMRXVsSHVpaTBN?=  =?utf-8?B?MlhWcjVGU2hXYnVCNEI3blVCTVZlb1ExeVdFUi9CUkc0VnUyNGRYbXBhVDYv?=  =?utf-8?B?OUlsMFBraGtDajFBajNyMSttb0pXTTR0Vmg4enRweklWcU9GaENQc0lJL3ZZ?=  =?utf-8?B?RmtTZEdDTEZMK25nMDJUMlN3ZHU3M1NkYlVEamF4YjZYRFhNZzZ4WkF1SGYw?=  =?utf-8?B?UGZuZldiSS9tcnl6cHE5V3k1WHhxWTZsRkhmczNKQUd1WjBzblRIeHpMQzVt?=  =?utf-8?Q?omuEpM/vH9k1Gh89?= X-Forefront-Antispam-Report: CIP:130.85.33.105;CTRY:US;LANG:en;SCL:1;SRV:;IPV:NLI;SFV:NSPM;H:mx.mail.umbc.edu;PTR:mx2-v3.mail.umbc.edu;CAT:NONE;SFS:(13230040)(12012899012);DIR:INB; X-MS-Exchange-ATPSafeLinks-Stat: 0 X-MS-Exchange-ATPSafeLinks-BitVector: 80000:0x0|0x0|0x0|0x0|0x0|0x80000; X-MS-Exchange-CrossTenant-OriginalArrivalTime: 10 Oct 2025 15:33:42.4323  (UTC) X-MS-Exchange-CrossTenant-Network-Message-Id: 4f9a9823-0ec9-479b-4676-08de081264c4 X-MS-Exchange-CrossTenant-Id: 8d281d1d-9c4d-4bf7-b16e-032d15de9f6c X-MS-Exchange-CrossTenant-AuthSource: SJ1PEPF00001CE6.namprd03.prod.outlook.com X-MS-Exchange-CrossTenant-AuthAs: Anonymous X-MS-Exchange-CrossTenant-FromEntityHeader: Internet X-MS-Exchange-Transport-CrossTenantHeadersStamped: CO1PR01MB6741 X-OrganizationHeadersPreserved: CO1PR01MB6741.prod.exchangelabs.com X-CrossPremisesHeadersPromoted: UTDEX03.campus.ad.utdallas.edu X-CrossPremisesHeadersFiltered: UTDEX03.campus.ad.utdallas.edu X-OriginatorOrg: utdallas.edu </pre> </font> </body> </html>"
3289729,72207567,Correspond,DoIT-Research-Computing,2025-10-10 15:33:50.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zweck,stalled,Greg Ballantine,gballan1,John Zweck,zweck,zweck@umbc.edu,NULL,exchangeadmins@utdallas.edu,"Greetings,  This message has been automatically generated in response to the creation of a ticket regarding:  ------------------------------------------------------------------------- Subject: ""Migrating Research Storage Volume to Ceph Cluster - pi_zweck""  Message:=20  Dear John,  As per the communication via myUMBC earlier this summer (June HPCF Newslett= er ), DoIT is in the process of migrating data off of an older storage server = to our new RRStor Ceph storage cluster. Your group is using 36.2 GB of a 97.7 = GB quota on the old storage server.  To perform these migrations, we need to take individual storage volumes off= line while we migrate them to the Ceph cluster. Thus we are reaching out to sche= dule a date where we can migrate your volume located at =E2=80=9C/umbc/rs/zweck= =E2=80=9D. During the migration, we will take your volume offline and will terminate any jobs run= ning on the chip compute cluster that are accessing this volume.  Below we=E2=80=99ve listed two options for handling this data migration - p= lease let us know which of these you=E2=80=99d prefer.  Option 1: Schedule a group-wide downtime date during standard business hour= s, which can be done by responding to this email with your preferred date(s) to perform the migration. During this time, DoIT staff will work to migrate yo= ur volume to the Ceph storage cluster. DoIT staff will send an email alert on = this email thread when the migration has begun and when it has completed. For mo= st storage volumes, this process should take less than a business day. Option 2: If you don=E2=80=99t respond to this email by October 15th, DoIT = staff will assign a day over the following month (October 16th through November 15th) = to migrate your volume. The day chosen will be random and will occur during business hours. You will be notified of the date chosen to perform the migration, and will be notified when the migration begins and completes.  Note: After this process has completed, the new storage volume will have a = new name. For example, group =E2=80=9Cpi_doit=E2=80=9D will find its data under=  =E2=80=9C/umbc/rs/pi_doit=E2=80=9D, or in your group=E2=80=99s case you will find your volume under =E2=80=9C/u= mbc/rs/pi_zweck=E2=80=9D.  Thank you, Elliot   ------------------------------------------------------------------------- =20 There is no need to reply to this message right now.=20=20  Your ticket has been assigned an ID of [Research Computing #3289729] or you=  can go there directly by clicking the link below.  Ticket <URL: https://nam02.safelinks.protection.outlook.com/?url=3Dhttps%3A= %2F%2Frt.umbc.edu%2FTicket%2FDisplay.html%3Fid%3D3289729&data=3D05%7C02%7Cj= wz120030%40utdallas.edu%7C4f9a98230ec9479b467608de081264c4%7C8d281d1d9c4d4b= f7b16e032d15de9f6c%7C0%7C0%7C638957072251751895%7CUnknown%7CTWFpbGZsb3d8eyJ= FbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIl= dUIjoyfQ%3D%3D%7C80000%7C%7C%7C&sdata=3DBCnSZcc2HnOrHuOz1E6UfSvDJQxswM9fh5d= dnL1Gea0%3D&reserved=3D0 >  You can login to view your open tickets at any time by visiting https://nam= 02.safelinks.protection.outlook.com/?url=3Dhttp%3A%2F%2Fmy.umbc.edu%2F&data= =3D05%7C02%7Cjwz120030%40utdallas.edu%7C4f9a98230ec9479b467608de081264c4%7C= 8d281d1d9c4d4bf7b16e032d15de9f6c%7C0%7C0%7C638957072251783959%7CUnknown%7CT= WFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFO= IjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C80000%7C%7C%7C&sdata=3D6eNirFugOGfXypAILjmA4= I%2FS8HsN5SMrgs%2FlSFBipFE%3D&reserved=3D0 and clicking on ""Help"" and ""Requ= est Help"".=20  Alternately you can click on https://nam02.safelinks.protection.outlook.com= /?url=3Dhttp%3A%2F%2Fmy.umbc.edu%2Fhelp&data=3D05%7C02%7Cjwz120030%40utdall= as.edu%7C4f9a98230ec9479b467608de081264c4%7C8d281d1d9c4d4bf7b16e032d15de9f6= c%7C0%7C0%7C638957072251805892%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRy= dWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C= 80000%7C%7C%7C&sdata=3DuX5mRg8OGdtB2KWHLw%2BKS80IkBZfFH3apXE83TXIEC0%3D&res= erved=3D0                          Thank you  "
3289729,72207587,Comment,DoIT-Research-Computing,2025-10-10 15:34:13.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zweck,stalled,Greg Ballantine,gballan1,John Zweck,zweck,zweck@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Cc&#39;ing myself to this ticket, in case there&#39;s any more problems</p> "
3289729,72207588,CommentEmailRecord,DoIT-Research-Computing,2025-10-10 15:34:14.0000000,Migrating Research Storage Volume to Ceph Cluster - pi_zweck,stalled,Greg Ballantine,gballan1,John Zweck,zweck,zweck@umbc.edu,The RT System itself,NULL,"Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3289729 >  Comment just added.    Cc'ing myself to this ticket, in case there's any more problems  "
3289747,72208189,Create,DoIT-Research-Computing,2025-10-10 15:51:15.0000000,HPC User Account: burnsm in Student Group,resolved,Elliot Gobbert,elliotg2,Mercedes Burns,burnsm,burnsm@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Mercedes Last Name:                 Burns Email:                     burnsm@umbc.edu Campus ID:                 OF19978  Request Type:              High Performance Cluster  Create/Modify account in Student group  I would like to add Lais Grossel (VE32332; laisg1) to my lab's group for access to the chip cluster. She should have access through 6/30/2026.   "
3289747,72209828,Comment,DoIT-Research-Computing,2025-10-10 16:35:39.0000000,HPC User Account: burnsm in Student Group,resolved,Elliot Gobbert,elliotg2,Mercedes Burns,burnsm,burnsm@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hi Lais,</p>  <p>Your account (laisg1) has been created on chip.rs.umbc.edu.<br /> Your primary group is pi_burnsm.<br /> Your home directory has 500M of storage.<br /> You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip<br /> Please read through the documentation found at hpcf.umbc.edu &gt; User Support.<br /> All available modules can be viewed using the command &#39;module avail&#39;.<br /> Please submit additional questions or issues as separate tickets via the following link.<br /> (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/)<br /> <br /> <br /> THIS DID NOT WORK, but saving this response for later.</p> "
3289747,72214552,Correspond,DoIT-Research-Computing,2025-10-10 19:07:35.0000000,HPC User Account: burnsm in Student Group,resolved,Elliot Gobbert,elliotg2,Mercedes Burns,burnsm,burnsm@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hi Lais,</p>  <p>Your account (laisg1) has been created on chip.rs.umbc.edu.<br /> Your primary group is pi_burnsm.<br /> Your home directory has 500M of storage.<br /> You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip<br /> Please read through the documentation found at hpcf.umbc.edu &gt; User Support.<br /> All available modules can be viewed using the command &#39;module avail&#39;.<br /> Please submit additional questions or issues as separate tickets via the following link.<br /> (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/)<br /> <br /> Let me know if you have any more questions!</p>  <p>Elliot</p> "
3289785,72209705,Create,DoIT-Research-Computing,2025-10-10 16:32:34.0000000,HPC User Account: rmwillia in misc-lab,new,Nobody in particular," ",Rebecca Williams,rmwillia,rmwillia@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Rebecca Last Name:                 Williams Email:                     rmwillia@umbc.edu Campus ID:                 KC16614  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    rmwillia@umbc.edu Existing Group:       misc-lab Project Title:        Misc Lab Project Abstract:     group for the Misc Lab  requesting new PI group  "
3289788,72209865,Create,DoIT-Research-Computing,2025-10-10 16:36:53.0000000,create new HPC PI group,resolved,Max Breitmeyer,mb17,Rebecca Williams,rmwillia,rmwillia@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Rebecca Last Name:                 Williams Email:                     rmwillia@umbc.edu Campus ID:                 KC16614  Request Type:              Help with something else  group name:  misc-lab list of members: rmwillia@umbc.edu  "
3290086,72219457,Create,DoIT-Research-Computing,2025-10-11 16:40:16.0000000,HPC Other Issue: Not able to connect to JupyterLab on Chip,resolved,Danielle Esposito,desposi1,Rajarshi Basak,rbasak1,rbasak1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Rajarshi<br /> Last Name:                 Basak<br /> Email:                     rbasak1@umbc.edu<br /> Campus ID:                 AT69971<br /> <br /> Request Type:              High Performance Cluster<br /> <br /> Hello, =0D<br /> =0D<br /> I am referring to this guide - https://umbc.atlassian.net/wiki/spaces/faq/p= ages/1104805915/How+do+I+run+a+new+jupyter+notebook+on+chip - to run ""an ex= isting jupyter notebook on chip"" in the ""/umbc/rs/pi_slaha/users/rbasak1/as= trophysics-anom_det"" folder.  I am stuck on Step 7 - i.e. in the .err file,=  I am not able to see the required section. I cannot find the line that sta= rts with =E2=80=9Chttp://127.0.0.1=E2=80=9D. What I see instead is attached=  as a screenshot.=0D<br /> =0D<br /> It says the following:=0D<br /> =0D<br /> /usr/bin/which: no jupyter-lab in (/usr/ebuild/installs/software/Anaconda3/= 2024.02-1:/usr/ebuild/installs/software/Anac\=0D<br /> onda3/2024.02-1/sbin:/usr/ebuild/installs/software/Anaconda3/2024.02-1/bin:= /usr/ebuild/installs/software/Anaconda3/2024\=0D<br /> .02-1/condabin:/cm/shared/apps/git/2.33.1/bin:/cm/shared/apps/slurm/current= /sbin:/cm/shared/apps/slurm/current/bin:/usr\=0D<br /> /local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/rbasak1)=0D<br /> /cm/local/apps/slurm/var/spool/job456706/slurm_script: line 63: jupyter-lab= : command not found=0D<br /> =0D<br /> Can you please help fix this?=0D<br /> =0D<br /> Thank you.=0D<br /> =0D<br /> Regards,=0D<br /> =0D<br /> =0D<br /> Rajarshi Basak<br /> <br /> Attachment 1: <a href=3D""https://umbc.box.com/s/tq970hxeqys60wq4p9g2t2whgf6= 92z9f"" target=3D""_blank"">Screenshot 2025-10-11 123702.png</a><br /> "
3290086,72241108,Correspond,DoIT-Research-Computing,2025-10-13 16:36:08.0000000,HPC Other Issue: Not able to connect to JupyterLab on Chip,resolved,Danielle Esposito,desposi1,Rajarshi Basak,rbasak1,rbasak1@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Rajarshi,</p>  <p>Based on the error you attached as a screenshot, it appears that the system was unable to find the binary for jupyter-lab. This binary is provided via the Anaconda3 package, which is loaded at the start of the Jupyter SBATCH file. The PATH for this module is also already present in your PATH env variable. This is as expected, however it is still unable to find the binary.&nbsp;</p>  <p>Do you happen to know what node the job was running on? It is possible that this could be due to a specific node. Is this error reproducible? Based on some of the more recent .err/.out files in your working directory, it seems like the notebooks have been running fine recently.&nbsp;</p>  <p>Let me know!&nbsp;</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Sat Oct 11 12:40:16 2025, ZZ99999 wrote: <blockquote>First Name: Rajarshi<br /> Last Name: Basak<br /> Email: rbasak1@umbc.edu<br /> Campus ID: AT69971<br /> <br /> Request Type: High Performance Cluster<br /> <br /> Hello,<br /> <br /> I am referring to this guide - https://umbc.atlassian.net/wiki/spaces/faq/pages/1104805915/How+do+I+run+a+new+jupyter+notebook+on+chip - to run &quot;an existing jupyter notebook on chip&quot; in the &quot;/umbc/rs/pi_slaha/users/rbasak1/astrophysics-anom_det&quot; folder. I am stuck on Step 7 - i.e. in the .err file, I am not able to see the required section. I cannot find the line that starts with &ldquo;http://127.0.0.1&rdquo;. What I see instead is attached as a screenshot.<br /> <br /> It says the following:<br /> <br /> /usr/bin/which: no jupyter-lab in (/usr/ebuild/installs/software/Anaconda3/2024.02-1:/usr/ebuild/installs/software/Anac\<br /> onda3/2024.02-1/sbin:/usr/ebuild/installs/software/Anaconda3/2024.02-1/bin:/usr/ebuild/installs/software/Anaconda3/2024\<br /> .02-1/condabin:/cm/shared/apps/git/2.33.1/bin:/cm/shared/apps/slurm/current/sbin:/cm/shared/apps/slurm/current/bin:/usr\<br /> /local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/rbasak1)<br /> /cm/local/apps/slurm/var/spool/job456706/slurm_script: line 63: jupyter-lab: command not found<br /> <br /> Can you please help fix this?<br /> <br /> Thank you.<br /> <br /> Regards,<br /> <br /> <br /> Rajarshi Basak<br /> <br /> Attachment 1: Screenshot 2025-10-11 123702.png</blockquote> </div> "
3290086,72251977,Correspond,DoIT-Research-Computing,2025-10-14 03:11:20.0000000,HPC Other Issue: Not able to connect to JupyterLab on Chip,resolved,Danielle Esposito,desposi1,Rajarshi Basak,rbasak1,rbasak1@umbc.edu,Rajarshi Basak,rbasak1@umbc.edu,"Hi Danielle,  I have seen this error a few times, but I did not note the node on which it occurred. I will do that the next time I notice it.  You are right about "" some of the more recent .err/.out files in the working directory ... running fine recently."" I actually restarted my machine, and SSHed into CHIP again, and got it to run. It could very well be a node issue.  In the meantime, please keep the ticket open so that we can address the issue the next time it pops up.  For the Office Hours tomorrow, if I still don't observe the same issue, I was wondering if we can do it anyway to discuss a new thing I'm trying to do - open two jupyter notebooks on chip (through two tunnels) using a single jupyter.slurm file from a single folder. Is that possible? Or do we need two jupyter.slurm files for that?  Thank you.  Regards,   Rajarshi  On Mon, Oct 13, 2025 at 12:36=E2=80=AFPM Danielle Esposito via RT < UMBCHelp@rt.umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3290086 > > > Last Update From Ticket: > > Hi Rajarshi, > > Based on the error you attached as a screenshot, it appears that the > system was > unable to find the binary for jupyter-lab. This binary is provided via the > Anaconda3 package, which is loaded at the start of the Jupyter SBATCH > file. The > PATH for this module is also already present in your PATH env variable. > This is > as expected, however it is still unable to find the binary. > > Do you happen to know what node the job was running on? It is possible th= at > this could be due to a specific node. Is this error reproducible? Based on > some > of the more recent .err/.out files in your working directory, it seems > like the > notebooks have been running fine recently. > > Let me know! > > -- > > Kind regards, > Danielle Esposito (she/her/hers) > DoIT Unix Infra Student Worker > > On Sat Oct 11 12:40:16 2025, ZZ99999 wrote: > > > First Name: Rajarshi > > Last Name: Basak > > Email: rbasak1@umbc.edu > > Campus ID: AT69971 > > > Request Type: High Performance Cluster > > > Hello, > > > I am referring to this guide - > > > https://umbc.atlassian.net/wiki/spaces/faq/pages/1104805915/How+do+I+run+= a+new+jupyter+notebook+on+chip > > - to run ""an existing jupyter notebook on chip"" in the > > ""/umbc/rs/pi_slaha/users/rbasak1/astrophysics-anom_det"" folder. I am > stuck > > on Step 7 - i.e. in the .err file, I am not able to see the required > > section. I cannot find the line that starts with =E2=80=9Chttp://127.0.= 0.1=E2=80=9D. > What I > > see instead is attached as a screenshot. > > > It says the following: > > > /usr/bin/which: no jupyter-lab in > > > (/usr/ebuild/installs/software/Anaconda3/2024.02-1:/usr/ebuild/installs/s= oftware/Anac\ > > > onda3/2024.02-1/sbin:/usr/ebuild/installs/software/Anaconda3/2024.02-1/bi= n:/usr/ebuild/installs/software/Anaconda3/2024\ > > > .02-1/condabin:/cm/shared/apps/git/2.33.1/bin:/cm/shared/apps/slurm/curre= nt/sbin:/cm/shared/apps/slurm/current/bin:/usr\ > > /local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/rbasak1) > > /cm/local/apps/slurm/var/spool/job456706/slurm_script: line 63: > > jupyter-lab: command not found > > > Can you please help fix this? > > > Thank you. > > > Regards, > > > > Rajarshi Basak > > > Attachment 1: Screenshot 2025-10-11 123702.png > > "
3290086,72251977,Correspond,DoIT-Research-Computing,2025-10-14 03:11:20.0000000,HPC Other Issue: Not able to connect to JupyterLab on Chip,resolved,Danielle Esposito,desposi1,Rajarshi Basak,rbasak1,rbasak1@umbc.edu,Rajarshi Basak,rbasak1@umbc.edu,"<div dir=3D""ltr"">Hi Danielle,<div><br></div><div>I have seen this error a f= ew times, but I did not note the node on which it occurred. I will do that = the next time I notice=C2=A0it.</div><div><br></div><div>You=C2=A0are right=  about &quot; some of the more recent .err/.out files in the working direct= ory ... running fine recently.&quot; I actually=C2=A0restarted my machine, = and SSHed into CHIP again, and got it to run. It could very well be a node = issue.</div><div><br></div><div>In the meantime, please keep the ticket ope= n so that we can address the issue the next time it pops up.</div><div><br>= </div><div>For the Office Hours tomorrow, if I still don&#39;t observe the= =C2=A0same issue, I was wondering if we can do it anyway to discuss a new t= hing I&#39;m trying to do - open two jupyter notebooks on chip (through two=  tunnels) using a single jupyter.slurm file from a single folder. Is that p= ossible? Or do we need two jupyter.slurm files for that?=C2=A0</div><div><b= r></div><div>Thank you.</div><div><br></div><div>Regards,</div><div><br></d= iv><div><br></div><div>Rajarshi</div></div><br><div class=3D""gmail_quote gm= ail_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Mon, Oct 13, = 2025 at 12:36=E2=80=AFPM Danielle Esposito via RT &lt;<a href=3D""mailto:UMB= CHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote=  class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px so= lid rgb(204,204,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://r= t.umbc.edu/Ticket/Display.html?id=3D3290086"" rel=3D""noreferrer"" target=3D""_= blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3290086</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Rajarshi,<br> <br> Based on the error you attached as a screenshot, it appears that the system=  was<br> unable to find the binary for jupyter-lab. This binary is provided via the<= br> Anaconda3 package, which is loaded at the start of the Jupyter SBATCH file.=  The<br> PATH for this module is also already present in your PATH env variable. Thi= s is<br> as expected, however it is still unable to find the binary.<br> <br> Do you happen to know what node the job was running on? It is possible that= <br> this could be due to a specific node. Is this error reproducible? Based on = some<br> of the more recent .err/.out files in your working directory, it seems like=  the<br> notebooks have been running fine recently.<br> <br> Let me know!<br> <br> --<br> <br> Kind regards,<br> Danielle Esposito (she/her/hers)<br> DoIT Unix Infra Student Worker<br> <br> On Sat Oct 11 12:40:16 2025, ZZ99999 wrote:<br> <br> &gt; First Name: Rajarshi<br> &gt; Last Name: Basak<br> &gt; Email: <a href=3D""mailto:rbasak1@umbc.edu"" target=3D""_blank"">rbasak1@u= mbc.edu</a><br> &gt; Campus ID: AT69971<br> <br> &gt; Request Type: High Performance Cluster<br> <br> &gt; Hello,<br> <br> &gt; I am referring to this guide -<br> &gt; <a href=3D""https://umbc.atlassian.net/wiki/spaces/faq/pages/1104805915= /How+do+I+run+a+new+jupyter+notebook+on+chip"" rel=3D""noreferrer"" target=3D""= _blank"">https://umbc.atlassian.net/wiki/spaces/faq/pages/1104805915/How+do+= I+run+a+new+jupyter+notebook+on+chip</a><br> &gt; - to run &quot;an existing jupyter notebook on chip&quot; in the<br> &gt; &quot;/umbc/rs/pi_slaha/users/rbasak1/astrophysics-anom_det&quot; fold= er. I am stuck<br> &gt; on Step 7 - i.e. in the .err file, I am not able to see the required<b= r> &gt; section. I cannot find the line that starts with =E2=80=9C<a href=3D""h= ttp://127.0.0.1"" rel=3D""noreferrer"" target=3D""_blank"">http://127.0.0.1</a>= =E2=80=9D. What I<br> &gt; see instead is attached as a screenshot.<br> <br> &gt; It says the following:<br> <br> &gt; /usr/bin/which: no jupyter-lab in<br> &gt; (/usr/ebuild/installs/software/Anaconda3/2024.02-1:/usr/ebuild/install= s/software/Anac\<br> &gt; onda3/2024.02-1/sbin:/usr/ebuild/installs/software/Anaconda3/2024.02-1= /bin:/usr/ebuild/installs/software/Anaconda3/2024\<br> &gt; .02-1/condabin:/cm/shared/apps/git/2.33.1/bin:/cm/shared/apps/slurm/cu= rrent/sbin:/cm/shared/apps/slurm/current/bin:/usr\<br> &gt; /local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/rbasak1)<br> &gt; /cm/local/apps/slurm/var/spool/job456706/slurm_script: line 63:<br> &gt; jupyter-lab: command not found<br> <br> &gt; Can you please help fix this?<br> <br> &gt; Thank you.<br> <br> &gt; Regards,<br> <br> <br> &gt; Rajarshi Basak<br> <br> &gt; Attachment 1: Screenshot 2025-10-11 123702.png<br> <br> </blockquote></div> "
3290086,72268108,Correspond,DoIT-Research-Computing,2025-10-14 17:31:42.0000000,HPC Other Issue: Not able to connect to JupyterLab on Chip,resolved,Danielle Esposito,desposi1,Rajarshi Basak,rbasak1,rbasak1@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Rajarshi,&nbsp;</p>  <p>I identified two nodes (c21-15 and c21-16) that had a missing symbolic l= ink in /usr/ebuild/installs. This is what likely prevented you from finding=  the jupyter-lab binaries. It has now been resolved, so you shouldn&#39;t e= xperience issues now.&nbsp;</p>  <p>I will close this ticket now, however if you do encounter another issue,=  feel free to submit a new ticket. Have a nice day!</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Mon Oct 13 23:11:20 2025, AT69971 wrote: <blockquote> <div>Hi Danielle, <div>&nbsp;</div>  <div>I have seen this error a few times, but I did not note the node on whi= ch it occurred. I will do that the next time I notice&nbsp;it.</div>  <div>&nbsp;</div>  <div>You&nbsp;are right about &quot; some of the more recent .err/.out file= s in the working directory ... running fine recently.&quot; I actually&nbsp= ;restarted my machine, and SSHed into CHIP again, and got it to run. It cou= ld very well be a node issue.</div>  <div>&nbsp;</div>  <div>In the meantime, please keep the ticket open so that we can address th= e issue the next time it pops up.</div>  <div>&nbsp;</div>  <div>For the Office Hours tomorrow, if I still don&#39;t observe the&nbsp;s= ame issue, I was wondering if we can do it anyway to discuss a new thing I&= #39;m trying to do - open two jupyter notebooks on chip (through two tunnel= s) using a single jupyter.slurm file from a single folder. Is that possible= ? Or do we need two jupyter.slurm files for that?&nbsp;</div>  <div>&nbsp;</div>  <div>Thank you.</div>  <div>&nbsp;</div>  <div>Regards,</div>  <div>&nbsp;</div>  <div>&nbsp;</div>  <div>Rajarshi</div> </div> &nbsp;  <div> <div>On Mon, Oct 13, 2025 at 12:36=E2=80=AFPM Danielle Esposito via RT &lt;= UMBCHelp@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 90086 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hi Rajarshi,<br /> <br /> Based on the error you attached as a screenshot, it appears that the system=  was<br /> unable to find the binary for jupyter-lab. This binary is provided via the<= br /> Anaconda3 package, which is loaded at the start of the Jupyter SBATCH file.=  The<br /> PATH for this module is also already present in your PATH env variable. Thi= s is<br /> as expected, however it is still unable to find the binary.<br /> <br /> Do you happen to know what node the job was running on? It is possible that= <br /> this could be due to a specific node. Is this error reproducible? Based on = some<br /> of the more recent .err/.out files in your working directory, it seems like=  the<br /> notebooks have been running fine recently.<br /> <br /> Let me know!<br /> <br /> --<br /> <br /> Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker<br /> <br /> On Sat Oct 11 12:40:16 2025, ZZ99999 wrote:<br /> <br /> &gt; First Name: Rajarshi<br /> &gt; Last Name: Basak<br /> &gt; Email: rbasak1@umbc.edu<br /> &gt; Campus ID: AT69971<br /> <br /> &gt; Request Type: High Performance Cluster<br /> <br /> &gt; Hello,<br /> <br /> &gt; I am referring to this guide -<br /> &gt; https://umbc.atlassian.net/wiki/spaces/faq/pages/1104805915/How+do+I+r= un+a+new+jupyter+notebook+on+chip<br /> &gt; - to run &quot;an existing jupyter notebook on chip&quot; in the<br /> &gt; &quot;/umbc/rs/pi_slaha/users/rbasak1/astrophysics-anom_det&quot; fold= er. I am stuck<br /> &gt; on Step 7 - i.e. in the .err file, I am not able to see the required<b= r /> &gt; section. I cannot find the line that starts with &ldquo;http://127.0.0= .1&rdquo;. What I<br /> &gt; see instead is attached as a screenshot.<br /> <br /> &gt; It says the following:<br /> <br /> &gt; /usr/bin/which: no jupyter-lab in<br /> &gt; (/usr/ebuild/installs/software/Anaconda3/2024.02-1:/usr/ebuild/install= s/software/Anac\<br /> &gt; onda3/2024.02-1/sbin:/usr/ebuild/installs/software/Anaconda3/2024.02-1= /bin:/usr/ebuild/installs/software/Anaconda3/2024\<br /> &gt; .02-1/condabin:/cm/shared/apps/git/2.33.1/bin:/cm/shared/apps/slurm/cu= rrent/sbin:/cm/shared/apps/slurm/current/bin:/usr\<br /> &gt; /local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/rbasak1)<br /> &gt; /cm/local/apps/slurm/var/spool/job456706/slurm_script: line 63:<br /> &gt; jupyter-lab: command not found<br /> <br /> &gt; Can you please help fix this?<br /> <br /> &gt; Thank you.<br /> <br /> &gt; Regards,<br /> <br /> <br /> &gt; Rajarshi Basak<br /> <br /> &gt; Attachment 1: Screenshot 2025-10-11 123702.png<br /> &nbsp;</blockquote> </div> </blockquote> </div> "
3290310,72223261,Create,DoIT-Research-Computing,2025-10-12 20:09:15.0000000,HPC User Account: khoffman in Hoffman_electric_fish_project,resolved,Danielle Esposito,desposi1,Kathleen Hoffman,khoffman,khoffman@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Kathleen Last Name:                 Hoffman Email:                     khoffman@umbc.edu Campus ID:                 AT36888  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    khoffman@umbc.edu Existing Group:       Hoffman_electric_fish_project Project Title:        Computational Mechanisms for State-Driven Active Sens= ing Project Abstract:     Dr. Hoffman and an undergraduate student supervised b= y Dr. Hoffman will perform the research for the project =E2=80=9CComputatio= nal Mechanisms for State-Driven Active Sensing=E2=80=9D at UMBC. The overal= l goal of the project is to understand sensing and information gathering be= haviors through experiments with electric fish. The experiments will be con= ducted at partner institutions (Johns Hopkins University and New Jersey Ins= titute of Technology), and Dr. Hoffman=E2=80=99s team at UMBC will focus on=  advanced time-series analysis of experimental data for all three specific = aims. The deliverables will be analysis of time-series data for identifying=  causal relationships among neural and behavioral signals, analysis of time= -series data for detecting changes in neural and behavioral data, and infor= ming  control-theoretic models of the observed behaviors (developed at UMN).      We need to use software called Deep Lab Cut https://deeplabcut.github.io/= DeepLabCut/docs/installation.html to analyze experimental data. As I understand it, the first step is to use = Deep Lab Cut on a laptop to recreate an *.eml file. That file would then be=  used within a batch job on a GPU computer to generate other files. I have = two undergraduate students who will be working on this project.  A guess is=  that each batch run will generate 5G of files.  The undergraduate students working on this project are  Prince Michael Kemani=20 Samuel Nemirovsky=20  "
3290310,72240064,Correspond,DoIT-Research-Computing,2025-10-13 16:06:20.0000000,HPC User Account: khoffman in Hoffman_electric_fish_project,resolved,Danielle Esposito,desposi1,Kathleen Hoffman,khoffman,khoffman@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Kathleen,&nbsp;</p>  <p>Welcome to chip, UMBC&#39;s High Performance Computing Cluster!</p>  <p>The group, pi_khoffman, now exists on the chip cluster. Members of this group can access and contibute to the research storage space allocated to the group.</p>  <p>Additionally, I created user accounts for you, and the two students requested.&nbsp;</p>  <p>This storage space is located at /umbc/rs/pi_khoffman, and currently has a quota of 10T.</p>  <p>&nbsp;For information on accessing the cluster, adding accounts to your group, and getting started using the cluster, check out the tutorial on our wiki: https://umbc.atlassian.net/wiki/x/R4BPQg</p>  <p>Additional documentation is also available here: https://umbc.atlassian.net/wiki/x/FwCHQ</p>  <p>For the software, Deep Lab Cut, you should be able to install it via a Conda environment. Here is a wiki page for getting started with anaconda on chip:&nbsp;https://umbc.atlassian.net/wiki/x/LYCPPQ</p>  <p>If you have any questions or issues, please submit a new RT ticket at: https://doit.umbc.edu/request-tracker-rt/doit-research-computing/</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Sun Oct 12 16:09:15 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Kathleen Last Name:                 Hoffman Email:                     khoffman@umbc.edu Campus ID:                 AT36888  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    khoffman@umbc.edu Existing Group:       Hoffman_electric_fish_project Project Title:        Computational Mechanisms for State-Driven Active Sensing Project Abstract:     Dr. Hoffman and an undergraduate student supervised by Dr. Hoffman will perform the research for the project &ldquo;Computational Mechanisms for State-Driven Active Sensing&rdquo; at UMBC. The overall goal of the project is to understand sensing and information gathering behaviors through experiments with electric fish. The experiments will be conducted at partner institutions (Johns Hopkins University and New Jersey Institute of Technology), and Dr. Hoffman&rsquo;s team at UMBC will focus on advanced time-series analysis of experimental data for all three specific aims. The deliverables will be analysis of time-series data for identifying causal relationships among neural and behavioral signals, analysis of time-series data for detecting changes in neural and behavioral data, and informing  control-theoretic models of the observed behaviors (developed at UMN).      We need to use software called Deep Lab Cut https://deeplabcut.github.io/DeepLabCut/docs/installation.html to analyze experimental data. As I understand it, the first step is to use Deep Lab Cut on a laptop to recreate an *.eml file. That file would then be used within a batch job on a GPU computer to generate other files. I have two undergraduate students who will be working on this project.  A guess is that each batch run will generate 5G of files.  The undergraduate students working on this project are  Prince Michael Kemani  Samuel Nemirovsky   </pre> </blockquote> </div> "
3291129,72250779,Create,DoIT-Research-Computing,2025-10-13 21:33:26.0000000,HPC User Account: laisg1 in Student Group,resolved,Roy Prouty,proutyr1,Lais Grossel,laisg1,laisg1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Lais<br /> Last Name:                 Grossel<br /> Email:                     laisg1@umbc.edu<br /> Campus ID:                 VE32332<br /> <br /> Request Type:              High Performance Cluster<br /> <br /> Create/Modify account in Student group<br /> <br /> Hello all!=0D<br /> My supervisor, Dr. Mercedes Burns (cc), asked you to include me for access = to the chip cluster (laisg1). The VPN is working and I'm able to find my di= rectory (home). But when I try to access my folder in her group (pi_burnsm)= , to upload heavier files (molecular sequences), my permission is denied. I=  checked and there's a folder for me in her group.=0D<br /> Could you give me some orientation about this issue, please? Let me know if=  you need any additional information. Thank you!=0D<br /> Best,=0D<br /> La=C3=ADs<br /> <br /> Attachment 1: <a href=3D""https://umbc.box.com/s/0xg49ifmvly0425rlpedl1e4fu7= oz7z0"" target=3D""_blank"">printscreen_chipHPC.jpg</a><br /> "
3291129,72250889,Correspond,DoIT-Research-Computing,2025-10-13 21:58:15.0000000,HPC User Account: laisg1 in Student Group,resolved,Roy Prouty,proutyr1,Lais Grossel,laisg1,laisg1@umbc.edu,Roy Prouty,proutyr1@umbc.edu,"<div>This has been fixed. Please try again and let us know if there is still a permission denied issue.</div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Roy Prouty<br /> DoIT Research Computing Team</p> "
3291162,72251350,Create,DoIT-Research-Computing,2025-10-13 23:37:28.0000000,HPC Other Issue: change with Intel C compiler?,stalled,Elliot Gobbert,elliotg2,Matthias Gobbert,gobbert,gobbert@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Matthias Last Name:                 Gobbert Email:                     gobbert@umbc.edu Campus ID:                 AX68683  Request Type:              High Performance Cluster  Hi,  A particular case of a run used to take 0.34 seconds, in fact for some years, I recall. And we also had this result about three weeks ago. But then suddenly, this same run became 0.61 seconds, not just for me, but all students.  This prompts my question: has there been any change around the Intel C compiler in the last weeks?  Matthias  "
3291162,72321819,Correspond,DoIT-Research-Computing,2025-10-16 13:25:39.0000000,HPC Other Issue: change with Intel C compiler?,stalled,Elliot Gobbert,elliotg2,Matthias Gobbert,gobbert,gobbert@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Matthias,</p>  <p>We&#39;ve not changed any of the compilers. If we ever do, it would only ever be to add a new module which shouldn&#39;t affect the old one. May just be that the node needs a reboot to get it back into gear. If you tell me what code you&#39;re running and on what node, I can take a look.&nbsp;</p>  <p>On Mon Oct 13 19:37:28 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Matthias Last Name:                 Gobbert Email:                     gobbert@umbc.edu Campus ID:                 AX68683  Request Type:              High Performance Cluster  Hi,  A particular case of a run used to take 0.34 seconds, in fact for some years, I recall. And we also had this result about three weeks ago. But then suddenly, this same run became 0.61 seconds, not just for me, but all students.  This prompts my question: has there been any change around the Intel C compiler in the last weeks?  Matthias  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3291162,72349229,Correspond,DoIT-Research-Computing,2025-10-17 15:48:17.0000000,HPC Other Issue: change with Intel C compiler?,stalled,Elliot Gobbert,elliotg2,Matthias Gobbert,gobbert,gobbert@umbc.edu,Matthias Gobbert,gobbert@umbc.edu,"Dear Max,  Thank you for taking an interest. In what appears to be a tiny issue. However, this is actually indicative of problematic issues -- that I had been trying to identify since January. I had to take a serious break, since I find the observations disturbing. Let me explain slowly where to find what. Do not continue unless you have a little time to focus.  I found code from 2022 that I can demonstrate -- even now! -- to work better. The directory is  /umbc/rs/pi_gobbert/users/gobbert/teaching/math627/homework/hw2_trap/240912= _Fall2024/ver1.0solution  Here, you will see the code and executable trap that was compiled in 2022; this is HW 2 early in the semester of Math 447/627, so it is compiled just by mpiicc without any options (like -O3). This ./trap here is from 2022, as you can see from the date stamp. I have now created two slurm scripts that use chip and current options for --account, --cluster, --partition, --qos, etc. One is for 2024 chip and one for 2018 chip. If you use ""ls -ltr"" you can see the time stamps of the output files. The last line of the stdout lists observed wall clock time in seconds. This code compiled in 2022 runs faster on the 2018 hardware than the 2024 hardware, by 0.36 seconds compared to 0.64 seconds! These are similar to the numbers that I mentioned in my original ticket.  I have now created the directory  /umbc/rs/pi_gobbert/users/gobbert/teaching/math627/homework/hw2_trap/250916= _Fall2025/ver1.0solution_2022  I copied over the trap.c code from the previous directory, and I addressed some ANSI-C standard issues like return type of main() and return at the end of main program, but otherwise identical C code. I compiled anew right now using mpiicc without any options. Then I used the same slurm scripts as in above directory. Notice I am using ""mpirun -print-rank-map"", so the stdout file identifies the node used, to remove any uncertainty, which output file was on which node, 2018 vs. 2024. Notice that the 2018 node is again faster than the 2024 node!!! Cutting through all other crap, this is just disturbing. I compiled this in an interactive session on a 2024 node, so this should not be a problem.  Maybe, the issues can be resolved by using the right options (-O3, AVX, etc.), but then we must really figure this out. It is just really problematic that a new node -- with code compiled on a new node -- performs worse than an old node. That's not what my original ticket said, but this is the disturbing problem.  I will experiment with some options eventually. Let me know if you see some mistake.  Matthias  Matthias K. Gobbert, Ph.D., Professor of Mathematics Department of Mathematics and Statistics Center for Interdisciplinary Research and Consulting (circ.umbc.edu) UMBC High Performance Computing Facility (hpcf.umbc.edu) REU Site: Online Interdisciplinary Big Data Analytics (BigDataREU.umbc.edu <http://bigdatareu.umbc.edu>) University of Maryland, Baltimore County 1000 Hilltop Circle, Baltimore, MD 21250 http://www.umbc.edu/~gobbert   On Thu, Oct 16, 2025 at 9:25=E2=80=AFAM Max Breitmeyer via RT <UMBCHelp@rt.= umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3291162 > > > Last Update From Ticket: > > Hi Matthias, > > We've not changed any of the compilers. If we ever do, it would only ever > be to > add a new module which shouldn't affect the old one. May just be that the > node > needs a reboot to get it back into gear. If you tell me what code you're > running and on what node, I can take a look. > > On Mon Oct 13 19:37:28 2025, ZZ99999 wrote: > > > First Name:                Matthias > > Last Name:                 Gobbert > > Email:                     gobbert@umbc.edu > > Campus ID:                 AX68683 > > > > Request Type:              High Performance Cluster > > > > Hi, > > > > A particular case of a run used to take 0.34 seconds, in fact for some > years, I recall. And we also had this result about three weeks ago. > > But then suddenly, this same run became 0.61 seconds, not just for me, > but all students. > > > > This prompts my question: has there been any change around the Intel C > compiler in the last weeks? > > > > Matthias > > > -- > > Best, > Max Breitmeyer > DOIT HPC System Administrator > > "
3291162,72349229,Correspond,DoIT-Research-Computing,2025-10-17 15:48:17.0000000,HPC Other Issue: change with Intel C compiler?,stalled,Elliot Gobbert,elliotg2,Matthias Gobbert,gobbert,gobbert@umbc.edu,Matthias Gobbert,gobbert@umbc.edu,"<div dir=3D""ltr""><div>Dear Max,</div><div><br></div><div>Thank you for taki= ng an interest. In what appears to be a tiny issue. However, this is actual= ly indicative of problematic issues -- that I had been trying to identify s= ince January. I had to take a serious break, since I find the observations = disturbing. Let me explain slowly where to find what. Do not continue unles= s you have a little time to focus.</div><div><br></div><div>I found code fr= om 2022 that I can demonstrate -- even now! -- to work better. The director= y is</div><div><br></div><div>/umbc/rs/pi_gobbert/users/gobbert/teaching/ma= th627/homework/hw2_trap/240912_Fall2024/ver1.0solution</div><div><br></div>= <div>Here, you will see the code and executable trap that was compiled in 2= 022; this is HW 2 early in the semester of Math 447/627, so it is compiled = just by mpiicc without any options (like -O3). This ./trap here is from 202= 2, as you can see from the date stamp.=C2=A0<br>I have now created two slur= m scripts that use chip and current options for --account, --cluster, --par= tition, --qos, etc. One is for 2024 chip and one for 2018 chip. If you use = &quot;ls -ltr&quot; you can see the time stamps=C2=A0of the output files. T= he last line of the stdout lists observed wall clock time in seconds. This = code compiled in 2022 runs faster on the 2018 hardware than the 2024 hardwa= re, by 0.36 seconds compared to 0.64 seconds! These are similar to the numb= ers that I mentioned in my original ticket.</div><div><br></div><div>I have=  now created the directory</div><div><br></div><div>/umbc/rs/pi_gobbert/use= rs/gobbert/teaching/math627/homework/hw2_trap/250916_Fall2025/ver1.0solutio= n_2022</div><div><br></div><div>I copied over the trap.c code from the prev= ious directory, and I addressed some ANSI-C standard issues like return typ= e of main() and return at the end of main program, but otherwise identical = C code. I compiled anew right now using mpiicc without any options. Then I = used the same slurm scripts as in above directory. Notice I am using &quot;= mpirun -print-rank-map&quot;, so the stdout file identifies the node used, = to remove any uncertainty, which output file was on which node, 2018 vs. 20= 24. Notice that the 2018 node is again faster than the 2024 node!!! Cutting=  through all other crap, this is just disturbing. I compiled this in an int= eractive session on a 2024 node, so this should not be a problem.</div><div= ><br></div><div>Maybe, the issues can be resolved by using the right option= s (-O3, AVX, etc.), but then we must really figure this out. It is just rea= lly problematic that a new node -- with code compiled on a new node -- perf= orms worse than an old node. That&#39;s not what my original ticket said, b= ut this is the disturbing problem.</div><div><br></div><div>I will experime= nt with some options eventually. Let me know if you see some mistake.</div>= <div><br></div><div>Matthias</div><div><div dir=3D""ltr"" class=3D""gmail_sign= ature"" data-smartmail=3D""gmail_signature""><div dir=3D""ltr""><div><br></div><= div>Matthias K. Gobbert, Ph.D., Professor of Mathematics</div><div>Departme= nt of Mathematics and Statistics</div><div>Center for Interdisciplinary Res= earch and Consulting (<a href=3D""http://circ.umbc.edu"" target=3D""_blank"">ci= rc.umbc.edu</a>)</div><div>UMBC High Performance Computing Facility (<a hre= f=3D""http://hpcf.umbc.edu"" target=3D""_blank"">hpcf.umbc.edu</a>)</div><div>R= EU Site: Online Interdisciplinary Big Data Analytics (<a href=3D""http://big= datareu.umbc.edu"" target=3D""_blank"">BigDataREU.umbc.edu</a>)</div><div>Univ= ersity of Maryland, Baltimore County</div><div>1000 Hilltop Circle, Baltimo= re, MD 21250</div><div><a href=3D""http://www.umbc.edu/~gobbert"" target=3D""_= blank"">http://www.umbc.edu/~gobbert</a></div></div></div></div><br></div><b= r><div class=3D""gmail_quote gmail_quote_container""><div dir=3D""ltr"" class= =3D""gmail_attr"">On Thu, Oct 16, 2025 at 9:25=E2=80=AFAM Max Breitmeyer via = RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt;=  wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""margin:0px 0px = 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"">Ticket &= lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D3291162"" re= l=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?= id=3D3291162</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Matthias,<br> <br> We&#39;ve not changed any of the compilers. If we ever do, it would only ev= er be to<br> add a new module which shouldn&#39;t affect the old one. May just be that t= he node<br> needs a reboot to get it back into gear. If you tell me what code you&#39;r= e<br> running and on what node, I can take a look.<br> <br> On Mon Oct 13 19:37:28 2025, ZZ99999 wrote:<br> <br> &gt; First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Mat= thias<br> &gt; Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0Gobbert<br> &gt; Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 =C2=A0<a href=3D""mailto:gobbert@umbc.edu"" target=3D""_blank"">gobbert@= umbc.edu</a><br> &gt; Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0AX68683<br> &gt; <br> &gt; Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Per= formance Cluster<br> &gt; <br> &gt; Hi,<br> &gt; <br> &gt; A particular case of a run used to take 0.34 seconds, in fact for some=  years, I recall. And we also had this result about three weeks ago.<br> &gt; But then suddenly, this same run became 0.61 seconds, not just for me,=  but all students.<br> &gt; <br> &gt; This prompts my question: has there been any change around the Intel C=  compiler in the last weeks?<br> &gt; <br> &gt; Matthias<br> <br> <br> --<br> <br> Best,<br> Max Breitmeyer<br> DOIT HPC System Administrator<br> <br> </blockquote></div> "
3291162,72351013,Correspond,DoIT-Research-Computing,2025-10-17 16:26:38.0000000,HPC Other Issue: change with Intel C compiler?,stalled,Elliot Gobbert,elliotg2,Matthias Gobbert,gobbert,gobbert@umbc.edu,Matthias Gobbert,gobbert@umbc.edu,"<div> <p>I did not think, it would be this quick, but I experimented with some co= mpiler options in</p>  <p>/umbc/rs/pi_gobbert/users/gobbert/teaching/math627/homework/hw2_trap/250= 916_Fall2025/ver1.0solution_2022_options</p>  <p>The README file shows the options that I used, namely</p>  <p>mpiicc -O3 -std=3Dc99 -Wall -ipo -axCORE-AVX2,CORE-AVX512 trap.c -o trap= </p>  <p>In short, the real problem (to me) is that the 2018 nodes are faster tha= n 2024 nodes, like 0.4 seconds faster than 0.6 seconds, without any compile= r options. But with the above options, both hardware is capable to give 0.2=  seconds, with the 2024 node now being slightly faster than the 2018 node. = See stdout files, last line.</p>  <p>We had tried to experiment with options earlier this year, that&#39;s wh= ere I took those options above. But this test code is much simpler and clea= ner than the one considered in the spring (the Poisson code, a 2-D PDE). So= , this might be instructive for guidance to users.</p>  <p>Matthias<br /> &nbsp;</p>  <p>&nbsp;</p>  <p>&nbsp;</p>  <p>On Fri Oct 17 11:48:17 2025, AX68683 wrote:</p>  <blockquote> <div> <div>Dear Max,</div>  <div>&nbsp;</div>  <div>Thank you for taking an interest. In what appears to be a tiny issue. = However, this is actually indicative of problematic issues -- that I had be= en trying to identify since January. I had to take a serious break, since I=  find the observations disturbing. Let me explain slowly where to find what= . Do not continue unless you have a little time to focus.</div>  <div>&nbsp;</div>  <div>I found code from 2022 that I can demonstrate -- even now! -- to work = better. The directory is</div>  <div>&nbsp;</div>  <div>/umbc/rs/pi_gobbert/users/gobbert/teaching/math627/homework/hw2_trap/2= 40912_Fall2024/ver1.0solution</div>  <div>&nbsp;</div>  <div>Here, you will see the code and executable trap that was compiled in 2= 022; this is HW 2 early in the semester of Math 447/627, so it is compiled = just by mpiicc without any options (like -O3). This ./trap here is from 202= 2, as you can see from the date stamp.&nbsp;<br /> I have now created two slurm scripts that use chip and current options for = --account, --cluster, --partition, --qos, etc. One is for 2024 chip and one=  for 2018 chip. If you use &quot;ls -ltr&quot; you can see the time stamps&= nbsp;of the output files. The last line of the stdout lists observed wall c= lock time in seconds. This code compiled in 2022 runs faster on the 2018 ha= rdware than the 2024 hardware, by 0.36 seconds compared to 0.64 seconds! Th= ese are similar to the numbers that I mentioned in my original ticket.</div>  <div>&nbsp;</div>  <div>I have now created the directory</div>  <div>&nbsp;</div>  <div>/umbc/rs/pi_gobbert/users/gobbert/teaching/math627/homework/hw2_trap/2= 50916_Fall2025/ver1.0solution_2022</div>  <div>&nbsp;</div>  <div>I copied over the trap.c code from the previous directory, and I addre= ssed some ANSI-C standard issues like return type of main() and return at t= he end of main program, but otherwise identical C code. I compiled anew rig= ht now using mpiicc without any options. Then I used the same slurm scripts=  as in above directory. Notice I am using &quot;mpirun -print-rank-map&quot= ;, so the stdout file identifies the node used, to remove any uncertainty, = which output file was on which node, 2018 vs. 2024. Notice that the 2018 no= de is again faster than the 2024 node!!! Cutting through all other crap, th= is is just disturbing. I compiled this in an interactive session on a 2024 = node, so this should not be a problem.</div>  <div>&nbsp;</div>  <div>Maybe, the issues can be resolved by using the right options (-O3, AVX= , etc.), but then we must really figure this out. It is just really problem= atic that a new node -- with code compiled on a new node -- performs worse = than an old node. That&#39;s not what my original ticket said, but this is = the disturbing problem.</div>  <div>&nbsp;</div>  <div>I will experiment with some options eventually. Let me know if you see=  some mistake.</div>  <div>&nbsp;</div>  <div>Matthias</div>  <div> <div> <div> <div>&nbsp;</div>  <div>Matthias K. Gobbert, Ph.D., Professor of Mathematics</div>  <div>Department of Mathematics and Statistics</div>  <div>Center for Interdisciplinary Research and Consulting (circ.umbc.edu)</= div>  <div>UMBC High Performance Computing Facility (hpcf.umbc.edu)</div>  <div>REU Site: Online Interdisciplinary Big Data Analytics (BigDataREU.umbc= .edu)</div>  <div>University of Maryland, Baltimore County</div>  <div>1000 Hilltop Circle, Baltimore, MD 21250</div>  <div>http://www.umbc.edu/~gobbert</div> </div> </div> </div> </div> &nbsp;  <div> <div>On Thu, Oct 16, 2025 at 9:25=E2=80=AFAM Max Breitmeyer via RT &lt;UMBC= Help@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 91162 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hi Matthias,<br /> <br /> We&#39;ve not changed any of the compilers. If we ever do, it would only ev= er be to<br /> add a new module which shouldn&#39;t affect the old one. May just be that t= he node<br /> needs a reboot to get it back into gear. If you tell me what code you&#39;r= e<br /> running and on what node, I can take a look.<br /> <br /> On Mon Oct 13 19:37:28 2025, ZZ99999 wrote:<br /> <br /> &gt; First Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Mat= thias<br /> &gt; Last Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p;Gobbert<br /> &gt; Email:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &= nbsp; &nbsp;gobbert@umbc.edu<br /> &gt; Campus ID:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p;AX68683<br /> &gt;<br /> &gt; Request Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; High Per= formance Cluster<br /> &gt;<br /> &gt; Hi,<br /> &gt;<br /> &gt; A particular case of a run used to take 0.34 seconds, in fact for some=  years, I recall. And we also had this result about three weeks ago.<br /> &gt; But then suddenly, this same run became 0.61 seconds, not just for me,=  but all students.<br /> &gt;<br /> &gt; This prompts my question: has there been any change around the Intel C=  compiler in the last weeks?<br /> &gt;<br /> &gt; Matthias<br /> <br /> <br /> --<br /> <br /> Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator<br /> &nbsp;</blockquote> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;<br /> Matthias Gobbert<br /> Professor, Mathematics and Statistics<br /> gobbert@umbc.edu</p> "
3291162,72530187,Correspond,DoIT-Research-Computing,2025-10-28 15:44:08.0000000,HPC Other Issue: change with Intel C compiler?,stalled,Elliot Gobbert,elliotg2,Matthias Gobbert,gobbert,gobbert@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>This is all very interesting stuff, I&#39;ll take a look at this and do some tests, fair warning though, this is a bit lower on my stack, so might take me some time to get to this.</p>  <p>&nbsp;</p>  <p>Elliot<br /> <br /> also, including the info you emailed me into this ticket, its best to keep this centralized:</p>  <div>Hi, Elliot,</div>  <div>&nbsp;</div>  <div>The power method code of my HW 3 in Math 447/627, which is also what we do at the end in SEA, is under the directory</div>  <div>&nbsp;</div>  <div>/umbc/rs/pi_gobbert/users/gobbert/teaching/math627/homework/hw3_power/250925_Fall2025</div>  <div>&nbsp;</div>  <div>The version directories here are not totally clear in order; those with ver2.X are for a different homework (HW4a), you should ignore them; they are purely on BLAS. Also ignore the ver1.XHW4b that have 4b in their name; those are like 3b, but with one BLAS function used for the most important operation. This shows that using BLAS is faster than writing double-for-loops ourselves.</div>  <div>&nbsp;</div>  <div>What you want to look at are those ver1.X that have HW3b in their name, that is, ver1.2, ver1.4, ver1.6, ver1.7. Compare the Makefiles yourself! ver1.2 has only -O3. ver1.4 has all options including -O3 -ipo -AVX2,AVX512, while ver1.6 leaves out -ipo and ver1.7 only has -O3 -ipo. That -ipo is inter procedural optimization, so it sounds like that it tries to optimize code across multiple C code files.</div>  <div>&nbsp;</div>  <div>Note that all use -O3.</div>  <div>&nbsp;</div>  <div>Each ver1.X directory has a Study directory. Each of these has a table of results in a file like Study....strong.... with the word &quot;strong&quot; in the filename. Clearly, using only -O3 (without -ipo or AVX) is sub-optimal in ver1.2. But -ipo or not does not make the biggest difference,&nbsp;that&nbsp;is, ver1.7 with -O3 -ipo (but without AVX) is faster than ver1.2, but not the best. What makes the difference are the AVX2,AVX512 options, in ver1.4 and ver1.6 (without -ipo), compared to the other two ver1.2 and ver1.7 without the AVX options. Again, all four use -O3.</div>  <div>&nbsp;</div>  <div>Please analyze the results yourself!</div>  <div>&nbsp;</div>  <div>Matthias <div>Elliot,</div>  <div>&nbsp;</div>  <div>I am trying to convince you that my little case of 0.2 sec. vs. 0.6 sec. is very relevant. Consider for instance&nbsp;this study, where runs take longer. It is the same fraction of savings, namely significant.</div>  <div>&nbsp;</div>  <div>If you sudo to my account, you should be able to execute these commands. (Not as simple user.) Make screen rather wide to see all columns nicely without linebreaks. The first is with just -O3, the second with -O3 -ipo -AVX2 -AVX512, etc.</div>  <div>&nbsp;</div>  <div>more /umbc/rs/pi_gobbert/users/gobbert/teaching/math627/homework/hw3_power/250925_Fall2025/ver1.2HW3b_diagout/Study251013b/Study251013b_strongPerformanceTable.tex</div>  <div>&nbsp;</div>  <div>more /umbc/rs/pi_gobbert/users/gobbert/teaching/math627/homework/hw3_power/250925_Fall2025/ver1.4HW3b_options/Study251017/Study251017_strongPerformanceTable.tex</div>  <div>&nbsp; <div>So, the results mentioned in this e-mail were for the power method (that&#39;s what we did at the end of the SEA program, by the way).</div>  <div>&nbsp;</div>  <div>Here are the Poisson equation results, old and new:</div>  <div>&nbsp;</div>  <div>more&nbsp;/umbc/rs/pi_gobbert/common/research/poisson/chip2024/source250801latestOptions/Study250731/Study250731_strongPerformanceTable.tex</div>  <div>&nbsp;</div>  <div>more&nbsp;/umbc/rs/pi_gobbert/common/research/poisson/chip2024/source251020finalOptions/Study251020/Study251020_strongPerformanceTable.tex</div>  <div>&nbsp;</div>  <div>If you compare the Makefile&#39;s&nbsp;in the two source code directories (one higher than the Study&#39;s), it is really</div>  <div>&nbsp;</div>  <div>&lt; CFLAGS := -O3 -std=c99 -Wall -ipo -axCORE-AVX2,CORE-AVX512 -qmkl<br /> ---<br /> &gt; CFLAGS := -O3 -std=c99 -Wall -ipo -march=native -qopenmp -qmkl -ffast-math -fprofile-generate -funroll-loops</div>  <div>&nbsp;</div>  <div>So, it seems to me that the AVX options are really the vital ingredient. I am not sure what the other options do. Certainly, -O3 is not enough.</div>  <div>&nbsp;</div>  <div>Now, I should really acknowledge one additional difference that there is with the new studies here: based on some class discussions and comparing their results to mine, it seems that there is a real benefit to providing more than enough memory with the --mem option. Not just enough, so that the job can run. But substantially more. This cleared up a lot of inconsistent behavior that I used to always experience for some runs. Of course, it may not really be the memory as such, but it may be the potentially exclusive use of the node that might result from requesting quite large memory per node for my job.</div>  <div>&nbsp;</div>  <div><span style=""color:#888888"">Matthias</span><br /> <br /> &nbsp;</div> </div> </div> "
3291162,72641805,Correspond,DoIT-Research-Computing,2025-10-31 19:15:07.0000000,HPC Other Issue: change with Intel C compiler?,stalled,Elliot Gobbert,elliotg2,Matthias Gobbert,gobbert,gobbert@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Alright, I&#39;ve gotten to workin on this now, to be clear, the directory I&#39;m copying is this one:</p>  <p>/umbc/rs/pi_gobbert/users/gobbert/teaching/math627/homework/hw2_trap/250916_Fall2025/ver1.0solution_2022_options</p>  <p>&nbsp;</p>  <p>Elliot</p> "
3291162,72641918,Correspond,DoIT-Research-Computing,2025-10-31 19:20:57.0000000,HPC Other Issue: change with Intel C compiler?,stalled,Elliot Gobbert,elliotg2,Matthias Gobbert,gobbert,gobbert@umbc.edu,Matthias Gobbert,gobbert@umbc.edu,"What's the purpose of the copy?  Okay, for HW 2 =3D trapezoidal rule, this is reasonable. But that's the 0.6 sec. vs. 0.2 sec.  I recommended before to look at HW 3 =3D power method, which has A*x =3D ma= tvec product and thus more, but not too much.  I sent a long e-mail on this before explaining about directories that have HW3b in their name. Those tests compared various Makefil COPS. The upshot is always: for serial or 1 process per node, the difference is significant, while for 64 processes per node it becomes quite immaterial.   Matthias K. Gobbert, Ph.D., Professor of Mathematics Department of Mathematics and Statistics Center for Interdisciplinary Research and Consulting (circ.umbc.edu) UMBC High Performance Computing Facility (hpcf.umbc.edu) REU Site: Online Interdisciplinary Big Data Analytics (BigDataREU.umbc.edu <http://bigdatareu.umbc.edu>) University of Maryland, Baltimore County 1000 Hilltop Circle, Baltimore, MD 21250 http://www.umbc.edu/~gobbert   On Fri, Oct 31, 2025 at 3:15=E2=80=AFPM Elliot Gobbert via RT <UMBCHelp@rt.= umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3291162 > > > Last Update From Ticket: > > Alright, I've gotten to workin on this now, to be clear, the directory I'm > copying is this one: > > > /umbc/rs/pi_gobbert/users/gobbert/teaching/math627/homework/hw2_trap/2509= 16_Fall2025/ver1.0solution_2022_options > > Elliot > > "
3291162,72641918,Correspond,DoIT-Research-Computing,2025-10-31 19:20:57.0000000,HPC Other Issue: change with Intel C compiler?,stalled,Elliot Gobbert,elliotg2,Matthias Gobbert,gobbert,gobbert@umbc.edu,Matthias Gobbert,gobbert@umbc.edu,"<div dir=3D""ltr""><div>What&#39;s the purpose of the copy?</div><div><br></d= iv><div>Okay, for HW 2 =3D trapezoidal rule, this is reasonable. But that&#= 39;s the 0.6 sec. vs. 0.2 sec.</div><div><br></div><div>I recommended befor= e to look at HW 3 =3D power method, which has A*x =3D matvec product and th= us more, but not too much.<br><br></div><div>I sent a long e-mail on this b= efore explaining about directories that have HW3b in their name. Those test= s compared various Makefil COPS. The upshot is always: for serial or 1 proc= ess per node, the difference is significant, while for 64 processes per nod= e it becomes quite immaterial.</div><div><br></div><div><div dir=3D""ltr"" cl= ass=3D""gmail_signature"" data-smartmail=3D""gmail_signature""><div dir=3D""ltr""= ><div><br></div><div>Matthias K. Gobbert, Ph.D., Professor of Mathematics</= div><div>Department of Mathematics and Statistics</div><div>Center for Inte= rdisciplinary Research and Consulting (<a href=3D""http://circ.umbc.edu"" tar= get=3D""_blank"">circ.umbc.edu</a>)</div><div>UMBC High Performance Computing=  Facility (<a href=3D""http://hpcf.umbc.edu"" target=3D""_blank"">hpcf.umbc.edu= </a>)</div><div>REU Site: Online Interdisciplinary Big Data Analytics (<a h= ref=3D""http://bigdatareu.umbc.edu"" target=3D""_blank"">BigDataREU.umbc.edu</a= >)</div><div>University of Maryland, Baltimore County</div><div>1000 Hillto= p Circle, Baltimore, MD 21250</div><div><a href=3D""http://www.umbc.edu/~gob= bert"" target=3D""_blank"">http://www.umbc.edu/~gobbert</a></div></div></div><= /div><br></div><br><div class=3D""gmail_quote gmail_quote_container""><div di= r=3D""ltr"" class=3D""gmail_attr"">On Fri, Oct 31, 2025 at 3:15=E2=80=AFPM Elli= ot Gobbert via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.u= mbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""= margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-lef= t:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?i= d=3D3291162"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticke= t/Display.html?id=3D3291162</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Alright, I&#39;ve gotten to workin on this now, to be clear, the directory = I&#39;m<br> copying is this one:<br> <br> /umbc/rs/pi_gobbert/users/gobbert/teaching/math627/homework/hw2_trap/250916= _Fall2025/ver1.0solution_2022_options<br> <br> Elliot<br> <br> </blockquote></div> "
3291162,72642047,Correspond,DoIT-Research-Computing,2025-10-31 19:25:36.0000000,HPC Other Issue: change with Intel C compiler?,stalled,Elliot Gobbert,elliotg2,Matthias Gobbert,gobbert,gobbert@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Yes I have all the emails in front of me and what getting confused.</p>  <p>&nbsp;</p>  <p>my point is, I want to know what the &quot;better&quot; performance test is for nodes, better than poison at least, and then I can run that on all the nodes, and then we can check to see if something is wrong with the 2024 nodes.</p>  <p>&nbsp;</p>  <p>tldr gimme the root path for the director of the better perf. test.</p> "
3291162,72642577,Correspond,DoIT-Research-Computing,2025-10-31 19:36:36.0000000,HPC Other Issue: change with Intel C compiler?,stalled,Elliot Gobbert,elliotg2,Matthias Gobbert,gobbert,gobbert@umbc.edu,Matthias Gobbert,gobbert@umbc.edu,"I suggest HW 3 as in  /umbc/rs/pi_gobbert/users/gobbert/teaching/math627/homework/hw3_power/25092= 5_Fall2025  and there ver1.2, 1.4, 1.6, 1.7, which all have HW3b in their name. (Those do NOT use BLAS; the ones with HW4b in their name use BLAS, which shows that BLAS are a lot faster!) Just do a diff among the Makefile's to check which one(s) you want to focus on; their code is the same, it's only the CFLAGS in Makefile that are different. See here:  [gobbert@chip-login1 250925_Fall2025]$ grep CFLAGS ver1.2HW3b_diagout/Makefile  | grep =3D CFLAGS :=3D -O3 -std=3Dc99 -Wall # CFLAGS :=3D -O3 -std=3Dc99 -Wall -Wno-unused-variable  [gobbert@chip-login1 250925_Fall2025]$ grep CFLAGS ver1.4HW3b_options/Makefile  | grep =3D #CFLAGS :=3D -O3 -std=3Dc99 -Wall -qmkl CFLAGS :=3D -O3 -std=3Dc99 -Wall -ipo -axCORE-AVX2,CORE-AVX512 -qmkl # CFLAGS :=3D -O3 -std=3Dc99 -Wall -Wno-unused-variable  [gobbert@chip-login1 250925_Fall2025]$ grep CFLAGS ver1.6HW3b_options/Makefile  | grep =3D #CFLAGS :=3D -O3 -std=3Dc99 -Wall -qmkl #CFLAGS :=3D -O3 -std=3Dc99 -Wall -ipo -axCORE-AVX2,CORE-AVX512 -qmkl CFLAGS :=3D -O3 -std=3Dc99 -Wall      -axCORE-AVX2,CORE-AVX512 -qmkl # CFLAGS :=3D -O3 -std=3Dc99 -Wall -Wno-unused-variable  [gobbert@chip-login1 250925_Fall2025]$ grep CFLAGS ver1.7HW3b_ipoOnly/Makefile  | grep =3D #CFLAGS :=3D -O3 -std=3Dc99 -Wall -qmkl #CFLAGS :=3D -O3 -std=3Dc99 -Wall -ipo -axCORE-AVX2,CORE-AVX512 -qmkl CFLAGS :=3D -O3 -std=3Dc99 -Wall -ipo                          -qmkl # CFLAGS :=3D -O3 -std=3Dc99 -Wall -Wno-unused-variable  Ignore the lines that are commented out! Focus on actual code! Then ver1.4 should be optimal.  Matthias K. Gobbert, Ph.D., Professor of Mathematics Department of Mathematics and Statistics Center for Interdisciplinary Research and Consulting (circ.umbc.edu) UMBC High Performance Computing Facility (hpcf.umbc.edu) REU Site: Online Interdisciplinary Big Data Analytics (BigDataREU.umbc.edu <http://bigdatareu.umbc.edu>) University of Maryland, Baltimore County 1000 Hilltop Circle, Baltimore, MD 21250 http://www.umbc.edu/~gobbert   On Fri, Oct 31, 2025 at 3:25=E2=80=AFPM Elliot Gobbert via RT <UMBCHelp@rt.= umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3291162 > > > Last Update From Ticket: > > Yes I have all the emails in front of me and what getting confused. > > my point is, I want to know what the ""better"" performance test is for > nodes, > better than poison at least, and then I can run that on all the nodes, and > then > we can check to see if something is wrong with the 2024 nodes. > > tldr gimme the root path for the director of the better perf. test. > > "
3291162,72642577,Correspond,DoIT-Research-Computing,2025-10-31 19:36:36.0000000,HPC Other Issue: change with Intel C compiler?,stalled,Elliot Gobbert,elliotg2,Matthias Gobbert,gobbert,gobbert@umbc.edu,Matthias Gobbert,gobbert@umbc.edu,"<div dir=3D""ltr""><div>I suggest HW 3 as in</div><div><br></div><div>/umbc/r= s/pi_gobbert/users/gobbert/teaching/math627/homework/hw3_power/250925_Fall2= 025</div><div><br></div><div>and there ver1.2, 1.4, 1.6, 1.7, which all hav= e HW3b in their name. (Those do NOT use BLAS; the ones with HW4b in their n= ame use BLAS, which shows that BLAS are a lot faster!) Just do a diff among=  the Makefile&#39;s=C2=A0to=C2=A0check which one(s) you want to focus on; t= heir code is the same, it&#39;s only the CFLAGS in Makefile that are differ= ent. See here:</div><div><br></div><div>[gobbert@chip-login1 250925_Fall202= 5]$ grep CFLAGS ver1.2HW3b_diagout/Makefile =C2=A0| grep =3D<br>CFLAGS :=3D=  -O3 -std=3Dc99 -Wall<br># CFLAGS :=3D -O3 -std=3Dc99 -Wall -Wno-unused-var= iable</div><div><br>[gobbert@chip-login1 250925_Fall2025]$ grep CFLAGS ver1= .4HW3b_options/Makefile =C2=A0| grep =3D<br>#CFLAGS :=3D -O3 -std=3Dc99 -Wa= ll -qmkl<br>CFLAGS :=3D -O3 -std=3Dc99 -Wall -ipo -axCORE-AVX2,CORE-AVX512 = -qmkl<br># CFLAGS :=3D -O3 -std=3Dc99 -Wall -Wno-unused-variable</div><div>= <br>[gobbert@chip-login1 250925_Fall2025]$ grep CFLAGS ver1.6HW3b_options/M= akefile =C2=A0| grep =3D<br>#CFLAGS :=3D -O3 -std=3Dc99 -Wall -qmkl<br>#CFL= AGS :=3D -O3 -std=3Dc99 -Wall -ipo -axCORE-AVX2,CORE-AVX512 -qmkl<br>CFLAGS=  :=3D -O3 -std=3Dc99 -Wall =C2=A0 =C2=A0 =C2=A0-axCORE-AVX2,CORE-AVX512 -qm= kl<br># CFLAGS :=3D -O3 -std=3Dc99 -Wall -Wno-unused-variable</div><div><br= >[gobbert@chip-login1 250925_Fall2025]$ grep CFLAGS ver1.7HW3b_ipoOnly/Make= file =C2=A0| grep =3D<br>#CFLAGS :=3D -O3 -std=3Dc99 -Wall -qmkl<br>#CFLAGS=  :=3D -O3 -std=3Dc99 -Wall -ipo -axCORE-AVX2,CORE-AVX512 -qmkl<br>CFLAGS := =3D -O3 -std=3Dc99 -Wall -ipo =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0-qmkl<br># CFLAGS :=3D -O3 -st= d=3Dc99 -Wall -Wno-unused-variable<br></div><div><br></div><div>Ignore the = lines that are commented out! Focus on actual code! Then ver1.4 should be o= ptimal.</div><div><div dir=3D""ltr"" class=3D""gmail_signature"" data-smartmail= =3D""gmail_signature""><div dir=3D""ltr""><div><br></div><div>Matthias K. Gobbe= rt, Ph.D., Professor of Mathematics</div><div>Department of Mathematics and=  Statistics</div><div>Center for Interdisciplinary Research and Consulting = (<a href=3D""http://circ.umbc.edu"" target=3D""_blank"">circ.umbc.edu</a>)</div= ><div>UMBC High Performance Computing Facility (<a href=3D""http://hpcf.umbc= .edu"" target=3D""_blank"">hpcf.umbc.edu</a>)</div><div>REU Site: Online Inter= disciplinary Big Data Analytics (<a href=3D""http://bigdatareu.umbc.edu"" tar= get=3D""_blank"">BigDataREU.umbc.edu</a>)</div><div>University of Maryland, B= altimore County</div><div>1000 Hilltop Circle, Baltimore, MD 21250</div><di= v><a href=3D""http://www.umbc.edu/~gobbert"" target=3D""_blank"">http://www.umb= c.edu/~gobbert</a></div></div></div></div><br></div><br><div class=3D""gmail= _quote gmail_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Fri,=  Oct 31, 2025 at 3:25=E2=80=AFPM Elliot Gobbert via RT &lt;<a href=3D""mailt= o:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><block= quote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1= px solid rgb(204,204,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""http= s://rt.umbc.edu/Ticket/Display.html?id=3D3291162"" rel=3D""noreferrer"" target= =3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3291162</a> &gt;<b= r> <br> Last Update From Ticket:<br> <br> Yes I have all the emails in front of me and what getting confused.<br> <br> my point is, I want to know what the &quot;better&quot; performance test is=  for nodes,<br> better than poison at least, and then I can run that on all the nodes, and = then<br> we can check to see if something is wrong with the 2024 nodes.<br> <br> tldr gimme the root path for the director of the better perf. test.<br> <br> </blockquote></div> "
3291162,72711249,Comment,DoIT-Research-Computing,2025-11-04 16:11:19.0000000,HPC Other Issue: change with Intel C compiler?,stalled,Elliot Gobbert,elliotg2,Matthias Gobbert,gobbert,gobbert@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>So, lets say i want to run the ver 1.4 on every node in the cluster, to test performance. Do you think I should recompile it on each node? or just run the same executable?</p> "
3291162,72712866,Comment,DoIT-Research-Computing,2025-11-04 16:40:25.0000000,HPC Other Issue: change with Intel C compiler?,stalled,Elliot Gobbert,elliotg2,Matthias Gobbert,gobbert,gobbert@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>You shouldn&#39;t need to recompile on each node, just each partition since each partition is using the same cpu architecture</p>  <p>On Tue Nov 04 11:11:19 2025, IO12693 wrote:</p>  <blockquote> <p>So, lets say i want to run the ver 1.4 on every node in the cluster, to test performance. Do you think I should recompile it on each node? or just run the same executable?</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3291162,72712867,CommentEmailRecord,DoIT-Research-Computing,2025-11-04 16:40:26.0000000,HPC Other Issue: change with Intel C compiler?,stalled,Elliot Gobbert,elliotg2,Matthias Gobbert,gobbert,gobbert@umbc.edu,The RT System itself,NULL,"Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3291162 >  Comment just added.    You shouldn't need to recompile on each node, just each partition since each partition is using the same cpu architecture  On Tue Nov 04 11:11:19 2025, IO12693 wrote:  > So, lets say i want to run the ver 1.4 on every node in the cluster, to > test performance. Do you think I should recompile it on each node? or just > run the same executable?  --  Best, Max Breitmeyer DOIT HPC System Administrator  "
3291564,72265159,Create,DoIT-Research-Computing,2025-10-14 16:18:39.0000000,HPC Other Issue: g24-12 issue,resolved,Hakim Fessuh,hfessuh1,Shubhashis Roy Dipta,sroydip1,sroydip1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Shubhashis<br /> Last Name:                 Roy Dipta<br /> Email:                     sroydip1@umbc.edu<br /> Campus ID:                 JS93659<br /> <br /> Request Type:              High Performance Cluster<br /> <br /> One gpu on g24-12 is not working. attached screenshot.<br /> <br /> Attachment 1: <a href=""https://umbc.box.com/s/ohnr1qz2p3f3vj78ses2zoxeka6nzx8d"" target=""_blank"">Screenshot 2025-10-14 at 12.17.01.png</a><br /> "
3291564,72330677,Comment,DoIT-Research-Computing,2025-10-16 17:35:20.0000000,HPC Other Issue: g24-12 issue,resolved,Hakim Fessuh,hfessuh1,Shubhashis Roy Dipta,sroydip1,sroydip1@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>g24-12 now has all 4 gpus showing up again. its currently drained. unsure if yall already messed with it after i drained it or if it just randomly appeared again&nbsp;</p>  <p>&nbsp;</p>  <div>On Tue Oct 14 12:18:39 2025, ZZ99999 wrote: <blockquote>First Name: Shubhashis<br /> Last Name: Roy Dipta<br /> Email: sroydip1@umbc.edu<br /> Campus ID: JS93659<br /> <br /> Request Type: High Performance Cluster<br /> <br /> One gpu on g24-12 is not working. attached screenshot.<br /> <br /> Attachment 1: Screenshot 2025-10-14 at 12.17.01.png</blockquote> </div> "
3291564,72330678,CommentEmailRecord,DoIT-Research-Computing,2025-10-16 17:35:21.0000000,HPC Other Issue: g24-12 issue,resolved,Hakim Fessuh,hfessuh1,Shubhashis Roy Dipta,sroydip1,sroydip1@umbc.edu,The RT System itself,NULL,"Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3291564 >  Comment just added.    g24-12 now has all 4 gpus showing up again. its currently drained. unsure if yall already messed with it after i drained it or if it just randomly appeared again  On Tue Oct 14 12:18:39 2025, ZZ99999 wrote:  > First Name: Shubhashis > Last Name: Roy Dipta > Email: sroydip1@umbc.edu > Campus ID: JS93659  > Request Type: High Performance Cluster  > One gpu on g24-12 is not working. attached screenshot.  > Attachment 1: Screenshot 2025-10-14 at 12.17.01.png  "
3291564,72337137,Correspond,DoIT-Research-Computing,2025-10-16 19:35:40.0000000,HPC Other Issue: g24-12 issue,resolved,Hakim Fessuh,hfessuh1,Shubhashis Roy Dipta,sroydip1,sroydip1@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi&nbsp;Shubhashis,</p>  <p>We&#39;ve confirmed the machine is functioning normally again. Thank you for the email!</p>  <p>On Tue Oct 14 12:18:39 2025, ZZ99999 wrote:</p>  <blockquote>First Name: Shubhashis<br /> Last Name: Roy Dipta<br /> Email: sroydip1@umbc.edu<br /> Campus ID: JS93659<br /> <br /> Request Type: High Performance Cluster<br /> <br /> One gpu on g24-12 is not working. attached screenshot.<br /> <br /> Attachment 1: Screenshot 2025-10-14 at 12.17.01.png</blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3293453,72308391,Create,DoIT-Research-Computing,2025-10-15 16:53:03.0000000,HPC Other Issue: My jobs not running on Chip today,resolved,Beamlak Bekele,bbekele1,Zoe McLaren,zmclaren,zmclaren@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Zoe Last Name:                 McLaren Email:                     zmclaren@umbc.edu Campus ID:                 OE75135  Request Type:              High Performance Cluster  Hi, I'm using the same code that has been running fine the past few weeks but today my jobs are sitting in Chip and not running. Is there a migration going on? Or another reason that would prevent me from accessing pi_gobbert nodes like I usually do? I can't run on the general nodes either. Thank you!  [zmclaren@chip-login1 zmclaren]$ squeue -u zmclaren CLUSTER: chip-cpu              JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)             526469   general  weather zmclaren PD       0:00      1 (Priority)             517473     match     temp zmclaren PD       0:00      1 (Priority)             526413     match      cal zmclaren PD       0:00      1 (Priority)             526432     match  dhsregs zmclaren PD       0:00      1 (Priority)    "
3293453,72309050,Correspond,DoIT-Research-Computing,2025-10-15 17:13:25.0000000,HPC Other Issue: My jobs not running on Chip today,resolved,Beamlak Bekele,bbekele1,Zoe McLaren,zmclaren,zmclaren@umbc.edu,Zoe McLaren,zmclaren@umbc.edu,"My jobs are now running. There must have been a very big job using up computing power and delaying mine more than usual. YOu can disregard this issue. Thanks!   On Wed, Oct 15, 2025 at 12:53=E2=80=AFPM via RT <UMBCHelp@rt.umbc.edu> wrot= e:  > Greetings, > > This message has been automatically generated in response to the > creation of a ticket regarding: > > ------------------------------------------------------------------------- > Subject: ""HPC Other Issue: My jobs not running on Chip today"" > > Message: > > First Name:                Zoe > Last Name:                 McLaren > Email:                     zmclaren@umbc.edu > Campus ID:                 OE75135 > > Request Type:              High Performance Cluster > > Hi, > I'm using the same code that has been running fine the past few weeks but > today my jobs are sitting in Chip and not running. Is there a migration > going on? Or another reason that would prevent me from accessing pi_gobbe= rt > nodes like I usually do? I can't run on the general nodes either. Thank y= ou! > > [zmclaren@chip-login1 zmclaren]$ squeue -u zmclaren > CLUSTER: chip-cpu >              JOBID PARTITION     NAME     USER ST       TIME  NODES > NODELIST(REASON) >             526469   general  weather zmclaren PD       0:00      1 > (Priority) >             517473     match     temp zmclaren PD       0:00      1 > (Priority) >             526413     match      cal zmclaren PD       0:00      1 > (Priority) >             526432     match  dhsregs zmclaren PD       0:00      1 > (Priority) > > > > > ------------------------------------------------------------------------- > > There is no need to reply to this message right now. > > Your ticket has been assigned an ID of [Research Computing #3293453] or > you can go there directly by clicking the link below. > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3293453 > > > You can login to view your open tickets at any time by visiting > http://my.umbc.edu and clicking on ""Help"" and ""Request Help"". > > Alternately you can click on http://my.umbc.edu/help > >                         Thank you > >  --=20 Zo=C3=AB M. McLaren Associate Professor School of Public Policy University of Maryland Baltimore County Baltimore, MD CV and paper downloads: http://zoemclaren.com Faculty website: https://publicpolicy.umbc.edu/zoe-m-mclaren/ Twitter: https://twitter.com/zoemclaren "
3293453,72309050,Correspond,DoIT-Research-Computing,2025-10-15 17:13:25.0000000,HPC Other Issue: My jobs not running on Chip today,resolved,Beamlak Bekele,bbekele1,Zoe McLaren,zmclaren,zmclaren@umbc.edu,Zoe McLaren,zmclaren@umbc.edu,"<div dir=3D""ltr""><div class=3D""gmail_default"" style=3D""font-family:tahoma,s= ans-serif"">My jobs are now running. There must have been a very big job usi= ng up computing power and delaying mine more than usual. YOu can disregard = this issue. Thanks!</div><div class=3D""gmail_default"" style=3D""font-family:= tahoma,sans-serif""><br></div></div><br><div class=3D""gmail_quote gmail_quot= e_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Wed, Oct 15, 2025 at = 12:53=E2=80=AFPM via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHel= p@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" sty= le=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);paddi= ng-left:1ex"">Greetings,<br> <br> This message has been automatically generated in response to the<br> creation of a ticket regarding:<br> <br> -------------------------------------------------------------------------<b= r> Subject: &quot;HPC Other Issue: My jobs not running on Chip today&quot;<br> <br> Message: <br> <br> First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Zoe<br> Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0McL= aren<br> Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=  =C2=A0<a href=3D""mailto:zmclaren@umbc.edu"" target=3D""_blank"">zmclaren@umbc= .edu</a><br> Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0OE7= 5135<br> <br> Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Performa= nce Cluster<br> <br> Hi,<br> I&#39;m using the same code that has been running fine the past few weeks b= ut today my jobs are sitting in Chip and not running. Is there a migration = going on? Or another reason that would prevent me from accessing pi_gobbert=  nodes like I usually do? I can&#39;t run on the general nodes either. Than= k you!<br> <br> [zmclaren@chip-login1 zmclaren]$ squeue -u zmclaren<br> CLUSTER: chip-cpu<br> =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0JOBID PARTITION=C2=A0 =C2= =A0 =C2=A0NAME=C2=A0 =C2=A0 =C2=A0USER ST=C2=A0 =C2=A0 =C2=A0 =C2=A0TIME=C2= =A0 NODES NODELIST(REASON)<br> =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 526469=C2=A0 =C2=A0general=C2=A0 = weather zmclaren PD=C2=A0 =C2=A0 =C2=A0 =C2=A00:00=C2=A0 =C2=A0 =C2=A0 1 (P= riority)<br> =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 517473=C2=A0 =C2=A0 =C2=A0match= =C2=A0 =C2=A0 =C2=A0temp zmclaren PD=C2=A0 =C2=A0 =C2=A0 =C2=A00:00=C2=A0 = =C2=A0 =C2=A0 1 (Priority)<br> =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 526413=C2=A0 =C2=A0 =C2=A0match= =C2=A0 =C2=A0 =C2=A0 cal zmclaren PD=C2=A0 =C2=A0 =C2=A0 =C2=A00:00=C2=A0 = =C2=A0 =C2=A0 1 (Priority)<br> =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 526432=C2=A0 =C2=A0 =C2=A0match= =C2=A0 dhsregs zmclaren PD=C2=A0 =C2=A0 =C2=A0 =C2=A00:00=C2=A0 =C2=A0 =C2= =A0 1 (Priority)<br> <br> <br> <br> <br> -------------------------------------------------------------------------<b= r> <br> There is no need to reply to this message right now.=C2=A0 <br> <br> Your ticket has been assigned an ID of [Research Computing #3293453] or you=  can go there directly by clicking the link below.<br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D329= 3453"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3293453</a> &gt;<br> <br> You can login to view your open tickets at any time by visiting <a href=3D""= http://my.umbc.edu"" rel=3D""noreferrer"" target=3D""_blank"">http://my.umbc.edu= </a> and clicking on &quot;Help&quot; and &quot;Request Help&quot;. <br> <br> Alternately you can click on <a href=3D""http://my.umbc.edu/help"" rel=3D""nor= eferrer"" target=3D""_blank"">http://my.umbc.edu/help</a><br> <br> =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 Thank you<br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr"">Zo=C3=AB M. McLaren<br>Associate Professor<br>Sc= hool of Public Policy<br>University of Maryland Baltimore County<br>Baltimo= re, MD<br>CV and paper downloads: <a href=3D""http://zoemclaren.com"" target= =3D""_blank"">http://zoemclaren.com</a><br>Faculty website: <a href=3D""https:= //publicpolicy.umbc.edu/zoe-m-mclaren/"" target=3D""_blank"">https://publicpol= icy.umbc.edu/zoe-m-mclaren/</a><div>Twitter:=C2=A0<a href=3D""https://twitte= r.com/zoemclaren"" target=3D""_blank"">https://twitter.com/zoemclaren</a></div= ></div></div> "
3293474,72308965,Create,DoIT-Research-Computing,2025-10-15 17:09:11.0000000,CMake issue on Cluster (Office Hours Follow up),resolved,Tartela Tabassum,tartelt1,Hakim Fessuh,hfessuh1,hfessuh1@umbc.edu,Hakim Fessuh,hfessuh1@umbc.edu,"<p>Hello Sylvia,</p>  <p>I hope this email finds you well.&nbsp;</p>  <p>Thank you for working with us during our office hours&#39; time at 12:15-12:45. We wanted to provide you a step-by-step guide on how to use CMake so you are able to use this moving forward. Please also refer to the documents below that talk about &quot;How to run an interactive job&quot;, &quot;SBATCH&quot;, and information on how to use modules.&nbsp;</p>  <p>1.&nbsp;https://umbc.atlassian.net/wiki/spaces/faq/pages/1134264329</p>  <p>2.&nbsp;https://umbc.atlassian.net/wiki/spaces/faq/pages/1335951387/Basic+Slurm+Commands#sbatch</p>  <p>3.&nbsp;https://umbc.atlassian.net/wiki/spaces/faq/pages/1330053125/How+to+use+modules</p>  <p>4. Here is also attached the Cmake documentation for any issues you might have when building (https://cmake.org/cmake/help/latest/guide/tutorial/index.html)</p>  <p>&nbsp;</p>  <p>When using CMake:</p>  <p>1. Log into chip&nbsp;</p>  <p>2. run an interactive job using the srun command (This can be found in the 1st article we linked)</p>  <p>3. module load &lt;modulename&gt; (Cmake)</p>  <p>4. Optional (if you would like to view the current modules being ran you can run &lt;module list&gt;</p>  <p>5. Start working with cmake :)</p>  <p>If you have any issues, please reach back out!</p>  <p>--&nbsp;</p>  <p>Best regards,</p>  <p>Hakim Fessuh</p> "
3293526,72310605,Create,DoIT-Research-Computing,2025-10-15 17:55:11.0000000,HPC Slurm/Software Issue: Conda environment,resolved,Danielle Esposito,desposi1,Matthew Baker,mbaker,mbaker@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Matthew Last Name:                 Baker Email:                     mbaker@umbc.edu Campus ID:                 UI11613  Request Type:              High Performance Cluster   HI Folks,  Sorry to be a bother, but since our storage was migrated we've noticed a ne= w issue:  The new issue seems to be related to the HPC environment. It appears that t= he Conda environment is not being applied consistently across all nodes aft= er we load it. Below are the steps and results for reference. Does your tea= m have a potential solution?=20=20  To allocate resources, we ran the following command:  srun --time=3D24:00:00 --mem=3D100G --partition=3D2018 --qos=3Dmedium --clu= ster=3Dchip-cpu --nodes=3D10 --ntasks-per-node=3D7 --pty bash  Then we loaded the computation environment using:  module load GRASS/8.2.0-foss-2021b module load Anaconda3/2024.02-1 conda activate /umbc/rs/mbaker/common/huc_test/Essential_Scripts/Hydrograph= yWorkflow/Cang/condaEnv_3  However, when we test the environment with:  mpirun python -c ""import fiona; print(fiona.__version__)""  only one node can use the fiona library (the version is printed), while the=  others return:  ModuleNotFoundError: No module named 'fiona'   I further tested with:  mpirun python -c ""import sys; print(sys.executable)""  and found that only one node uses the Conda environment=E2=80=99s Python in= terpreter (/umbc/rs/mbaker/common/huc_test/Essential_Scripts/HydrographyWor= kflow/Cang/condaEnv_3/bin/python), while the others default to the system P= ython at:  /usr/ebuild/installs/software/Python/3.9.6-GCCcore-11.2.0/bin/python  It seems that the Conda environment is only being applied to one node. Coul= d this be related to how the environment is initialized across nodes in the=  current cluster setup?  Thank you!    "
3293526,72312318,Correspond,DoIT-Research-Computing,2025-10-15 18:42:46.0000000,HPC Slurm/Software Issue: Conda environment,resolved,Danielle Esposito,desposi1,Matthew Baker,mbaker,mbaker@umbc.edu,Matthew Baker,mbaker@umbc.edu,"Hi All  Update:  the problem did not appear when we tried again at noon.  However, any explanation might help us navigate future issues in the future, and would be appreciated.  M  On Wed, Oct 15, 2025 at 1:55=E2=80=AFPM via RT <UMBCHelp@rt.umbc.edu> wrote:  > Greetings, > > This message has been automatically generated in response to the > creation of a ticket regarding: > > ------------------------------------------------------------------------- > Subject: ""HPC Slurm/Software Issue: Conda environment"" > > Message: > > First Name:                Matthew > Last Name:                 Baker > Email:                     mbaker@umbc.edu > Campus ID:                 UI11613 > > Request Type:              High Performance Cluster > > > HI Folks, > > Sorry to be a bother, but since our storage was migrated we've noticed a > new issue: > > The new issue seems to be related to the HPC environment. It appears that > the Conda environment is not being applied consistently across all nodes > after we load it. Below are the steps and results for reference. Does your > team have a potential solution? > > To allocate resources, we ran the following command: > > srun --time=3D24:00:00 --mem=3D100G --partition=3D2018 --qos=3Dmedium > --cluster=3Dchip-cpu --nodes=3D10 --ntasks-per-node=3D7 --pty bash > > Then we loaded the computation environment using: > > module load GRASS/8.2.0-foss-2021b > module load Anaconda3/2024.02-1 > conda activate > /umbc/rs/mbaker/common/huc_test/Essential_Scripts/HydrographyWorkflow/Can= g/condaEnv_3 > > However, when we test the environment with: > > mpirun python -c ""import fiona; print(fiona.__version__)"" > > only one node can use the fiona library (the version is printed), while > the others return: > > ModuleNotFoundError: No module named 'fiona' > > > I further tested with: > > mpirun python -c ""import sys; print(sys.executable)"" > > and found that only one node uses the Conda environment=E2=80=99s Python > interpreter > (/umbc/rs/mbaker/common/huc_test/Essential_Scripts/HydrographyWorkflow/Ca= ng/condaEnv_3/bin/python), > while the others default to the system Python at: > > /usr/ebuild/installs/software/Python/3.9.6-GCCcore-11.2.0/bin/python > > It seems that the Conda environment is only being applied to one node. > Could this be related to how the environment is initialized across nodes = in > the current cluster setup? > > Thank you! > > > > > ------------------------------------------------------------------------- > > There is no need to reply to this message right now. > > Your ticket has been assigned an ID of [Research Computing #3293526] or > you can go there directly by clicking the link below. > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3293526 > > > You can login to view your open tickets at any time by visiting > http://my.umbc.edu and clicking on ""Help"" and ""Request Help"". > > Alternately you can click on http://my.umbc.edu/help > >                         Thank you > > "
3293526,72312318,Correspond,DoIT-Research-Computing,2025-10-15 18:42:46.0000000,HPC Slurm/Software Issue: Conda environment,resolved,Danielle Esposito,desposi1,Matthew Baker,mbaker,mbaker@umbc.edu,Matthew Baker,mbaker@umbc.edu,"<div dir=3D""ltr"">Hi All<div><br></div><div>Update:=C2=A0 the problem did no= t appear when we tried again at noon.=C2=A0 However, any explanation might = help us navigate future issues in the future, and would be appreciated.</di= v><div><br></div><div>M</div></div><br><div class=3D""gmail_quote gmail_quot= e_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Wed, Oct 15, 2025 at = 1:55=E2=80=AFPM via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp= @rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" styl= e=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);paddin= g-left:1ex"">Greetings,<br> <br> This message has been automatically generated in response to the<br> creation of a ticket regarding:<br> <br> -------------------------------------------------------------------------<b= r> Subject: &quot;HPC Slurm/Software Issue: Conda environment&quot;<br> <br> Message: <br> <br> First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Matthew<= br> Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0Bak= er<br> Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=  =C2=A0<a href=3D""mailto:mbaker@umbc.edu"" target=3D""_blank"">mbaker@umbc.edu= </a><br> Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0UI1= 1613<br> <br> Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Performa= nce Cluster<br> <br> <br> HI Folks,<br> <br> Sorry to be a bother, but since our storage was migrated we&#39;ve noticed = a new issue:<br> <br> The new issue seems to be related to the HPC environment. It appears that t= he Conda environment is not being applied consistently across all nodes aft= er we load it. Below are the steps and results for reference. Does your tea= m have a potential solution?=C2=A0 <br> <br> To allocate resources, we ran the following command:<br> <br> srun --time=3D24:00:00 --mem=3D100G --partition=3D2018 --qos=3Dmedium --clu= ster=3Dchip-cpu --nodes=3D10 --ntasks-per-node=3D7 --pty bash<br> <br> Then we loaded the computation environment using:<br> <br> module load GRASS/8.2.0-foss-2021b<br> module load Anaconda3/2024.02-1<br> conda activate /umbc/rs/mbaker/common/huc_test/Essential_Scripts/Hydrograph= yWorkflow/Cang/condaEnv_3<br> <br> However, when we test the environment with:<br> <br> mpirun python -c &quot;import fiona; print(fiona.__version__)&quot;<br> <br> only one node can use the fiona library (the version is printed), while the=  others return:<br> <br> ModuleNotFoundError: No module named &#39;fiona&#39;<br> <br> <br> I further tested with:<br> <br> mpirun python -c &quot;import sys; print(sys.executable)&quot;<br> <br> and found that only one node uses the Conda environment=E2=80=99s Python in= terpreter (/umbc/rs/mbaker/common/huc_test/Essential_Scripts/HydrographyWor= kflow/Cang/condaEnv_3/bin/python), while the others default to the system P= ython at:<br> <br> /usr/ebuild/installs/software/Python/3.9.6-GCCcore-11.2.0/bin/python<br> <br> It seems that the Conda environment is only being applied to one node. Coul= d this be related to how the environment is initialized across nodes in the=  current cluster setup?<br> <br> Thank you!<br> <br> <br> <br> <br> -------------------------------------------------------------------------<b= r> <br> There is no need to reply to this message right now.=C2=A0 <br> <br> Your ticket has been assigned an ID of [Research Computing #3293526] or you=  can go there directly by clicking the link below.<br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D329= 3526"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3293526</a> &gt;<br> <br> You can login to view your open tickets at any time by visiting <a href=3D""= http://my.umbc.edu"" rel=3D""noreferrer"" target=3D""_blank"">http://my.umbc.edu= </a> and clicking on &quot;Help&quot; and &quot;Request Help&quot;. <br> <br> Alternately you can click on <a href=3D""http://my.umbc.edu/help"" rel=3D""nor= eferrer"" target=3D""_blank"">http://my.umbc.edu/help</a><br> <br> =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 Thank you<br> <br> </blockquote></div> "
3293526,72326368,Correspond,DoIT-Research-Computing,2025-10-16 15:15:35.0000000,HPC Slurm/Software Issue: Conda environment,resolved,Danielle Esposito,desposi1,Matthew Baker,mbaker,mbaker@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Matthew,</p>  <p>Since your group has migrated to ceph, the path for your groups research=  storage is slightly different.&nbsp;</p>  <p>Previously, on Isilon, the path was &#39;/umbc/rs/mbaker/...&#39;</p>  <p>However, Ceph research volumes are differentiated by adding the pi_: &#3= 9;/umbc/rs/pi_mbaker&#39;.&nbsp;</p>  <p>I noticed in your conda activate command, the path was still using the p= revious isilion research volume, which no longer exists. Can you confirm th= at you are activating the conda environment with the correct path?</p>  <p>Thanks!</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Wed Oct 15 14:42:46 2025, UI11613 wrote: <blockquote> <div>Hi All <div>&nbsp;</div>  <div>Update:&nbsp; the problem did not appear when we tried again at noon.&= nbsp; However, any explanation might help us navigate future issues in the = future, and would be appreciated.</div>  <div>&nbsp;</div>  <div>M</div> </div> &nbsp;  <div> <div>On Wed, Oct 15, 2025 at 1:55=E2=80=AFPM via RT &lt;UMBCHelp@rt.umbc.ed= u&gt; wrote:</div>  <blockquote>Greetings,<br /> <br /> This message has been automatically generated in response to the<br /> creation of a ticket regarding:<br /> <br /> -------------------------------------------------------------------------<b= r /> Subject: &quot;HPC Slurm/Software Issue: Conda environment&quot;<br /> <br /> Message:<br /> <br /> First Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Matthew<= br /> Last Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Bak= er<br /> Email:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;=  &nbsp;mbaker@umbc.edu<br /> Campus ID:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;UI1= 1613<br /> <br /> Request Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; High Performa= nce Cluster<br /> <br /> <br /> HI Folks,<br /> <br /> Sorry to be a bother, but since our storage was migrated we&#39;ve noticed = a new issue:<br /> <br /> The new issue seems to be related to the HPC environment. It appears that t= he Conda environment is not being applied consistently across all nodes aft= er we load it. Below are the steps and results for reference. Does your tea= m have a potential solution?&nbsp;<br /> <br /> To allocate resources, we ran the following command:<br /> <br /> srun --time=3D24:00:00 --mem=3D100G --partition=3D2018 --qos=3Dmedium --clu= ster=3Dchip-cpu --nodes=3D10 --ntasks-per-node=3D7 --pty bash<br /> <br /> Then we loaded the computation environment using:<br /> <br /> module load GRASS/8.2.0-foss-2021b<br /> module load Anaconda3/2024.02-1<br /> conda activate /umbc/rs/mbaker/common/huc_test/Essential_Scripts/Hydrograph= yWorkflow/Cang/condaEnv_3<br /> <br /> However, when we test the environment with:<br /> <br /> mpirun python -c &quot;import fiona; print(fiona.__version__)&quot;<br /> <br /> only one node can use the fiona library (the version is printed), while the=  others return:<br /> <br /> ModuleNotFoundError: No module named &#39;fiona&#39;<br /> <br /> <br /> I further tested with:<br /> <br /> mpirun python -c &quot;import sys; print(sys.executable)&quot;<br /> <br /> and found that only one node uses the Conda environment&rsquo;s Python inte= rpreter (/umbc/rs/mbaker/common/huc_test/Essential_Scripts/HydrographyWorkf= low/Cang/condaEnv_3/bin/python), while the others default to the system Pyt= hon at:<br /> <br /> /usr/ebuild/installs/software/Python/3.9.6-GCCcore-11.2.0/bin/python<br /> <br /> It seems that the Conda environment is only being applied to one node. Coul= d this be related to how the environment is initialized across nodes in the=  current cluster setup?<br /> <br /> Thank you!<br /> <br /> <br /> <br /> <br /> -------------------------------------------------------------------------<b= r /> <br /> There is no need to reply to this message right now.&nbsp;<br /> <br /> Your ticket has been assigned an ID of [Research Computing #3293526] or you=  can go there directly by clicking the link below.<br /> <br /> Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3293526 &gt;<b= r /> <br /> You can login to view your open tickets at any time by visiting http://my.u= mbc.edu and clicking on &quot;Help&quot; and &quot;Request Help&quot;.<br /> <br /> Alternately you can click on http://my.umbc.edu/help<br /> <br /> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp= ; &nbsp; Thank you<br /> &nbsp;</blockquote> </div> </blockquote> </div> "
3293526,72337633,Correspond,DoIT-Research-Computing,2025-10-16 19:48:50.0000000,HPC Slurm/Software Issue: Conda environment,resolved,Danielle Esposito,desposi1,Matthew Baker,mbaker,mbaker@umbc.edu,Matthew Baker,mbaker@umbc.edu,"It's possible that is the explanation, it is also possible that we miscopied the paths in, as we are doing this late at night to get node time.  What is going on on the HPC lately, it's tough to get adequate node time.  On Thu, Oct 16, 2025 at 11:15=E2=80=AFAM Danielle Esposito via RT < UMBCHelp@rt.umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3293526 > > > Last Update From Ticket: > > Hi Matthew, > > Since your group has migrated to ceph, the path for your groups research > storage is slightly different. > > Previously, on Isilon, the path was '/umbc/rs/mbaker/...' > > However, Ceph research volumes are differentiated by adding the pi_: > '/umbc/rs/pi_mbaker'. > > I noticed in your conda activate command, the path was still using the > previous > isilion research volume, which no longer exists. Can you confirm that you > are > activating the conda environment with the correct path? > > Thanks! > > -- > > Kind regards, > Danielle Esposito (she/her/hers) > DoIT Unix Infra Student Worker > > On Wed Oct 15 14:42:46 2025, UI11613 wrote: > > > Hi All Update: the problem did not appear when we tried again at noon. > > However, any explanation might help us navigate future issues in the > > future, and would be appreciated. M On Wed, Oct 15, 2025 at 1:55 PM via > RT > > <UMBCHelp@rt.umbc.edu> wrote: > > >> Greetings, > > >> This message has been automatically generated in response to the > >> creation of a ticket regarding: > > >> > ------------------------------------------------------------------------- > >> Subject: ""HPC Slurm/Software Issue: Conda environment"" > > >> Message: > > >> First Name: Matthew > >> Last Name: Baker > >> Email: mbaker@umbc.edu > >> Campus ID: UI11613 > > >> Request Type: High Performance Cluster > > > >> HI Folks, > > >> Sorry to be a bother, but since our storage was migrated we've noticed > >> a new issue: > > >> The new issue seems to be related to the HPC environment. It appears > >> that the Conda environment is not being applied consistently across all > >> nodes after we load it. Below are the steps and results for reference. > >> Does your team have a potential solution? > > >> To allocate resources, we ran the following command: > > >> srun --time=3D24:00:00 --mem=3D100G --partition=3D2018 --qos=3Dmedium > >> --cluster=3Dchip-cpu --nodes=3D10 --ntasks-per-node=3D7 --pty bash > > >> Then we loaded the computation environment using: > > >> module load GRASS/8.2.0-foss-2021b > >> module load Anaconda3/2024.02-1 > >> conda activate > >> > /umbc/rs/mbaker/common/huc_test/Essential_Scripts/HydrographyWorkflow/Can= g/condaEnv_3 > > >> However, when we test the environment with: > > >> mpirun python -c ""import fiona; print(fiona.__version__)"" > > >> only one node can use the fiona library (the version is printed), while > >> the others return: > > >> ModuleNotFoundError: No module named 'fiona' > > > >> I further tested with: > > >> mpirun python -c ""import sys; print(sys.executable)"" > > >> and found that only one node uses the Conda environment=E2=80=99s Pyth= on > >> interpreter > >> > (/umbc/rs/mbaker/common/huc_test/Essential_Scripts/HydrographyWorkflow/Ca= ng/condaEnv_3/bin/python), > >> while the others default to the system Python at: > > >> /usr/ebuild/installs/software/Python/3.9.6-GCCcore-11.2.0/bin/python > > >> It seems that the Conda environment is only being applied to one node. > >> Could this be related to how the environment is initialized across > >> nodes in the current cluster setup? > > >> Thank you! > > > > > >> > ------------------------------------------------------------------------- > > >> There is no need to reply to this message right now. > > >> Your ticket has been assigned an ID of [Research Computing #3293526] or > >> you can go there directly by clicking the link below. > > >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3293526 > > > >> You can login to view your open tickets at any time by visiting > >> http://my.umbc.edu and clicking on ""Help"" and ""Request Help"". > > >> Alternately you can click on http://my.umbc.edu/help > > >> Thank you > > "
3293526,72337633,Correspond,DoIT-Research-Computing,2025-10-16 19:48:50.0000000,HPC Slurm/Software Issue: Conda environment,resolved,Danielle Esposito,desposi1,Matthew Baker,mbaker,mbaker@umbc.edu,Matthew Baker,mbaker@umbc.edu,"<div dir=3D""ltr"">It&#39;s possible that is the explanation, it is also poss= ible that we miscopied the paths in, as we are doing this late at night to = get node time.=C2=A0 What is going on on the HPC lately, it&#39;s tough=C2= =A0to get adequate node time.</div><br><div class=3D""gmail_quote gmail_quot= e_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Thu, Oct 16, 2025 at = 11:15=E2=80=AFAM Danielle Esposito via RT &lt;<a href=3D""mailto:UMBCHelp@rt= .umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class= =3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rg= b(204,204,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc= .edu/Ticket/Display.html?id=3D3293526"" rel=3D""noreferrer"" target=3D""_blank""= >https://rt.umbc.edu/Ticket/Display.html?id=3D3293526</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Matthew,<br> <br> Since your group has migrated to ceph, the path for your groups research<br> storage is slightly different.<br> <br> Previously, on Isilon, the path was &#39;/umbc/rs/mbaker/...&#39;<br> <br> However, Ceph research volumes are differentiated by adding the pi_:<br> &#39;/umbc/rs/pi_mbaker&#39;.<br> <br> I noticed in your conda activate command, the path was still using the prev= ious<br> isilion research volume, which no longer exists. Can you confirm that you a= re<br> activating the conda environment with the correct path?<br> <br> Thanks!<br> <br> --<br> <br> Kind regards,<br> Danielle Esposito (she/her/hers)<br> DoIT Unix Infra Student Worker<br> <br> On Wed Oct 15 14:42:46 2025, UI11613 wrote:<br> <br> &gt; Hi All Update: the problem did not appear when we tried again at noon.= <br> &gt; However, any explanation might help us navigate future issues in the<b= r> &gt; future, and would be appreciated. M On Wed, Oct 15, 2025 at 1:55 PM vi= a RT<br> &gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp= @rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; Greetings,<br> <br> &gt;&gt; This message has been automatically generated in response to the<b= r> &gt;&gt; creation of a ticket regarding:<br> <br> &gt;&gt; ------------------------------------------------------------------= -------<br> &gt;&gt; Subject: &quot;HPC Slurm/Software Issue: Conda environment&quot;<b= r> <br> &gt;&gt; Message:<br> <br> &gt;&gt; First Name: Matthew<br> &gt;&gt; Last Name: Baker<br> &gt;&gt; Email: <a href=3D""mailto:mbaker@umbc.edu"" target=3D""_blank"">mbaker= @umbc.edu</a><br> &gt;&gt; Campus ID: UI11613<br> <br> &gt;&gt; Request Type: High Performance Cluster<br> <br> <br> &gt;&gt; HI Folks,<br> <br> &gt;&gt; Sorry to be a bother, but since our storage was migrated we&#39;ve=  noticed<br> &gt;&gt; a new issue:<br> <br> &gt;&gt; The new issue seems to be related to the HPC environment. It appea= rs<br> &gt;&gt; that the Conda environment is not being applied consistently acros= s all<br> &gt;&gt; nodes after we load it. Below are the steps and results for refere= nce.<br> &gt;&gt; Does your team have a potential solution?<br> <br> &gt;&gt; To allocate resources, we ran the following command:<br> <br> &gt;&gt; srun --time=3D24:00:00 --mem=3D100G --partition=3D2018 --qos=3Dmed= ium<br> &gt;&gt; --cluster=3Dchip-cpu --nodes=3D10 --ntasks-per-node=3D7 --pty bash= <br> <br> &gt;&gt; Then we loaded the computation environment using:<br> <br> &gt;&gt; module load GRASS/8.2.0-foss-2021b<br> &gt;&gt; module load Anaconda3/2024.02-1<br> &gt;&gt; conda activate<br> &gt;&gt; /umbc/rs/mbaker/common/huc_test/Essential_Scripts/HydrographyWorkf= low/Cang/condaEnv_3<br> <br> &gt;&gt; However, when we test the environment with:<br> <br> &gt;&gt; mpirun python -c &quot;import fiona; print(fiona.__version__)&quot= ;<br> <br> &gt;&gt; only one node can use the fiona library (the version is printed), = while<br> &gt;&gt; the others return:<br> <br> &gt;&gt; ModuleNotFoundError: No module named &#39;fiona&#39;<br> <br> <br> &gt;&gt; I further tested with:<br> <br> &gt;&gt; mpirun python -c &quot;import sys; print(sys.executable)&quot;<br> <br> &gt;&gt; and found that only one node uses the Conda environment=E2=80=99s = Python<br> &gt;&gt; interpreter<br> &gt;&gt; (/umbc/rs/mbaker/common/huc_test/Essential_Scripts/HydrographyWork= flow/Cang/condaEnv_3/bin/python),<br> &gt;&gt; while the others default to the system Python at:<br> <br> &gt;&gt; /usr/ebuild/installs/software/Python/3.9.6-GCCcore-11.2.0/bin/pyth= on<br> <br> &gt;&gt; It seems that the Conda environment is only being applied to one n= ode.<br> &gt;&gt; Could this be related to how the environment is initialized across= <br> &gt;&gt; nodes in the current cluster setup?<br> <br> &gt;&gt; Thank you!<br> <br> <br> <br> <br> &gt;&gt; ------------------------------------------------------------------= -------<br> <br> &gt;&gt; There is no need to reply to this message right now.<br> <br> &gt;&gt; Your ticket has been assigned an ID of [Research Computing #329352= 6] or<br> &gt;&gt; you can go there directly by clicking the link below.<br> <br> &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html= ?id=3D3293526"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tic= ket/Display.html?id=3D3293526</a> &gt;<br> <br> &gt;&gt; You can login to view your open tickets at any time by visiting<br> &gt;&gt; <a href=3D""http://my.umbc.edu"" rel=3D""noreferrer"" target=3D""_blank= "">http://my.umbc.edu</a> and clicking on &quot;Help&quot; and &quot;Request=  Help&quot;.<br> <br> &gt;&gt; Alternately you can click on <a href=3D""http://my.umbc.edu/help"" r= el=3D""noreferrer"" target=3D""_blank"">http://my.umbc.edu/help</a><br> <br> &gt;&gt; Thank you<br> <br> </blockquote></div> "
3293526,72350632,Correspond,DoIT-Research-Computing,2025-10-17 16:16:44.0000000,HPC Slurm/Software Issue: Conda environment,resolved,Danielle Esposito,desposi1,Matthew Baker,mbaker,mbaker@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Matthew,</p>  <p>You&#39;ll have to be a little more specific when you say &quot;it&#39;s=  tough&nbsp;to get adequate node time.&quot; What kind of workflows are you=  running? Are these things that you typically try to use interactive nodes = for, or something that you can send an &quot;srun&quot; or &quot;sbatch&quo= t; for and leave it alone? We have definitely seen an increase in the numbe= r of users and PIs who are using our machines in the last few months, so ce= rtainly that is probably a part of it, but is there something else you&#39;= re noticing?&nbsp;</p>  <p>On Thu Oct 16 15:48:50 2025, UI11613 wrote:</p>  <blockquote> <div>It&#39;s possible that is the explanation, it is also possible that we=  miscopied the paths in, as we are doing this late at night to get node tim= e.&nbsp; What is going on on the HPC lately, it&#39;s tough&nbsp;to get ade= quate node time.</div> &nbsp;  <div> <div>On Thu, Oct 16, 2025 at 11:15=E2=80=AFAM Danielle Esposito via RT &lt;= UMBCHelp@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 93526 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hi Matthew,<br /> <br /> Since your group has migrated to ceph, the path for your groups research<br=  /> storage is slightly different.<br /> <br /> Previously, on Isilon, the path was &#39;/umbc/rs/mbaker/...&#39;<br /> <br /> However, Ceph research volumes are differentiated by adding the pi_:<br /> &#39;/umbc/rs/pi_mbaker&#39;.<br /> <br /> I noticed in your conda activate command, the path was still using the prev= ious<br /> isilion research volume, which no longer exists. Can you confirm that you a= re<br /> activating the conda environment with the correct path?<br /> <br /> Thanks!<br /> <br /> --<br /> <br /> Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker<br /> <br /> On Wed Oct 15 14:42:46 2025, UI11613 wrote:<br /> <br /> &gt; Hi All Update: the problem did not appear when we tried again at noon.= <br /> &gt; However, any explanation might help us navigate future issues in the<b= r /> &gt; future, and would be appreciated. M On Wed, Oct 15, 2025 at 1:55 PM vi= a RT<br /> &gt; &lt;UMBCHelp@rt.umbc.edu&gt; wrote:<br /> <br /> &gt;&gt; Greetings,<br /> <br /> &gt;&gt; This message has been automatically generated in response to the<b= r /> &gt;&gt; creation of a ticket regarding:<br /> <br /> &gt;&gt; ------------------------------------------------------------------= -------<br /> &gt;&gt; Subject: &quot;HPC Slurm/Software Issue: Conda environment&quot;<b= r /> <br /> &gt;&gt; Message:<br /> <br /> &gt;&gt; First Name: Matthew<br /> &gt;&gt; Last Name: Baker<br /> &gt;&gt; Email: mbaker@umbc.edu<br /> &gt;&gt; Campus ID: UI11613<br /> <br /> &gt;&gt; Request Type: High Performance Cluster<br /> <br /> <br /> &gt;&gt; HI Folks,<br /> <br /> &gt;&gt; Sorry to be a bother, but since our storage was migrated we&#39;ve=  noticed<br /> &gt;&gt; a new issue:<br /> <br /> &gt;&gt; The new issue seems to be related to the HPC environment. It appea= rs<br /> &gt;&gt; that the Conda environment is not being applied consistently acros= s all<br /> &gt;&gt; nodes after we load it. Below are the steps and results for refere= nce.<br /> &gt;&gt; Does your team have a potential solution?<br /> <br /> &gt;&gt; To allocate resources, we ran the following command:<br /> <br /> &gt;&gt; srun --time=3D24:00:00 --mem=3D100G --partition=3D2018 --qos=3Dmed= ium<br /> &gt;&gt; --cluster=3Dchip-cpu --nodes=3D10 --ntasks-per-node=3D7 --pty bash= <br /> <br /> &gt;&gt; Then we loaded the computation environment using:<br /> <br /> &gt;&gt; module load GRASS/8.2.0-foss-2021b<br /> &gt;&gt; module load Anaconda3/2024.02-1<br /> &gt;&gt; conda activate<br /> &gt;&gt; /umbc/rs/mbaker/common/huc_test/Essential_Scripts/HydrographyWorkf= low/Cang/condaEnv_3<br /> <br /> &gt;&gt; However, when we test the environment with:<br /> <br /> &gt;&gt; mpirun python -c &quot;import fiona; print(fiona.__version__)&quot= ;<br /> <br /> &gt;&gt; only one node can use the fiona library (the version is printed), = while<br /> &gt;&gt; the others return:<br /> <br /> &gt;&gt; ModuleNotFoundError: No module named &#39;fiona&#39;<br /> <br /> <br /> &gt;&gt; I further tested with:<br /> <br /> &gt;&gt; mpirun python -c &quot;import sys; print(sys.executable)&quot;<br = /> <br /> &gt;&gt; and found that only one node uses the Conda environment&rsquo;s Py= thon<br /> &gt;&gt; interpreter<br /> &gt;&gt; (/umbc/rs/mbaker/common/huc_test/Essential_Scripts/HydrographyWork= flow/Cang/condaEnv_3/bin/python),<br /> &gt;&gt; while the others default to the system Python at:<br /> <br /> &gt;&gt; /usr/ebuild/installs/software/Python/3.9.6-GCCcore-11.2.0/bin/pyth= on<br /> <br /> &gt;&gt; It seems that the Conda environment is only being applied to one n= ode.<br /> &gt;&gt; Could this be related to how the environment is initialized across= <br /> &gt;&gt; nodes in the current cluster setup?<br /> <br /> &gt;&gt; Thank you!<br /> <br /> <br /> <br /> <br /> &gt;&gt; ------------------------------------------------------------------= -------<br /> <br /> &gt;&gt; There is no need to reply to this message right now.<br /> <br /> &gt;&gt; Your ticket has been assigned an ID of [Research Computing #329352= 6] or<br /> &gt;&gt; you can go there directly by clicking the link below.<br /> <br /> &gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32935= 26 &gt;<br /> <br /> &gt;&gt; You can login to view your open tickets at any time by visiting<br=  /> &gt;&gt; http://my.umbc.edu and clicking on &quot;Help&quot; and &quot;Requ= est Help&quot;.<br /> <br /> &gt;&gt; Alternately you can click on http://my.umbc.edu/help<br /> <br /> &gt;&gt; Thank you<br /> &nbsp;</blockquote> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3293526,72358985,Correspond,DoIT-Research-Computing,2025-10-17 21:32:39.0000000,HPC Slurm/Software Issue: Conda environment,resolved,Danielle Esposito,desposi1,Matthew Baker,mbaker,mbaker@umbc.edu,Matthew Baker,mbaker@umbc.edu,"Hi Max  I think I am probably noticing the uptick in competition for node time, per= haps via a larger number of active users.  At least that would be a suffici= ent explanation.=20  I am not specifically talking about a problem though, but that is for letti= ng me know.  Matthew   Sent from my iPhone  > On Oct 17, 2025, at 12:16=E2=80=AFPM, Max Breitmeyer via RT <UMBCHelp@rt.= umbc.edu> wrote: >=20 > =EF=BB=BFTicket <URL: https://www.google.com/url?q=3Dhttps://rt.umbc.edu/= Ticket/Display.html?id%3D3293526&source=3Dgmail-imap&ust=3D1761322609000000= &usg=3DAOvVaw0wv3hH6RptKPjt58gdEvLr > >=20 > Last Update From Ticket: >=20 > Hi Matthew, >=20 > You'll have to be a little more specific when you say ""it's tough to get > adequate node time."" What kind of workflows are you running? Are these th= ings > that you typically try to use interactive nodes for, or something that yo= u can > send an ""srun"" or ""sbatch"" for and leave it alone? We have definitely see= n an > increase in the number of users and PIs who are using our machines in the=  last > few months, so certainly that is probably a part of it, but is there some= thing > else you're noticing? >=20 >> On Thu Oct 16 15:48:50 2025, UI11613 wrote: >>=20 >> It's possible that is the explanation, it is also possible that we >> miscopied the paths in, as we are doing this late at night to get node >> time. What is going on on the HPC lately, it's tough to get adequate node >> time. On Thu, Oct 16, 2025 at 11:15 AM Danielle Esposito via RT >> <UMBCHelp@rt.umbc.edu> wrote: >=20 >>> Ticket <URL: https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/= Display.html?id%3D3293526&source=3Dgmail-imap&ust=3D1761322609000000&usg=3D= AOvVaw0wv3hH6RptKPjt58gdEvLr > >=20 >>> Last Update From Ticket: >=20 >>> Hi Matthew, >=20 >>> Since your group has migrated to ceph, the path for your groups >>> research >>> storage is slightly different. >=20 >>> Previously, on Isilon, the path was '/umbc/rs/mbaker/...' >=20 >>> However, Ceph research volumes are differentiated by adding the pi_: >>> '/umbc/rs/pi_mbaker'. >=20 >>> I noticed in your conda activate command, the path was still using the >>> previous >>> isilion research volume, which no longer exists. Can you confirm that >>> you are >>> activating the conda environment with the correct path? >=20 >>> Thanks! >=20 >>> -- >=20 >>> Kind regards, >>> Danielle Esposito (she/her/hers) >>> DoIT Unix Infra Student Worker >=20 >>> On Wed Oct 15 14:42:46 2025, UI11613 wrote: >=20 >>>> Hi All Update: the problem did not appear when we tried again at >>> noon. >>>> However, any explanation might help us navigate future issues in the >>>> future, and would be appreciated. M On Wed, Oct 15, 2025 at 1:55 PM >>> via RT >>>> <UMBCHelp@rt.umbc.edu> wrote: >=20 >>>>> Greetings, >=20 >>>>> This message has been automatically generated in response to the >>>>> creation of a ticket regarding: >=20 >>>>>=20 >>> -----------------------------------------------------------------------= -- >>>>> Subject: ""HPC Slurm/Software Issue: Conda environment"" >=20 >>>>> Message: >=20 >>>>> First Name: Matthew >>>>> Last Name: Baker >>>>> Email: mbaker@umbc.edu >>>>> Campus ID: UI11613 >=20 >>>>> Request Type: High Performance Cluster >=20 >=20 >>>>> HI Folks, >=20 >>>>> Sorry to be a bother, but since our storage was migrated we've >>> noticed >>>>> a new issue: >=20 >>>>> The new issue seems to be related to the HPC environment. It appears >>>>> that the Conda environment is not being applied consistently across >>> all >>>>> nodes after we load it. Below are the steps and results for >>> reference. >>>>> Does your team have a potential solution? >=20 >>>>> To allocate resources, we ran the following command: >=20 >>>>> srun --time=3D24:00:00 --mem=3D100G --partition=3D2018 --qos=3Dmedium >>>>> --cluster=3Dchip-cpu --nodes=3D10 --ntasks-per-node=3D7 --pty bash >=20 >>>>> Then we loaded the computation environment using: >=20 >>>>> module load GRASS/8.2.0-foss-2021b >>>>> module load Anaconda3/2024.02-1 >>>>> conda activate >>>>>=20 >>> /umbc/rs/mbaker/common/huc_test/Essential_Scripts/HydrographyWorkflow/C= ang/condaEnv_3 >=20 >>>>> However, when we test the environment with: >=20 >>>>> mpirun python -c ""import fiona; print(fiona.__version__)"" >=20 >>>>> only one node can use the fiona library (the version is printed), >>> while >>>>> the others return: >=20 >>>>> ModuleNotFoundError: No module named 'fiona' >=20 >=20 >>>>> I further tested with: >=20 >>>>> mpirun python -c ""import sys; print(sys.executable)"" >=20 >>>>> and found that only one node uses the Conda environment=E2=80=99s Pyt= hon >>>>> interpreter >>>>>=20 >>> (/umbc/rs/mbaker/common/huc_test/Essential_Scripts/HydrographyWorkflow/= Cang/condaEnv_3/bin/python), >>>>> while the others default to the system Python at: >=20 >>>>> /usr/ebuild/installs/software/Python/3.9.6-GCCcore-11.2.0/bin/python >=20 >>>>> It seems that the Conda environment is only being applied to one >>> node. >>>>> Could this be related to how the environment is initialized across >>>>> nodes in the current cluster setup? >=20 >>>>> Thank you! >=20 >=20 >=20 >=20 >>>>>=20 >>> -----------------------------------------------------------------------= -- >=20 >>>>> There is no need to reply to this message right now. >=20 >>>>> Your ticket has been assigned an ID of [Research Computing #3293526] >>> or >>>>> you can go there directly by clicking the link below. >=20 >>>>> Ticket <URL: https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticke= t/Display.html?id%3D3293526&source=3Dgmail-imap&ust=3D1761322609000000&usg= =3DAOvVaw0wv3hH6RptKPjt58gdEvLr > >=20 >>>>> You can login to view your open tickets at any time by visiting >>>>> https://www.google.com/url?q=3Dhttp://my.umbc.edu&source=3Dgmail-imap= &ust=3D1761322609000000&usg=3DAOvVaw08kV6MdknFvhwlnejGaers and clicking on = ""Help"" and ""Request Help"". >=20 >>>>> Alternately you can click on https://www.google.com/url?q=3Dhttp://my= .umbc.edu/help&source=3Dgmail-imap&ust=3D1761322609000000&usg=3DAOvVaw3vEPa= y5opBUnoGf4F79fS6 >=20 >>>>> Thank you >=20 > -- >=20 > Best, > Max Breitmeyer > DOIT HPC System Administrator >=20  "
3293526,72364302,Correspond,DoIT-Research-Computing,2025-10-19 22:53:36.0000000,HPC Slurm/Software Issue: Conda environment,resolved,Danielle Esposito,desposi1,Matthew Baker,mbaker,mbaker@umbc.edu,Matthew Baker,mbaker@umbc.edu,"Here's another conundrum:  My colleague is unable to to create a directory even though I have explicitly given my group rwx permissions and he is recognized as a member of my group.  Can you advice?  (base) [xcang@c18-01 Cang]$ pwd/umbc/rs/pi_mbaker/common/huc_test/Essential_Scripts/HydrographyWorkflow= /Cang  (base) [xcang@c18-01 Cang]$ mkdir a_new_dir mkdir: cannot create directory =E2=80=98a_new_dir=E2=80=99: Permission deni= ed  (base) [xcang@c18-01 Cang]$ ls -ld /umbc/rs/pi_mbaker/common/huc_test/Essential_Scripts/HydrographyWorkflow/Ca= ng drwxrws---+ 35 mbaker pi_mbaker 42 Oct 18 15:43 /umbc/rs/pi_mbaker/common/huc_test/Essential_Scripts/HydrographyWorkflow/Ca= ng (base) [xcang@c18-01 Cang]$ getent group pi_mbaker pi_mbaker:*:1054:mbaker,barajasc,xcang      On Fri, Oct 17, 2025 at 5:32=E2=80=AFPM Matthew Baker <mbaker@umbc.edu> wro= te:  > Hi Max > > I think I am probably noticing the uptick in competition for node time, > perhaps via a larger number of active users.  At least that would be a > sufficient explanation. > > I am not specifically talking about a problem though, but that is for > letting me know. > > Matthew > > > Sent from my iPhone > > > On Oct 17, 2025, at 12:16=E2=80=AFPM, Max Breitmeyer via RT < > UMBCHelp@rt.umbc.edu> wrote: > > > > =EF=BB=BFTicket <URL: > https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id= %3D3293526&source=3Dgmail-imap&ust=3D1761322609000000&usg=3DAOvVaw0wv3hH6Rp= tKPjt58gdEvLr > > > > > > Last Update From Ticket: > > > > Hi Matthew, > > > > You'll have to be a little more specific when you say ""it's tough to get > > adequate node time."" What kind of workflows are you running? Are these > things > > that you typically try to use interactive nodes for, or something that > you can > > send an ""srun"" or ""sbatch"" for and leave it alone? We have definitely > seen an > > increase in the number of users and PIs who are using our machines in > the last > > few months, so certainly that is probably a part of it, but is there > something > > else you're noticing? > > > >> On Thu Oct 16 15:48:50 2025, UI11613 wrote: > >> > >> It's possible that is the explanation, it is also possible that we > >> miscopied the paths in, as we are doing this late at night to get node > >> time. What is going on on the HPC lately, it's tough to get adequate > node > >> time. On Thu, Oct 16, 2025 at 11:15 AM Danielle Esposito via RT > >> <UMBCHelp@rt.umbc.edu> wrote: > > > >>> Ticket <URL: > https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id= %3D3293526&source=3Dgmail-imap&ust=3D1761322609000000&usg=3DAOvVaw0wv3hH6Rp= tKPjt58gdEvLr > > > > > >>> Last Update From Ticket: > > > >>> Hi Matthew, > > > >>> Since your group has migrated to ceph, the path for your groups > >>> research > >>> storage is slightly different. > > > >>> Previously, on Isilon, the path was '/umbc/rs/mbaker/...' > > > >>> However, Ceph research volumes are differentiated by adding the pi_: > >>> '/umbc/rs/pi_mbaker'. > > > >>> I noticed in your conda activate command, the path was still using the > >>> previous > >>> isilion research volume, which no longer exists. Can you confirm that > >>> you are > >>> activating the conda environment with the correct path? > > > >>> Thanks! > > > >>> -- > > > >>> Kind regards, > >>> Danielle Esposito (she/her/hers) > >>> DoIT Unix Infra Student Worker > > > >>> On Wed Oct 15 14:42:46 2025, UI11613 wrote: > > > >>>> Hi All Update: the problem did not appear when we tried again at > >>> noon. > >>>> However, any explanation might help us navigate future issues in the > >>>> future, and would be appreciated. M On Wed, Oct 15, 2025 at 1:55 PM > >>> via RT > >>>> <UMBCHelp@rt.umbc.edu> wrote: > > > >>>>> Greetings, > > > >>>>> This message has been automatically generated in response to the > >>>>> creation of a ticket regarding: > > > >>>>> > >>> > ------------------------------------------------------------------------- > >>>>> Subject: ""HPC Slurm/Software Issue: Conda environment"" > > > >>>>> Message: > > > >>>>> First Name: Matthew > >>>>> Last Name: Baker > >>>>> Email: mbaker@umbc.edu > >>>>> Campus ID: UI11613 > > > >>>>> Request Type: High Performance Cluster > > > > > >>>>> HI Folks, > > > >>>>> Sorry to be a bother, but since our storage was migrated we've > >>> noticed > >>>>> a new issue: > > > >>>>> The new issue seems to be related to the HPC environment. It appears > >>>>> that the Conda environment is not being applied consistently across > >>> all > >>>>> nodes after we load it. Below are the steps and results for > >>> reference. > >>>>> Does your team have a potential solution? > > > >>>>> To allocate resources, we ran the following command: > > > >>>>> srun --time=3D24:00:00 --mem=3D100G --partition=3D2018 --qos=3Dmedi= um > >>>>> --cluster=3Dchip-cpu --nodes=3D10 --ntasks-per-node=3D7 --pty bash > > > >>>>> Then we loaded the computation environment using: > > > >>>>> module load GRASS/8.2.0-foss-2021b > >>>>> module load Anaconda3/2024.02-1 > >>>>> conda activate > >>>>> > >>> > /umbc/rs/mbaker/common/huc_test/Essential_Scripts/HydrographyWorkflow/Can= g/condaEnv_3 > > > >>>>> However, when we test the environment with: > > > >>>>> mpirun python -c ""import fiona; print(fiona.__version__)"" > > > >>>>> only one node can use the fiona library (the version is printed), > >>> while > >>>>> the others return: > > > >>>>> ModuleNotFoundError: No module named 'fiona' > > > > > >>>>> I further tested with: > > > >>>>> mpirun python -c ""import sys; print(sys.executable)"" > > > >>>>> and found that only one node uses the Conda environment=E2=80=99s P= ython > >>>>> interpreter > >>>>> > >>> > (/umbc/rs/mbaker/common/huc_test/Essential_Scripts/HydrographyWorkflow/Ca= ng/condaEnv_3/bin/python), > >>>>> while the others default to the system Python at: > > > >>>>> /usr/ebuild/installs/software/Python/3.9.6-GCCcore-11.2.0/bin/python > > > >>>>> It seems that the Conda environment is only being applied to one > >>> node. > >>>>> Could this be related to how the environment is initialized across > >>>>> nodes in the current cluster setup? > > > >>>>> Thank you! > > > > > > > > > >>>>> > >>> > ------------------------------------------------------------------------- > > > >>>>> There is no need to reply to this message right now. > > > >>>>> Your ticket has been assigned an ID of [Research Computing #3293526] > >>> or > >>>>> you can go there directly by clicking the link below. > > > >>>>> Ticket <URL: > https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id= %3D3293526&source=3Dgmail-imap&ust=3D1761322609000000&usg=3DAOvVaw0wv3hH6Rp= tKPjt58gdEvLr > > > > > >>>>> You can login to view your open tickets at any time by visiting > >>>>> > https://www.google.com/url?q=3Dhttp://my.umbc.edu&source=3Dgmail-imap&ust= =3D1761322609000000&usg=3DAOvVaw08kV6MdknFvhwlnejGaers > and clicking on ""Help"" and ""Request Help"". > > > >>>>> Alternately you can click on > https://www.google.com/url?q=3Dhttp://my.umbc.edu/help&source=3Dgmail-ima= p&ust=3D1761322609000000&usg=3DAOvVaw3vEPay5opBUnoGf4F79fS6 > > > >>>>> Thank you > > > > -- > > > > Best, > > Max Breitmeyer > > DOIT HPC System Administrator > > > "
3293526,72364302,Correspond,DoIT-Research-Computing,2025-10-19 22:53:36.0000000,HPC Slurm/Software Issue: Conda environment,resolved,Danielle Esposito,desposi1,Matthew Baker,mbaker,mbaker@umbc.edu,Matthew Baker,mbaker@umbc.edu,"<div dir=3D""ltr""><div>Here&#39;s another=C2=A0conundrum:=C2=A0 My colleague=  is unable to to create a directory even though I have explicitly given my = group rwx permissions=C2=A0and he is recognized as a member of my group.</d= iv><div><br></div><div>Can you advice?</div><div><br></div><div><pre><span = class=3D""gmail-im""><div dir=3D""ltr""><code>(base) [xcang<span>@c18</span><sp= an>-</span><span>01</span> <span>Cang</span>]$ pwd <span>/umbc/</span>rs<span>/pi_mbaker/</span>common<span>/huc_test/</span><= span>Essential_Scripts</span><span>/HydrographyWorkflow/</span><span>Cang</= span></code></div></span><div dir=3D""ltr""><code> <br>(base) [xcang@c18-01 Cang]$ mkdir a_new_dir<br>mkdir: cannot create dir= ectory =E2=80=98a_new_dir=E2=80=99: Permission denied</code></div><div dir= =3D""ltr""><code><span class=3D""gmail-im""> (base) [xcang<span>@c18</span><span>-</span><span>01</span> <span>Cang</spa= n>]$ ls <span>-</span>ld <span>/umbc/</span>rs<span>/pi_mbaker/</span>commo= n<span>/huc_test/</span><span>Essential_Scripts</span><span>/HydrographyWor= kflow/</span><span>Cang</span></span> drwxrws<span>---+</span> <span>35</span> mbaker pi_mbaker <span>42</span> <= span>Oct</span> <span>18</span> <span>15</span>:<span>43</span> <span>/umbc= /</span>rs<span>/pi_mbaker/</span>common<span>/huc_test/</span><span>Essent= ial_Scripts</span><span>/HydrographyWorkflow/</span><span>Cang</span> <br></code></div><span class=3D""gmail-im""><div dir=3D""ltr""><code>(base) [xc= ang<span>@c18</span><span>-</span><span>01</span> <span>Cang</span>]$ geten= t group pi_mbaker pi_mbaker:<span>*</span>:<span>1054</span>:mbaker,barajasc,xcang </code></div></span></pre><br></div><div><br></div><div><br></div></div><br= ><div class=3D""gmail_quote""><div dir=3D""ltr"" class=3D""gmail_attr"">On Fri, O= ct 17, 2025 at 5:32=E2=80=AFPM Matthew Baker &lt;<a href=3D""mailto:mbaker@u= mbc.edu"" target=3D""_blank"">mbaker@umbc.edu</a>&gt; wrote:<br></div><blockqu= ote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px=  solid rgb(204,204,204);padding-left:1ex"">Hi Max<br> <br> I think I am probably noticing the uptick in competition for node time, per= haps via a larger number of active users.=C2=A0 At least that would be a su= fficient explanation. <br> <br> I am not specifically talking about a problem though, but that is for letti= ng me know.<br> <br> Matthew<br> <br> <br> Sent from my iPhone<br> <br> &gt; On Oct 17, 2025, at 12:16=E2=80=AFPM, Max Breitmeyer via RT &lt;<a hre= f=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a= >&gt; wrote:<br> &gt; <br> &gt; =EF=BB=BFTicket &lt;URL: <a href=3D""https://www.google.com/url?q=3Dhtt= ps://rt.umbc.edu/Ticket/Display.html?id%3D3293526&amp;source=3Dgmail-imap&a= mp;ust=3D1761322609000000&amp;usg=3DAOvVaw0wv3hH6RptKPjt58gdEvLr"" rel=3D""no= referrer"" target=3D""_blank"">https://www.google.com/url?q=3Dhttps://rt.umbc.= edu/Ticket/Display.html?id%3D3293526&amp;source=3Dgmail-imap&amp;ust=3D1761= 322609000000&amp;usg=3DAOvVaw0wv3hH6RptKPjt58gdEvLr</a> &gt;<br> &gt; <br> &gt; Last Update From Ticket:<br> &gt; <br> &gt; Hi Matthew,<br> &gt; <br> &gt; You&#39;ll have to be a little more specific when you say &quot;it&#39= ;s tough to get<br> &gt; adequate node time.&quot; What kind of workflows are you running? Are = these things<br> &gt; that you typically try to use interactive nodes for, or something that=  you can<br> &gt; send an &quot;srun&quot; or &quot;sbatch&quot; for and leave it alone?=  We have definitely seen an<br> &gt; increase in the number of users and PIs who are using our machines in = the last<br> &gt; few months, so certainly that is probably a part of it, but is there s= omething<br> &gt; else you&#39;re noticing?<br> &gt; <br> &gt;&gt; On Thu Oct 16 15:48:50 2025, UI11613 wrote:<br> &gt;&gt; <br> &gt;&gt; It&#39;s possible that is the explanation, it is also possible tha= t we<br> &gt;&gt; miscopied the paths in, as we are doing this late at night to get = node<br> &gt;&gt; time. What is going on on the HPC lately, it&#39;s tough to get ad= equate node<br> &gt;&gt; time. On Thu, Oct 16, 2025 at 11:15 AM Danielle Esposito via RT<br> &gt;&gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBC= Help@rt.umbc.edu</a>&gt; wrote:<br> &gt; <br> &gt;&gt;&gt; Ticket &lt;URL: <a href=3D""https://www.google.com/url?q=3Dhttp= s://rt.umbc.edu/Ticket/Display.html?id%3D3293526&amp;source=3Dgmail-imap&am= p;ust=3D1761322609000000&amp;usg=3DAOvVaw0wv3hH6RptKPjt58gdEvLr"" rel=3D""nor= eferrer"" target=3D""_blank"">https://www.google.com/url?q=3Dhttps://rt.umbc.e= du/Ticket/Display.html?id%3D3293526&amp;source=3Dgmail-imap&amp;ust=3D17613= 22609000000&amp;usg=3DAOvVaw0wv3hH6RptKPjt58gdEvLr</a> &gt;<br> &gt; <br> &gt;&gt;&gt; Last Update From Ticket:<br> &gt; <br> &gt;&gt;&gt; Hi Matthew,<br> &gt; <br> &gt;&gt;&gt; Since your group has migrated to ceph, the path for your group= s<br> &gt;&gt;&gt; research<br> &gt;&gt;&gt; storage is slightly different.<br> &gt; <br> &gt;&gt;&gt; Previously, on Isilon, the path was &#39;/umbc/rs/mbaker/...&#= 39;<br> &gt; <br> &gt;&gt;&gt; However, Ceph research volumes are differentiated by adding th= e pi_:<br> &gt;&gt;&gt; &#39;/umbc/rs/pi_mbaker&#39;.<br> &gt; <br> &gt;&gt;&gt; I noticed in your conda activate command, the path was still u= sing the<br> &gt;&gt;&gt; previous<br> &gt;&gt;&gt; isilion research volume, which no longer exists. Can you confi= rm that<br> &gt;&gt;&gt; you are<br> &gt;&gt;&gt; activating the conda environment with the correct path?<br> &gt; <br> &gt;&gt;&gt; Thanks!<br> &gt; <br> &gt;&gt;&gt; --<br> &gt; <br> &gt;&gt;&gt; Kind regards,<br> &gt;&gt;&gt; Danielle Esposito (she/her/hers)<br> &gt;&gt;&gt; DoIT Unix Infra Student Worker<br> &gt; <br> &gt;&gt;&gt; On Wed Oct 15 14:42:46 2025, UI11613 wrote:<br> &gt; <br> &gt;&gt;&gt;&gt; Hi All Update: the problem did not appear when we tried ag= ain at<br> &gt;&gt;&gt; noon.<br> &gt;&gt;&gt;&gt; However, any explanation might help us navigate future iss= ues in the<br> &gt;&gt;&gt;&gt; future, and would be appreciated. M On Wed, Oct 15, 2025 a= t 1:55 PM<br> &gt;&gt;&gt; via RT<br> &gt;&gt;&gt;&gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_bla= nk"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> &gt; <br> &gt;&gt;&gt;&gt;&gt; Greetings,<br> &gt; <br> &gt;&gt;&gt;&gt;&gt; This message has been automatically generated in respo= nse to the<br> &gt;&gt;&gt;&gt;&gt; creation of a ticket regarding:<br> &gt; <br> &gt;&gt;&gt;&gt;&gt; <br> &gt;&gt;&gt; --------------------------------------------------------------= -----------<br> &gt;&gt;&gt;&gt;&gt; Subject: &quot;HPC Slurm/Software Issue: Conda environ= ment&quot;<br> &gt; <br> &gt;&gt;&gt;&gt;&gt; Message:<br> &gt; <br> &gt;&gt;&gt;&gt;&gt; First Name: Matthew<br> &gt;&gt;&gt;&gt;&gt; Last Name: Baker<br> &gt;&gt;&gt;&gt;&gt; Email: <a href=3D""mailto:mbaker@umbc.edu"" target=3D""_b= lank"">mbaker@umbc.edu</a><br> &gt;&gt;&gt;&gt;&gt; Campus ID: UI11613<br> &gt; <br> &gt;&gt;&gt;&gt;&gt; Request Type: High Performance Cluster<br> &gt; <br> &gt; <br> &gt;&gt;&gt;&gt;&gt; HI Folks,<br> &gt; <br> &gt;&gt;&gt;&gt;&gt; Sorry to be a bother, but since our storage was migrat= ed we&#39;ve<br> &gt;&gt;&gt; noticed<br> &gt;&gt;&gt;&gt;&gt; a new issue:<br> &gt; <br> &gt;&gt;&gt;&gt;&gt; The new issue seems to be related to the HPC environme= nt. It appears<br> &gt;&gt;&gt;&gt;&gt; that the Conda environment is not being applied consis= tently across<br> &gt;&gt;&gt; all<br> &gt;&gt;&gt;&gt;&gt; nodes after we load it. Below are the steps and result= s for<br> &gt;&gt;&gt; reference.<br> &gt;&gt;&gt;&gt;&gt; Does your team have a potential solution?<br> &gt; <br> &gt;&gt;&gt;&gt;&gt; To allocate resources, we ran the following command:<b= r> &gt; <br> &gt;&gt;&gt;&gt;&gt; srun --time=3D24:00:00 --mem=3D100G --partition=3D2018=  --qos=3Dmedium<br> &gt;&gt;&gt;&gt;&gt; --cluster=3Dchip-cpu --nodes=3D10 --ntasks-per-node=3D= 7 --pty bash<br> &gt; <br> &gt;&gt;&gt;&gt;&gt; Then we loaded the computation environment using:<br> &gt; <br> &gt;&gt;&gt;&gt;&gt; module load GRASS/8.2.0-foss-2021b<br> &gt;&gt;&gt;&gt;&gt; module load Anaconda3/2024.02-1<br> &gt;&gt;&gt;&gt;&gt; conda activate<br> &gt;&gt;&gt;&gt;&gt; <br> &gt;&gt;&gt; /umbc/rs/mbaker/common/huc_test/Essential_Scripts/HydrographyW= orkflow/Cang/condaEnv_3<br> &gt; <br> &gt;&gt;&gt;&gt;&gt; However, when we test the environment with:<br> &gt; <br> &gt;&gt;&gt;&gt;&gt; mpirun python -c &quot;import fiona; print(fiona.__ver= sion__)&quot;<br> &gt; <br> &gt;&gt;&gt;&gt;&gt; only one node can use the fiona library (the version i= s printed),<br> &gt;&gt;&gt; while<br> &gt;&gt;&gt;&gt;&gt; the others return:<br> &gt; <br> &gt;&gt;&gt;&gt;&gt; ModuleNotFoundError: No module named &#39;fiona&#39;<b= r> &gt; <br> &gt; <br> &gt;&gt;&gt;&gt;&gt; I further tested with:<br> &gt; <br> &gt;&gt;&gt;&gt;&gt; mpirun python -c &quot;import sys; print(sys.executabl= e)&quot;<br> &gt; <br> &gt;&gt;&gt;&gt;&gt; and found that only one node uses the Conda environmen= t=E2=80=99s Python<br> &gt;&gt;&gt;&gt;&gt; interpreter<br> &gt;&gt;&gt;&gt;&gt; <br> &gt;&gt;&gt; (/umbc/rs/mbaker/common/huc_test/Essential_Scripts/Hydrography= Workflow/Cang/condaEnv_3/bin/python),<br> &gt;&gt;&gt;&gt;&gt; while the others default to the system Python at:<br> &gt; <br> &gt;&gt;&gt;&gt;&gt; /usr/ebuild/installs/software/Python/3.9.6-GCCcore-11.= 2.0/bin/python<br> &gt; <br> &gt;&gt;&gt;&gt;&gt; It seems that the Conda environment is only being appl= ied to one<br> &gt;&gt;&gt; node.<br> &gt;&gt;&gt;&gt;&gt; Could this be related to how the environment is initia= lized across<br> &gt;&gt;&gt;&gt;&gt; nodes in the current cluster setup?<br> &gt; <br> &gt;&gt;&gt;&gt;&gt; Thank you!<br> &gt; <br> &gt; <br> &gt; <br> &gt; <br> &gt;&gt;&gt;&gt;&gt; <br> &gt;&gt;&gt; --------------------------------------------------------------= -----------<br> &gt; <br> &gt;&gt;&gt;&gt;&gt; There is no need to reply to this message right now.<b= r> &gt; <br> &gt;&gt;&gt;&gt;&gt; Your ticket has been assigned an ID of [Research Compu= ting #3293526]<br> &gt;&gt;&gt; or<br> &gt;&gt;&gt;&gt;&gt; you can go there directly by clicking the link below.<= br> &gt; <br> &gt;&gt;&gt;&gt;&gt; Ticket &lt;URL: <a href=3D""https://www.google.com/url?= q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3293526&amp;source=3Dgmail= -imap&amp;ust=3D1761322609000000&amp;usg=3DAOvVaw0wv3hH6RptKPjt58gdEvLr"" re= l=3D""noreferrer"" target=3D""_blank"">https://www.google.com/url?q=3Dhttps://r= t.umbc.edu/Ticket/Display.html?id%3D3293526&amp;source=3Dgmail-imap&amp;ust= =3D1761322609000000&amp;usg=3DAOvVaw0wv3hH6RptKPjt58gdEvLr</a> &gt;<br> &gt; <br> &gt;&gt;&gt;&gt;&gt; You can login to view your open tickets at any time by=  visiting<br> &gt;&gt;&gt;&gt;&gt; <a href=3D""https://www.google.com/url?q=3Dhttp://my.um= bc.edu&amp;source=3Dgmail-imap&amp;ust=3D1761322609000000&amp;usg=3DAOvVaw0= 8kV6MdknFvhwlnejGaers"" rel=3D""noreferrer"" target=3D""_blank"">https://www.goo= gle.com/url?q=3Dhttp://my.umbc.edu&amp;source=3Dgmail-imap&amp;ust=3D176132= 2609000000&amp;usg=3DAOvVaw08kV6MdknFvhwlnejGaers</a> and clicking on &quot= ;Help&quot; and &quot;Request Help&quot;.<br> &gt; <br> &gt;&gt;&gt;&gt;&gt; Alternately you can click on <a href=3D""https://www.go= ogle.com/url?q=3Dhttp://my.umbc.edu/help&amp;source=3Dgmail-imap&amp;ust=3D= 1761322609000000&amp;usg=3DAOvVaw3vEPay5opBUnoGf4F79fS6"" rel=3D""noreferrer""=  target=3D""_blank"">https://www.google.com/url?q=3Dhttp://my.umbc.edu/help&a= mp;source=3Dgmail-imap&amp;ust=3D1761322609000000&amp;usg=3DAOvVaw3vEPay5op= BUnoGf4F79fS6</a><br> &gt; <br> &gt;&gt;&gt;&gt;&gt; Thank you<br> &gt; <br> &gt; --<br> &gt; <br> &gt; Best,<br> &gt; Max Breitmeyer<br> &gt; DOIT HPC System Administrator<br> &gt; <br> </blockquote></div> "
3293526,72371321,Correspond,DoIT-Research-Computing,2025-10-20 14:08:56.0000000,HPC Slurm/Software Issue: Conda environment,resolved,Danielle Esposito,desposi1,Matthew Baker,mbaker,mbaker@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Matthew,&nbsp;</p>  <p>Thanks for letting me know about the permission issue. There was an inco= rrect extended acl set on the directory, which I have fixed. Let me know if=  it works for your colleague.&nbsp;</p>  <p>Can you confirm if you were able to get your anaconda environment workin= g successfully? Or are you still experiencing the original issue?</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Sun Oct 19 18:53:36 2025, UI11613 wrote: <blockquote> <div> <div>Here&#39;s another&nbsp;conundrum:&nbsp; My colleague is unable to to = create a directory even though I have explicitly given my group rwx permiss= ions&nbsp;and he is recognized as a member of my group.</div>  <div>&nbsp;</div>  <div>Can you advice?</div>  <div>&nbsp;</div>  <div> <pre>  &nbsp;</pre>  <div><code>(base) [xcang@c18-01 Cang]$ pwd /umbc/rs/pi_mbaker/common/huc_te= st/Essential_Scripts/HydrographyWorkflow/Cang</code></div>  <div><br /> <code>(base) [xcang@c18-01 Cang]$ mkdir a_new_dir<br /> mkdir: cannot create directory &lsquo;a_new_dir&rsquo;: Permission denied</= code></div>  <div><code>(base) [xcang@c18-01 Cang]$ ls -ld /umbc/rs/pi_mbaker/common/huc= _test/Essential_Scripts/HydrographyWorkflow/Cang drwxrws---+ 35 mbaker pi_m= baker 42 Oct 18 15:43 /umbc/rs/pi_mbaker/common/huc_test/Essential_Scripts/= HydrographyWorkflow/Cang </code></div>  <div><code>(base) [xcang@c18-01 Cang]$ getent group pi_mbaker pi_mbaker:*:1= 054:mbaker,barajasc,xcang </code></div> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> </div> &nbsp;  <div> <div>On Fri, Oct 17, 2025 at 5:32=E2=80=AFPM Matthew Baker &lt;mbaker@umbc.= edu&gt; wrote:</div>  <blockquote>Hi Max<br /> <br /> I think I am probably noticing the uptick in competition for node time, per= haps via a larger number of active users.&nbsp; At least that would be a su= fficient explanation.<br /> <br /> I am not specifically talking about a problem though, but that is for letti= ng me know.<br /> <br /> Matthew<br /> <br /> <br /> Sent from my iPhone<br /> <br /> &gt; On Oct 17, 2025, at 12:16=E2=80=AFPM, Max Breitmeyer via RT &lt;UMBCHe= lp@rt.umbc.edu&gt; wrote:<br /> &gt;<br /> &gt; =EF=BB=BFTicket &lt;URL: https://www.google.com/url?q=3Dhttps://rt.umb= c.edu/Ticket/Display.html?id%3D3293526&amp;source=3Dgmail-imap&amp;ust=3D17= 61322609000000&amp;usg=3DAOvVaw0wv3hH6RptKPjt58gdEvLr &gt;<br /> &gt;<br /> &gt; Last Update From Ticket:<br /> &gt;<br /> &gt; Hi Matthew,<br /> &gt;<br /> &gt; You&#39;ll have to be a little more specific when you say &quot;it&#39= ;s tough to get<br /> &gt; adequate node time.&quot; What kind of workflows are you running? Are = these things<br /> &gt; that you typically try to use interactive nodes for, or something that=  you can<br /> &gt; send an &quot;srun&quot; or &quot;sbatch&quot; for and leave it alone?=  We have definitely seen an<br /> &gt; increase in the number of users and PIs who are using our machines in = the last<br /> &gt; few months, so certainly that is probably a part of it, but is there s= omething<br /> &gt; else you&#39;re noticing?<br /> &gt;<br /> &gt;&gt; On Thu Oct 16 15:48:50 2025, UI11613 wrote:<br /> &gt;&gt;<br /> &gt;&gt; It&#39;s possible that is the explanation, it is also possible tha= t we<br /> &gt;&gt; miscopied the paths in, as we are doing this late at night to get = node<br /> &gt;&gt; time. What is going on on the HPC lately, it&#39;s tough to get ad= equate node<br /> &gt;&gt; time. On Thu, Oct 16, 2025 at 11:15 AM Danielle Esposito via RT<br=  /> &gt;&gt; &lt;UMBCHelp@rt.umbc.edu&gt; wrote:<br /> &gt;<br /> &gt;&gt;&gt; Ticket &lt;URL: https://www.google.com/url?q=3Dhttps://rt.umbc= .edu/Ticket/Display.html?id%3D3293526&amp;source=3Dgmail-imap&amp;ust=3D176= 1322609000000&amp;usg=3DAOvVaw0wv3hH6RptKPjt58gdEvLr &gt;<br /> &gt;<br /> &gt;&gt;&gt; Last Update From Ticket:<br /> &gt;<br /> &gt;&gt;&gt; Hi Matthew,<br /> &gt;<br /> &gt;&gt;&gt; Since your group has migrated to ceph, the path for your group= s<br /> &gt;&gt;&gt; research<br /> &gt;&gt;&gt; storage is slightly different.<br /> &gt;<br /> &gt;&gt;&gt; Previously, on Isilon, the path was &#39;/umbc/rs/mbaker/...&#= 39;<br /> &gt;<br /> &gt;&gt;&gt; However, Ceph research volumes are differentiated by adding th= e pi_:<br /> &gt;&gt;&gt; &#39;/umbc/rs/pi_mbaker&#39;.<br /> &gt;<br /> &gt;&gt;&gt; I noticed in your conda activate command, the path was still u= sing the<br /> &gt;&gt;&gt; previous<br /> &gt;&gt;&gt; isilion research volume, which no longer exists. Can you confi= rm that<br /> &gt;&gt;&gt; you are<br /> &gt;&gt;&gt; activating the conda environment with the correct path?<br /> &gt;<br /> &gt;&gt;&gt; Thanks!<br /> &gt;<br /> &gt;&gt;&gt; --<br /> &gt;<br /> &gt;&gt;&gt; Kind regards,<br /> &gt;&gt;&gt; Danielle Esposito (she/her/hers)<br /> &gt;&gt;&gt; DoIT Unix Infra Student Worker<br /> &gt;<br /> &gt;&gt;&gt; On Wed Oct 15 14:42:46 2025, UI11613 wrote:<br /> &gt;<br /> &gt;&gt;&gt;&gt; Hi All Update: the problem did not appear when we tried ag= ain at<br /> &gt;&gt;&gt; noon.<br /> &gt;&gt;&gt;&gt; However, any explanation might help us navigate future iss= ues in the<br /> &gt;&gt;&gt;&gt; future, and would be appreciated. M On Wed, Oct 15, 2025 a= t 1:55 PM<br /> &gt;&gt;&gt; via RT<br /> &gt;&gt;&gt;&gt; &lt;UMBCHelp@rt.umbc.edu&gt; wrote:<br /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; Greetings,<br /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; This message has been automatically generated in respo= nse to the<br /> &gt;&gt;&gt;&gt;&gt; creation of a ticket regarding:<br /> &gt;<br /> &gt;&gt;&gt;&gt;&gt;<br /> &gt;&gt;&gt; --------------------------------------------------------------= -----------<br /> &gt;&gt;&gt;&gt;&gt; Subject: &quot;HPC Slurm/Software Issue: Conda environ= ment&quot;<br /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; Message:<br /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; First Name: Matthew<br /> &gt;&gt;&gt;&gt;&gt; Last Name: Baker<br /> &gt;&gt;&gt;&gt;&gt; Email: mbaker@umbc.edu<br /> &gt;&gt;&gt;&gt;&gt; Campus ID: UI11613<br /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; Request Type: High Performance Cluster<br /> &gt;<br /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; HI Folks,<br /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; Sorry to be a bother, but since our storage was migrat= ed we&#39;ve<br /> &gt;&gt;&gt; noticed<br /> &gt;&gt;&gt;&gt;&gt; a new issue:<br /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; The new issue seems to be related to the HPC environme= nt. It appears<br /> &gt;&gt;&gt;&gt;&gt; that the Conda environment is not being applied consis= tently across<br /> &gt;&gt;&gt; all<br /> &gt;&gt;&gt;&gt;&gt; nodes after we load it. Below are the steps and result= s for<br /> &gt;&gt;&gt; reference.<br /> &gt;&gt;&gt;&gt;&gt; Does your team have a potential solution?<br /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; To allocate resources, we ran the following command:<b= r /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; srun --time=3D24:00:00 --mem=3D100G --partition=3D2018=  --qos=3Dmedium<br /> &gt;&gt;&gt;&gt;&gt; --cluster=3Dchip-cpu --nodes=3D10 --ntasks-per-node=3D= 7 --pty bash<br /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; Then we loaded the computation environment using:<br /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; module load GRASS/8.2.0-foss-2021b<br /> &gt;&gt;&gt;&gt;&gt; module load Anaconda3/2024.02-1<br /> &gt;&gt;&gt;&gt;&gt; conda activate<br /> &gt;&gt;&gt;&gt;&gt;<br /> &gt;&gt;&gt; /umbc/rs/mbaker/common/huc_test/Essential_Scripts/HydrographyW= orkflow/Cang/condaEnv_3<br /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; However, when we test the environment with:<br /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; mpirun python -c &quot;import fiona; print(fiona.__ver= sion__)&quot;<br /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; only one node can use the fiona library (the version i= s printed),<br /> &gt;&gt;&gt; while<br /> &gt;&gt;&gt;&gt;&gt; the others return:<br /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; ModuleNotFoundError: No module named &#39;fiona&#39;<b= r /> &gt;<br /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; I further tested with:<br /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; mpirun python -c &quot;import sys; print(sys.executabl= e)&quot;<br /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; and found that only one node uses the Conda environmen= t&rsquo;s Python<br /> &gt;&gt;&gt;&gt;&gt; interpreter<br /> &gt;&gt;&gt;&gt;&gt;<br /> &gt;&gt;&gt; (/umbc/rs/mbaker/common/huc_test/Essential_Scripts/Hydrography= Workflow/Cang/condaEnv_3/bin/python),<br /> &gt;&gt;&gt;&gt;&gt; while the others default to the system Python at:<br /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; /usr/ebuild/installs/software/Python/3.9.6-GCCcore-11.= 2.0/bin/python<br /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; It seems that the Conda environment is only being appl= ied to one<br /> &gt;&gt;&gt; node.<br /> &gt;&gt;&gt;&gt;&gt; Could this be related to how the environment is initia= lized across<br /> &gt;&gt;&gt;&gt;&gt; nodes in the current cluster setup?<br /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; Thank you!<br /> &gt;<br /> &gt;<br /> &gt;<br /> &gt;<br /> &gt;&gt;&gt;&gt;&gt;<br /> &gt;&gt;&gt; --------------------------------------------------------------= -----------<br /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; There is no need to reply to this message right now.<b= r /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; Your ticket has been assigned an ID of [Research Compu= ting #3293526]<br /> &gt;&gt;&gt; or<br /> &gt;&gt;&gt;&gt;&gt; you can go there directly by clicking the link below.<= br /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; Ticket &lt;URL: https://www.google.com/url?q=3Dhttps:/= /rt.umbc.edu/Ticket/Display.html?id%3D3293526&amp;source=3Dgmail-imap&amp;u= st=3D1761322609000000&amp;usg=3DAOvVaw0wv3hH6RptKPjt58gdEvLr &gt;<br /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; You can login to view your open tickets at any time by=  visiting<br /> &gt;&gt;&gt;&gt;&gt; https://www.google.com/url?q=3Dhttp://my.umbc.edu&amp;= source=3Dgmail-imap&amp;ust=3D1761322609000000&amp;usg=3DAOvVaw08kV6MdknFvh= wlnejGaers and clicking on &quot;Help&quot; and &quot;Request Help&quot;.<b= r /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; Alternately you can click on https://www.google.com/ur= l?q=3Dhttp://my.umbc.edu/help&amp;source=3Dgmail-imap&amp;ust=3D17613226090= 00000&amp;usg=3DAOvVaw3vEPay5opBUnoGf4F79fS6<br /> &gt;<br /> &gt;&gt;&gt;&gt;&gt; Thank you<br /> &gt;<br /> &gt; --<br /> &gt;<br /> &gt; Best,<br /> &gt; Max Breitmeyer<br /> &gt; DOIT HPC System Administrator<br /> &gt;</blockquote> </div> </blockquote> </div> "
3293526,72371410,Correspond,DoIT-Research-Computing,2025-10-20 14:10:34.0000000,HPC Slurm/Software Issue: Conda environment,resolved,Danielle Esposito,desposi1,Matthew Baker,mbaker,mbaker@umbc.edu,Matthew Baker,mbaker@umbc.edu,"Confirmed that conda env is working successfully.  I dont think it was just a path issue but it did seem to resolve itself.  Thanks!  On Mon, Oct 20, 2025 at 10:08=E2=80=AFAM Danielle Esposito via RT < UMBCHelp@rt.umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3293526 > > > Last Update From Ticket: > > Hi Matthew, > > Thanks for letting me know about the permission issue. There was an > incorrect > extended acl set on the directory, which I have fixed. Let me know if it > works > for your colleague. > > Can you confirm if you were able to get your anaconda environment working > successfully? Or are you still experiencing the original issue? > > -- > > Kind regards, > Danielle Esposito (she/her/hers) > DoIT Unix Infra Student Worker > > On Sun Oct 19 18:53:36 2025, UI11613 wrote: > > > Here's another conundrum: My colleague is unable to to create a directo= ry > > even though I have explicitly given my group rwx permissions and he is > > recognized as a member of my group. Can you advice? > > > > > > > > (base) [xcang@c18-01 Cang]$ pwd > > > /umbc/rs/pi_mbaker/common/huc_test/Essential_Scripts/HydrographyWorkflow/= Cang > > (base) [xcang@c18-01 Cang]$ mkdir a_new_dir > > mkdir: cannot create directory =E2=80=98a_new_dir=E2=80=99: Permission = denied(base) > > [xcang@c18-01 Cang]$ ls -ld > > > /umbc/rs/pi_mbaker/common/huc_test/Essential_Scripts/HydrographyWorkflow/= Cang > > drwxrws---+ 35 mbaker pi_mbaker 42 Oct 18 15:43 > > > /umbc/rs/pi_mbaker/common/huc_test/Essential_Scripts/HydrographyWorkflow/= Cang > > (base) [xcang@c18-01 Cang]$ getent group pi_mbaker > > pi_mbaker:*:1054:mbaker,barajasc,xcang On Fri, Oct 17, 2025 at 5:32 PM > > Matthew Baker <mbaker@umbc.edu> wrote: > > >> Hi Max > > >> I think I am probably noticing the uptick in competition for node time, > >> perhaps via a larger number of active users. At least that would be a > >> sufficient explanation. > > >> I am not specifically talking about a problem though, but that is for > >> letting me know. > > >> Matthew > > > >> Sent from my iPhone > > >> > On Oct 17, 2025, at 12:16 PM, Max Breitmeyer via RT > >> <UMBCHelp@rt.umbc.edu> wrote: > >> > > >> > =EF=BB=BFTicket <URL: > >> > https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id= %3D3293526&source=3Dgmail-imap&ust=3D1761322609000000&usg=3DAOvVaw0wv3hH6Rp= tKPjt58gdEvLr > >> > > >> > > >> > Last Update From Ticket: > >> > > >> > Hi Matthew, > >> > > >> > You'll have to be a little more specific when you say ""it's tough to > >> get > >> > adequate node time."" What kind of workflows are you running? Are > >> these things > >> > that you typically try to use interactive nodes for, or something > >> that you can > >> > send an ""srun"" or ""sbatch"" for and leave it alone? We have definitely > >> seen an > >> > increase in the number of users and PIs who are using our machines in > >> the last > >> > few months, so certainly that is probably a part of it, but is there > >> something > >> > else you're noticing? > >> > > >> >> On Thu Oct 16 15:48:50 2025, UI11613 wrote: > >> >> > >> >> It's possible that is the explanation, it is also possible that we > >> >> miscopied the paths in, as we are doing this late at night to get > >> node > >> >> time. What is going on on the HPC lately, it's tough to get adequate > >> node > >> >> time. On Thu, Oct 16, 2025 at 11:15 AM Danielle Esposito via RT > >> >> <UMBCHelp@rt.umbc.edu> wrote: > >> > > >> >>> Ticket <URL: > >> > https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id= %3D3293526&source=3Dgmail-imap&ust=3D1761322609000000&usg=3DAOvVaw0wv3hH6Rp= tKPjt58gdEvLr > >> > > >> > > >> >>> Last Update From Ticket: > >> > > >> >>> Hi Matthew, > >> > > >> >>> Since your group has migrated to ceph, the path for your groups > >> >>> research > >> >>> storage is slightly different. > >> > > >> >>> Previously, on Isilon, the path was '/umbc/rs/mbaker/...' > >> > > >> >>> However, Ceph research volumes are differentiated by adding the > >> pi_: > >> >>> '/umbc/rs/pi_mbaker'. > >> > > >> >>> I noticed in your conda activate command, the path was still using > >> the > >> >>> previous > >> >>> isilion research volume, which no longer exists. Can you confirm > >> that > >> >>> you are > >> >>> activating the conda environment with the correct path? > >> > > >> >>> Thanks! > >> > > >> >>> -- > >> > > >> >>> Kind regards, > >> >>> Danielle Esposito (she/her/hers) > >> >>> DoIT Unix Infra Student Worker > >> > > >> >>> On Wed Oct 15 14:42:46 2025, UI11613 wrote: > >> > > >> >>>> Hi All Update: the problem did not appear when we tried again at > >> >>> noon. > >> >>>> However, any explanation might help us navigate future issues in > >> the > >> >>>> future, and would be appreciated. M On Wed, Oct 15, 2025 at 1:55 > >> PM > >> >>> via RT > >> >>>> <UMBCHelp@rt.umbc.edu> wrote: > >> > > >> >>>>> Greetings, > >> > > >> >>>>> This message has been automatically generated in response to the > >> >>>>> creation of a ticket regarding: > >> > > >> >>>>> > >> >>> > >> > ------------------------------------------------------------------------- > >> >>>>> Subject: ""HPC Slurm/Software Issue: Conda environment"" > >> > > >> >>>>> Message: > >> > > >> >>>>> First Name: Matthew > >> >>>>> Last Name: Baker > >> >>>>> Email: mbaker@umbc.edu > >> >>>>> Campus ID: UI11613 > >> > > >> >>>>> Request Type: High Performance Cluster > >> > > >> > > >> >>>>> HI Folks, > >> > > >> >>>>> Sorry to be a bother, but since our storage was migrated we've > >> >>> noticed > >> >>>>> a new issue: > >> > > >> >>>>> The new issue seems to be related to the HPC environment. It > >> appears > >> >>>>> that the Conda environment is not being applied consistently > >> across > >> >>> all > >> >>>>> nodes after we load it. Below are the steps and results for > >> >>> reference. > >> >>>>> Does your team have a potential solution? > >> > > >> >>>>> To allocate resources, we ran the following command: > >> > > >> >>>>> srun --time=3D24:00:00 --mem=3D100G --partition=3D2018 --qos=3Dm= edium > >> >>>>> --cluster=3Dchip-cpu --nodes=3D10 --ntasks-per-node=3D7 --pty ba= sh > >> > > >> >>>>> Then we loaded the computation environment using: > >> > > >> >>>>> module load GRASS/8.2.0-foss-2021b > >> >>>>> module load Anaconda3/2024.02-1 > >> >>>>> conda activate > >> >>>>> > >> >>> > >> > /umbc/rs/mbaker/common/huc_test/Essential_Scripts/HydrographyWorkflow/Can= g/condaEnv_3 > >> > > >> >>>>> However, when we test the environment with: > >> > > >> >>>>> mpirun python -c ""import fiona; print(fiona.__version__)"" > >> > > >> >>>>> only one node can use the fiona library (the version is printed), > >> >>> while > >> >>>>> the others return: > >> > > >> >>>>> ModuleNotFoundError: No module named 'fiona' > >> > > >> > > >> >>>>> I further tested with: > >> > > >> >>>>> mpirun python -c ""import sys; print(sys.executable)"" > >> > > >> >>>>> and found that only one node uses the Conda environment=E2=80=99= s Python > >> >>>>> interpreter > >> >>>>> > >> >>> > >> > (/umbc/rs/mbaker/common/huc_test/Essential_Scripts/HydrographyWorkflow/Ca= ng/condaEnv_3/bin/python), > >> >>>>> while the others default to the system Python at: > >> > > >> >>>>> > >> /usr/ebuild/installs/software/Python/3.9.6-GCCcore-11.2.0/bin/python > >> > > >> >>>>> It seems that the Conda environment is only being applied to one > >> >>> node. > >> >>>>> Could this be related to how the environment is initialized > >> across > >> >>>>> nodes in the current cluster setup? > >> > > >> >>>>> Thank you! > >> > > >> > > >> > > >> > > >> >>>>> > >> >>> > >> > ------------------------------------------------------------------------- > >> > > >> >>>>> There is no need to reply to this message right now. > >> > > >> >>>>> Your ticket has been assigned an ID of [Research Computing > >> #3293526] > >> >>> or > >> >>>>> you can go there directly by clicking the link below. > >> > > >> >>>>> Ticket <URL: > >> > https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id= %3D3293526&source=3Dgmail-imap&ust=3D1761322609000000&usg=3DAOvVaw0wv3hH6Rp= tKPjt58gdEvLr > >> > > >> > > >> >>>>> You can login to view your open tickets at any time by visiting > >> >>>>> > >> > https://www.google.com/url?q=3Dhttp://my.umbc.edu&source=3Dgmail-imap&ust= =3D1761322609000000&usg=3DAOvVaw08kV6MdknFvhwlnejGaers > >> and clicking on ""Help"" and ""Request Help"". > >> > > >> >>>>> Alternately you can click on > >> > https://www.google.com/url?q=3Dhttp://my.umbc.edu/help&source=3Dgmail-ima= p&ust=3D1761322609000000&usg=3DAOvVaw3vEPay5opBUnoGf4F79fS6 > >> > > >> >>>>> Thank you > >> > > >> > -- > >> > > >> > Best, > >> > Max Breitmeyer > >> > DOIT HPC System Administrator > >> > > > "
3293526,72371410,Correspond,DoIT-Research-Computing,2025-10-20 14:10:34.0000000,HPC Slurm/Software Issue: Conda environment,resolved,Danielle Esposito,desposi1,Matthew Baker,mbaker,mbaker@umbc.edu,Matthew Baker,mbaker@umbc.edu,"<div dir=3D""ltr""><div>Confirmed that conda env is working successfully.=C2= =A0 I dont think it was just a path issue but it did seem to resolve itself= .</div><div><br></div><div>Thanks!</div></div><br><div class=3D""gmail_quote=  gmail_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Mon, Oct 2= 0, 2025 at 10:08=E2=80=AFAM Danielle Esposito via RT &lt;<a href=3D""mailto:= UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockqu= ote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px=  solid rgb(204,204,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https:= //rt.umbc.edu/Ticket/Display.html?id=3D3293526"" rel=3D""noreferrer"" target= =3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3293526</a> &gt;<b= r> <br> Last Update From Ticket:<br> <br> Hi Matthew,<br> <br> Thanks for letting me know about the permission issue. There was an incorre= ct<br> extended acl set on the directory, which I have fixed. Let me know if it wo= rks<br> for your colleague.<br> <br> Can you confirm if you were able to get your anaconda environment working<b= r> successfully? Or are you still experiencing the original issue?<br> <br> --<br> <br> Kind regards,<br> Danielle Esposito (she/her/hers)<br> DoIT Unix Infra Student Worker<br> <br> On Sun Oct 19 18:53:36 2025, UI11613 wrote:<br> <br> &gt; Here&#39;s another conundrum: My colleague is unable to to create a di= rectory<br> &gt; even though I have explicitly given my group rwx permissions and he is= <br> &gt; recognized as a member of my group. Can you advice?<br> <br> &gt; <br> &gt; =C2=A0=C2=A0 <br> <br> &gt; (base) [xcang@c18-01 Cang]$ pwd<br> &gt; /umbc/rs/pi_mbaker/common/huc_test/Essential_Scripts/HydrographyWorkfl= ow/Cang<br> &gt; (base) [xcang@c18-01 Cang]$ mkdir a_new_dir<br> &gt; mkdir: cannot create directory =E2=80=98a_new_dir=E2=80=99: Permission=  denied(base)<br> &gt; [xcang@c18-01 Cang]$ ls -ld<br> &gt; /umbc/rs/pi_mbaker/common/huc_test/Essential_Scripts/HydrographyWorkfl= ow/Cang<br> &gt; drwxrws---+ 35 mbaker pi_mbaker 42 Oct 18 15:43<br> &gt; /umbc/rs/pi_mbaker/common/huc_test/Essential_Scripts/HydrographyWorkfl= ow/Cang<br> &gt; (base) [xcang@c18-01 Cang]$ getent group pi_mbaker<br> &gt; pi_mbaker:*:1054:mbaker,barajasc,xcang On Fri, Oct 17, 2025 at 5:32 PM= <br> &gt; Matthew Baker &lt;<a href=3D""mailto:mbaker@umbc.edu"" target=3D""_blank""= >mbaker@umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; Hi Max<br> <br> &gt;&gt; I think I am probably noticing the uptick in competition for node = time,<br> &gt;&gt; perhaps via a larger number of active users. At least that would b= e a<br> &gt;&gt; sufficient explanation.<br> <br> &gt;&gt; I am not specifically talking about a problem though, but that is = for<br> &gt;&gt; letting me know.<br> <br> &gt;&gt; Matthew<br> <br> <br> &gt;&gt; Sent from my iPhone<br> <br> &gt;&gt; &gt; On Oct 17, 2025, at 12:16 PM, Max Breitmeyer via RT<br> &gt;&gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBC= Help@rt.umbc.edu</a>&gt; wrote:<br> &gt;&gt; &gt;<br> &gt;&gt; &gt; =EF=BB=BFTicket &lt;URL:<br> &gt;&gt; <a href=3D""https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Tick= et/Display.html?id%3D3293526&amp;source=3Dgmail-imap&amp;ust=3D176132260900= 0000&amp;usg=3DAOvVaw0wv3hH6RptKPjt58gdEvLr"" rel=3D""noreferrer"" target=3D""_= blank"">https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.ht= ml?id%3D3293526&amp;source=3Dgmail-imap&amp;ust=3D1761322609000000&amp;usg= =3DAOvVaw0wv3hH6RptKPjt58gdEvLr</a><br> &gt;&gt; &gt;<br> &gt;&gt; &gt;<br> &gt;&gt; &gt; Last Update From Ticket:<br> &gt;&gt; &gt;<br> &gt;&gt; &gt; Hi Matthew,<br> &gt;&gt; &gt;<br> &gt;&gt; &gt; You&#39;ll have to be a little more specific when you say &qu= ot;it&#39;s tough to<br> &gt;&gt; get<br> &gt;&gt; &gt; adequate node time.&quot; What kind of workflows are you runn= ing? Are<br> &gt;&gt; these things<br> &gt;&gt; &gt; that you typically try to use interactive nodes for, or somet= hing<br> &gt;&gt; that you can<br> &gt;&gt; &gt; send an &quot;srun&quot; or &quot;sbatch&quot; for and leave = it alone? We have definitely<br> &gt;&gt; seen an<br> &gt;&gt; &gt; increase in the number of users and PIs who are using our mac= hines in<br> &gt;&gt; the last<br> &gt;&gt; &gt; few months, so certainly that is probably a part of it, but i= s there<br> &gt;&gt; something<br> &gt;&gt; &gt; else you&#39;re noticing?<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt; On Thu Oct 16 15:48:50 2025, UI11613 wrote:<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; It&#39;s possible that is the explanation, it is also pos= sible that we<br> &gt;&gt; &gt;&gt; miscopied the paths in, as we are doing this late at nigh= t to get<br> &gt;&gt; node<br> &gt;&gt; &gt;&gt; time. What is going on on the HPC lately, it&#39;s tough = to get adequate<br> &gt;&gt; node<br> &gt;&gt; &gt;&gt; time. On Thu, Oct 16, 2025 at 11:15 AM Danielle Esposito = via RT<br> &gt;&gt; &gt;&gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_bl= ank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt; Ticket &lt;URL:<br> &gt;&gt; <a href=3D""https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Tick= et/Display.html?id%3D3293526&amp;source=3Dgmail-imap&amp;ust=3D176132260900= 0000&amp;usg=3DAOvVaw0wv3hH6RptKPjt58gdEvLr"" rel=3D""noreferrer"" target=3D""_= blank"">https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.ht= ml?id%3D3293526&amp;source=3Dgmail-imap&amp;ust=3D1761322609000000&amp;usg= =3DAOvVaw0wv3hH6RptKPjt58gdEvLr</a><br> &gt;&gt; &gt;<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt; Last Update From Ticket:<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt; Hi Matthew,<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt; Since your group has migrated to ceph, the path for y= our groups<br> &gt;&gt; &gt;&gt;&gt; research<br> &gt;&gt; &gt;&gt;&gt; storage is slightly different.<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt; Previously, on Isilon, the path was &#39;/umbc/rs/mba= ker/...&#39;<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt; However, Ceph research volumes are differentiated by = adding the<br> &gt;&gt; pi_:<br> &gt;&gt; &gt;&gt;&gt; &#39;/umbc/rs/pi_mbaker&#39;.<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt; I noticed in your conda activate command, the path wa= s still using<br> &gt;&gt; the<br> &gt;&gt; &gt;&gt;&gt; previous<br> &gt;&gt; &gt;&gt;&gt; isilion research volume, which no longer exists. Can = you confirm<br> &gt;&gt; that<br> &gt;&gt; &gt;&gt;&gt; you are<br> &gt;&gt; &gt;&gt;&gt; activating the conda environment with the correct pat= h?<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt; Thanks!<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt; --<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt; Kind regards,<br> &gt;&gt; &gt;&gt;&gt; Danielle Esposito (she/her/hers)<br> &gt;&gt; &gt;&gt;&gt; DoIT Unix Infra Student Worker<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt; On Wed Oct 15 14:42:46 2025, UI11613 wrote:<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt; Hi All Update: the problem did not appear when we=  tried again at<br> &gt;&gt; &gt;&gt;&gt; noon.<br> &gt;&gt; &gt;&gt;&gt;&gt; However, any explanation might help us navigate f= uture issues in<br> &gt;&gt; the<br> &gt;&gt; &gt;&gt;&gt;&gt; future, and would be appreciated. M On Wed, Oct 1= 5, 2025 at 1:55<br> &gt;&gt; PM<br> &gt;&gt; &gt;&gt;&gt; via RT<br> &gt;&gt; &gt;&gt;&gt;&gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" targe= t=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Greetings,<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; This message has been automatically generated=  in response to the<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; creation of a ticket regarding:<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt;<br> &gt;&gt; &gt;&gt;&gt;<br> &gt;&gt; ------------------------------------------------------------------= -------<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Subject: &quot;HPC Slurm/Software Issue: Cond= a environment&quot;<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Message:<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; First Name: Matthew<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Last Name: Baker<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Email: <a href=3D""mailto:mbaker@umbc.edu"" tar= get=3D""_blank"">mbaker@umbc.edu</a><br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Campus ID: UI11613<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Request Type: High Performance Cluster<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; HI Folks,<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Sorry to be a bother, but since our storage w= as migrated we&#39;ve<br> &gt;&gt; &gt;&gt;&gt; noticed<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; a new issue:<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; The new issue seems to be related to the HPC = environment. It<br> &gt;&gt; appears<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; that the Conda environment is not being appli= ed consistently<br> &gt;&gt; across<br> &gt;&gt; &gt;&gt;&gt; all<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; nodes after we load it. Below are the steps a= nd results for<br> &gt;&gt; &gt;&gt;&gt; reference.<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Does your team have a potential solution?<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; To allocate resources, we ran the following c= ommand:<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; srun --time=3D24:00:00 --mem=3D100G --partiti= on=3D2018 --qos=3Dmedium<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; --cluster=3Dchip-cpu --nodes=3D10 --ntasks-pe= r-node=3D7 --pty bash<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Then we loaded the computation environment us= ing:<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; module load GRASS/8.2.0-foss-2021b<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; module load Anaconda3/2024.02-1<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; conda activate<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt;<br> &gt;&gt; &gt;&gt;&gt;<br> &gt;&gt; /umbc/rs/mbaker/common/huc_test/Essential_Scripts/HydrographyWorkf= low/Cang/condaEnv_3<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; However, when we test the environment with:<b= r> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; mpirun python -c &quot;import fiona; print(fi= ona.__version__)&quot;<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; only one node can use the fiona library (the = version is printed),<br> &gt;&gt; &gt;&gt;&gt; while<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; the others return:<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; ModuleNotFoundError: No module named &#39;fio= na&#39;<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; I further tested with:<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; mpirun python -c &quot;import sys; print(sys.= executable)&quot;<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; and found that only one node uses the Conda e= nvironment=E2=80=99s Python<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; interpreter<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt;<br> &gt;&gt; &gt;&gt;&gt;<br> &gt;&gt; (/umbc/rs/mbaker/common/huc_test/Essential_Scripts/HydrographyWork= flow/Cang/condaEnv_3/bin/python),<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; while the others default to the system Python=  at:<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt;<br> &gt;&gt; /usr/ebuild/installs/software/Python/3.9.6-GCCcore-11.2.0/bin/pyth= on<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; It seems that the Conda environment is only b= eing applied to one<br> &gt;&gt; &gt;&gt;&gt; node.<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Could this be related to how the environment = is initialized<br> &gt;&gt; across<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; nodes in the current cluster setup?<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Thank you!<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt;<br> &gt;&gt; &gt;&gt;&gt;<br> &gt;&gt; ------------------------------------------------------------------= -------<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; There is no need to reply to this message rig= ht now.<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Your ticket has been assigned an ID of [Resea= rch Computing<br> &gt;&gt; #3293526]<br> &gt;&gt; &gt;&gt;&gt; or<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; you can go there directly by clicking the lin= k below.<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Ticket &lt;URL:<br> &gt;&gt; <a href=3D""https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Tick= et/Display.html?id%3D3293526&amp;source=3Dgmail-imap&amp;ust=3D176132260900= 0000&amp;usg=3DAOvVaw0wv3hH6RptKPjt58gdEvLr"" rel=3D""noreferrer"" target=3D""_= blank"">https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.ht= ml?id%3D3293526&amp;source=3Dgmail-imap&amp;ust=3D1761322609000000&amp;usg= =3DAOvVaw0wv3hH6RptKPjt58gdEvLr</a><br> &gt;&gt; &gt;<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; You can login to view your open tickets at an= y time by visiting<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt;<br> &gt;&gt; <a href=3D""https://www.google.com/url?q=3Dhttp://my.umbc.edu&amp;s= ource=3Dgmail-imap&amp;ust=3D1761322609000000&amp;usg=3DAOvVaw08kV6MdknFvhw= lnejGaers"" rel=3D""noreferrer"" target=3D""_blank"">https://www.google.com/url?= q=3Dhttp://my.umbc.edu&amp;source=3Dgmail-imap&amp;ust=3D1761322609000000&a= mp;usg=3DAOvVaw08kV6MdknFvhwlnejGaers</a><br> &gt;&gt; and clicking on &quot;Help&quot; and &quot;Request Help&quot;.<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Alternately you can click on<br> &gt;&gt; <a href=3D""https://www.google.com/url?q=3Dhttp://my.umbc.edu/help&= amp;source=3Dgmail-imap&amp;ust=3D1761322609000000&amp;usg=3DAOvVaw3vEPay5o= pBUnoGf4F79fS6"" rel=3D""noreferrer"" target=3D""_blank"">https://www.google.com= /url?q=3Dhttp://my.umbc.edu/help&amp;source=3Dgmail-imap&amp;ust=3D17613226= 09000000&amp;usg=3DAOvVaw3vEPay5opBUnoGf4F79fS6</a><br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Thank you<br> &gt;&gt; &gt;<br> &gt;&gt; &gt; --<br> &gt;&gt; &gt;<br> &gt;&gt; &gt; Best,<br> &gt;&gt; &gt; Max Breitmeyer<br> &gt;&gt; &gt; DOIT HPC System Administrator<br> &gt;&gt; &gt;<br> <br> </blockquote></div> "
3293526,72398003,Correspond,DoIT-Research-Computing,2025-10-21 16:00:08.0000000,HPC Slurm/Software Issue: Conda environment,resolved,Danielle Esposito,desposi1,Matthew Baker,mbaker,mbaker@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Matthew,</p>  <p>Glad to hear! I will go ahead and resolve this ticket now, but if you ru= n into any issues in the future feel free to submit a new ticket! Have a gr= eat day!</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Mon Oct 20 10:10:34 2025, UI11613 wrote: <blockquote> <div> <div>Confirmed that conda env is working successfully.&nbsp; I dont think i= t was just a path issue but it did seem to resolve itself.</div>  <div>&nbsp;</div>  <div>Thanks!</div> </div> &nbsp;  <div> <div>On Mon, Oct 20, 2025 at 10:08=E2=80=AFAM Danielle Esposito via RT &lt;= UMBCHelp@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 93526 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hi Matthew,<br /> <br /> Thanks for letting me know about the permission issue. There was an incorre= ct<br /> extended acl set on the directory, which I have fixed. Let me know if it wo= rks<br /> for your colleague.<br /> <br /> Can you confirm if you were able to get your anaconda environment working<b= r /> successfully? Or are you still experiencing the original issue?<br /> <br /> --<br /> <br /> Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker<br /> <br /> On Sun Oct 19 18:53:36 2025, UI11613 wrote:<br /> <br /> &gt; Here&#39;s another conundrum: My colleague is unable to to create a di= rectory<br /> &gt; even though I have explicitly given my group rwx permissions and he is= <br /> &gt; recognized as a member of my group. Can you advice?<br /> <br /> &gt;<br /> &gt; &nbsp;&nbsp;<br /> <br /> &gt; (base) [xcang@c18-01 Cang]$ pwd<br /> &gt; /umbc/rs/pi_mbaker/common/huc_test/Essential_Scripts/HydrographyWorkfl= ow/Cang<br /> &gt; (base) [xcang@c18-01 Cang]$ mkdir a_new_dir<br /> &gt; mkdir: cannot create directory &lsquo;a_new_dir&rsquo;: Permission den= ied(base)<br /> &gt; [xcang@c18-01 Cang]$ ls -ld<br /> &gt; /umbc/rs/pi_mbaker/common/huc_test/Essential_Scripts/HydrographyWorkfl= ow/Cang<br /> &gt; drwxrws---+ 35 mbaker pi_mbaker 42 Oct 18 15:43<br /> &gt; /umbc/rs/pi_mbaker/common/huc_test/Essential_Scripts/HydrographyWorkfl= ow/Cang<br /> &gt; (base) [xcang@c18-01 Cang]$ getent group pi_mbaker<br /> &gt; pi_mbaker:*:1054:mbaker,barajasc,xcang On Fri, Oct 17, 2025 at 5:32 PM= <br /> &gt; Matthew Baker &lt;mbaker@umbc.edu&gt; wrote:<br /> <br /> &gt;&gt; Hi Max<br /> <br /> &gt;&gt; I think I am probably noticing the uptick in competition for node = time,<br /> &gt;&gt; perhaps via a larger number of active users. At least that would b= e a<br /> &gt;&gt; sufficient explanation.<br /> <br /> &gt;&gt; I am not specifically talking about a problem though, but that is = for<br /> &gt;&gt; letting me know.<br /> <br /> &gt;&gt; Matthew<br /> <br /> <br /> &gt;&gt; Sent from my iPhone<br /> <br /> &gt;&gt; &gt; On Oct 17, 2025, at 12:16 PM, Max Breitmeyer via RT<br /> &gt;&gt; &lt;UMBCHelp@rt.umbc.edu&gt; wrote:<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt; =EF=BB=BFTicket &lt;URL:<br /> &gt;&gt; https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.= html?id%3D3293526&amp;source=3Dgmail-imap&amp;ust=3D1761322609000000&amp;us= g=3DAOvVaw0wv3hH6RptKPjt58gdEvLr<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt; Last Update From Ticket:<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt; Hi Matthew,<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt; You&#39;ll have to be a little more specific when you say &qu= ot;it&#39;s tough to<br /> &gt;&gt; get<br /> &gt;&gt; &gt; adequate node time.&quot; What kind of workflows are you runn= ing? Are<br /> &gt;&gt; these things<br /> &gt;&gt; &gt; that you typically try to use interactive nodes for, or somet= hing<br /> &gt;&gt; that you can<br /> &gt;&gt; &gt; send an &quot;srun&quot; or &quot;sbatch&quot; for and leave = it alone? We have definitely<br /> &gt;&gt; seen an<br /> &gt;&gt; &gt; increase in the number of users and PIs who are using our mac= hines in<br /> &gt;&gt; the last<br /> &gt;&gt; &gt; few months, so certainly that is probably a part of it, but i= s there<br /> &gt;&gt; something<br /> &gt;&gt; &gt; else you&#39;re noticing?<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt; On Thu Oct 16 15:48:50 2025, UI11613 wrote:<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; It&#39;s possible that is the explanation, it is also pos= sible that we<br /> &gt;&gt; &gt;&gt; miscopied the paths in, as we are doing this late at nigh= t to get<br /> &gt;&gt; node<br /> &gt;&gt; &gt;&gt; time. What is going on on the HPC lately, it&#39;s tough = to get adequate<br /> &gt;&gt; node<br /> &gt;&gt; &gt;&gt; time. On Thu, Oct 16, 2025 at 11:15 AM Danielle Esposito = via RT<br /> &gt;&gt; &gt;&gt; &lt;UMBCHelp@rt.umbc.edu&gt; wrote:<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt; Ticket &lt;URL:<br /> &gt;&gt; https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.= html?id%3D3293526&amp;source=3Dgmail-imap&amp;ust=3D1761322609000000&amp;us= g=3DAOvVaw0wv3hH6RptKPjt58gdEvLr<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt; Last Update From Ticket:<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt; Hi Matthew,<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt; Since your group has migrated to ceph, the path for y= our groups<br /> &gt;&gt; &gt;&gt;&gt; research<br /> &gt;&gt; &gt;&gt;&gt; storage is slightly different.<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt; Previously, on Isilon, the path was &#39;/umbc/rs/mba= ker/...&#39;<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt; However, Ceph research volumes are differentiated by = adding the<br /> &gt;&gt; pi_:<br /> &gt;&gt; &gt;&gt;&gt; &#39;/umbc/rs/pi_mbaker&#39;.<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt; I noticed in your conda activate command, the path wa= s still using<br /> &gt;&gt; the<br /> &gt;&gt; &gt;&gt;&gt; previous<br /> &gt;&gt; &gt;&gt;&gt; isilion research volume, which no longer exists. Can = you confirm<br /> &gt;&gt; that<br /> &gt;&gt; &gt;&gt;&gt; you are<br /> &gt;&gt; &gt;&gt;&gt; activating the conda environment with the correct pat= h?<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt; Thanks!<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt; --<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt; Kind regards,<br /> &gt;&gt; &gt;&gt;&gt; Danielle Esposito (she/her/hers)<br /> &gt;&gt; &gt;&gt;&gt; DoIT Unix Infra Student Worker<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt; On Wed Oct 15 14:42:46 2025, UI11613 wrote:<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt; Hi All Update: the problem did not appear when we=  tried again at<br /> &gt;&gt; &gt;&gt;&gt; noon.<br /> &gt;&gt; &gt;&gt;&gt;&gt; However, any explanation might help us navigate f= uture issues in<br /> &gt;&gt; the<br /> &gt;&gt; &gt;&gt;&gt;&gt; future, and would be appreciated. M On Wed, Oct 1= 5, 2025 at 1:55<br /> &gt;&gt; PM<br /> &gt;&gt; &gt;&gt;&gt; via RT<br /> &gt;&gt; &gt;&gt;&gt;&gt; &lt;UMBCHelp@rt.umbc.edu&gt; wrote:<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Greetings,<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; This message has been automatically generated=  in response to the<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; creation of a ticket regarding:<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt;<br /> &gt;&gt; ------------------------------------------------------------------= -------<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Subject: &quot;HPC Slurm/Software Issue: Cond= a environment&quot;<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Message:<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; First Name: Matthew<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Last Name: Baker<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Email: mbaker@umbc.edu<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Campus ID: UI11613<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Request Type: High Performance Cluster<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; HI Folks,<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Sorry to be a bother, but since our storage w= as migrated we&#39;ve<br /> &gt;&gt; &gt;&gt;&gt; noticed<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; a new issue:<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; The new issue seems to be related to the HPC = environment. It<br /> &gt;&gt; appears<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; that the Conda environment is not being appli= ed consistently<br /> &gt;&gt; across<br /> &gt;&gt; &gt;&gt;&gt; all<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; nodes after we load it. Below are the steps a= nd results for<br /> &gt;&gt; &gt;&gt;&gt; reference.<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Does your team have a potential solution?<br = /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; To allocate resources, we ran the following c= ommand:<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; srun --time=3D24:00:00 --mem=3D100G --partiti= on=3D2018 --qos=3Dmedium<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; --cluster=3Dchip-cpu --nodes=3D10 --ntasks-pe= r-node=3D7 --pty bash<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Then we loaded the computation environment us= ing:<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; module load GRASS/8.2.0-foss-2021b<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; module load Anaconda3/2024.02-1<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; conda activate<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt;<br /> &gt;&gt; /umbc/rs/mbaker/common/huc_test/Essential_Scripts/HydrographyWorkf= low/Cang/condaEnv_3<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; However, when we test the environment with:<b= r /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; mpirun python -c &quot;import fiona; print(fi= ona.__version__)&quot;<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; only one node can use the fiona library (the = version is printed),<br /> &gt;&gt; &gt;&gt;&gt; while<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; the others return:<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; ModuleNotFoundError: No module named &#39;fio= na&#39;<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; I further tested with:<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; mpirun python -c &quot;import sys; print(sys.= executable)&quot;<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; and found that only one node uses the Conda e= nvironment&rsquo;s Python<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; interpreter<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt;<br /> &gt;&gt; (/umbc/rs/mbaker/common/huc_test/Essential_Scripts/HydrographyWork= flow/Cang/condaEnv_3/bin/python),<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; while the others default to the system Python=  at:<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt;<br /> &gt;&gt; /usr/ebuild/installs/software/Python/3.9.6-GCCcore-11.2.0/bin/pyth= on<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; It seems that the Conda environment is only b= eing applied to one<br /> &gt;&gt; &gt;&gt;&gt; node.<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Could this be related to how the environment = is initialized<br /> &gt;&gt; across<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; nodes in the current cluster setup?<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Thank you!<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt;<br /> &gt;&gt; ------------------------------------------------------------------= -------<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; There is no need to reply to this message rig= ht now.<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Your ticket has been assigned an ID of [Resea= rch Computing<br /> &gt;&gt; #3293526]<br /> &gt;&gt; &gt;&gt;&gt; or<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; you can go there directly by clicking the lin= k below.<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Ticket &lt;URL:<br /> &gt;&gt; https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.= html?id%3D3293526&amp;source=3Dgmail-imap&amp;ust=3D1761322609000000&amp;us= g=3DAOvVaw0wv3hH6RptKPjt58gdEvLr<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; You can login to view your open tickets at an= y time by visiting<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt;<br /> &gt;&gt; https://www.google.com/url?q=3Dhttp://my.umbc.edu&amp;source=3Dgma= il-imap&amp;ust=3D1761322609000000&amp;usg=3DAOvVaw08kV6MdknFvhwlnejGaers<b= r /> &gt;&gt; and clicking on &quot;Help&quot; and &quot;Request Help&quot;.<br = /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Alternately you can click on<br /> &gt;&gt; https://www.google.com/url?q=3Dhttp://my.umbc.edu/help&amp;source= =3Dgmail-imap&amp;ust=3D1761322609000000&amp;usg=3DAOvVaw3vEPay5opBUnoGf4F7= 9fS6<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt;&gt; Thank you<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt; --<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt; Best,<br /> &gt;&gt; &gt; Max Breitmeyer<br /> &gt;&gt; &gt; DOIT HPC System Administrator<br /> &gt;&gt; &gt;<br /> &nbsp;</blockquote> </div> </blockquote> </div> "
3294124,72331166,Create,DoIT-Research-Computing,2025-10-16 17:49:55.0000000,HPC User Account: skhisa1 in FSI,resolved,Danielle Esposito,desposi1,Soham Khisa,skhisa1,skhisa1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Soham Last Name:                 Khisa Email:                     skhisa1@umbc.edu Campus ID:                 CQ12379  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    dli@umbc.edu Existing Group:       FSI Project Title:        Sleep Disorder Monitoring Project Abstract:     This project aims to create a sleep stage monitoring system that utilizes contact-free multi-modal data. The long term goal is to extend the system to be able to detect and monitor sleep disorders. By using cutting-edge signal processing and machine learning methods, the system will be capable of providing precise and non-invasive sleep pattern analysis. In order to make the system scalable and capable of real-time execution, the ultimate solution will be refined for deployment on edge devices.   I am requesting access to a GPU cluster to help experiments on contact-free, multi-modal sleep stage monitoring and sleep disorder detection. This is a project to create and train machine learning models that will be used on edge devices.   "
3294124,72331225,Correspond,DoIT-Research-Computing,2025-10-16 17:51:33.0000000,HPC User Account: skhisa1 in FSI,resolved,Danielle Esposito,desposi1,Soham Khisa,skhisa1,skhisa1@umbc.edu,Dong Li,dli@umbc.edu,"I approve this account creation.  Best, Dong  On Thu, Oct 16, 2025 at 1:49=E2=80=AFPM RT API via RT <UMBCHelp@rt.umbc.edu= > wrote:  > This e-mail is a notification that a UMBC user: Soham Khisa < > skhisa1@umbc.edu> has requested an account within UMBC's HPC environment > in your group <FSI>. As the PI, we request that you acknowledge and appro= ve > this account creation by replying to this message. Alternatively you can = go > to this link and review the ticket and indicate your decision here: > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3294124 > > > Once we have your approval, we will create the account and you and the new > user will receive another e-mail notifying you that the account has been > created. If you have any other questions or concerns please contact us. > > - UMBC DoIT Research Computing Support Staff >   --=20 Dong Li, Ph.D. Assistant Professor Department of Computer Science and Electrical Engineering University of Maryland Baltimore County Homepage: https://leetton.github.io/ Schedule a meeting: Google Calendar <https://calendar.app.google/xYK55Uy8HUuaM2ic8> "
3294124,72331225,Correspond,DoIT-Research-Computing,2025-10-16 17:51:33.0000000,HPC User Account: skhisa1 in FSI,resolved,Danielle Esposito,desposi1,Soham Khisa,skhisa1,skhisa1@umbc.edu,Dong Li,dli@umbc.edu,"<div dir=3D""ltr"">I approve this account creation.<div><br></div><div>Best,<= /div><div>Dong</div></div><br><div class=3D""gmail_quote gmail_quote_contain= er""><div dir=3D""ltr"" class=3D""gmail_attr"">On Thu, Oct 16, 2025 at 1:49=E2= =80=AFPM RT API via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp= @rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" styl= e=3D""margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-style:solid= ;border-left-color:rgb(204,204,204);padding-left:1ex"">This e-mail is a noti= fication that a UMBC user: Soham Khisa &lt;<a href=3D""mailto:skhisa1@umbc.e= du"" target=3D""_blank"">skhisa1@umbc.edu</a>&gt; has requested an account wit= hin UMBC&#39;s HPC environment in your group &lt;FSI&gt;. As the PI, we req= uest that you acknowledge and approve this account creation by replying to = this message. Alternatively you can go to this link and review the ticket a= nd indicate your decision here:<br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D329= 4124"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3294124</a> &gt;<br> <br> Once we have your approval, we will create the account and you and the new = user will receive another e-mail notifying you that the account has been cr= eated. If you have any other questions or concerns please contact us.<br> <br> - UMBC DoIT Research Computing Support Staff<br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><div>Dong Li, Ph.D.<br></div><div><div>Assistant=  Professor</div><div>Department of Computer Science and Electrical Engineer= ing</div><div>University of Maryland Baltimore County</div></div><div>Homep= age:=C2=A0<a href=3D""https://leetton.github.io/"" target=3D""_blank"">https://= leetton.github.io/</a><br></div><div>Schedule a meeting: <a href=3D""https:/= /calendar.app.google/xYK55Uy8HUuaM2ic8"" target=3D""_blank"">Google Calendar</= a></div><div><br></div></div></div> "
3294124,72331579,Correspond,DoIT-Research-Computing,2025-10-16 18:03:42.0000000,HPC User Account: skhisa1 in FSI,resolved,Danielle Esposito,desposi1,Soham Khisa,skhisa1,skhisa1@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Soham,</p>  <p>Your account (skhisa1) has been created on chip.rs.umbc.edu.<br /> Your primary group is pi_dli.<br /> Your home directory has 500M of storage.<br /> You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip<br /> Please read through the documentation found at hpcf.umbc.edu &gt; User Support.<br /> All available modules can be viewed using the command &#39;module avail&#39;.<br /> Please submit additional questions or issues as separate tickets via the following link.<br /> (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/)</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Thu Oct 16 13:49:55 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Soham Last Name:                 Khisa Email:                     skhisa1@umbc.edu Campus ID:                 CQ12379  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    dli@umbc.edu Existing Group:       FSI Project Title:        Sleep Disorder Monitoring Project Abstract:     This project aims to create a sleep stage monitoring system that utilizes contact-free multi-modal data. The long term goal is to extend the system to be able to detect and monitor sleep disorders. By using cutting-edge signal processing and machine learning methods, the system will be capable of providing precise and non-invasive sleep pattern analysis. In order to make the system scalable and capable of real-time execution, the ultimate solution will be refined for deployment on edge devices.   I am requesting access to a GPU cluster to help experiments on contact-free, multi-modal sleep stage monitoring and sleep disorder detection. This is a project to create and train machine learning models that will be used on edge devices.   </pre> </blockquote> </div> "
3295053,72357319,Create,DoIT-Research-Computing,2025-10-17 19:50:27.0000000,HPC User Account: kkanike1 in FSI,resolved,Danielle Esposito,desposi1,Keerthana Kanike,kkanike1,kkanike1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Keerthana Last Name:                 Kanike Email:                     kkanike1@umbc.edu Campus ID:                 YS76658  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    dli@umbc.edu Existing Group:       FSI Project Title:        Sleep Disorder Monitoring Project Abstract:     This project aims to create a sleep stage monitoring system that utilizes contact-free multi-modal data. The long term goal is to extend the system to be able to detect and monitor sleep disorders. By using cutting-edge signal processing and machine learning methods, the system will be capable of providing precise and non-invasive sleep pattern analysis. In order to make the system scalable and capable of real-time execution, the ultimate solution will be refined for deployment on edge devices.  I am requesting access to a GPU cluster to help experiments on contact-free, multi-modal sleep stage monitoring and sleep disorder detection. This is a project to create and train machine learning models that will be used on edge devices.  "
3295053,72357349,Correspond,DoIT-Research-Computing,2025-10-17 19:51:24.0000000,HPC User Account: kkanike1 in FSI,resolved,Danielle Esposito,desposi1,Keerthana Kanike,kkanike1,kkanike1@umbc.edu,Dong Li,dli@umbc.edu,"I acknowledge and approve the account creation.  On Fri, Oct 17, 2025 at 3:50=E2=80=AFPM RT API via RT <UMBCHelp@rt.umbc.edu= > wrote:  > This e-mail is a notification that a UMBC user: Keerthana Kanike < > kkanike1@umbc.edu> has requested an account within UMBC's HPC environment > in your group <FSI>. As the PI, we request that you acknowledge and appro= ve > this account creation by replying to this message. Alternatively you can = go > to this link and review the ticket and indicate your decision here: > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3295053 > > > Once we have your approval, we will create the account and you and the new > user will receive another e-mail notifying you that the account has been > created. If you have any other questions or concerns please contact us. > > - UMBC DoIT Research Computing Support Staff >   --=20 Dong Li, Ph.D. Assistant Professor Department of Computer Science and Electrical Engineering University of Maryland Baltimore County Homepage: https://leetton.github.io/ Schedule a meeting: Google Calendar <https://calendar.app.google/xYK55Uy8HUuaM2ic8> "
3295053,72357349,Correspond,DoIT-Research-Computing,2025-10-17 19:51:24.0000000,HPC User Account: kkanike1 in FSI,resolved,Danielle Esposito,desposi1,Keerthana Kanike,kkanike1,kkanike1@umbc.edu,Dong Li,dli@umbc.edu,"<div dir=3D""ltr"">I acknowledge and approve the account creation.</div><br><= div class=3D""gmail_quote gmail_quote_container""><div dir=3D""ltr"" class=3D""g= mail_attr"">On Fri, Oct 17, 2025 at 3:50=E2=80=AFPM RT API via RT &lt;<a hre= f=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></= div><blockquote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;bor= der-left-width:1px;border-left-style:solid;border-left-color:rgb(204,204,20= 4);padding-left:1ex"">This e-mail is a notification that a UMBC user: Keerth= ana Kanike &lt;<a href=3D""mailto:kkanike1@umbc.edu"" target=3D""_blank"">kkani= ke1@umbc.edu</a>&gt; has requested an account within UMBC&#39;s HPC environ= ment in your group &lt;FSI&gt;. As the PI, we request that you acknowledge = and approve this account creation by replying to this message. Alternativel= y you can go to this link and review the ticket and indicate your decision = here:<br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D329= 5053"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3295053</a> &gt;<br> <br> Once we have your approval, we will create the account and you and the new = user will receive another e-mail notifying you that the account has been cr= eated. If you have any other questions or concerns please contact us.<br> <br> - UMBC DoIT Research Computing Support Staff<br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><div>Dong Li, Ph.D.<br></div><div><div>Assistant=  Professor</div><div>Department of Computer Science and Electrical Engineer= ing</div><div>University of Maryland Baltimore County</div></div><div>Homep= age:=C2=A0<a href=3D""https://leetton.github.io/"" target=3D""_blank"">https://= leetton.github.io/</a><br></div><div>Schedule a meeting: <a href=3D""https:/= /calendar.app.google/xYK55Uy8HUuaM2ic8"" target=3D""_blank"">Google Calendar</= a></div><div><br></div></div></div> "
3295053,72375618,Correspond,DoIT-Research-Computing,2025-10-20 18:36:23.0000000,HPC User Account: kkanike1 in FSI,resolved,Danielle Esposito,desposi1,Keerthana Kanike,kkanike1,kkanike1@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Keerthana,</p>  <p>Your account (kkanike1) has been created on chip.rs.umbc.edu.<br /> Your primary group is pi_dli.<br /> Your home directory has 500M of storage.<br /> You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip<br /> Please read through the documentation found at hpcf.umbc.edu &gt; User Support.<br /> All available modules can be viewed using the command &#39;module avail&#39;.<br /> Please submit additional questions or issues as separate tickets via the following link.<br /> (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/)</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Fri Oct 17 15:50:27 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Keerthana Last Name:                 Kanike Email:                     kkanike1@umbc.edu Campus ID:                 YS76658  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    dli@umbc.edu Existing Group:       FSI Project Title:        Sleep Disorder Monitoring Project Abstract:     This project aims to create a sleep stage monitoring system that utilizes contact-free multi-modal data. The long term goal is to extend the system to be able to detect and monitor sleep disorders. By using cutting-edge signal processing and machine learning methods, the system will be capable of providing precise and non-invasive sleep pattern analysis. In order to make the system scalable and capable of real-time execution, the ultimate solution will be refined for deployment on edge devices.  I am requesting access to a GPU cluster to help experiments on contact-free, multi-modal sleep stage monitoring and sleep disorder detection. This is a project to create and train machine learning models that will be used on edge devices.  </pre> </blockquote> </div> "
3295071,72358088,Create,DoIT-Research-Computing,2025-10-17 20:17:16.0000000,HPC User Account: jchen35 in Machine Learning for Signals Processing Lab,resolved,Danielle Esposito,desposi1,Justin Chen,jchen35,jchen35@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Justin Last Name:                 Chen Email:                     jchen35@umbc.edu Campus ID:                 QZ46739  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    adali@umbc.edu Existing Group:       Machine Learning for Signals Processing Lab Project Title:        Multivariate feature selection for fMRI analysis Project Abstract:     fMRI has become a widely used imaging tool for exploring the normal neural functions as well as disordered brain functions like schizophrenia. Among all fMRI  data analysis strategies, data-driven-based methods have a unique advantage of capturing the whole picture of available information since they effectively minimize assumptions imposed on the brain activity. With the increasing number of multimodal data and multisite data, the problem of balancing the computation cost and analysis performance is becoming more important than ever before. In this project, our interest is in identifying the most informative multivariate  features when analyzing multiple fMRI datasets. Our goal is the development of flexible new decomposition methods as well as identifying the best feature extraction strategy for a given problem.    None  "
3295071,72358773,Correspond,DoIT-Research-Computing,2025-10-17 21:03:40.0000000,HPC User Account: jchen35 in Machine Learning for Signals Processing Lab,resolved,Danielle Esposito,desposi1,Justin Chen,jchen35,jchen35@umbc.edu,Tulay Adali,adali@umbc.edu,"Approved!  Best, Tulay   > On Oct 17, 2025, at 4:17 PM, RT API via RT <UMBCHelp@rt.umbc.edu> wrote: >  > This e-mail is a notification that a UMBC user: Justin Chen <jchen35@umbc.edu> has requested an account within UMBC's HPC environment in your group <Machine Learning for Signals Processing Lab>. As the PI, we request that you acknowledge and approve this account creation by replying to this message. Alternatively you can go to this link and review the ticket and indicate your decision here: >  > Ticket <URL: https://www.google.com/url?q=https://rt.umbc.edu/Ticket/Display.html?id%3D3295071&source=gmail-imap&ust=1761337039000000&usg=AOvVaw07BhdxVJ9wnYeXCTTY8oBY > >  > Once we have your approval, we will create the account and you and the new user will receive another e-mail notifying you that the account has been created. If you have any other questions or concerns please contact us. >  > - UMBC DoIT Research Computing Support Staff  "
3295071,72358773,Correspond,DoIT-Research-Computing,2025-10-17 21:03:40.0000000,HPC User Account: jchen35 in Machine Learning for Signals Processing Lab,resolved,Danielle Esposito,desposi1,Justin Chen,jchen35,jchen35@umbc.edu,Tulay Adali,adali@umbc.edu,"<html><head><meta http-equiv=""content-type"" content=""text/html; charset=us-ascii""></head><body style=""overflow-wrap: break-word; -webkit-nbsp-mode: space; line-break: after-white-space;"">Approved!<div><br><div> <meta charset=""UTF-8""><div>Best,</div><div>Tulay&nbsp;</div> </div> <div><br><blockquote type=""cite""><div>On Oct 17, 2025, at 4:17 PM, RT API via RT &lt;UMBCHelp@rt.umbc.edu&gt; wrote:</div><br class=""Apple-interchange-newline""><div><div>This e-mail is a notification that a UMBC user: Justin Chen &lt;jchen35@umbc.edu&gt; has requested an account within UMBC's HPC environment in your group &lt;Machine Learning for Signals Processing Lab&gt;. As the PI, we request that you acknowledge and approve this account creation by replying to this message. Alternatively you can go to this link and review the ticket and indicate your decision here:<br><br>Ticket &lt;URL: https://www.google.com/url?q=https://rt.umbc.edu/Ticket/Display.html?id%3D3295071&amp;source=gmail-imap&amp;ust=1761337039000000&amp;usg=AOvVaw07BhdxVJ9wnYeXCTTY8oBY &gt;<br><br>Once we have your approval, we will create the account and you and the new user will receive another e-mail notifying you that the account has been created. If you have any other questions or concerns please contact us.<br><br>- UMBC DoIT Research Computing Support Staff<br></div></div></blockquote></div><br></div></body></html>"
3295071,72375456,Correspond,DoIT-Research-Computing,2025-10-20 18:33:56.0000000,HPC User Account: jchen35 in Machine Learning for Signals Processing Lab,resolved,Danielle Esposito,desposi1,Justin Chen,jchen35,jchen35@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Justin,</p>  <p>Your account (jchen35) has been created on chip.rs.umbc.edu.<br /> Your primary group is pi_adali.<br /> Your home directory has 500M of storage.<br /> You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip<br /> Please read through the documentation found at hpcf.umbc.edu &gt; User Support.<br /> All available modules can be viewed using the command &#39;module avail&#39;.<br /> Please submit additional questions or issues as separate tickets via the following link.<br /> (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/)</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Fri Oct 17 16:17:16 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Justin Last Name:                 Chen Email:                     jchen35@umbc.edu Campus ID:                 QZ46739  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    adali@umbc.edu Existing Group:       Machine Learning for Signals Processing Lab Project Title:        Multivariate feature selection for fMRI analysis Project Abstract:     fMRI has become a widely used imaging tool for exploring the normal neural functions as well as disordered brain functions like schizophrenia. Among all fMRI  data analysis strategies, data-driven-based methods have a unique advantage of capturing the whole picture of available information since they effectively minimize assumptions imposed on the brain activity. With the increasing number of multimodal data and multisite data, the problem of balancing the computation cost and analysis performance is becoming more important than ever before. In this project, our interest is in identifying the most informative multivariate  features when analyzing multiple fMRI datasets. Our goal is the development of flexible new decomposition methods as well as identifying the best feature extraction strategy for a given problem.    None  </pre> </blockquote> </div> "
3295672,72374816,Create,DoIT-Research-Computing,2025-10-20 18:28:29.0000000,Request for Additional Shared Storage (2TB),resolved,Roy Prouty,proutyr1,Soham Khisa,skhisa1,skhisa1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Soham Last Name:                 Khisa Email:                     skhisa1@umbc.edu Campus ID:                 CQ12379  Request Type:              Help with something else  Dear HPCF Team,  I am a member of the FSI research group led by Dr. Dong Li. For our project titled ""Sleep Disorder Monitoring"", we are currently working with large datasets and require an additional 2TB of shared storage to support our ongoing research activities. We would appreciate your support in accommodating this request. Thank you for your understanding.  Best regards, Soham Khisa  "
3295672,72389751,Correspond,DoIT-Research-Computing,2025-10-21 13:24:05.0000000,Request for Additional Shared Storage (2TB),resolved,Roy Prouty,proutyr1,Soham Khisa,skhisa1,skhisa1@umbc.edu,Roy Prouty,proutyr1@umbc.edu,"<div> <p>Hi Soham,</p>  <p>As I write this, your shared research group volume is using 0% of it&#39;s 25TiB of space. This volume is located at /umbc/rs/pi_dli. I&#39;ll mark this as resolved for now.</p>  <p>See this wiki page for additional information:&nbsp;https://umbc.atlassian.net/wiki/spaces/faq/pages/1072267344/Storage</p>  <p>&nbsp;</p>  <p>On Mon Oct 20 14:28:29 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Soham Last Name:                 Khisa Email:                     skhisa1@umbc.edu Campus ID:                 CQ12379  Request Type:              Help with something else  Dear HPCF Team,  I am a member of the FSI research group led by Dr. Dong Li. For our project titled &quot;Sleep Disorder Monitoring&quot;, we are currently working with large datasets and require an additional 2TB of shared storage to support our ongoing research activities. We would appreciate your support in accommodating this request. Thank you for your understanding.  Best regards, Soham Khisa  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Roy Prouty<br /> DoIT Research Computing Team</p> "
3295788,72376929,Create,DoIT-Research-Computing,2025-10-20 18:51:37.0000000,Requesting Additional Shared Storage (2TB),resolved,Roy Prouty,proutyr1,Soham Khisa,skhisa1,skhisa1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Soham Last Name:                 Khisa Email:                     skhisa1@umbc.edu Campus ID:                 CQ12379  Request Type:              Help with something else  Dear HPCF Team,  I am a member of the FSI research group led by Dr. Dong Li. For our project titled ""Sleep Disorder Monitoring"", we are currently working with large datasets and require an additional 2TB of shared storage to support our ongoing research activities. We would appreciate your support in accommodating this request. Thank you for your understanding.  Best regards, Soham Khisa  "
3295788,72378520,Correspond,DoIT-Research-Computing,2025-10-20 19:08:52.0000000,Requesting Additional Shared Storage (2TB),resolved,Roy Prouty,proutyr1,Soham Khisa,skhisa1,skhisa1@umbc.edu,Roy Prouty,proutyr1@umbc.edu,"<div> <p>Hi&nbsp;Soham,</p>  <p>As I write this, the pi_dli group has used 0% of its 25TiB allocation on chip. The root for this storage volume is located at /umbc/rs/pi_dli . Let me know if you have any questions about this, but I&#39;ll resolve this request for now.</p>  <p>On Mon Oct 20 14:51:37 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Soham Last Name:                 Khisa Email:                     skhisa1@umbc.edu Campus ID:                 CQ12379  Request Type:              Help with something else  Dear HPCF Team,  I am a member of the FSI research group led by Dr. Dong Li. For our project titled &quot;Sleep Disorder Monitoring&quot;, we are currently working with large datasets and require an additional 2TB of shared storage to support our ongoing research activities. We would appreciate your support in accommodating this request. Thank you for your understanding.  Best regards, Soham Khisa  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Roy Prouty<br /> DoIT Research Computing Team</p> "
3296672,72402231,Create,DoIT-Research-Computing,2025-10-21 17:22:34.0000000,HPC User Account: sp65610 in pi_weltyc,resolved,Danielle Esposito,desposi1,Mahjabin Afroj Mila,sp65610,sp65610@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Mahjabin Afroj Last Name:                 Mila Email:                     sp65610@umbc.edu Campus ID:                 SP65610  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    weltyc@umbc.edu Existing Group:       pi_weltyc Project Title:        Coupled Groundwater=E2=80=93Surface Water Modeling of=  Baltimore Using ParFlow.CLM Project Abstract:     This study develops high-resolution ParFlow.CLM model= s for urban watersheds in Baltimore, Maryland, to investigate the interacti= ons between groundwater and surface water under recent climatic conditions.=  The model setups include: acquisition and processing of digital elevation = data; slope generation using GRASS GIS; an overland flow test to ensure tha= t the domains drain completely; acquisition and reclassification of land co= ver; generation of hydrogeologic layers with variable permeability; and pre= paration of NLDAS-2 meteorological forcing data. The models are being used = for three objectives: (1) to evaluate the potential to predict whether simu= lated groundwater levels can predict basement flooding during storms, (2) t= o apply backward particle tracking with ECOSLIM to assess the correlation b= etween stream water chemistry and land use/land cover, and (3) to compare w= ater quantity results (prediction of stream discharge, aquifer levels) to o= ther hydrologic models (HEC-RAS, SWMM, CityCat) being applied to the waters= heds. This implementation provides a basis to evaluate hydrologic model rob= ustness and improve understanding of hydrology in urban settings.   N/A  "
3296672,72410236,Correspond,DoIT-Research-Computing,2025-10-21 20:07:27.0000000,HPC User Account: sp65610 in pi_weltyc,resolved,Danielle Esposito,desposi1,Mahjabin Afroj Mila,sp65610,sp65610@umbc.edu,Claire Welty,weltyc@umbc.edu,"<html><head><meta http-equiv=3D""content-type"" content=3D""text/html; charset= =3Dutf-8""></head><body style=3D""overflow-wrap: break-word; -webkit-nbsp-mod= e: space; line-break: after-white-space;"">I acknowledge and approve.&nbsp;<= br id=3D""lineBreakAtBeginningOfMessage""><div><br><blockquote type=3D""cite"">= <div>On Oct 21, 2025, at 1:22=E2=80=AFPM, RT API via RT &lt;UMBCHelp@rt.umb= c.edu&gt; wrote:</div><br class=3D""Apple-interchange-newline""><div><div>Thi= s e-mail is a notification that a UMBC user: Mahjabin Afroj Mila &lt;sp6561= 0@umbc.edu&gt; has requested an account within UMBC's HPC environment in yo= ur group &lt;pi_weltyc&gt;. As the PI, we request that you acknowledge and = approve this account creation by replying to this message. Alternatively yo= u can go to this link and review the ticket and indicate your decision here= :<br><br>Ticket &lt;URL: https://www.google.com/url?q=3Dhttps://rt.umbc.edu= /Ticket/Display.html?id%3D3296672&amp;source=3Dgmail-imap&amp;ust=3D1761672= 157000000&amp;usg=3DAOvVaw2grBtVAldnCIfHMQjDfulg &gt;<br><br>Once we have y= our approval, we will create the account and you and the new user will rece= ive another e-mail notifying you that the account has been created. If you = have any other questions or concerns please contact us.<br><br>- UMBC DoIT = Research Computing Support Staff<br></div></div></blockquote></div><br><div> <meta charset=3D""UTF-8""><div dir=3D""auto"" style=3D""caret-color: rgb(0, 0, 0= ); color: rgb(0, 0, 0); letter-spacing: normal; text-align: start; text-ind= ent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; -we= bkit-text-stroke-width: 0px; text-decoration: none; word-wrap: break-word; = -webkit-nbsp-mode: space; line-break: after-white-space;""><div dir=3D""auto""=  style=3D""caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0); letter-spacing: n= ormal; text-align: start; text-indent: 0px; text-transform: none; white-spa= ce: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decorat= ion: none; word-wrap: break-word; -webkit-nbsp-mode: space; line-break: aft= er-white-space;"">Claire Welty<br>Director,&nbsp;<a href=3D""https://cuere.um= bc.edu/"">C</a><a href=3D""https://cuere.umbc.edu/"">enter for Urban&nbsp;Envi= ronmental Research and&nbsp;Education</a><br>Professor of Chemical, Biochem= ical,&nbsp;and Environmental Engineering<br>UMBC<br>TRC 102<br>1000 Hilltop=  Circle<br>Baltimore, MD 21250<br><a href=3D""https://urbanhydrology.umbc.ed= u/"">Urban Hydrology at UMBC</a><br><br><br></div></div> </div> <br></body></html>= "
3296672,72410236,Correspond,DoIT-Research-Computing,2025-10-21 20:07:27.0000000,HPC User Account: sp65610 in pi_weltyc,resolved,Danielle Esposito,desposi1,Mahjabin Afroj Mila,sp65610,sp65610@umbc.edu,Claire Welty,weltyc@umbc.edu,"I acknowledge and approve.=20  > On Oct 21, 2025, at 1:22=E2=80=AFPM, RT API via RT <UMBCHelp@rt.umbc.edu>=  wrote: >=20 > This e-mail is a notification that a UMBC user: Mahjabin Afroj Mila <sp65= 610@umbc.edu> has requested an account within UMBC's HPC environment in you= r group <pi_weltyc>. As the PI, we request that you acknowledge and approve=  this account creation by replying to this message. Alternatively you can g= o to this link and review the ticket and indicate your decision here: >=20 > Ticket <URL: https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Di= splay.html?id%3D3296672&source=3Dgmail-imap&ust=3D1761672157000000&usg=3DAO= vVaw2grBtVAldnCIfHMQjDfulg > >=20 > Once we have your approval, we will create the account and you and the ne= w user will receive another e-mail notifying you that the account has been = created. If you have any other questions or concerns please contact us. >=20 > - UMBC DoIT Research Computing Support Staff  Claire Welty Director, C <https://cuere.umbc.edu/>enter for Urban=C2=A0Environmental Res= earch and=C2=A0Education <https://cuere.umbc.edu/> Professor of Chemical, Biochemical, and Environmental Engineering UMBC TRC 102 1000 Hilltop Circle Baltimore, MD 21250 Urban Hydrology at UMBC <https://urbanhydrology.umbc.edu/>    "
3296672,72449233,Comment,DoIT-Research-Computing,2025-10-23 13:23:19.0000000,HPC User Account: sp65610 in pi_weltyc,resolved,Danielle Esposito,desposi1,Mahjabin Afroj Mila,sp65610,sp65610@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hi Authentication,</p>  <p>Your account (sp65610) has been created on chip.rs.umbc.edu.<br /> Your primary group is pi_weltyc.<br /> Your home directory has 500M of storage.<br /> Please read through the documentation found at hpcf.umbc.edu &gt; User Support.<br /> All available modules can be viewed using the command &#39;module avail&#39;.<br /> Please submit additional questions or issues as separate tickets via the following link.<br /> (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/)</p> "
3296672,72458889,Correspond,DoIT-Research-Computing,2025-10-23 16:45:16.0000000,HPC User Account: sp65610 in pi_weltyc,resolved,Danielle Esposito,desposi1,Mahjabin Afroj Mila,sp65610,sp65610@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Mahjabin,</p>  <p>Your account (sp65610) has been created on chip.rs.umbc.edu.<br /> Your primary group is pi_weltyc.<br /> Your home directory has 500M of storage.<br /> Please read through the documentation found at hpcf.umbc.edu &gt; User Support.<br /> All available modules can be viewed using the command &#39;module avail&#39;.<br /> Please submit additional questions or issues as separate tickets via the following link.<br /> (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/)</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Tue Oct 21 13:22:34 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Mahjabin Afroj Last Name:                 Mila Email:                     sp65610@umbc.edu Campus ID:                 SP65610  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    weltyc@umbc.edu Existing Group:       pi_weltyc Project Title:        Coupled Groundwater&ndash;Surface Water Modeling of Baltimore Using ParFlow.CLM Project Abstract:     This study develops high-resolution ParFlow.CLM models for urban watersheds in Baltimore, Maryland, to investigate the interactions between groundwater and surface water under recent climatic conditions. The model setups include: acquisition and processing of digital elevation data; slope generation using GRASS GIS; an overland flow test to ensure that the domains drain completely; acquisition and reclassification of land cover; generation of hydrogeologic layers with variable permeability; and preparation of NLDAS-2 meteorological forcing data. The models are being used for three objectives: (1) to evaluate the potential to predict whether simulated groundwater levels can predict basement flooding during storms, (2) to apply backward particle tracking with ECOSLIM to assess the correlation between stream water chemistry and land use/land cover, and (3) to compare water quantity results (prediction of stream discharge, aquifer levels) to other hydrologic models (HEC-RAS, SWMM, CityCat) being applied to the watersheds. This implementation provides a basis to evaluate hydrologic model robustness and improve understanding of hydrology in urban settings.   N/A  </pre> </blockquote> </div> "
3296766,72404830,Create,DoIT-Research-Computing,2025-10-21 18:18:03.0000000,HPC Other Issue: Ollama running unexpectedly slow,stalled,Beamlak Bekele,bbekele1,Roy Prouty,proutyr1,proutyr1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Roy Last Name:                 Prouty Email:                     proutyr1@umbc.edu Campus ID:                 WH39335  Request Type:              High Performance Cluster  User Mostafa Cham reports following ollama setup according to:  https://umbc.atlassian.net/wiki/spaces/faq/pages/1408335873/Running+an+LLM+using+Ollama+on+chip  User reports that running LLMs via ollama is running unexpectedly slow. Please follow-up with user to replicate the issue and perhaps set a meeting to understand the issue and to resolve.  "
3296766,72408022,Correspond,DoIT-Research-Computing,2025-10-21 19:20:39.0000000,HPC Other Issue: Ollama running unexpectedly slow,stalled,Beamlak Bekele,bbekele1,Roy Prouty,proutyr1,proutyr1@umbc.edu,Beamlak Bekele,bbekele1@umbc.edu,"<div> <p>Hi Mostafa</p>  <p>I need some more information about your LLM job.&nbsp;</p>  <p>Can you please provide your slurm script, and working directory? Are you working with a data, if so how big is it and where is it located?&nbsp;</p>  <p>How much time do you expect the job to take ?&nbsp;&nbsp;</p>  <p>On Tue Oct 21 14:18:03 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Roy Last Name:                 Prouty Email:                     proutyr1@umbc.edu Campus ID:                 WH39335  Request Type:              High Performance Cluster  User Mostafa Cham reports following ollama setup according to:  https://umbc.atlassian.net/wiki/spaces/faq/pages/1408335873/Running+an+LLM+using+Ollama+on+chip  User reports that running LLMs via ollama is running unexpectedly slow. Please follow-up with user to replicate the issue and perhaps set a meeting to understand the issue and to resolve.  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> "
3296766,72434050,Correspond,DoIT-Research-Computing,2025-10-22 17:44:19.0000000,HPC Other Issue: Ollama running unexpectedly slow,stalled,Beamlak Bekele,bbekele1,Roy Prouty,proutyr1,proutyr1@umbc.edu,Mostafa Cham,mcham2@umbc.edu,"<p>Hi Beamlak,</p>  <p>I followed the instructions on the HPCF wiki page (https://umbc.atlassian.net/wiki/spaces/faq/pages/1408335873/Running+an+LLM+using+Ollama+on+chip) and simply ran the example on that page. It took more than 30minutes to generate only 2 words. I used an interactive job to run the OLLAMA. I even tried to use the GPU partition for pi_jianwu. It was a little faster but still super slow. I share the ollama_server.log file in the attachment for your reference. Thank you in advance for your help.</p>  <p>&nbsp;</p>  <p>Best regards,</p>  <p>Mostafa</p>  <p>&nbsp;</p>  <p>On Tue Oct 21 15:20:39 2025, PG41777 wrote:</p>  <blockquote> <div> <p>Hi Mostafa</p>  <p>I need some more information about your LLM job.&nbsp;</p>  <p>Can you please provide your slurm script, and working directory? Are you working with a data, if so how big is it and where is it located?&nbsp;</p>  <p>How much time do you expect the job to take ?&nbsp;&nbsp;</p>  <p>On Tue Oct 21 14:18:03 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Roy Last Name:                 Prouty Email:                     proutyr1@umbc.edu Campus ID:                 WH39335  Request Type:              High Performance Cluster  User Mostafa Cham reports following ollama setup according to:  https://umbc.atlassian.net/wiki/spaces/faq/pages/1408335873/Running+an+LLM+using+Ollama+on+chip  User reports that running LLMs via ollama is running unexpectedly slow. Please follow-up with user to replicate the issue and perhaps set a meeting to understand the issue and to resolve.  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> </blockquote> "
3296766,72462863,Correspond,DoIT-Research-Computing,2025-10-23 18:21:16.0000000,HPC Other Issue: Ollama running unexpectedly slow,stalled,Beamlak Bekele,bbekele1,Roy Prouty,proutyr1,proutyr1@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Mostafa,&nbsp;</p>  <p>There were some issues with flags that were automatically generating incorrect slurm variables, which may have been causing the slowness. Please try it again and let us know if it seems faster.</p>  <p>On Wed Oct 22 13:44:19 2025, QT51576 wrote:</p>  <blockquote> <p>Hi Beamlak,</p>  <p>I followed the instructions on the HPCF wiki page (https://umbc.atlassian.net/wiki/spaces/faq/pages/1408335873/Running+an+LLM+using+Ollama+on+chip) and simply ran the example on that page. It took more than 30minutes to generate only 2 words. I used an interactive job to run the OLLAMA. I even tried to use the GPU partition for pi_jianwu. It was a little faster but still super slow. I share the ollama_server.log file in the attachment for your reference. Thank you in advance for your help.</p>  <p>&nbsp;</p>  <p>Best regards,</p>  <p>Mostafa</p>  <p>&nbsp;</p>  <p>On Tue Oct 21 15:20:39 2025, PG41777 wrote:</p>  <blockquote> <div> <p>Hi Mostafa</p>  <p>I need some more information about your LLM job.&nbsp;</p>  <p>Can you please provide your slurm script, and working directory? Are you working with a data, if so how big is it and where is it located?&nbsp;</p>  <p>How much time do you expect the job to take ?&nbsp;&nbsp;</p>  <p>On Tue Oct 21 14:18:03 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Roy Last Name:                 Prouty Email:                     proutyr1@umbc.edu Campus ID:                 WH39335  Request Type:              High Performance Cluster  User Mostafa Cham reports following ollama setup according to:  https://umbc.atlassian.net/wiki/spaces/faq/pages/1408335873/Running+an+LLM+using+Ollama+on+chip  User reports that running LLMs via ollama is running unexpectedly slow. Please follow-up with user to replicate the issue and perhaps set a meeting to understand the issue and to resolve.  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> </blockquote> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3296910,72409997,Create,DoIT-Research-Computing,2025-10-21 19:59:49.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Mohammad Last Name:                 Ebrahimabadi Email:                     e127@umbc.edu Campus ID:                 NQ23652  Request Type:              High Performance Cluster  Dear Team,  I hope you=E2=80=99re doing well. I=E2=80=99m a Ph.D. student in Computer Engineering and have a question reg= arding running a software on the HPCF. On the department server, I usually = run software using shell scripts located at /umbc/software/scripts/ which i= s provided by the IT group pf the department (Geoff Weiss Team). However, i= n case of HPCF, it seems that this path is not accessible from the HPCF env= ironment.  Could you please let me know if there is any specific configuration or acce= ss permission required to reach this directory through the HPCF? or direct = me to a related person that can help me.  Best regards, Mohammad  "
3296910,72432497,Correspond,DoIT-Research-Computing,2025-10-22 17:15:12.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Tartela Tabassum,tartelt1@umbc.edu,"<p>Hello,</p>  <p>Thanks for reaching out. Could you clarify which specific <strong>machine or server</strong> you&rsquo;re referring to when you mention the &ldquo;departmental server&rdquo;?</p>  <p>The <code>/umbc/software/scripts/</code> directory isn&rsquo;t accessible from HPCF (e.g., Chip) by default &mdash; those systems have separate storage and don&rsquo;t automatically mount departmental shares.</p>  <p>Could you check where <code>/umbc/software</code> is mounted on your departmental system (e.g., by running <code>df -h /umbc/software</code> or <code>mount | grep /umbc/software</code>)? That&rsquo;ll tell us which server hosts it. Once we know that, we can confirm whether it&rsquo;s possible or appropriate to make it visible from HPCF.</p>  <p>Best,</p>  <p>Tartela</p>  <p>&nbsp;</p>  <p>On Tue Oct 21 15:59:49 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Mohammad Last Name:                 Ebrahimabadi Email:                     e127@umbc.edu Campus ID:                 NQ23652  Request Type:              High Performance Cluster  Dear Team,  I hope you&rsquo;re doing well. I&rsquo;m a Ph.D. student in Computer Engineering and have a question regarding running a software on the HPCF. On the department server, I usually run software using shell scripts located at /umbc/software/scripts/ which is provided by the IT group pf the department (Geoff Weiss Team). However, in case of HPCF, it seems that this path is not accessible from the HPCF environment.  Could you please let me know if there is any specific configuration or access permission required to reach this directory through the HPCF? or direct me to a related person that can help me.  Best regards, Mohammad  </pre> </blockquote> "
3296910,72433180,Correspond,DoIT-Research-Computing,2025-10-22 17:26:05.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Mohammad Ebrahimabadi,e127@umbc.edu,"<div> <p>Thanks Tartela for your help to resolve my issue!</p>  <p>Actually it seems all server in CSEE department have access to that path. When I ran the command that you gave me I got the following report:</p>  <p>mohammad@alborz:~$ df -h /umbc/software<br /> Filesystem &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Size &nbsp;Used Avail Use% Mounted on<br /> nfs.iss.rs.umbc.edu:/ifs/data/csee/software &nbsp;2.5T &nbsp;2.2T &nbsp;326G &nbsp;88% /umbc/software</p>  <p>&nbsp;</p>  <p>mohammad@alborz:~$ mount | grep /umbc/software<br /> nfs.iss.rs.umbc.edu:/ifs/data/csee/software on /umbc/software type nfs (rw,relatime,vers=3,rsize=131072,wsize=524288,namlen=255,acregmin=15,acregmax=15,acdirmin=15,acdirmax=15,hard,noacl,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=10.2.44.60,mountvers=3,mountport=300,mountproto=tcp,local_lock=none,addr=10.2.44.60)<br /> &nbsp;</p>  <p>Please let me know if you need other information.</p>  <p>&nbsp;</p>  <p>Regards,</p>  <p>Mohammad</p>  <p>&nbsp;</p>  <p>&nbsp;</p>  <p>&nbsp;</p>  <p>&nbsp;</p>  <p>&nbsp;</p>  <p>&nbsp;</p>  <p>On Wed Oct 22 13:15:12 2025, HK41259 wrote:</p>  <blockquote> <p>Hello,</p>  <p>Thanks for reaching out. Could you clarify which specific <strong>machine or server</strong> you&rsquo;re referring to when you mention the &ldquo;departmental server&rdquo;?</p>  <p>The <code>/umbc/software/scripts/</code> directory isn&rsquo;t accessible from HPCF (e.g., Chip) by default &mdash; those systems have separate storage and don&rsquo;t automatically mount departmental shares.</p>  <p>Could you check where <code>/umbc/software</code> is mounted on your departmental system (e.g., by running <code>df -h /umbc/software</code> or <code>mount | grep /umbc/software</code>)? That&rsquo;ll tell us which server hosts it. Once we know that, we can confirm whether it&rsquo;s possible or appropriate to make it visible from HPCF.</p>  <p>Best,</p>  <p>Tartela</p>  <p>&nbsp;</p>  <p>On Tue Oct 21 15:59:49 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Mohammad Last Name:                 Ebrahimabadi Email:                     e127@umbc.edu Campus ID:                 NQ23652  Request Type:              High Performance Cluster  Dear Team,  I hope you&rsquo;re doing well. I&rsquo;m a Ph.D. student in Computer Engineering and have a question regarding running a software on the HPCF. On the department server, I usually run software using shell scripts located at /umbc/software/scripts/ which is provided by the IT group pf the department (Geoff Weiss Team). However, in case of HPCF, it seems that this path is not accessible from the HPCF environment.  Could you please let me know if there is any specific configuration or access permission required to reach this directory through the HPCF? or direct me to a related person that can help me.  Best regards, Mohammad  </pre> </blockquote> </blockquote> </div> "
3296910,72434774,Correspond,DoIT-Research-Computing,2025-10-22 18:01:32.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi&nbsp;Mohammad,</p>  <p>We can get the software mounted for you on chip, but it will have to be read-only as we won&#39;t be able to manage who can make changes to it otherwise.&nbsp;</p>  <p>As a reminder, the machines on that run on chip are not the same as those run by the CSEE department, so we can&#39;t guarantee that these will run the same way or as effectively.&nbsp;</p>  <p>If that&#39;s all ok with you, please give me some time to get the directory mounted.&nbsp;</p>  <p>On Wed Oct 22 13:26:05 2025, NQ23652 wrote:</p>  <blockquote> <div> <p>Thanks Tartela for your help to resolve my issue!</p>  <p>Actually it seems all server in CSEE department have access to that path. When I ran the command that you gave me I got the following report:</p>  <p>mohammad@alborz:~$ df -h /umbc/software<br /> Filesystem &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Size &nbsp;Used Avail Use% Mounted on<br /> nfs.iss.rs.umbc.edu:/ifs/data/csee/software &nbsp;2.5T &nbsp;2.2T &nbsp;326G &nbsp;88% /umbc/software</p>  <p>&nbsp;</p>  <p>mohammad@alborz:~$ mount | grep /umbc/software<br /> nfs.iss.rs.umbc.edu:/ifs/data/csee/software on /umbc/software type nfs (rw,relatime,vers=3,rsize=131072,wsize=524288,namlen=255,acregmin=15,acregmax=15,acdirmin=15,acdirmax=15,hard,noacl,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=10.2.44.60,mountvers=3,mountport=300,mountproto=tcp,local_lock=none,addr=10.2.44.60)<br /> &nbsp;</p>  <p>Please let me know if you need other information.</p>  <p>&nbsp;</p>  <p>Regards,</p>  <p>Mohammad</p>  <p>&nbsp;</p>  <p>&nbsp;</p>  <p>&nbsp;</p>  <p>&nbsp;</p>  <p>&nbsp;</p>  <p>&nbsp;</p>  <p>On Wed Oct 22 13:15:12 2025, HK41259 wrote:</p>  <blockquote> <p>Hello,</p>  <p>Thanks for reaching out. Could you clarify which specific <strong>machine or server</strong> you&rsquo;re referring to when you mention the &ldquo;departmental server&rdquo;?</p>  <p>The <code>/umbc/software/scripts/</code> directory isn&rsquo;t accessible from HPCF (e.g., Chip) by default &mdash; those systems have separate storage and don&rsquo;t automatically mount departmental shares.</p>  <p>Could you check where <code>/umbc/software</code> is mounted on your departmental system (e.g., by running <code>df -h /umbc/software</code> or <code>mount | grep /umbc/software</code>)? That&rsquo;ll tell us which server hosts it. Once we know that, we can confirm whether it&rsquo;s possible or appropriate to make it visible from HPCF.</p>  <p>Best,</p>  <p>Tartela</p>  <p>&nbsp;</p>  <p>On Tue Oct 21 15:59:49 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Mohammad Last Name:                 Ebrahimabadi Email:                     e127@umbc.edu Campus ID:                 NQ23652  Request Type:              High Performance Cluster  Dear Team,  I hope you&rsquo;re doing well. I&rsquo;m a Ph.D. student in Computer Engineering and have a question regarding running a software on the HPCF. On the department server, I usually run software using shell scripts located at /umbc/software/scripts/ which is provided by the IT group pf the department (Geoff Weiss Team). However, in case of HPCF, it seems that this path is not accessible from the HPCF environment.  Could you please let me know if there is any specific configuration or access permission required to reach this directory through the HPCF? or direct me to a related person that can help me.  Best regards, Mohammad  </pre> </blockquote> </blockquote> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3296910,72435117,Correspond,DoIT-Research-Computing,2025-10-22 18:06:55.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Mohammad Ebrahimabadi,e127@umbc.edu,"<div> <p>Hi Max,</p>  <p>Thanks for your help! Let&rsquo;s proceed based on your suggestion, hopefully it works as expected.</p>  <p>I just have one quick question: since the IT team in the department can modify or add new shell scripts in that directory, will we also have access to those updated or newly added scripts from the mounted directory on HPCF?</p>  <p>Thanks again,<br /> Mohammad</p>  <p>&nbsp;</p>  <p>On Wed Oct 22 14:01:32 2025, OL73413 wrote:</p>  <blockquote> <div> <p>Hi&nbsp;Mohammad,</p>  <p>We can get the software mounted for you on chip, but it will have to be read-only as we won&#39;t be able to manage who can make changes to it otherwise.&nbsp;</p>  <p>As a reminder, the machines on that run on chip are not the same as those run by the CSEE department, so we can&#39;t guarantee that these will run the same way or as effectively.&nbsp;</p>  <p>If that&#39;s all ok with you, please give me some time to get the directory mounted.&nbsp;</p>  <p>On Wed Oct 22 13:26:05 2025, NQ23652 wrote:</p>  <blockquote> <div> <p>Thanks Tartela for your help to resolve my issue!</p>  <p>Actually it seems all server in CSEE department have access to that path. When I ran the command that you gave me I got the following report:</p>  <p>mohammad@alborz:~$ df -h /umbc/software<br /> Filesystem &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Size &nbsp;Used Avail Use% Mounted on<br /> nfs.iss.rs.umbc.edu:/ifs/data/csee/software &nbsp;2.5T &nbsp;2.2T &nbsp;326G &nbsp;88% /umbc/software</p>  <p>&nbsp;</p>  <p>mohammad@alborz:~$ mount | grep /umbc/software<br /> nfs.iss.rs.umbc.edu:/ifs/data/csee/software on /umbc/software type nfs (rw,relatime,vers=3,rsize=131072,wsize=524288,namlen=255,acregmin=15,acregmax=15,acdirmin=15,acdirmax=15,hard,noacl,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=10.2.44.60,mountvers=3,mountport=300,mountproto=tcp,local_lock=none,addr=10.2.44.60)<br /> &nbsp;</p>  <p>Please let me know if you need other information.</p>  <p>&nbsp;</p>  <p>Regards,</p>  <p>Mohammad</p>  <p>&nbsp;</p>  <p>&nbsp;</p>  <p>&nbsp;</p>  <p>&nbsp;</p>  <p>&nbsp;</p>  <p>&nbsp;</p>  <p>On Wed Oct 22 13:15:12 2025, HK41259 wrote:</p>  <blockquote> <p>Hello,</p>  <p>Thanks for reaching out. Could you clarify which specific <strong>machine or server</strong> you&rsquo;re referring to when you mention the &ldquo;departmental server&rdquo;?</p>  <p>The <code>/umbc/software/scripts/</code> directory isn&rsquo;t accessible from HPCF (e.g., Chip) by default &mdash; those systems have separate storage and don&rsquo;t automatically mount departmental shares.</p>  <p>Could you check where <code>/umbc/software</code> is mounted on your departmental system (e.g., by running <code>df -h /umbc/software</code> or <code>mount | grep /umbc/software</code>)? That&rsquo;ll tell us which server hosts it. Once we know that, we can confirm whether it&rsquo;s possible or appropriate to make it visible from HPCF.</p>  <p>Best,</p>  <p>Tartela</p>  <p>&nbsp;</p>  <p>On Tue Oct 21 15:59:49 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Mohammad Last Name:                 Ebrahimabadi Email:                     e127@umbc.edu Campus ID:                 NQ23652  Request Type:              High Performance Cluster  Dear Team,  I hope you&rsquo;re doing well. I&rsquo;m a Ph.D. student in Computer Engineering and have a question regarding running a software on the HPCF. On the department server, I usually run software using shell scripts located at /umbc/software/scripts/ which is provided by the IT group pf the department (Geoff Weiss Team). However, in case of HPCF, it seems that this path is not accessible from the HPCF environment.  Could you please let me know if there is any specific configuration or access permission required to reach this directory through the HPCF? or direct me to a related person that can help me.  Best regards, Mohammad  </pre> </blockquote> </blockquote> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> </blockquote> </div> "
3296910,72441219,Correspond,DoIT-Research-Computing,2025-10-22 20:14:51.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,NULL,naghmeh.karimi@umbc.edu,"Hi Max, Roy, Skye,  Thank you all for your help.  I added Skye here as she is our IT exert on adding the capability of running synopsis tools (Hspice) in servers. Skye we want to use the HPC cluster to run hspice an other synopsys tools. Could you please help on this?  Thanks, Naghmeh  On Wed, Oct 22, 2025 at 2:06=E2=80=AFPM Mohammad Ebrahimabadi via RT < UMBCHelp@rt.umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 > > > Last Update From Ticket: > > Hi Max, > > Thanks for your help! Let=E2=80=99s proceed based on your suggestion, hop= efully it > works as expected. > > I just have one quick question: since the IT team in the department can > modify > or add new shell scripts in that directory, will we also have access to > those > updated or newly added scripts from the mounted directory on HPCF? > > Thanks again, > Mohammad > > On Wed Oct 22 14:01:32 2025, OL73413 wrote: > > > Hi Mohammad, > > > We can get the software mounted for you on chip, but it will have to be > > read-only as we won't be able to manage who can make changes to it > > otherwise. > > > As a reminder, the machines on that run on chip are not the same as tho= se > > run by the CSEE department, so we can't guarantee that these will run t= he > > same way or as effectively. > > > If that's all ok with you, please give me some time to get the directory > > mounted. > > > On Wed Oct 22 13:26:05 2025, NQ23652 wrote: > > >> Thanks Tartela for your help to resolve my issue! > > >> Actually it seems all server in CSEE department have access to that > >> path. When I ran the command that you gave me I got the following > >> report: > > >> mohammad@alborz:~$ df -h /umbc/software > >> Filesystem Size Used Avail Use% Mounted on > >> nfs.iss.rs.umbc.edu:/ifs/data/csee/software 2.5T 2.2T 326G 88% > >> /umbc/software > > >> mohammad@alborz:~$ mount | grep /umbc/software > >> nfs.iss.rs.umbc.edu:/ifs/data/csee/software on /umbc/software type nfs > >> > (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namlen=3D255,acregmin= =3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,proto=3Dtcp,time= o=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountvers=3D3,mountpor= t=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.60) > > >> Please let me know if you need other information. > > >> Regards, > > >> Mohammad > > >> On Wed Oct 22 13:15:12 2025, HK41259 wrote: > > >>> Hello, > > >>> Thanks for reaching out. Could you clarify which specific machine > >>> or server you=E2=80=99re referring to when you mention the =E2=80=9Cd= epartmental > >>> server=E2=80=9D? > > >>> The /umbc/software/scripts/ directory isn=E2=80=99t accessible from H= PCF > >>> (e.g., Chip) by default =E2=80=94 those systems have separate storage=  and > >>> don=E2=80=99t automatically mount departmental shares. > > >>> Could you check where /umbc/software is mounted on your > >>> departmental system (e.g., by running df -h /umbc/software or mount > >>> | grep /umbc/software)? That=E2=80=99ll tell us which server hosts it= . Once > >>> we know that, we can confirm whether it=E2=80=99s possible or appropr= iate > >>> to make it visible from HPCF. > > >>> Best, > > >>> Tartela > > >>> On Tue Oct 21 15:59:49 2025, ZZ99999 wrote: > > >>>> First Name:                Mohammad > >>>> Last Name:                 Ebrahimabadi > >>>> Email:                     e127@umbc.edu > >>>> Campus ID:                 NQ23652 > >>>> > >>>> Request Type:              High Performance Cluster > >>>> > >>>> Dear Team, > >>>> > >>>> I hope you=E2=80=99re doing well. > >>>> I=E2=80=99m a Ph.D. student in Computer Engineering and have a quest= ion > regarding running a software on the HPCF. On the department server, I > usually run software using shell scripts located at /umbc/software/script= s/ > which is provided by the IT group pf the department (Geoff Weiss Team). > However, in case of HPCF, it seems that this path is not accessible from > the HPCF environment. > >>>> > >>>> Could you please let me know if there is any specific configuration > or access permission required to reach this directory through the HPCF? or > direct me to a related person that can help me. > >>>> > >>>> Best regards, > >>>> Mohammad > >>> > > > -- > > > Best, > > Max Breitmeyer > > DOIT HPC System Administrator > > >  --=20 Naghmeh Karimi, Ph.D. Associate Professor Department of Computer Science and Electrical Engineering University of Maryland, Baltimore County Baltimore, MD 21250 Tel: *410-455-3965* E-mail: *nkarimi@umbc.edu <nkarimi@umbc.edu>* Web: *http://www.csee.umbc.edu/~nkarimi/ <http://www.csee.umbc.edu/~nkarimi/>* "
3296910,72441219,Correspond,DoIT-Research-Computing,2025-10-22 20:14:51.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,NULL,naghmeh.karimi@umbc.edu,"<div dir=3D""ltr"">Hi Max, Roy, Skye,<div><br></div><div>Thank you all for yo= ur help.</div><div><br></div><div>I added Skye here as she is our IT exert = on adding the=C2=A0capability of running=C2=A0synopsis=C2=A0tools (Hspice) = in servers.</div><div>Skye we want to use the HPC cluster to run hspice an = other synopsys tools. Could you please help on this?</div><div><br></div><d= iv>Thanks,</div><div>Naghmeh</div></div><br><div class=3D""gmail_quote gmail= _quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Wed, Oct 22, 202= 5 at 2:06=E2=80=AFPM Mohammad Ebrahimabadi via RT &lt;<a href=3D""mailto:UMB= CHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote=  class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px so= lid rgb(204,204,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://r= t.umbc.edu/Ticket/Display.html?id=3D3296910"" rel=3D""noreferrer"" target=3D""_= blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3296910</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Max,<br> <br> Thanks for your help! Let=E2=80=99s proceed based on your suggestion, hopef= ully it<br> works as expected.<br> <br> I just have one quick question: since the IT team in the department can mod= ify<br> or add new shell scripts in that directory, will we also have access to tho= se<br> updated or newly added scripts from the mounted directory on HPCF?<br> <br> Thanks again,<br> Mohammad<br> <br> On Wed Oct 22 14:01:32 2025, OL73413 wrote:<br> <br> &gt; Hi Mohammad,<br> <br> &gt; We can get the software mounted for you on chip, but it will have to b= e<br> &gt; read-only as we won&#39;t be able to manage who can make changes to it= <br> &gt; otherwise.<br> <br> &gt; As a reminder, the machines on that run on chip are not the same as th= ose<br> &gt; run by the CSEE department, so we can&#39;t guarantee that these will = run the<br> &gt; same way or as effectively.<br> <br> &gt; If that&#39;s all ok with you, please give me some time to get the dir= ectory<br> &gt; mounted.<br> <br> &gt; On Wed Oct 22 13:26:05 2025, NQ23652 wrote:<br> <br> &gt;&gt; Thanks Tartela for your help to resolve my issue!<br> <br> &gt;&gt; Actually it seems all server in CSEE department have access to tha= t<br> &gt;&gt; path. When I ran the command that you gave me I got the following<= br> &gt;&gt; report:<br> <br> &gt;&gt; mohammad@alborz:~$ df -h /umbc/software<br> &gt;&gt; Filesystem Size Used Avail Use% Mounted on<br> &gt;&gt; nfs.iss.rs.umbc.edu:/ifs/data/csee/software 2.5T 2.2T 326G 88%<br> &gt;&gt; /umbc/software<br> <br> &gt;&gt; mohammad@alborz:~$ mount | grep /umbc/software<br> &gt;&gt; nfs.iss.rs.umbc.edu:/ifs/data/csee/software on /umbc/software type=  nfs<br> &gt;&gt; (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namlen=3D255,a= cregmin=3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,proto=3Dt= cp,timeo=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountvers=3D3,m= ountport=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.60)<br> <br> &gt;&gt; Please let me know if you need other information.<br> <br> &gt;&gt; Regards,<br> <br> &gt;&gt; Mohammad<br> <br> &gt;&gt; On Wed Oct 22 13:15:12 2025, HK41259 wrote:<br> <br> &gt;&gt;&gt; Hello,<br> <br> &gt;&gt;&gt; Thanks for reaching out. Could you clarify which specific mach= ine<br> &gt;&gt;&gt; or server you=E2=80=99re referring to when you mention the =E2= =80=9Cdepartmental<br> &gt;&gt;&gt; server=E2=80=9D?<br> <br> &gt;&gt;&gt; The /umbc/software/scripts/ directory isn=E2=80=99t accessible=  from HPCF<br> &gt;&gt;&gt; (e.g., Chip) by default =E2=80=94 those systems have separate = storage and<br> &gt;&gt;&gt; don=E2=80=99t automatically mount departmental shares.<br> <br> &gt;&gt;&gt; Could you check where /umbc/software is mounted on your<br> &gt;&gt;&gt; departmental system (e.g., by running df -h /umbc/software or = mount<br> &gt;&gt;&gt; | grep /umbc/software)? That=E2=80=99ll tell us which server h= osts it. Once<br> &gt;&gt;&gt; we know that, we can confirm whether it=E2=80=99s possible or = appropriate<br> &gt;&gt;&gt; to make it visible from HPCF.<br> <br> &gt;&gt;&gt; Best,<br> <br> &gt;&gt;&gt; Tartela<br> <br> &gt;&gt;&gt; On Tue Oct 21 15:59:49 2025, ZZ99999 wrote:<br> <br> &gt;&gt;&gt;&gt; First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 Mohammad<br> &gt;&gt;&gt;&gt; Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=  =C2=A0 =C2=A0Ebrahimabadi<br> &gt;&gt;&gt;&gt; Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 =C2=A0 =C2=A0<a href=3D""mailto:e127@umbc.edu"" target=3D""_blank"">= e127@umbc.edu</a><br> &gt;&gt;&gt;&gt; Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=  =C2=A0 =C2=A0NQ23652<br> &gt;&gt;&gt;&gt; <br> &gt;&gt;&gt;&gt; Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 High Performance Cluster<br> &gt;&gt;&gt;&gt; <br> &gt;&gt;&gt;&gt; Dear Team,<br> &gt;&gt;&gt;&gt; <br> &gt;&gt;&gt;&gt; I hope you=E2=80=99re doing well.<br> &gt;&gt;&gt;&gt; I=E2=80=99m a Ph.D. student in Computer Engineering and ha= ve a question regarding running a software on the HPCF. On the department s= erver, I usually run software using shell scripts located at /umbc/software= /scripts/ which is provided by the IT group pf the department (Geoff Weiss = Team). However, in case of HPCF, it seems that this path is not accessible = from the HPCF environment.<br> &gt;&gt;&gt;&gt; <br> &gt;&gt;&gt;&gt; Could you please let me know if there is any specific conf= iguration or access permission required to reach this directory through the=  HPCF? or direct me to a related person that can help me.<br> &gt;&gt;&gt;&gt; <br> &gt;&gt;&gt;&gt; Best regards,<br> &gt;&gt;&gt;&gt; Mohammad<br> &gt;&gt;&gt; <br> <br> &gt; --<br> <br> &gt; Best,<br> &gt; Max Breitmeyer<br> &gt; DOIT HPC System Administrator<br> <br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><span style=3D""font-size:9.5pt;line-height:115%;= font-family:Arial,sans-serif;color:rgb(136,136,136);background-image:initia= l;background-position:initial;background-repeat:initial"">Naghmeh=C2=A0Karim= i, Ph.D.<br> Associate Professor<br> Department of Computer Science and Electrical Engineering<br> University of Maryland, Baltimore County<br> Baltimore, MD 21250<br> Tel:</span><span style=3D""font-size:11pt;line-height:115%;font-family:Calib= ri,&quot;sans-serif&quot;;color:black"">=C2=A0</span><u><span style=3D""font-= size:11pt;line-height:115%;font-family:Calibri,&quot;sans-serif&quot;;color= :rgb(17,85,204)"">410-455-3965</span></u><span style=3D""font-size:11pt;line-= height:115%;font-family:Calibri,&quot;sans-serif&quot;;color:black""> </span= ><span style=3D""font-size:9.5pt;line-height:115%;font-family:Arial,sans-ser= if;color:rgb(136,136,136);background-image:initial;background-position:init= ial;background-repeat:initial"">E-mail:</span><span style=3D""font-size:11pt;= line-height:115%;font-family:Calibri,&quot;sans-serif&quot;;color:black"">= =C2=A0</span><u><span style=3D""font-size:11pt;line-height:115%;font-family:= Calibri,&quot;sans-serif&quot;;color:rgb(17,85,204)""><a href=3D""mailto:nkar= imi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a></span></u><span style= =3D""font-size:9.5pt;line-height:115%;font-family:Arial,sans-serif;color:rgb= (136,136,136);background-image:initial;background-position:initial;backgrou= nd-repeat:initial""><br> Web:</span><span style=3D""font-size:9.5pt;line-height:115%;font-family:Aria= l,sans-serif;color:black;background-image:initial;background-position:initi= al;background-repeat:initial"">=C2=A0</span><u><span style=3D""font-size:11pt= ;line-height:115%;font-family:Calibri,sans-serif;color:rgb(17,85,204);backg= round-image:initial;background-position:initial;background-repeat:initial"">= <a href=3D""http://www.csee.umbc.edu/~nkarimi/"" target=3D""_blank"">http://www= .csee.umbc.edu/~nkarimi/</a></span></u></div></div> "
3296910,72441430,Correspond,DoIT-Research-Computing,2025-10-22 20:22:04.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Skye Jonke,ii69854@umbc.edu,"<html aria-label=3D""message body""><head><meta http-equiv=3D""content-type"" c= ontent=3D""text/html; charset=3Dutf-8""></head><body style=3D""overflow-wrap: = break-word; -webkit-nbsp-mode: space; line-break: after-white-space;"">Moham= mad,&nbsp;<div><br></div><div>So long as you continue to access them throug= h /umbc/software/scripts, they should remain up to date and you will see an= y new scripts. If you copy them to a local directory on the HPC cluster, th= ey will not be updated.&nbsp;</div><div><br id=3D""lineBreakAtBeginningOfMes= sage""><div> <meta charset=3D""UTF-8""><div dir=3D""auto"" style=3D""caret-color: rgb(0, 0, 0= ); color: rgb(0, 0, 0); letter-spacing: normal; text-align: start; text-ind= ent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; -we= bkit-text-stroke-width: 0px; text-decoration: none; overflow-wrap: break-wo= rd; -webkit-nbsp-mode: space; line-break: after-white-space;"">---</div><div=  dir=3D""auto"" style=3D""caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0); lett= er-spacing: normal; text-align: start; text-indent: 0px; text-transform: no= ne; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px;=  text-decoration: none; overflow-wrap: break-word; -webkit-nbsp-mode: space= ; line-break: after-white-space;"">Skye Jonke<br><br>she/her<br><br>Speciali= st, Linux System Administrator &amp;<span class=3D""Apple-converted-space"">&= nbsp;</span>Lab Technical Support<br></div> </div> <div><br><blockquote type=3D""cite""><div>On Oct 22, 2025, at 16:14, Naghmeh = Karimi &lt;nkarimi@umbc.edu&gt; wrote:</div><br class=3D""Apple-interchange-= newline""><div><div dir=3D""ltr"">Hi Max, Roy, Skye,<div><br></div><div>Thank = you all for your help.</div><div><br></div><div>I added Skye here as she is=  our IT exert on adding the&nbsp;capability of running&nbsp;synopsis&nbsp;t= ools (Hspice) in servers.</div><div>Skye we want to use the HPC cluster to = run hspice an other synopsys tools. Could you please help on this?</div><di= v><br></div><div>Thanks,</div><div>Naghmeh</div></div><br><div class=3D""gma= il_quote gmail_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On We= d, Oct 22, 2025 at 2:06=E2=80=AFPM Mohammad Ebrahimabadi via RT &lt;<a href= =3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></d= iv><blockquote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;bord= er-left:1px solid rgb(204,204,204);padding-left:1ex"">Ticket &lt;URL: <a hre= f=3D""https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html= ?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761768891000000&amp;usg=3D= AOvVaw2jxB3nArHBD7-ij9ZN8ZsW"" rel=3D""noreferrer"" target=3D""_blank"">https://= rt.umbc.edu/Ticket/Display.html?id=3D3296910</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Max,<br> <br> Thanks for your help! Let=E2=80=99s proceed based on your suggestion, hopef= ully it<br> works as expected.<br> <br> I just have one quick question: since the IT team in the department can mod= ify<br> or add new shell scripts in that directory, will we also have access to tho= se<br> updated or newly added scripts from the mounted directory on HPCF?<br> <br> Thanks again,<br> Mohammad<br> <br> On Wed Oct 22 14:01:32 2025, OL73413 wrote:<br> <br> &gt; Hi Mohammad,<br> <br> &gt; We can get the software mounted for you on chip, but it will have to b= e<br> &gt; read-only as we won't be able to manage who can make changes to it<br> &gt; otherwise.<br> <br> &gt; As a reminder, the machines on that run on chip are not the same as th= ose<br> &gt; run by the CSEE department, so we can't guarantee that these will run = the<br> &gt; same way or as effectively.<br> <br> &gt; If that's all ok with you, please give me some time to get the directo= ry<br> &gt; mounted.<br> <br> &gt; On Wed Oct 22 13:26:05 2025, NQ23652 wrote:<br> <br> &gt;&gt; Thanks Tartela for your help to resolve my issue!<br> <br> &gt;&gt; Actually it seems all server in CSEE department have access to tha= t<br> &gt;&gt; path. When I ran the command that you gave me I got the following<= br> &gt;&gt; report:<br> <br> &gt;&gt; mohammad@alborz:~$ df -h /umbc/software<br> &gt;&gt; Filesystem Size Used Avail Use% Mounted on<br> &gt;&gt; nfs.iss.rs.umbc.edu:/ifs/data/csee/software 2.5T 2.2T 326G 88%<br> &gt;&gt; /umbc/software<br> <br> &gt;&gt; mohammad@alborz:~$ mount | grep /umbc/software<br> &gt;&gt; nfs.iss.rs.umbc.edu:/ifs/data/csee/software on /umbc/software type=  nfs<br> &gt;&gt; (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namlen=3D255,a= cregmin=3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,proto=3Dt= cp,timeo=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountvers=3D3,m= ountport=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.60)<br> <br> &gt;&gt; Please let me know if you need other information.<br> <br> &gt;&gt; Regards,<br> <br> &gt;&gt; Mohammad<br> <br> &gt;&gt; On Wed Oct 22 13:15:12 2025, HK41259 wrote:<br> <br> &gt;&gt;&gt; Hello,<br> <br> &gt;&gt;&gt; Thanks for reaching out. Could you clarify which specific mach= ine<br> &gt;&gt;&gt; or server you=E2=80=99re referring to when you mention the =E2= =80=9Cdepartmental<br> &gt;&gt;&gt; server=E2=80=9D?<br> <br> &gt;&gt;&gt; The /umbc/software/scripts/ directory isn=E2=80=99t accessible=  from HPCF<br> &gt;&gt;&gt; (e.g., Chip) by default =E2=80=94 those systems have separate = storage and<br> &gt;&gt;&gt; don=E2=80=99t automatically mount departmental shares.<br> <br> &gt;&gt;&gt; Could you check where /umbc/software is mounted on your<br> &gt;&gt;&gt; departmental system (e.g., by running df -h /umbc/software or = mount<br> &gt;&gt;&gt; | grep /umbc/software)? That=E2=80=99ll tell us which server h= osts it. Once<br> &gt;&gt;&gt; we know that, we can confirm whether it=E2=80=99s possible or = appropriate<br> &gt;&gt;&gt; to make it visible from HPCF.<br> <br> &gt;&gt;&gt; Best,<br> <br> &gt;&gt;&gt; Tartela<br> <br> &gt;&gt;&gt; On Tue Oct 21 15:59:49 2025, ZZ99999 wrote:<br> <br> &gt;&gt;&gt;&gt; First Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp= ; &nbsp; Mohammad<br> &gt;&gt;&gt;&gt; Last Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;=  &nbsp; &nbsp;Ebrahimabadi<br> &gt;&gt;&gt;&gt; Email:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nb= sp; &nbsp; &nbsp; &nbsp;<a href=3D""mailto:e127@umbc.edu"" target=3D""_blank"">= e127@umbc.edu</a><br> &gt;&gt;&gt;&gt; Campus ID:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;=  &nbsp; &nbsp;NQ23652<br> &gt;&gt;&gt;&gt; <br> &gt;&gt;&gt;&gt; Request Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nb= sp; High Performance Cluster<br> &gt;&gt;&gt;&gt; <br> &gt;&gt;&gt;&gt; Dear Team,<br> &gt;&gt;&gt;&gt; <br> &gt;&gt;&gt;&gt; I hope you=E2=80=99re doing well.<br> &gt;&gt;&gt;&gt; I=E2=80=99m a Ph.D. student in Computer Engineering and ha= ve a question regarding running a software on the HPCF. On the department s= erver, I usually run software using shell scripts located at /umbc/software= /scripts/ which is provided by the IT group pf the department (Geoff Weiss = Team). However, in case of HPCF, it seems that this path is not accessible = from the HPCF environment.<br> &gt;&gt;&gt;&gt; <br> &gt;&gt;&gt;&gt; Could you please let me know if there is any specific conf= iguration or access permission required to reach this directory through the=  HPCF? or direct me to a related person that can help me.<br> &gt;&gt;&gt;&gt; <br> &gt;&gt;&gt;&gt; Best regards,<br> &gt;&gt;&gt;&gt; Mohammad<br> &gt;&gt;&gt; <br> <br> &gt; --<br> <br> &gt; Best,<br> &gt; Max Breitmeyer<br> &gt; DOIT HPC System Administrator<br> <br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><span style=3D""font-size:9.5pt;line-height:115%;= font-family:Arial,sans-serif;color:rgb(136,136,136);background-image:initia= l;background-position:initial;background-repeat:initial"">Naghmeh&nbsp;Karim= i, Ph.D.<br> Associate Professor<br> Department of Computer Science and Electrical Engineering<br> University of Maryland, Baltimore County<br> Baltimore, MD 21250<br> Tel:</span><span style=3D""font-size: 11pt; line-height: 115%; font-family: = Calibri, sans-serif;"">&nbsp;</span><u><span style=3D""font-size:11pt;line-he= ight:115%;font-family:Calibri,&quot;sans-serif&quot;;color:rgb(17,85,204)"">= 410-455-3965</span></u><span style=3D""font-size: 11pt; line-height: 115%; f= ont-family: Calibri, sans-serif;""> </span><span style=3D""font-size:9.5pt;li= ne-height:115%;font-family:Arial,sans-serif;color:rgb(136,136,136);backgrou= nd-image:initial;background-position:initial;background-repeat:initial"">E-m= ail:</span><span style=3D""font-size: 11pt; line-height: 115%; font-family: = Calibri, sans-serif;"">&nbsp;</span><u><span style=3D""font-size:11pt;line-he= ight:115%;font-family:Calibri,&quot;sans-serif&quot;;color:rgb(17,85,204)"">= <a href=3D""mailto:nkarimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a><= /span></u><span style=3D""font-size:9.5pt;line-height:115%;font-family:Arial= ,sans-serif;color:rgb(136,136,136);background-image:initial;background-posi= tion:initial;background-repeat:initial""><br> Web:</span><span style=3D""font-size: 9.5pt; line-height: 115%; font-family:=  Arial, sans-serif; background-image: initial; background-position: initial= ; background-repeat: initial;"">&nbsp;</span><u><span style=3D""font-size:11p= t;line-height:115%;font-family:Calibri,sans-serif;color:rgb(17,85,204);back= ground-image:initial;background-position:initial;background-repeat:initial""= ><a href=3D""https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarim= i/&amp;source=3Dgmail-imap&amp;ust=3D1761768891000000&amp;usg=3DAOvVaw0xLE0= 2sw5-ueWRHyHyCwkN"" target=3D""_blank"">http://www.csee.umbc.edu/~nkarimi/</a>= </span></u></div></div> </div></blockquote></div><br></div></body></html>= "
3296910,72441430,Correspond,DoIT-Research-Computing,2025-10-22 20:22:04.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Skye Jonke,ii69854@umbc.edu,"Mohammad,=20  So long as you continue to access them through /umbc/software/scripts, they=  should remain up to date and you will see any new scripts. If you copy the= m to a local directory on the HPC cluster, they will not be updated.=20  --- Skye Jonke  she/her  Specialist, Linux System Administrator & Lab Technical Support  > On Oct 22, 2025, at 16:14, Naghmeh Karimi <nkarimi@umbc.edu> wrote: >=20 > Hi Max, Roy, Skye, >=20 > Thank you all for your help. >=20 > I added Skye here as she is our IT exert on adding the capability of runn= ing synopsis tools (Hspice) in servers. > Skye we want to use the HPC cluster to run hspice an other synopsys tools= . Could you please help on this? >=20 > Thanks, > Naghmeh >=20 > On Wed, Oct 22, 2025 at 2:06=E2=80=AFPM Mohammad Ebrahimabadi via RT <UMB= CHelp@rt.umbc.edu <mailto:UMBCHelp@rt.umbc.edu>> wrote: >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 <https= ://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3296= 910&source=3Dgmail-imap&ust=3D1761768891000000&usg=3DAOvVaw2jxB3nArHBD7-ij9= ZN8ZsW> > >>=20 >> Last Update From Ticket: >>=20 >> Hi Max, >>=20 >> Thanks for your help! Let=E2=80=99s proceed based on your suggestion, ho= pefully it >> works as expected. >>=20 >> I just have one quick question: since the IT team in the department can = modify >> or add new shell scripts in that directory, will we also have access to = those >> updated or newly added scripts from the mounted directory on HPCF? >>=20 >> Thanks again, >> Mohammad >>=20 >> On Wed Oct 22 14:01:32 2025, OL73413 wrote: >>=20 >> > Hi Mohammad, >>=20 >> > We can get the software mounted for you on chip, but it will have to be >> > read-only as we won't be able to manage who can make changes to it >> > otherwise. >>=20 >> > As a reminder, the machines on that run on chip are not the same as th= ose >> > run by the CSEE department, so we can't guarantee that these will run = the >> > same way or as effectively. >>=20 >> > If that's all ok with you, please give me some time to get the directo= ry >> > mounted. >>=20 >> > On Wed Oct 22 13:26:05 2025, NQ23652 wrote: >>=20 >> >> Thanks Tartela for your help to resolve my issue! >>=20 >> >> Actually it seems all server in CSEE department have access to that >> >> path. When I ran the command that you gave me I got the following >> >> report: >>=20 >> >> mohammad@alborz:~$ df -h /umbc/software >> >> Filesystem Size Used Avail Use% Mounted on >> >> nfs.iss.rs.umbc.edu:/ifs/data/csee/software 2.5T 2.2T 326G 88% >> >> /umbc/software >>=20 >> >> mohammad@alborz:~$ mount | grep /umbc/software >> >> nfs.iss.rs.umbc.edu:/ifs/data/csee/software on /umbc/software type nfs >> >> (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namlen=3D255,acre= gmin=3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,proto=3Dtcp,= timeo=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountvers=3D3,moun= tport=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.60) >>=20 >> >> Please let me know if you need other information. >>=20 >> >> Regards, >>=20 >> >> Mohammad >>=20 >> >> On Wed Oct 22 13:15:12 2025, HK41259 wrote: >>=20 >> >>> Hello, >>=20 >> >>> Thanks for reaching out. Could you clarify which specific machine >> >>> or server you=E2=80=99re referring to when you mention the =E2=80=9C= departmental >> >>> server=E2=80=9D? >>=20 >> >>> The /umbc/software/scripts/ directory isn=E2=80=99t accessible from = HPCF >> >>> (e.g., Chip) by default =E2=80=94 those systems have separate storag= e and >> >>> don=E2=80=99t automatically mount departmental shares. >>=20 >> >>> Could you check where /umbc/software is mounted on your >> >>> departmental system (e.g., by running df -h /umbc/software or mount >> >>> | grep /umbc/software)? That=E2=80=99ll tell us which server hosts i= t. Once >> >>> we know that, we can confirm whether it=E2=80=99s possible or approp= riate >> >>> to make it visible from HPCF. >>=20 >> >>> Best, >>=20 >> >>> Tartela >>=20 >> >>> On Tue Oct 21 15:59:49 2025, ZZ99999 wrote: >>=20 >> >>>> First Name:                Mohammad >> >>>> Last Name:                 Ebrahimabadi >> >>>> Email:                     e127@umbc.edu <mailto:e127@umbc.edu> >> >>>> Campus ID:                 NQ23652 >> >>>>=20 >> >>>> Request Type:              High Performance Cluster >> >>>>=20 >> >>>> Dear Team, >> >>>>=20 >> >>>> I hope you=E2=80=99re doing well. >> >>>> I=E2=80=99m a Ph.D. student in Computer Engineering and have a ques= tion regarding running a software on the HPCF. On the department server, I = usually run software using shell scripts located at /umbc/software/scripts/=  which is provided by the IT group pf the department (Geoff Weiss Team). Ho= wever, in case of HPCF, it seems that this path is not accessible from the = HPCF environment. >> >>>>=20 >> >>>> Could you please let me know if there is any specific configuration=  or access permission required to reach this directory through the HPCF? or=  direct me to a related person that can help me. >> >>>>=20 >> >>>> Best regards, >> >>>> Mohammad >> >>>=20 >>=20 >> > -- >>=20 >> > Best, >> > Max Breitmeyer >> > DOIT HPC System Administrator >>=20 >>=20 >=20 >=20 >=20 > -- > Naghmeh Karimi, Ph.D. > Associate Professor > Department of Computer Science and Electrical Engineering > University of Maryland, Baltimore County > Baltimore, MD 21250 > Tel: 410-455-3965 E-mail: nkarimi@umbc.edu <mailto:nkarimi@umbc.edu> > Web: http://www.csee.umbc.edu/~nkarimi/ <https://www.google.com/url?q=3Dh= ttp://www.csee.umbc.edu/~nkarimi/&source=3Dgmail-imap&ust=3D176176889100000= 0&usg=3DAOvVaw0xLE02sw5-ueWRHyHyCwkN> "
3296910,72453066,Correspond,DoIT-Research-Computing,2025-10-23 14:36:41.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,NULL,naghmeh.karimi@umbc.edu,"Hi Skye and Max,  Could you please help on mounting this today so we can use them as we have a deadline?  Thanks, Naghmeh Karimi   On Wed, Oct 22, 2025 at 4:22=E2=80=AFPM Skye Jonke via RT <UMBCHelp@rt.umbc= .edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 > > > Last Update From Ticket: > > Mohammad, > > So long as you continue to access them through /umbc/software/scripts, > they should remain up to date and you will see any new scripts. If you co= py > them to a local directory on the HPC cluster, they will not be updated. > > --- > Skye Jonke > > she/her > > Specialist, Linux System Administrator & Lab Technical Support > > > On Oct 22, 2025, at 16:14, Naghmeh Karimi <nkarimi@umbc.edu> wrote: > > > > Hi Max, Roy, Skye, > > > > Thank you all for your help. > > > > I added Skye here as she is our IT exert on adding the capability of > running synopsis tools (Hspice) in servers. > > Skye we want to use the HPC cluster to run hspice an other synopsys > tools. Could you please help on this? > > > > Thanks, > > Naghmeh > > > > On Wed, Oct 22, 2025 at 2:06=E2=80=AFPM Mohammad Ebrahimabadi via RT < > UMBCHelp@rt.umbc.edu <mailto:UMBCHelp@rt.umbc.edu>> wrote: > >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 < > https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id= %3D3296910&source=3Dgmail-imap&ust=3D1761768891000000&usg=3DAOvVaw2jxB3nArH= BD7-ij9ZN8ZsW> > > > >> > >> Last Update From Ticket: > >> > >> Hi Max, > >> > >> Thanks for your help! Let=E2=80=99s proceed based on your suggestion, = hopefully > it > >> works as expected. > >> > >> I just have one quick question: since the IT team in the department can > modify > >> or add new shell scripts in that directory, will we also have access to > those > >> updated or newly added scripts from the mounted directory on HPCF? > >> > >> Thanks again, > >> Mohammad > >> > >> On Wed Oct 22 14:01:32 2025, OL73413 wrote: > >> > >> > Hi Mohammad, > >> > >> > We can get the software mounted for you on chip, but it will have to > be > >> > read-only as we won't be able to manage who can make changes to it > >> > otherwise. > >> > >> > As a reminder, the machines on that run on chip are not the same as > those > >> > run by the CSEE department, so we can't guarantee that these will run > the > >> > same way or as effectively. > >> > >> > If that's all ok with you, please give me some time to get the > directory > >> > mounted. > >> > >> > On Wed Oct 22 13:26:05 2025, NQ23652 wrote: > >> > >> >> Thanks Tartela for your help to resolve my issue! > >> > >> >> Actually it seems all server in CSEE department have access to that > >> >> path. When I ran the command that you gave me I got the following > >> >> report: > >> > >> >> mohammad@alborz:~$ df -h /umbc/software > >> >> Filesystem Size Used Avail Use% Mounted on > >> >> nfs.iss.rs.umbc.edu:/ifs/data/csee/software 2.5T 2.2T 326G 88% > >> >> /umbc/software > >> > >> >> mohammad@alborz:~$ mount | grep /umbc/software > >> >> nfs.iss.rs.umbc.edu:/ifs/data/csee/software on /umbc/software type > nfs > >> >> > (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namlen=3D255,acregmin= =3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,proto=3Dtcp,time= o=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountvers=3D3,mountpor= t=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.60) > >> > >> >> Please let me know if you need other information. > >> > >> >> Regards, > >> > >> >> Mohammad > >> > >> >> On Wed Oct 22 13:15:12 2025, HK41259 wrote: > >> > >> >>> Hello, > >> > >> >>> Thanks for reaching out. Could you clarify which specific machine > >> >>> or server you=E2=80=99re referring to when you mention the =E2=80= =9Cdepartmental > >> >>> server=E2=80=9D? > >> > >> >>> The /umbc/software/scripts/ directory isn=E2=80=99t accessible fro= m HPCF > >> >>> (e.g., Chip) by default =E2=80=94 those systems have separate stor= age and > >> >>> don=E2=80=99t automatically mount departmental shares. > >> > >> >>> Could you check where /umbc/software is mounted on your > >> >>> departmental system (e.g., by running df -h /umbc/software or mount > >> >>> | grep /umbc/software)? That=E2=80=99ll tell us which server hosts=  it. Once > >> >>> we know that, we can confirm whether it=E2=80=99s possible or appr= opriate > >> >>> to make it visible from HPCF. > >> > >> >>> Best, > >> > >> >>> Tartela > >> > >> >>> On Tue Oct 21 15:59:49 2025, ZZ99999 wrote: > >> > >> >>>> First Name:                Mohammad > >> >>>> Last Name:                 Ebrahimabadi > >> >>>> Email:                     e127@umbc.edu <mailto:e127@umbc.edu> > >> >>>> Campus ID:                 NQ23652 > >> >>>> > >> >>>> Request Type:              High Performance Cluster > >> >>>> > >> >>>> Dear Team, > >> >>>> > >> >>>> I hope you=E2=80=99re doing well. > >> >>>> I=E2=80=99m a Ph.D. student in Computer Engineering and have a qu= estion > regarding running a software on the HPCF. On the department server, I > usually run software using shell scripts located at /umbc/software/script= s/ > which is provided by the IT group pf the department (Geoff Weiss Team). > However, in case of HPCF, it seems that this path is not accessible from > the HPCF environment. > >> >>>> > >> >>>> Could you please let me know if there is any specific > configuration or access permission required to reach this directory throu= gh > the HPCF? or direct me to a related person that can help me. > >> >>>> > >> >>>> Best regards, > >> >>>> Mohammad > >> >>> > >> > >> > -- > >> > >> > Best, > >> > Max Breitmeyer > >> > DOIT HPC System Administrator > >> > >> > > > > > > > > -- > > Naghmeh Karimi, Ph.D. > > Associate Professor > > Department of Computer Science and Electrical Engineering > > University of Maryland, Baltimore County > > Baltimore, MD 21250 > > Tel: 410-455-3965 E-mail: nkarimi@umbc.edu <mailto:nkarimi@umbc.edu> > > Web: http://www.csee.umbc.edu/~nkarimi/ < > https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source= =3Dgmail-imap&ust=3D1761768891000000&usg=3DAOvVaw0xLE02sw5-ueWRHyHyCwkN > > > >  --=20 Naghmeh Karimi, Ph.D. Associate Professor Department of Computer Science and Electrical Engineering University of Maryland, Baltimore County Baltimore, MD 21250 Tel: *410-455-3965* E-mail: *nkarimi@umbc.edu <nkarimi@umbc.edu>* Web: *http://www.csee.umbc.edu/~nkarimi/ <http://www.csee.umbc.edu/~nkarimi/>* "
3296910,72453066,Correspond,DoIT-Research-Computing,2025-10-23 14:36:41.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,NULL,naghmeh.karimi@umbc.edu,"<div dir=3D""ltr""><div dir=3D""ltr"">Hi Skye and Max,<div><br></div><div>Could=  you please help on mounting this today so we can use them as we have a dea= dline?</div><div><br></div><div>Thanks,</div><div>Naghmeh Karimi</div><div>= <div><div><br></div></div></div></div><br><div class=3D""gmail_quote gmail_q= uote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Wed, Oct 22, 2025 = at 4:22=E2=80=AFPM Skye Jonke via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc= .edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gma= il_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,2= 04,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ti= cket/Display.html?id=3D3296910"" rel=3D""noreferrer"" target=3D""_blank"">https:= //rt.umbc.edu/Ticket/Display.html?id=3D3296910</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Mohammad, <br> <br> So long as you continue to access them through /umbc/software/scripts, they=  should remain up to date and you will see any new scripts. If you copy the= m to a local directory on the HPC cluster, they will not be updated. <br> <br> ---<br> Skye Jonke<br> <br> she/her<br> <br> Specialist, Linux System Administrator &amp; Lab Technical Support<br> <br> &gt; On Oct 22, 2025, at 16:14, Naghmeh Karimi &lt;<a href=3D""mailto:nkarim= i@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a>&gt; wrote:<br> &gt; <br> &gt; Hi Max, Roy, Skye,<br> &gt; <br> &gt; Thank you all for your help.<br> &gt; <br> &gt; I added Skye here as she is our IT exert on adding the capability of r= unning synopsis tools (Hspice) in servers.<br> &gt; Skye we want to use the HPC cluster to run hspice an other synopsys to= ols. Could you please help on this?<br> &gt; <br> &gt; Thanks,<br> &gt; Naghmeh<br> &gt; <br> &gt; On Wed, Oct 22, 2025 at 2:06=E2=80=AFPM Mohammad Ebrahimabadi via RT &= lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.um= bc.edu</a> &lt;mailto:<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_bl= ank"">UMBCHelp@rt.umbc.edu</a>&gt;&gt; wrote:<br> &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html= ?id=3D3296910"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tic= ket/Display.html?id=3D3296910</a> &lt;<a href=3D""https://www.google.com/url= ?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgmai= l-imap&amp;ust=3D1761768891000000&amp;usg=3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW"" r= el=3D""noreferrer"" target=3D""_blank"">https://www.google.com/url?q=3Dhttps://= rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;us= t=3D1761768891000000&amp;usg=3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW</a>&gt; &gt;<br> &gt;&gt; <br> &gt;&gt; Last Update From Ticket:<br> &gt;&gt; <br> &gt;&gt; Hi Max,<br> &gt;&gt; <br> &gt;&gt; Thanks for your help! Let=E2=80=99s proceed based on your suggesti= on, hopefully it<br> &gt;&gt; works as expected.<br> &gt;&gt; <br> &gt;&gt; I just have one quick question: since the IT team in the departmen= t can modify<br> &gt;&gt; or add new shell scripts in that directory, will we also have acce= ss to those<br> &gt;&gt; updated or newly added scripts from the mounted directory on HPCF?= <br> &gt;&gt; <br> &gt;&gt; Thanks again,<br> &gt;&gt; Mohammad<br> &gt;&gt; <br> &gt;&gt; On Wed Oct 22 14:01:32 2025, OL73413 wrote:<br> &gt;&gt; <br> &gt;&gt; &gt; Hi Mohammad,<br> &gt;&gt; <br> &gt;&gt; &gt; We can get the software mounted for you on chip, but it will = have to be<br> &gt;&gt; &gt; read-only as we won&#39;t be able to manage who can make chan= ges to it<br> &gt;&gt; &gt; otherwise.<br> &gt;&gt; <br> &gt;&gt; &gt; As a reminder, the machines on that run on chip are not the s= ame as those<br> &gt;&gt; &gt; run by the CSEE department, so we can&#39;t guarantee that th= ese will run the<br> &gt;&gt; &gt; same way or as effectively.<br> &gt;&gt; <br> &gt;&gt; &gt; If that&#39;s all ok with you, please give me some time to ge= t the directory<br> &gt;&gt; &gt; mounted.<br> &gt;&gt; <br> &gt;&gt; &gt; On Wed Oct 22 13:26:05 2025, NQ23652 wrote:<br> &gt;&gt; <br> &gt;&gt; &gt;&gt; Thanks Tartela for your help to resolve my issue!<br> &gt;&gt; <br> &gt;&gt; &gt;&gt; Actually it seems all server in CSEE department have acce= ss to that<br> &gt;&gt; &gt;&gt; path. When I ran the command that you gave me I got the f= ollowing<br> &gt;&gt; &gt;&gt; report:<br> &gt;&gt; <br> &gt;&gt; &gt;&gt; mohammad@alborz:~$ df -h /umbc/software<br> &gt;&gt; &gt;&gt; Filesystem Size Used Avail Use% Mounted on<br> &gt;&gt; &gt;&gt; nfs.iss.rs.umbc.edu:/ifs/data/csee/software 2.5T 2.2T 326= G 88%<br> &gt;&gt; &gt;&gt; /umbc/software<br> &gt;&gt; <br> &gt;&gt; &gt;&gt; mohammad@alborz:~$ mount | grep /umbc/software<br> &gt;&gt; &gt;&gt; nfs.iss.rs.umbc.edu:/ifs/data/csee/software on /umbc/soft= ware type nfs<br> &gt;&gt; &gt;&gt; (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namle= n=3D255,acregmin=3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,= proto=3Dtcp,timeo=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountv= ers=3D3,mountport=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.6= 0)<br> &gt;&gt; <br> &gt;&gt; &gt;&gt; Please let me know if you need other information.<br> &gt;&gt; <br> &gt;&gt; &gt;&gt; Regards,<br> &gt;&gt; <br> &gt;&gt; &gt;&gt; Mohammad<br> &gt;&gt; <br> &gt;&gt; &gt;&gt; On Wed Oct 22 13:15:12 2025, HK41259 wrote:<br> &gt;&gt; <br> &gt;&gt; &gt;&gt;&gt; Hello,<br> &gt;&gt; <br> &gt;&gt; &gt;&gt;&gt; Thanks for reaching out. Could you clarify which spec= ific machine<br> &gt;&gt; &gt;&gt;&gt; or server you=E2=80=99re referring to when you mentio= n the =E2=80=9Cdepartmental<br> &gt;&gt; &gt;&gt;&gt; server=E2=80=9D?<br> &gt;&gt; <br> &gt;&gt; &gt;&gt;&gt; The /umbc/software/scripts/ directory isn=E2=80=99t a= ccessible from HPCF<br> &gt;&gt; &gt;&gt;&gt; (e.g., Chip) by default =E2=80=94 those systems have = separate storage and<br> &gt;&gt; &gt;&gt;&gt; don=E2=80=99t automatically mount departmental shares= .<br> &gt;&gt; <br> &gt;&gt; &gt;&gt;&gt; Could you check where /umbc/software is mounted on yo= ur<br> &gt;&gt; &gt;&gt;&gt; departmental system (e.g., by running df -h /umbc/sof= tware or mount<br> &gt;&gt; &gt;&gt;&gt; | grep /umbc/software)? That=E2=80=99ll tell us which=  server hosts it. Once<br> &gt;&gt; &gt;&gt;&gt; we know that, we can confirm whether it=E2=80=99s pos= sible or appropriate<br> &gt;&gt; &gt;&gt;&gt; to make it visible from HPCF.<br> &gt;&gt; <br> &gt;&gt; &gt;&gt;&gt; Best,<br> &gt;&gt; <br> &gt;&gt; &gt;&gt;&gt; Tartela<br> &gt;&gt; <br> &gt;&gt; &gt;&gt;&gt; On Tue Oct 21 15:59:49 2025, ZZ99999 wrote:<br> &gt;&gt; <br> &gt;&gt; &gt;&gt;&gt;&gt; First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 =C2=A0 Mohammad<br> &gt;&gt; &gt;&gt;&gt;&gt; Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 =C2=A0 =C2=A0Ebrahimabadi<br> &gt;&gt; &gt;&gt;&gt;&gt; Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0<a href=3D""mailto:e127@umbc.edu"" target= =3D""_blank"">e127@umbc.edu</a> &lt;mailto:<a href=3D""mailto:e127@umbc.edu"" t= arget=3D""_blank"">e127@umbc.edu</a>&gt;<br> &gt;&gt; &gt;&gt;&gt;&gt; Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 =C2=A0 =C2=A0NQ23652<br> &gt;&gt; &gt;&gt;&gt;&gt; <br> &gt;&gt; &gt;&gt;&gt;&gt; Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 =C2=A0 High Performance Cluster<br> &gt;&gt; &gt;&gt;&gt;&gt; <br> &gt;&gt; &gt;&gt;&gt;&gt; Dear Team,<br> &gt;&gt; &gt;&gt;&gt;&gt; <br> &gt;&gt; &gt;&gt;&gt;&gt; I hope you=E2=80=99re doing well.<br> &gt;&gt; &gt;&gt;&gt;&gt; I=E2=80=99m a Ph.D. student in Computer Engineeri= ng and have a question regarding running a software on the HPCF. On the dep= artment server, I usually run software using shell scripts located at /umbc= /software/scripts/ which is provided by the IT group pf the department (Geo= ff Weiss Team). However, in case of HPCF, it seems that this path is not ac= cessible from the HPCF environment.<br> &gt;&gt; &gt;&gt;&gt;&gt; <br> &gt;&gt; &gt;&gt;&gt;&gt; Could you please let me know if there is any spec= ific configuration or access permission required to reach this directory th= rough the HPCF? or direct me to a related person that can help me.<br> &gt;&gt; &gt;&gt;&gt;&gt; <br> &gt;&gt; &gt;&gt;&gt;&gt; Best regards,<br> &gt;&gt; &gt;&gt;&gt;&gt; Mohammad<br> &gt;&gt; &gt;&gt;&gt; <br> &gt;&gt; <br> &gt;&gt; &gt; --<br> &gt;&gt; <br> &gt;&gt; &gt; Best,<br> &gt;&gt; &gt; Max Breitmeyer<br> &gt;&gt; &gt; DOIT HPC System Administrator<br> &gt;&gt; <br> &gt;&gt; <br> &gt; <br> &gt; <br> &gt; <br> &gt; --<br> &gt; Naghmeh Karimi, Ph.D.<br> &gt; Associate Professor<br> &gt; Department of Computer Science and Electrical Engineering<br> &gt; University of Maryland, Baltimore County<br> &gt; Baltimore, MD 21250<br> &gt; Tel: 410-455-3965 E-mail: <a href=3D""mailto:nkarimi@umbc.edu"" target= =3D""_blank"">nkarimi@umbc.edu</a> &lt;mailto:<a href=3D""mailto:nkarimi@umbc.= edu"" target=3D""_blank"">nkarimi@umbc.edu</a>&gt;<br> &gt; Web: <a href=3D""http://www.csee.umbc.edu/~nkarimi/"" rel=3D""noreferrer""=  target=3D""_blank"">http://www.csee.umbc.edu/~nkarimi/</a> &lt;<a href=3D""ht= tps://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source= =3Dgmail-imap&amp;ust=3D1761768891000000&amp;usg=3DAOvVaw0xLE02sw5-ueWRHyHy= CwkN"" rel=3D""noreferrer"" target=3D""_blank"">https://www.google.com/url?q=3Dh= ttp://www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761768= 891000000&amp;usg=3DAOvVaw0xLE02sw5-ueWRHyHyCwkN</a>&gt;<br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><span style=3D""font-size:9.5pt;line-height:115%;= font-family:Arial,sans-serif;color:rgb(136,136,136);background-image:initia= l;background-position:initial;background-repeat:initial"">Naghmeh=C2=A0Karim= i, Ph.D.<br> Associate Professor<br> Department of Computer Science and Electrical Engineering<br> University of Maryland, Baltimore County<br> Baltimore, MD 21250<br> Tel:</span><span style=3D""font-size:11pt;line-height:115%;font-family:Calib= ri,&quot;sans-serif&quot;;color:black"">=C2=A0</span><u><span style=3D""font-= size:11pt;line-height:115%;font-family:Calibri,&quot;sans-serif&quot;;color= :rgb(17,85,204)"">410-455-3965</span></u><span style=3D""font-size:11pt;line-= height:115%;font-family:Calibri,&quot;sans-serif&quot;;color:black""> </span= ><span style=3D""font-size:9.5pt;line-height:115%;font-family:Arial,sans-ser= if;color:rgb(136,136,136);background-image:initial;background-position:init= ial;background-repeat:initial"">E-mail:</span><span style=3D""font-size:11pt;= line-height:115%;font-family:Calibri,&quot;sans-serif&quot;;color:black"">= =C2=A0</span><u><span style=3D""font-size:11pt;line-height:115%;font-family:= Calibri,&quot;sans-serif&quot;;color:rgb(17,85,204)""><a href=3D""mailto:nkar= imi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a></span></u><span style= =3D""font-size:9.5pt;line-height:115%;font-family:Arial,sans-serif;color:rgb= (136,136,136);background-image:initial;background-position:initial;backgrou= nd-repeat:initial""><br> Web:</span><span style=3D""font-size:9.5pt;line-height:115%;font-family:Aria= l,sans-serif;color:black;background-image:initial;background-position:initi= al;background-repeat:initial"">=C2=A0</span><u><span style=3D""font-size:11pt= ;line-height:115%;font-family:Calibri,sans-serif;color:rgb(17,85,204);backg= round-image:initial;background-position:initial;background-repeat:initial"">= <a href=3D""http://www.csee.umbc.edu/~nkarimi/"" target=3D""_blank"">http://www= .csee.umbc.edu/~nkarimi/</a></span></u></div></div></div> "
3296910,72453822,Correspond,DoIT-Research-Computing,2025-10-23 14:54:59.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Nahmeh,</p>  <p>Apologies &mdash; I meant to update you yesterday after completing this,=  but I was pulled into a long meeting. The software is now available on chi= p&nbsp;at:</p>  <p><code>/umbc/software/csee</code></p>  <p>Please note that if the software contains any hardcoded file paths, it m= ay not function correctly. Unfortunately, we don&rsquo;t have control over = those cases.</p>  <p>Also, just to clarify, Skye and I work in separate departments. Similar = to how I don&rsquo;t have administrative access to the CSEE department serv= ers (which include the software directory), Skye is not an administrator on=  <strong>chip</strong>.</p>  <p>Please let me know if you encounter any issues accessing the software on=  the server. If there are problems <em>running</em> the software, I&rsquo;l= l coordinate with Skye to troubleshoot, but since we don&rsquo;t maintain t= his software, we can&rsquo;t guarantee full compatibility on our systems.</= p>  <p>On Thu Oct 23 10:36:41 2025, naghmeh.karimi@umbc.edu wrote:</p>  <blockquote> <div> <div>Hi Skye and Max, <div>&nbsp;</div>  <div>Could you please help on mounting this today so we can use them as we = have a deadline?</div>  <div>&nbsp;</div>  <div>Thanks,</div>  <div>Naghmeh Karimi</div>  <div> <div> <div>&nbsp;</div> </div> </div> </div> &nbsp;  <div> <div>On Wed, Oct 22, 2025 at 4:22=E2=80=AFPM Skye Jonke via RT &lt;UMBCHelp= @rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 96910 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Mohammad,<br /> <br /> So long as you continue to access them through /umbc/software/scripts, they=  should remain up to date and you will see any new scripts. If you copy the= m to a local directory on the HPC cluster, they will not be updated.<br /> <br /> ---<br /> Skye Jonke<br /> <br /> she/her<br /> <br /> Specialist, Linux System Administrator &amp; Lab Technical Support<br /> <br /> &gt; On Oct 22, 2025, at 16:14, Naghmeh Karimi &lt;nkarimi@umbc.edu&gt; wro= te:<br /> &gt;<br /> &gt; Hi Max, Roy, Skye,<br /> &gt;<br /> &gt; Thank you all for your help.<br /> &gt;<br /> &gt; I added Skye here as she is our IT exert on adding the capability of r= unning synopsis tools (Hspice) in servers.<br /> &gt; Skye we want to use the HPC cluster to run hspice an other synopsys to= ols. Could you please help on this?<br /> &gt;<br /> &gt; Thanks,<br /> &gt; Naghmeh<br /> &gt;<br /> &gt; On Wed, Oct 22, 2025 at 2:06=E2=80=AFPM Mohammad Ebrahimabadi via RT &= lt;UMBCHelp@rt.umbc.edu &lt;mailto:UMBCHelp@rt.umbc.edu&gt;&gt; wrote:<br /> &gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32969= 10 &lt;https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.ht= ml?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761768891000000&amp;usg= =3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW&gt; &gt;<br /> &gt;&gt;<br /> &gt;&gt; Last Update From Ticket:<br /> &gt;&gt;<br /> &gt;&gt; Hi Max,<br /> &gt;&gt;<br /> &gt;&gt; Thanks for your help! Let&rsquo;s proceed based on your suggestion= , hopefully it<br /> &gt;&gt; works as expected.<br /> &gt;&gt;<br /> &gt;&gt; I just have one quick question: since the IT team in the departmen= t can modify<br /> &gt;&gt; or add new shell scripts in that directory, will we also have acce= ss to those<br /> &gt;&gt; updated or newly added scripts from the mounted directory on HPCF?= <br /> &gt;&gt;<br /> &gt;&gt; Thanks again,<br /> &gt;&gt; Mohammad<br /> &gt;&gt;<br /> &gt;&gt; On Wed Oct 22 14:01:32 2025, OL73413 wrote:<br /> &gt;&gt;<br /> &gt;&gt; &gt; Hi Mohammad,<br /> &gt;&gt;<br /> &gt;&gt; &gt; We can get the software mounted for you on chip, but it will = have to be<br /> &gt;&gt; &gt; read-only as we won&#39;t be able to manage who can make chan= ges to it<br /> &gt;&gt; &gt; otherwise.<br /> &gt;&gt;<br /> &gt;&gt; &gt; As a reminder, the machines on that run on chip are not the s= ame as those<br /> &gt;&gt; &gt; run by the CSEE department, so we can&#39;t guarantee that th= ese will run the<br /> &gt;&gt; &gt; same way or as effectively.<br /> &gt;&gt;<br /> &gt;&gt; &gt; If that&#39;s all ok with you, please give me some time to ge= t the directory<br /> &gt;&gt; &gt; mounted.<br /> &gt;&gt;<br /> &gt;&gt; &gt; On Wed Oct 22 13:26:05 2025, NQ23652 wrote:<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt; Thanks Tartela for your help to resolve my issue!<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt; Actually it seems all server in CSEE department have acce= ss to that<br /> &gt;&gt; &gt;&gt; path. When I ran the command that you gave me I got the f= ollowing<br /> &gt;&gt; &gt;&gt; report:<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt; mohammad@alborz:~$ df -h /umbc/software<br /> &gt;&gt; &gt;&gt; Filesystem Size Used Avail Use% Mounted on<br /> &gt;&gt; &gt;&gt; nfs.iss.rs.umbc.edu:/ifs/data/csee/software 2.5T 2.2T 326= G 88%<br /> &gt;&gt; &gt;&gt; /umbc/software<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt; mohammad@alborz:~$ mount | grep /umbc/software<br /> &gt;&gt; &gt;&gt; nfs.iss.rs.umbc.edu:/ifs/data/csee/software on /umbc/soft= ware type nfs<br /> &gt;&gt; &gt;&gt; (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namle= n=3D255,acregmin=3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,= proto=3Dtcp,timeo=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountv= ers=3D3,mountport=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.6= 0)<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt; Please let me know if you need other information.<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt; Regards,<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt; Mohammad<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt; On Wed Oct 22 13:15:12 2025, HK41259 wrote:<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt; Hello,<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt; Thanks for reaching out. Could you clarify which spec= ific machine<br /> &gt;&gt; &gt;&gt;&gt; or server you&rsquo;re referring to when you mention = the &ldquo;departmental<br /> &gt;&gt; &gt;&gt;&gt; server&rdquo;?<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt; The /umbc/software/scripts/ directory isn&rsquo;t acc= essible from HPCF<br /> &gt;&gt; &gt;&gt;&gt; (e.g., Chip) by default &mdash; those systems have se= parate storage and<br /> &gt;&gt; &gt;&gt;&gt; don&rsquo;t automatically mount departmental shares.<= br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt; Could you check where /umbc/software is mounted on yo= ur<br /> &gt;&gt; &gt;&gt;&gt; departmental system (e.g., by running df -h /umbc/sof= tware or mount<br /> &gt;&gt; &gt;&gt;&gt; | grep /umbc/software)? That&rsquo;ll tell us which s= erver hosts it. Once<br /> &gt;&gt; &gt;&gt;&gt; we know that, we can confirm whether it&rsquo;s possi= ble or appropriate<br /> &gt;&gt; &gt;&gt;&gt; to make it visible from HPCF.<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt; Best,<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt; Tartela<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt; On Tue Oct 21 15:59:49 2025, ZZ99999 wrote:<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt; First Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nb= sp; &nbsp; &nbsp; Mohammad<br /> &gt;&gt; &gt;&gt;&gt;&gt; Last Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p; &nbsp; &nbsp; &nbsp;Ebrahimabadi<br /> &gt;&gt; &gt;&gt;&gt;&gt; Email:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &= nbsp; &nbsp; &nbsp; &nbsp; &nbsp;e127@umbc.edu &lt;mailto:e127@umbc.edu&gt;= <br /> &gt;&gt; &gt;&gt;&gt;&gt; Campus ID:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p; &nbsp; &nbsp; &nbsp;NQ23652<br /> &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt; Request Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &= nbsp; &nbsp; High Performance Cluster<br /> &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt; Dear Team,<br /> &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt; I hope you&rsquo;re doing well.<br /> &gt;&gt; &gt;&gt;&gt;&gt; I&rsquo;m a Ph.D. student in Computer Engineering=  and have a question regarding running a software on the HPCF. On the depar= tment server, I usually run software using shell scripts located at /umbc/s= oftware/scripts/ which is provided by the IT group pf the department (Geoff=  Weiss Team). However, in case of HPCF, it seems that this path is not acce= ssible from the HPCF environment.<br /> &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt; Could you please let me know if there is any spec= ific configuration or access permission required to reach this directory th= rough the HPCF? or direct me to a related person that can help me.<br /> &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt; Best regards,<br /> &gt;&gt; &gt;&gt;&gt;&gt; Mohammad<br /> &gt;&gt; &gt;&gt;&gt;<br /> &gt;&gt;<br /> &gt;&gt; &gt; --<br /> &gt;&gt;<br /> &gt;&gt; &gt; Best,<br /> &gt;&gt; &gt; Max Breitmeyer<br /> &gt;&gt; &gt; DOIT HPC System Administrator<br /> &gt;&gt;<br /> &gt;&gt;<br /> &gt;<br /> &gt;<br /> &gt;<br /> &gt; --<br /> &gt; Naghmeh Karimi, Ph.D.<br /> &gt; Associate Professor<br /> &gt; Department of Computer Science and Electrical Engineering<br /> &gt; University of Maryland, Baltimore County<br /> &gt; Baltimore, MD 21250<br /> &gt; Tel: 410-455-3965 E-mail: nkarimi@umbc.edu &lt;mailto:nkarimi@umbc.edu= &gt;<br /> &gt; Web: http://www.csee.umbc.edu/~nkarimi/ &lt;https://www.google.com/url= ?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1= 761768891000000&amp;usg=3DAOvVaw0xLE02sw5-ueWRHyHyCwkN&gt;<br /> &nbsp;</blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --  <div> <div><span style=3D""color:#888888"">Naghmeh&nbsp;Karimi, Ph.D.<br /> Associate Professor<br /> Department of Computer Science and Electrical Engineering<br /> University of Maryland, Baltimore County<br /> Baltimore, MD 21250<br /> Tel:</span><span style=3D""color:black"">&nbsp;</span><span style=3D""color:#1= 155cc"">410-455-3965</span><span style=3D""color:black""> </span><span style= =3D""color:#888888"">E-mail:</span><span style=3D""color:black"">&nbsp;</span><= span style=3D""color:#1155cc"">nkarimi@umbc.edu</span><br /> <span style=3D""color:#888888"">Web:</span><span style=3D""color:black"">&nbsp;= </span><span style=3D""color:#1155cc"">http://www.csee.umbc.edu/~nkarimi/</sp= an></div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3296910,72455972,Correspond,DoIT-Research-Computing,2025-10-23 15:41:53.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Mohammad Ebrahimabadi,e127@umbc.edu,"<div> <p>Thank you Max!</p>  <p>Actually we have still problem for accessing that directory. I uploaded = two screenshots that shows what we can see in the department servers and wh= at we can see in CHIP.</p>  <p>For example by running this shell file we can launch HSPICE software in = the department server:<br /> &nbsp;/umbc/software/scripts/launch_synopsys_hspice.sh<br /> But it is not still accessible on CHIP.</p>  <p>&nbsp;</p>  <p>Regards,</p>  <p>Mohammad<br /> &nbsp;</p>  <p>On Thu Oct 23 10:54:59 2025, OL73413 wrote:</p>  <blockquote> <div> <p>Hi Nahmeh,</p>  <p>Apologies &mdash; I meant to update you yesterday after completing this,=  but I was pulled into a long meeting. The software is now available on chi= p&nbsp;at:</p>  <p><code>/umbc/software/csee</code></p>  <p>Please note that if the software contains any hardcoded file paths, it m= ay not function correctly. Unfortunately, we don&rsquo;t have control over = those cases.</p>  <p>Also, just to clarify, Skye and I work in separate departments. Similar = to how I don&rsquo;t have administrative access to the CSEE department serv= ers (which include the software directory), Skye is not an administrator on=  <strong>chip</strong>.</p>  <p>Please let me know if you encounter any issues accessing the software on=  the server. If there are problems <em>running</em> the software, I&rsquo;l= l coordinate with Skye to troubleshoot, but since we don&rsquo;t maintain t= his software, we can&rsquo;t guarantee full compatibility on our systems.</= p>  <p>On Thu Oct 23 10:36:41 2025, naghmeh.karimi@umbc.edu wrote:</p>  <blockquote> <div> <div>Hi Skye and Max, <div>&nbsp;</div>  <div>Could you please help on mounting this today so we can use them as we = have a deadline?</div>  <div>&nbsp;</div>  <div>Thanks,</div>  <div>Naghmeh Karimi</div>  <div> <div> <div>&nbsp;</div> </div> </div> </div> &nbsp;  <div> <div>On Wed, Oct 22, 2025 at 4:22=E2=80=AFPM Skye Jonke via RT &lt;UMBCHelp= @rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 96910 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Mohammad,<br /> <br /> So long as you continue to access them through /umbc/software/scripts, they=  should remain up to date and you will see any new scripts. If you copy the= m to a local directory on the HPC cluster, they will not be updated.<br /> <br /> ---<br /> Skye Jonke<br /> <br /> she/her<br /> <br /> Specialist, Linux System Administrator &amp; Lab Technical Support<br /> <br /> &gt; On Oct 22, 2025, at 16:14, Naghmeh Karimi &lt;nkarimi@umbc.edu&gt; wro= te:<br /> &gt;<br /> &gt; Hi Max, Roy, Skye,<br /> &gt;<br /> &gt; Thank you all for your help.<br /> &gt;<br /> &gt; I added Skye here as she is our IT exert on adding the capability of r= unning synopsis tools (Hspice) in servers.<br /> &gt; Skye we want to use the HPC cluster to run hspice an other synopsys to= ols. Could you please help on this?<br /> &gt;<br /> &gt; Thanks,<br /> &gt; Naghmeh<br /> &gt;<br /> &gt; On Wed, Oct 22, 2025 at 2:06=E2=80=AFPM Mohammad Ebrahimabadi via RT &= lt;UMBCHelp@rt.umbc.edu &lt;mailto:UMBCHelp@rt.umbc.edu&gt;&gt; wrote:<br /> &gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32969= 10 &lt;https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.ht= ml?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761768891000000&amp;usg= =3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW&gt; &gt;<br /> &gt;&gt;<br /> &gt;&gt; Last Update From Ticket:<br /> &gt;&gt;<br /> &gt;&gt; Hi Max,<br /> &gt;&gt;<br /> &gt;&gt; Thanks for your help! Let&rsquo;s proceed based on your suggestion= , hopefully it<br /> &gt;&gt; works as expected.<br /> &gt;&gt;<br /> &gt;&gt; I just have one quick question: since the IT team in the departmen= t can modify<br /> &gt;&gt; or add new shell scripts in that directory, will we also have acce= ss to those<br /> &gt;&gt; updated or newly added scripts from the mounted directory on HPCF?= <br /> &gt;&gt;<br /> &gt;&gt; Thanks again,<br /> &gt;&gt; Mohammad<br /> &gt;&gt;<br /> &gt;&gt; On Wed Oct 22 14:01:32 2025, OL73413 wrote:<br /> &gt;&gt;<br /> &gt;&gt; &gt; Hi Mohammad,<br /> &gt;&gt;<br /> &gt;&gt; &gt; We can get the software mounted for you on chip, but it will = have to be<br /> &gt;&gt; &gt; read-only as we won&#39;t be able to manage who can make chan= ges to it<br /> &gt;&gt; &gt; otherwise.<br /> &gt;&gt;<br /> &gt;&gt; &gt; As a reminder, the machines on that run on chip are not the s= ame as those<br /> &gt;&gt; &gt; run by the CSEE department, so we can&#39;t guarantee that th= ese will run the<br /> &gt;&gt; &gt; same way or as effectively.<br /> &gt;&gt;<br /> &gt;&gt; &gt; If that&#39;s all ok with you, please give me some time to ge= t the directory<br /> &gt;&gt; &gt; mounted.<br /> &gt;&gt;<br /> &gt;&gt; &gt; On Wed Oct 22 13:26:05 2025, NQ23652 wrote:<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt; Thanks Tartela for your help to resolve my issue!<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt; Actually it seems all server in CSEE department have acce= ss to that<br /> &gt;&gt; &gt;&gt; path. When I ran the command that you gave me I got the f= ollowing<br /> &gt;&gt; &gt;&gt; report:<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt; mohammad@alborz:~$ df -h /umbc/software<br /> &gt;&gt; &gt;&gt; Filesystem Size Used Avail Use% Mounted on<br /> &gt;&gt; &gt;&gt; nfs.iss.rs.umbc.edu:/ifs/data/csee/software 2.5T 2.2T 326= G 88%<br /> &gt;&gt; &gt;&gt; /umbc/software<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt; mohammad@alborz:~$ mount | grep /umbc/software<br /> &gt;&gt; &gt;&gt; nfs.iss.rs.umbc.edu:/ifs/data/csee/software on /umbc/soft= ware type nfs<br /> &gt;&gt; &gt;&gt; (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namle= n=3D255,acregmin=3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,= proto=3Dtcp,timeo=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountv= ers=3D3,mountport=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.6= 0)<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt; Please let me know if you need other information.<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt; Regards,<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt; Mohammad<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt; On Wed Oct 22 13:15:12 2025, HK41259 wrote:<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt; Hello,<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt; Thanks for reaching out. Could you clarify which spec= ific machine<br /> &gt;&gt; &gt;&gt;&gt; or server you&rsquo;re referring to when you mention = the &ldquo;departmental<br /> &gt;&gt; &gt;&gt;&gt; server&rdquo;?<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt; The /umbc/software/scripts/ directory isn&rsquo;t acc= essible from HPCF<br /> &gt;&gt; &gt;&gt;&gt; (e.g., Chip) by default &mdash; those systems have se= parate storage and<br /> &gt;&gt; &gt;&gt;&gt; don&rsquo;t automatically mount departmental shares.<= br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt; Could you check where /umbc/software is mounted on yo= ur<br /> &gt;&gt; &gt;&gt;&gt; departmental system (e.g., by running df -h /umbc/sof= tware or mount<br /> &gt;&gt; &gt;&gt;&gt; | grep /umbc/software)? That&rsquo;ll tell us which s= erver hosts it. Once<br /> &gt;&gt; &gt;&gt;&gt; we know that, we can confirm whether it&rsquo;s possi= ble or appropriate<br /> &gt;&gt; &gt;&gt;&gt; to make it visible from HPCF.<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt; Best,<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt; Tartela<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt; On Tue Oct 21 15:59:49 2025, ZZ99999 wrote:<br /> &gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt; First Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nb= sp; &nbsp; &nbsp; Mohammad<br /> &gt;&gt; &gt;&gt;&gt;&gt; Last Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p; &nbsp; &nbsp; &nbsp;Ebrahimabadi<br /> &gt;&gt; &gt;&gt;&gt;&gt; Email:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &= nbsp; &nbsp; &nbsp; &nbsp; &nbsp;e127@umbc.edu &lt;mailto:e127@umbc.edu&gt;= <br /> &gt;&gt; &gt;&gt;&gt;&gt; Campus ID:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p; &nbsp; &nbsp; &nbsp;NQ23652<br /> &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt; Request Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &= nbsp; &nbsp; High Performance Cluster<br /> &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt; Dear Team,<br /> &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt; I hope you&rsquo;re doing well.<br /> &gt;&gt; &gt;&gt;&gt;&gt; I&rsquo;m a Ph.D. student in Computer Engineering=  and have a question regarding running a software on the HPCF. On the depar= tment server, I usually run software using shell scripts located at /umbc/s= oftware/scripts/ which is provided by the IT group pf the department (Geoff=  Weiss Team). However, in case of HPCF, it seems that this path is not acce= ssible from the HPCF environment.<br /> &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt; Could you please let me know if there is any spec= ific configuration or access permission required to reach this directory th= rough the HPCF? or direct me to a related person that can help me.<br /> &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt;&gt;&gt; Best regards,<br /> &gt;&gt; &gt;&gt;&gt;&gt; Mohammad<br /> &gt;&gt; &gt;&gt;&gt;<br /> &gt;&gt;<br /> &gt;&gt; &gt; --<br /> &gt;&gt;<br /> &gt;&gt; &gt; Best,<br /> &gt;&gt; &gt; Max Breitmeyer<br /> &gt;&gt; &gt; DOIT HPC System Administrator<br /> &gt;&gt;<br /> &gt;&gt;<br /> &gt;<br /> &gt;<br /> &gt;<br /> &gt; --<br /> &gt; Naghmeh Karimi, Ph.D.<br /> &gt; Associate Professor<br /> &gt; Department of Computer Science and Electrical Engineering<br /> &gt; University of Maryland, Baltimore County<br /> &gt; Baltimore, MD 21250<br /> &gt; Tel: 410-455-3965 E-mail: nkarimi@umbc.edu &lt;mailto:nkarimi@umbc.edu= &gt;<br /> &gt; Web: http://www.csee.umbc.edu/~nkarimi/ &lt;https://www.google.com/url= ?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1= 761768891000000&amp;usg=3DAOvVaw0xLE02sw5-ueWRHyHyCwkN&gt;<br /> &nbsp;</blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --  <div> <div><span style=3D""color:#888888"">Naghmeh&nbsp;Karimi, Ph.D.<br /> Associate Professor<br /> Department of Computer Science and Electrical Engineering<br /> University of Maryland, Baltimore County<br /> Baltimore, MD 21250<br /> Tel:</span><span style=3D""color:black"">&nbsp;</span><span style=3D""color:#1= 155cc"">410-455-3965</span><span style=3D""color:black""> </span><span style= =3D""color:#888888"">E-mail:</span><span style=3D""color:black"">&nbsp;</span><= span style=3D""color:#1155cc"">nkarimi@umbc.edu</span><br /> <span style=3D""color:#888888"">Web:</span><span style=3D""color:black"">&nbsp;= </span><span style=3D""color:#1155cc"">http://www.csee.umbc.edu/~nkarimi/</sp= an></div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> </blockquote> </div> "
3296910,72456845,Correspond,DoIT-Research-Computing,2025-10-23 15:59:17.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"Hi Mohammad,  A good point. Chip uses a service called ""autofs"" to mount and unmount directories that aren't being actively used. This helps maintain the login nodes integrity. If you see in my screenshot below, the directory appears to be empty, but when I cd to the csee directory where your software is mounted, it is all available. Backing out of the directory and ls'ing the directory also shows that the directory is now mounted and available. [image: image.png]    On Thu, Oct 23, 2025 at 11:41=E2=80=AFAM Mohammad Ebrahimabadi via RT < UMBCHelp@rt.umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 > > > Comment From Ticket: > Thank you Max! > > Actually we have still problem for accessing that directory. I uploaded t= wo > screenshots that shows what we can see in the department servers and what > we > can see in CHIP. > > For example by running this shell file we can launch HSPICE software in t= he > department server: > /umbc/software/scripts/launch_synopsys_hspice.sh > But it is not still accessible on CHIP. > > Regards, > > Mohammad > > On Thu Oct 23 10:54:59 2025, OL73413 wrote: > > > Hi Nahmeh, > > > Apologies =E2=80=94 I meant to update you yesterday after completing th= is, but I > > was pulled into a long meeting. The software is now available on chip a= t: > > > /umbc/software/csee > > > Please note that if the software contains any hardcoded file paths, it > may > > not function correctly. Unfortunately, we don=E2=80=99t have control ov= er those > > cases. > > > Also, just to clarify, Skye and I work in separate departments. Similar > to > > how I don=E2=80=99t have administrative access to the CSEE department s= ervers > > (which include the software directory), Skye is not an administrator on > > chip. > > > Please let me know if you encounter any issues accessing the software on > > the server. If there are problems running the software, I=E2=80=99ll co= ordinate > > with Skye to troubleshoot, but since we don=E2=80=99t maintain this sof= tware, we > > can=E2=80=99t guarantee full compatibility on our systems. > > > On Thu Oct 23 10:36:41 2025, naghmeh.karimi@umbc.edu wrote: > > >> Hi Skye and Max, Could you please help on mounting this today so we can > >> use them as we have a deadline? Thanks,Naghmeh Karimi On Wed, Oct 22, > >> 2025 at 4:22 PM Skye Jonke via RT <UMBCHelp@rt.umbc.edu> wrote: > > >>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 > > > >>> Last Update From Ticket: > > >>> Mohammad, > > >>> So long as you continue to access them through > >>> /umbc/software/scripts, they should remain up to date and you will > >>> see any new scripts. If you copy them to a local directory on the > >>> HPC cluster, they will not be updated. > > >>> --- > >>> Skye Jonke > > >>> she/her > > >>> Specialist, Linux System Administrator & Lab Technical Support > > >>> > On Oct 22, 2025, at 16:14, Naghmeh Karimi <nkarimi@umbc.edu> > >>> wrote: > >>> > > >>> > Hi Max, Roy, Skye, > >>> > > >>> > Thank you all for your help. > >>> > > >>> > I added Skye here as she is our IT exert on adding the capability > >>> of running synopsis tools (Hspice) in servers. > >>> > Skye we want to use the HPC cluster to run hspice an other > >>> synopsys tools. Could you please help on this? > >>> > > >>> > Thanks, > >>> > Naghmeh > >>> > > >>> > On Wed, Oct 22, 2025 at 2:06 PM Mohammad Ebrahimabadi via RT > >>> <UMBCHelp@rt.umbc.edu <mailto:UMBCHelp@rt.umbc.edu>> wrote: > >>> >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 > >>> < > https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id= %3D3296910&source=3Dgmail-imap&ust=3D1761768891000000&usg=3DAOvVaw2jxB3nArH= BD7-ij9ZN8ZsW > > > >>> > > >>> >> > >>> >> Last Update From Ticket: > >>> >> > >>> >> Hi Max, > >>> >> > >>> >> Thanks for your help! Let=E2=80=99s proceed based on your suggesti= on, > >>> hopefully it > >>> >> works as expected. > >>> >> > >>> >> I just have one quick question: since the IT team in the > >>> department can modify > >>> >> or add new shell scripts in that directory, will we also have > >>> access to those > >>> >> updated or newly added scripts from the mounted directory on > >>> HPCF? > >>> >> > >>> >> Thanks again, > >>> >> Mohammad > >>> >> > >>> >> On Wed Oct 22 14:01:32 2025, OL73413 wrote: > >>> >> > >>> >> > Hi Mohammad, > >>> >> > >>> >> > We can get the software mounted for you on chip, but it will > >>> have to be > >>> >> > read-only as we won't be able to manage who can make changes > >>> to it > >>> >> > otherwise. > >>> >> > >>> >> > As a reminder, the machines on that run on chip are not the > >>> same as those > >>> >> > run by the CSEE department, so we can't guarantee that these > >>> will run the > >>> >> > same way or as effectively. > >>> >> > >>> >> > If that's all ok with you, please give me some time to get the > >>> directory > >>> >> > mounted. > >>> >> > >>> >> > On Wed Oct 22 13:26:05 2025, NQ23652 wrote: > >>> >> > >>> >> >> Thanks Tartela for your help to resolve my issue! > >>> >> > >>> >> >> Actually it seems all server in CSEE department have access > >>> to that > >>> >> >> path. When I ran the command that you gave me I got the > >>> following > >>> >> >> report: > >>> >> > >>> >> >> mohammad@alborz:~$ df -h /umbc/software > >>> >> >> Filesystem Size Used Avail Use% Mounted on > >>> >> >> nfs.iss.rs.umbc.edu:/ifs/data/csee/software 2.5T 2.2T 326G > >>> 88% > >>> >> >> /umbc/software > >>> >> > >>> >> >> mohammad@alborz:~$ mount | grep /umbc/software > >>> >> >> nfs.iss.rs.umbc.edu:/ifs/data/csee/software on /umbc/software > >>> type nfs > >>> >> >> > >>> > (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namlen=3D255,acregmin= =3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,proto=3Dtcp,time= o=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountvers=3D3,mountpor= t=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.60) > >>> >> > >>> >> >> Please let me know if you need other information. > >>> >> > >>> >> >> Regards, > >>> >> > >>> >> >> Mohammad > >>> >> > >>> >> >> On Wed Oct 22 13:15:12 2025, HK41259 wrote: > >>> >> > >>> >> >>> Hello, > >>> >> > >>> >> >>> Thanks for reaching out. Could you clarify which specific > >>> machine > >>> >> >>> or server you=E2=80=99re referring to when you mention the > >>> =E2=80=9Cdepartmental > >>> >> >>> server=E2=80=9D? > >>> >> > >>> >> >>> The /umbc/software/scripts/ directory isn=E2=80=99t accessible=  from > >>> HPCF > >>> >> >>> (e.g., Chip) by default =E2=80=94 those systems have separate > >>> storage and > >>> >> >>> don=E2=80=99t automatically mount departmental shares. > >>> >> > >>> >> >>> Could you check where /umbc/software is mounted on your > >>> >> >>> departmental system (e.g., by running df -h /umbc/software > >>> or mount > >>> >> >>> | grep /umbc/software)? That=E2=80=99ll tell us which server h= osts > >>> it. Once > >>> >> >>> we know that, we can confirm whether it=E2=80=99s possible or > >>> appropriate > >>> >> >>> to make it visible from HPCF. > >>> >> > >>> >> >>> Best, > >>> >> > >>> >> >>> Tartela > >>> >> > >>> >> >>> On Tue Oct 21 15:59:49 2025, ZZ99999 wrote: > >>> >> > >>> >> >>>> First Name: Mohammad > >>> >> >>>> Last Name: Ebrahimabadi > >>> >> >>>> Email: e127@umbc.edu <mailto:e127@umbc.edu> > >>> >> >>>> Campus ID: NQ23652 > >>> >> >>>> > >>> >> >>>> Request Type: High Performance Cluster > >>> >> >>>> > >>> >> >>>> Dear Team, > >>> >> >>>> > >>> >> >>>> I hope you=E2=80=99re doing well. > >>> >> >>>> I=E2=80=99m a Ph.D. student in Computer Engineering and have a > >>> question regarding running a software on the HPCF. On the > >>> department server, I usually run software using shell scripts > >>> located at /umbc/software/scripts/ which is provided by the IT > >>> group pf the department (Geoff Weiss Team). However, in case of > >>> HPCF, it seems that this path is not accessible from the HPCF > >>> environment. > >>> >> >>>> > >>> >> >>>> Could you please let me know if there is any specific > >>> configuration or access permission required to reach this directory > >>> through the HPCF? or direct me to a related person that can help > >>> me. > >>> >> >>>> > >>> >> >>>> Best regards, > >>> >> >>>> Mohammad > >>> >> >>> > >>> >> > >>> >> > -- > >>> >> > >>> >> > Best, > >>> >> > Max Breitmeyer > >>> >> > DOIT HPC System Administrator > >>> >> > >>> >> > >>> > > >>> > > >>> > > >>> > -- > >>> > Naghmeh Karimi, Ph.D. > >>> > Associate Professor > >>> > Department of Computer Science and Electrical Engineering > >>> > University of Maryland, Baltimore County > >>> > Baltimore, MD 21250 > >>> > Tel: 410-455-3965 E-mail: nkarimi@umbc.edu > >>> <mailto:nkarimi@umbc.edu> > >>> > Web: http://www.csee.umbc.edu/~nkarimi/ > >>> < > https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source= =3Dgmail-imap&ust=3D1761768891000000&usg=3DAOvVaw0xLE02sw5-ueWRHyHyCwkN > > > > >> -- Naghmeh Karimi, Ph.D. > >> Associate Professor > >> Department of Computer Science and Electrical Engineering > >> University of Maryland, Baltimore County > >> Baltimore, MD 21250 > >> Tel: 410-455-3965 E-mail: nkarimi@umbc.edu > >> Web: http://www.csee.umbc.edu/~nkarimi/ > > > -- > > > Best, > > Max Breitmeyer > > DOIT HPC System Administrator > > >  --=20 V/R, Maxwell Breitmeyer UMBC HPCF Specialist UMBC Observatory IT Manager Graduate Student (443) 835-8250 "
3296910,72456845,Correspond,DoIT-Research-Computing,2025-10-23 15:59:17.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div dir=3D""ltr""><p>Hi Mohammad,</p><p>A good point. Chip uses a service ca= lled &quot;autofs&quot; to mount and unmount directories that aren&#39;t be= ing actively used. This helps maintain the login nodes integrity. If you se= e in my screenshot below, the directory appears to be empty, but when I cd = to the csee directory where your software is mounted, it is all available. = Backing out of the directory and ls&#39;ing the directory also shows that t= he directory is now mounted and available.=C2=A0</p><img src=3D""cid:ii_mh3l= xc9f0"" alt=3D""image.png"" width=3D""476"" height=3D""404""><p><br></p></div><br>= <div class=3D""gmail_quote gmail_quote_container""><div dir=3D""ltr"" class=3D""= gmail_attr"">On Thu, Oct 23, 2025 at 11:41=E2=80=AFAM Mohammad Ebrahimabadi = via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>= &gt; wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""margin:0px = 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"">Tick= et &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D3296910= "" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.h= tml?id=3D3296910</a> &gt;<br> <br> Comment From Ticket:<br> Thank you Max!<br> <br> Actually we have still problem for accessing that directory. I uploaded two= <br> screenshots that shows what we can see in the department servers and what w= e<br> can see in CHIP.<br> <br> For example by running this shell file we can launch HSPICE software in the= <br> department server:<br> /umbc/software/scripts/launch_synopsys_hspice.sh<br> But it is not still accessible on CHIP.<br> <br> Regards,<br> <br> Mohammad<br> <br> On Thu Oct 23 10:54:59 2025, OL73413 wrote:<br> <br> &gt; Hi Nahmeh,<br> <br> &gt; Apologies =E2=80=94 I meant to update you yesterday after completing t= his, but I<br> &gt; was pulled into a long meeting. The software is now available on chip = at:<br> <br> &gt; /umbc/software/csee<br> <br> &gt; Please note that if the software contains any hardcoded file paths, it=  may<br> &gt; not function correctly. Unfortunately, we don=E2=80=99t have control o= ver those<br> &gt; cases.<br> <br> &gt; Also, just to clarify, Skye and I work in separate departments. Simila= r to<br> &gt; how I don=E2=80=99t have administrative access to the CSEE department = servers<br> &gt; (which include the software directory), Skye is not an administrator o= n<br> &gt; chip.<br> <br> &gt; Please let me know if you encounter any issues accessing the software = on<br> &gt; the server. If there are problems running the software, I=E2=80=99ll c= oordinate<br> &gt; with Skye to troubleshoot, but since we don=E2=80=99t maintain this so= ftware, we<br> &gt; can=E2=80=99t guarantee full compatibility on our systems.<br> <br> &gt; On Thu Oct 23 10:36:41 2025, <a href=3D""mailto:naghmeh.karimi@umbc.edu= "" target=3D""_blank"">naghmeh.karimi@umbc.edu</a> wrote:<br> <br> &gt;&gt; Hi Skye and Max, Could you please help on mounting this today so w= e can<br> &gt;&gt; use them as we have a deadline? Thanks,Naghmeh Karimi On Wed, Oct = 22,<br> &gt;&gt; 2025 at 4:22 PM Skye Jonke via RT &lt;<a href=3D""mailto:UMBCHelp@r= t.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.= html?id=3D3296910"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu= /Ticket/Display.html?id=3D3296910</a> &gt;<br> <br> &gt;&gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt;&gt; Mohammad,<br> <br> &gt;&gt;&gt; So long as you continue to access them through<br> &gt;&gt;&gt; /umbc/software/scripts, they should remain up to date and you = will<br> &gt;&gt;&gt; see any new scripts. If you copy them to a local directory on = the<br> &gt;&gt;&gt; HPC cluster, they will not be updated.<br> <br> &gt;&gt;&gt; ---<br> &gt;&gt;&gt; Skye Jonke<br> <br> &gt;&gt;&gt; she/her<br> <br> &gt;&gt;&gt; Specialist, Linux System Administrator &amp; Lab Technical Sup= port<br> <br> &gt;&gt;&gt; &gt; On Oct 22, 2025, at 16:14, Naghmeh Karimi &lt;<a href=3D""= mailto:nkarimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a>&gt;<br> &gt;&gt;&gt; wrote:<br> &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt; Hi Max, Roy, Skye,<br> &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt; Thank you all for your help.<br> &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt; I added Skye here as she is our IT exert on adding the ca= pability<br> &gt;&gt;&gt; of running synopsis tools (Hspice) in servers.<br> &gt;&gt;&gt; &gt; Skye we want to use the HPC cluster to run hspice an othe= r<br> &gt;&gt;&gt; synopsys tools. Could you please help on this?<br> &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt; Thanks,<br> &gt;&gt;&gt; &gt; Naghmeh<br> &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt; On Wed, Oct 22, 2025 at 2:06 PM Mohammad Ebrahimabadi via=  RT<br> &gt;&gt;&gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">= UMBCHelp@rt.umbc.edu</a> &lt;mailto:<a href=3D""mailto:UMBCHelp@rt.umbc.edu""=  target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt;&gt; wrote:<br> &gt;&gt;&gt; &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket= /Display.html?id=3D3296910"" rel=3D""noreferrer"" target=3D""_blank"">https://rt= .umbc.edu/Ticket/Display.html?id=3D3296910</a><br> &gt;&gt;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttps://rt.umbc.= edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761= 768891000000&amp;usg=3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW"" rel=3D""noreferrer"" tar= get=3D""_blank"">https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Di= splay.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761768891000000&= amp;usg=3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW</a>&gt;<br> &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; Last Update From Ticket:<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; Hi Max,<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; Thanks for your help! Let=E2=80=99s proceed based on = your suggestion,<br> &gt;&gt;&gt; hopefully it<br> &gt;&gt;&gt; &gt;&gt; works as expected.<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; I just have one quick question: since the IT team in = the<br> &gt;&gt;&gt; department can modify<br> &gt;&gt;&gt; &gt;&gt; or add new shell scripts in that directory, will we a= lso have<br> &gt;&gt;&gt; access to those<br> &gt;&gt;&gt; &gt;&gt; updated or newly added scripts from the mounted direc= tory on<br> &gt;&gt;&gt; HPCF?<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; Thanks again,<br> &gt;&gt;&gt; &gt;&gt; Mohammad<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; On Wed Oct 22 14:01:32 2025, OL73413 wrote:<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt; Hi Mohammad,<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt; We can get the software mounted for you on chip,=  but it will<br> &gt;&gt;&gt; have to be<br> &gt;&gt;&gt; &gt;&gt; &gt; read-only as we won&#39;t be able to manage who = can make changes<br> &gt;&gt;&gt; to it<br> &gt;&gt;&gt; &gt;&gt; &gt; otherwise.<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt; As a reminder, the machines on that run on chip = are not the<br> &gt;&gt;&gt; same as those<br> &gt;&gt;&gt; &gt;&gt; &gt; run by the CSEE department, so we can&#39;t guar= antee that these<br> &gt;&gt;&gt; will run the<br> &gt;&gt;&gt; &gt;&gt; &gt; same way or as effectively.<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt; If that&#39;s all ok with you, please give me so= me time to get the<br> &gt;&gt;&gt; directory<br> &gt;&gt;&gt; &gt;&gt; &gt; mounted.<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt; On Wed Oct 22 13:26:05 2025, NQ23652 wrote:<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thanks Tartela for your help to resolve my i= ssue!<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Actually it seems all server in CSEE departm= ent have access<br> &gt;&gt;&gt; to that<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; path. When I ran the command that you gave m= e I got the<br> &gt;&gt;&gt; following<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; report:<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; mohammad@alborz:~$ df -h /umbc/software<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Filesystem Size Used Avail Use% Mounted on<b= r> &gt;&gt;&gt; &gt;&gt; &gt;&gt; nfs.iss.rs.umbc.edu:/ifs/data/csee/software = 2.5T 2.2T 326G<br> &gt;&gt;&gt; 88%<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; /umbc/software<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; mohammad@alborz:~$ mount | grep /umbc/softwa= re<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; nfs.iss.rs.umbc.edu:/ifs/data/csee/software = on /umbc/software<br> &gt;&gt;&gt; type nfs<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namlen=3D2= 55,acregmin=3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,proto= =3Dtcp,timeo=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountvers= =3D3,mountport=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.60)<= br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Please let me know if you need other informa= tion.<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Regards,<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Mohammad<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt; On Wed Oct 22 13:15:12 2025, HK41259 wrote:<= br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Hello,<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Thanks for reaching out. Could you clari= fy which specific<br> &gt;&gt;&gt; machine<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; or server you=E2=80=99re referring to wh= en you mention the<br> &gt;&gt;&gt; =E2=80=9Cdepartmental<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; server=E2=80=9D?<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; The /umbc/software/scripts/ directory is= n=E2=80=99t accessible from<br> &gt;&gt;&gt; HPCF<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; (e.g., Chip) by default =E2=80=94 those = systems have separate<br> &gt;&gt;&gt; storage and<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; don=E2=80=99t automatically mount depart= mental shares.<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Could you check where /umbc/software is = mounted on your<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; departmental system (e.g., by running df=  -h /umbc/software<br> &gt;&gt;&gt; or mount<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; | grep /umbc/software)? That=E2=80=99ll = tell us which server hosts<br> &gt;&gt;&gt; it. Once<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; we know that, we can confirm whether it= =E2=80=99s possible or<br> &gt;&gt;&gt; appropriate<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; to make it visible from HPCF.<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Best,<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Tartela<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; On Tue Oct 21 15:59:49 2025, ZZ99999 wro= te:<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; First Name: Mohammad<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Last Name: Ebrahimabadi<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Email: <a href=3D""mailto:e127@umbc.e= du"" target=3D""_blank"">e127@umbc.edu</a> &lt;mailto:<a href=3D""mailto:e127@u= mbc.edu"" target=3D""_blank"">e127@umbc.edu</a>&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Campus ID: NQ23652<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Request Type: High Performance Clust= er<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Dear Team,<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; I hope you=E2=80=99re doing well.<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; I=E2=80=99m a Ph.D. student in Compu= ter Engineering and have a<br> &gt;&gt;&gt; question regarding running a software on the HPCF. On the<br> &gt;&gt;&gt; department server, I usually run software using shell scripts<= br> &gt;&gt;&gt; located at /umbc/software/scripts/ which is provided by the IT= <br> &gt;&gt;&gt; group pf the department (Geoff Weiss Team). However, in case o= f<br> &gt;&gt;&gt; HPCF, it seems that this path is not accessible from the HPCF<= br> &gt;&gt;&gt; environment.<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Could you please let me know if ther= e is any specific<br> &gt;&gt;&gt; configuration or access permission required to reach this dire= ctory<br> &gt;&gt;&gt; through the HPCF? or direct me to a related person that can he= lp<br> &gt;&gt;&gt; me.<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Best regards,<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Mohammad<br> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt; --<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt; &gt; Best,<br> &gt;&gt;&gt; &gt;&gt; &gt; Max Breitmeyer<br> &gt;&gt;&gt; &gt;&gt; &gt; DOIT HPC System Administrator<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt; --<br> &gt;&gt;&gt; &gt; Naghmeh Karimi, Ph.D.<br> &gt;&gt;&gt; &gt; Associate Professor<br> &gt;&gt;&gt; &gt; Department of Computer Science and Electrical Engineering= <br> &gt;&gt;&gt; &gt; University of Maryland, Baltimore County<br> &gt;&gt;&gt; &gt; Baltimore, MD 21250<br> &gt;&gt;&gt; &gt; Tel: 410-455-3965 E-mail: <a href=3D""mailto:nkarimi@umbc.= edu"" target=3D""_blank"">nkarimi@umbc.edu</a><br> &gt;&gt;&gt; &lt;mailto:<a href=3D""mailto:nkarimi@umbc.edu"" target=3D""_blan= k"">nkarimi@umbc.edu</a>&gt;<br> &gt;&gt;&gt; &gt; Web: <a href=3D""http://www.csee.umbc.edu/~nkarimi/"" rel= =3D""noreferrer"" target=3D""_blank"">http://www.csee.umbc.edu/~nkarimi/</a><br> &gt;&gt;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttp://www.csee.= umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761768891000000&amp;u= sg=3DAOvVaw0xLE02sw5-ueWRHyHyCwkN"" rel=3D""noreferrer"" target=3D""_blank"">htt= ps://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source= =3Dgmail-imap&amp;ust=3D1761768891000000&amp;usg=3DAOvVaw0xLE02sw5-ueWRHyHy= CwkN</a>&gt;<br> <br> &gt;&gt; -- Naghmeh Karimi, Ph.D.<br> &gt;&gt; Associate Professor<br> &gt;&gt; Department of Computer Science and Electrical Engineering<br> &gt;&gt; University of Maryland, Baltimore County<br> &gt;&gt; Baltimore, MD 21250<br> &gt;&gt; Tel: 410-455-3965 E-mail: <a href=3D""mailto:nkarimi@umbc.edu"" targ= et=3D""_blank"">nkarimi@umbc.edu</a><br> &gt;&gt; Web: <a href=3D""http://www.csee.umbc.edu/~nkarimi/"" rel=3D""norefer= rer"" target=3D""_blank"">http://www.csee.umbc.edu/~nkarimi/</a><br> <br> &gt; --<br> <br> &gt; Best,<br> &gt; Max Breitmeyer<br> &gt; DOIT HPC System Administrator<br> <br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><div dir=3D""ltr""><div>V/R,</div><div dir=3D""ltr""= >Maxwell Breitmeyer</div><div dir=3D""ltr"">UMBC HPCF Specialist</div><div di= r=3D""ltr"">UMBC Observatory IT Manager<br><div>Graduate Student</div><div>(4= 43) 835-8250<br></div></div></div></div></div> "
3296910,72457206,Correspond,DoIT-Research-Computing,2025-10-23 16:09:34.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Mohammad Ebrahimabadi,e127@umbc.edu,"<div> <p>Yes you are right, now I could see all shell files. However still I have=  problem when I run the sh file.</p>  <p>[e127@chip-login2 scripts]$ sh launch_synopsys_hspice.sh<br /> launch_synopsys_hspice.sh: line 3: /umbc/software/scripts/env_synopsys_hspi= ce.sh: No such file or directory<br /> [e127@chip-login2 scripts]$<br /> &nbsp;</p>  <p>Would you please take a look to it.</p>  <p>&nbsp;</p>  <p>Regards,</p>  <p>Mohammad</p>  <p>&nbsp;</p>  <p>On Thu Oct 23 11:59:17 2025, OL73413 wrote:</p>  <blockquote> <div> <p>Hi Mohammad,</p>  <p>A good point. Chip uses a service called &quot;autofs&quot; to mount and=  unmount directories that aren&#39;t being actively used. This helps mainta= in the login nodes integrity. If you see in my screenshot below, the direct= ory appears to be empty, but when I cd to the csee directory where your sof= tware is mounted, it is all available. Backing out of the directory and ls&= #39;ing the directory also shows that the directory is now mounted and avai= lable.&nbsp;</p>  <p>&nbsp;</p> </div> &nbsp;  <div> <div>On Thu, Oct 23, 2025 at 11:41=E2=80=AFAM Mohammad Ebrahimabadi via RT = &lt;UMBCHelp@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 96910 &gt;<br /> <br /> Comment From Ticket:<br /> Thank you Max!<br /> <br /> Actually we have still problem for accessing that directory. I uploaded two= <br /> screenshots that shows what we can see in the department servers and what w= e<br /> can see in CHIP.<br /> <br /> For example by running this shell file we can launch HSPICE software in the= <br /> department server:<br /> /umbc/software/scripts/launch_synopsys_hspice.sh<br /> But it is not still accessible on CHIP.<br /> <br /> Regards,<br /> <br /> Mohammad<br /> <br /> On Thu Oct 23 10:54:59 2025, OL73413 wrote:<br /> <br /> &gt; Hi Nahmeh,<br /> <br /> &gt; Apologies &mdash; I meant to update you yesterday after completing thi= s, but I<br /> &gt; was pulled into a long meeting. The software is now available on chip = at:<br /> <br /> &gt; /umbc/software/csee<br /> <br /> &gt; Please note that if the software contains any hardcoded file paths, it=  may<br /> &gt; not function correctly. Unfortunately, we don&rsquo;t have control ove= r those<br /> &gt; cases.<br /> <br /> &gt; Also, just to clarify, Skye and I work in separate departments. Simila= r to<br /> &gt; how I don&rsquo;t have administrative access to the CSEE department se= rvers<br /> &gt; (which include the software directory), Skye is not an administrator o= n<br /> &gt; chip.<br /> <br /> &gt; Please let me know if you encounter any issues accessing the software = on<br /> &gt; the server. If there are problems running the software, I&rsquo;ll coo= rdinate<br /> &gt; with Skye to troubleshoot, but since we don&rsquo;t maintain this soft= ware, we<br /> &gt; can&rsquo;t guarantee full compatibility on our systems.<br /> <br /> &gt; On Thu Oct 23 10:36:41 2025, naghmeh.karimi@umbc.edu wrote:<br /> <br /> &gt;&gt; Hi Skye and Max, Could you please help on mounting this today so w= e can<br /> &gt;&gt; use them as we have a deadline? Thanks,Naghmeh Karimi On Wed, Oct = 22,<br /> &gt;&gt; 2025 at 4:22 PM Skye Jonke via RT &lt;UMBCHelp@rt.umbc.edu&gt; wro= te:<br /> <br /> &gt;&gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3= 296910 &gt;<br /> <br /> &gt;&gt;&gt; Last Update From Ticket:<br /> <br /> &gt;&gt;&gt; Mohammad,<br /> <br /> &gt;&gt;&gt; So long as you continue to access them through<br /> &gt;&gt;&gt; /umbc/software/scripts, they should remain up to date and you = will<br /> &gt;&gt;&gt; see any new scripts. If you copy them to a local directory on = the<br /> &gt;&gt;&gt; HPC cluster, they will not be updated.<br /> <br /> &gt;&gt;&gt; ---<br /> &gt;&gt;&gt; Skye Jonke<br /> <br /> &gt;&gt;&gt; she/her<br /> <br /> &gt;&gt;&gt; Specialist, Linux System Administrator &amp; Lab Technical Sup= port<br /> <br /> &gt;&gt;&gt; &gt; On Oct 22, 2025, at 16:14, Naghmeh Karimi &lt;nkarimi@umb= c.edu&gt;<br /> &gt;&gt;&gt; wrote:<br /> &gt;&gt;&gt; &gt;<br /> &gt;&gt;&gt; &gt; Hi Max, Roy, Skye,<br /> &gt;&gt;&gt; &gt;<br /> &gt;&gt;&gt; &gt; Thank you all for your help.<br /> &gt;&gt;&gt; &gt;<br /> &gt;&gt;&gt; &gt; I added Skye here as she is our IT exert on adding the ca= pability<br /> &gt;&gt;&gt; of running synopsis tools (Hspice) in servers.<br /> &gt;&gt;&gt; &gt; Skye we want to use the HPC cluster to run hspice an othe= r<br /> &gt;&gt;&gt; synopsys tools. Could you please help on this?<br /> &gt;&gt;&gt; &gt;<br /> &gt;&gt;&gt; &gt; Thanks,<br /> &gt;&gt;&gt; &gt; Naghmeh<br /> &gt;&gt;&gt; &gt;<br /> &gt;&gt;&gt; &gt; On Wed, Oct 22, 2025 at 2:06 PM Mohammad Ebrahimabadi via=  RT<br /> &gt;&gt;&gt; &lt;UMBCHelp@rt.umbc.edu &lt;mailto:UMBCHelp@rt.umbc.edu&gt;&g= t; wrote:<br /> &gt;&gt;&gt; &gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.ht= ml?id=3D3296910<br /> &gt;&gt;&gt; &lt;https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/= Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D176176889100000= 0&amp;usg=3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW&gt;<br /> &gt;&gt;&gt; &gt;<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; Last Update From Ticket:<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; Hi Max,<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; Thanks for your help! Let&rsquo;s proceed based on yo= ur suggestion,<br /> &gt;&gt;&gt; hopefully it<br /> &gt;&gt;&gt; &gt;&gt; works as expected.<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; I just have one quick question: since the IT team in = the<br /> &gt;&gt;&gt; department can modify<br /> &gt;&gt;&gt; &gt;&gt; or add new shell scripts in that directory, will we a= lso have<br /> &gt;&gt;&gt; access to those<br /> &gt;&gt;&gt; &gt;&gt; updated or newly added scripts from the mounted direc= tory on<br /> &gt;&gt;&gt; HPCF?<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; Thanks again,<br /> &gt;&gt;&gt; &gt;&gt; Mohammad<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; On Wed Oct 22 14:01:32 2025, OL73413 wrote:<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt; Hi Mohammad,<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt; We can get the software mounted for you on chip,=  but it will<br /> &gt;&gt;&gt; have to be<br /> &gt;&gt;&gt; &gt;&gt; &gt; read-only as we won&#39;t be able to manage who = can make changes<br /> &gt;&gt;&gt; to it<br /> &gt;&gt;&gt; &gt;&gt; &gt; otherwise.<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt; As a reminder, the machines on that run on chip = are not the<br /> &gt;&gt;&gt; same as those<br /> &gt;&gt;&gt; &gt;&gt; &gt; run by the CSEE department, so we can&#39;t guar= antee that these<br /> &gt;&gt;&gt; will run the<br /> &gt;&gt;&gt; &gt;&gt; &gt; same way or as effectively.<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt; If that&#39;s all ok with you, please give me so= me time to get the<br /> &gt;&gt;&gt; directory<br /> &gt;&gt;&gt; &gt;&gt; &gt; mounted.<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt; On Wed Oct 22 13:26:05 2025, NQ23652 wrote:<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thanks Tartela for your help to resolve my i= ssue!<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Actually it seems all server in CSEE departm= ent have access<br /> &gt;&gt;&gt; to that<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; path. When I ran the command that you gave m= e I got the<br /> &gt;&gt;&gt; following<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; report:<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; mohammad@alborz:~$ df -h /umbc/software<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Filesystem Size Used Avail Use% Mounted on<b= r /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; nfs.iss.rs.umbc.edu:/ifs/data/csee/software = 2.5T 2.2T 326G<br /> &gt;&gt;&gt; 88%<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; /umbc/software<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; mohammad@alborz:~$ mount | grep /umbc/softwa= re<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; nfs.iss.rs.umbc.edu:/ifs/data/csee/software = on /umbc/software<br /> &gt;&gt;&gt; type nfs<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namlen=3D2= 55,acregmin=3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,proto= =3Dtcp,timeo=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountvers= =3D3,mountport=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.60)<= br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Please let me know if you need other informa= tion.<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Regards,<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Mohammad<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; On Wed Oct 22 13:15:12 2025, HK41259 wrote:<= br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Hello,<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Thanks for reaching out. Could you clari= fy which specific<br /> &gt;&gt;&gt; machine<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; or server you&rsquo;re referring to when=  you mention the<br /> &gt;&gt;&gt; &ldquo;departmental<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; server&rdquo;?<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; The /umbc/software/scripts/ directory is= n&rsquo;t accessible from<br /> &gt;&gt;&gt; HPCF<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; (e.g., Chip) by default &mdash; those sy= stems have separate<br /> &gt;&gt;&gt; storage and<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; don&rsquo;t automatically mount departme= ntal shares.<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Could you check where /umbc/software is = mounted on your<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; departmental system (e.g., by running df=  -h /umbc/software<br /> &gt;&gt;&gt; or mount<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; | grep /umbc/software)? That&rsquo;ll te= ll us which server hosts<br /> &gt;&gt;&gt; it. Once<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; we know that, we can confirm whether it&= rsquo;s possible or<br /> &gt;&gt;&gt; appropriate<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; to make it visible from HPCF.<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Best,<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Tartela<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; On Tue Oct 21 15:59:49 2025, ZZ99999 wro= te:<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; First Name: Mohammad<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Last Name: Ebrahimabadi<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Email: e127@umbc.edu &lt;mailto:e127= @umbc.edu&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Campus ID: NQ23652<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Request Type: High Performance Clust= er<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Dear Team,<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; I hope you&rsquo;re doing well.<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; I&rsquo;m a Ph.D. student in Compute= r Engineering and have a<br /> &gt;&gt;&gt; question regarding running a software on the HPCF. On the<br /> &gt;&gt;&gt; department server, I usually run software using shell scripts<= br /> &gt;&gt;&gt; located at /umbc/software/scripts/ which is provided by the IT= <br /> &gt;&gt;&gt; group pf the department (Geoff Weiss Team). However, in case o= f<br /> &gt;&gt;&gt; HPCF, it seems that this path is not accessible from the HPCF<= br /> &gt;&gt;&gt; environment.<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Could you please let me know if ther= e is any specific<br /> &gt;&gt;&gt; configuration or access permission required to reach this dire= ctory<br /> &gt;&gt;&gt; through the HPCF? or direct me to a related person that can he= lp<br /> &gt;&gt;&gt; me.<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Best regards,<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Mohammad<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt; --<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt; Best,<br /> &gt;&gt;&gt; &gt;&gt; &gt; Max Breitmeyer<br /> &gt;&gt;&gt; &gt;&gt; &gt; DOIT HPC System Administrator<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;<br /> &gt;&gt;&gt; &gt;<br /> &gt;&gt;&gt; &gt;<br /> &gt;&gt;&gt; &gt; --<br /> &gt;&gt;&gt; &gt; Naghmeh Karimi, Ph.D.<br /> &gt;&gt;&gt; &gt; Associate Professor<br /> &gt;&gt;&gt; &gt; Department of Computer Science and Electrical Engineering= <br /> &gt;&gt;&gt; &gt; University of Maryland, Baltimore County<br /> &gt;&gt;&gt; &gt; Baltimore, MD 21250<br /> &gt;&gt;&gt; &gt; Tel: 410-455-3965 E-mail: nkarimi@umbc.edu<br /> &gt;&gt;&gt; &lt;mailto:nkarimi@umbc.edu&gt;<br /> &gt;&gt;&gt; &gt; Web: http://www.csee.umbc.edu/~nkarimi/<br /> &gt;&gt;&gt; &lt;https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~n= karimi/&amp;source=3Dgmail-imap&amp;ust=3D1761768891000000&amp;usg=3DAOvVaw= 0xLE02sw5-ueWRHyHyCwkN&gt;<br /> <br /> &gt;&gt; -- Naghmeh Karimi, Ph.D.<br /> &gt;&gt; Associate Professor<br /> &gt;&gt; Department of Computer Science and Electrical Engineering<br /> &gt;&gt; University of Maryland, Baltimore County<br /> &gt;&gt; Baltimore, MD 21250<br /> &gt;&gt; Tel: 410-455-3965 E-mail: nkarimi@umbc.edu<br /> &gt;&gt; Web: http://www.csee.umbc.edu/~nkarimi/<br /> <br /> &gt; --<br /> <br /> &gt; Best,<br /> &gt; Max Breitmeyer<br /> &gt; DOIT HPC System Administrator<br /> <br /> &nbsp;</blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --  <div> <div> <div> <div>V/R,</div>  <div>Maxwell Breitmeyer</div>  <div>UMBC HPCF Specialist</div>  <div>UMBC Observatory IT Manager <div>Graduate Student</div>  <div>(443) 835-8250</div> </div> </div> </div> </div> </blockquote> </div> "
3296910,72457893,Correspond,DoIT-Research-Computing,2025-10-23 16:22:55.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Mohammad,</p>  <p>According to the error message, it&#39;s having an issue finding another=  file in the directory. This appears to be an issue due to the mounting loc= ations being different. I would point you to the previous email where I sai= d that any software with hard-coded locations would not work, and that soft= ware compatibility is not guaranteed. I would recommend working with Skye o= n porting what is in the directory to your local research directories on ch= ip.&nbsp;&nbsp;</p>  <p>On Thu Oct 23 12:09:34 2025, NQ23652 wrote:</p>  <blockquote> <div> <p>Yes you are right, now I could see all shell files. However still I have=  problem when I run the sh file.</p>  <p>[e127@chip-login2 scripts]$ sh launch_synopsys_hspice.sh<br /> launch_synopsys_hspice.sh: line 3: /umbc/software/scripts/env_synopsys_hspi= ce.sh: No such file or directory<br /> [e127@chip-login2 scripts]$<br /> &nbsp;</p>  <p>Would you please take a look to it.</p>  <p>&nbsp;</p>  <p>Regards,</p>  <p>Mohammad</p>  <p>&nbsp;</p>  <p>On Thu Oct 23 11:59:17 2025, OL73413 wrote:</p>  <blockquote> <div> <p>Hi Mohammad,</p>  <p>A good point. Chip uses a service called &quot;autofs&quot; to mount and=  unmount directories that aren&#39;t being actively used. This helps mainta= in the login nodes integrity. If you see in my screenshot below, the direct= ory appears to be empty, but when I cd to the csee directory where your sof= tware is mounted, it is all available. Backing out of the directory and ls&= #39;ing the directory also shows that the directory is now mounted and avai= lable.&nbsp;</p>  <p>&nbsp;</p> </div> &nbsp;  <div> <div>On Thu, Oct 23, 2025 at 11:41=E2=80=AFAM Mohammad Ebrahimabadi via RT = &lt;UMBCHelp@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32= 96910 &gt;<br /> <br /> Comment From Ticket:<br /> Thank you Max!<br /> <br /> Actually we have still problem for accessing that directory. I uploaded two= <br /> screenshots that shows what we can see in the department servers and what w= e<br /> can see in CHIP.<br /> <br /> For example by running this shell file we can launch HSPICE software in the= <br /> department server:<br /> /umbc/software/scripts/launch_synopsys_hspice.sh<br /> But it is not still accessible on CHIP.<br /> <br /> Regards,<br /> <br /> Mohammad<br /> <br /> On Thu Oct 23 10:54:59 2025, OL73413 wrote:<br /> <br /> &gt; Hi Nahmeh,<br /> <br /> &gt; Apologies &mdash; I meant to update you yesterday after completing thi= s, but I<br /> &gt; was pulled into a long meeting. The software is now available on chip = at:<br /> <br /> &gt; /umbc/software/csee<br /> <br /> &gt; Please note that if the software contains any hardcoded file paths, it=  may<br /> &gt; not function correctly. Unfortunately, we don&rsquo;t have control ove= r those<br /> &gt; cases.<br /> <br /> &gt; Also, just to clarify, Skye and I work in separate departments. Simila= r to<br /> &gt; how I don&rsquo;t have administrative access to the CSEE department se= rvers<br /> &gt; (which include the software directory), Skye is not an administrator o= n<br /> &gt; chip.<br /> <br /> &gt; Please let me know if you encounter any issues accessing the software = on<br /> &gt; the server. If there are problems running the software, I&rsquo;ll coo= rdinate<br /> &gt; with Skye to troubleshoot, but since we don&rsquo;t maintain this soft= ware, we<br /> &gt; can&rsquo;t guarantee full compatibility on our systems.<br /> <br /> &gt; On Thu Oct 23 10:36:41 2025, naghmeh.karimi@umbc.edu wrote:<br /> <br /> &gt;&gt; Hi Skye and Max, Could you please help on mounting this today so w= e can<br /> &gt;&gt; use them as we have a deadline? Thanks,Naghmeh Karimi On Wed, Oct = 22,<br /> &gt;&gt; 2025 at 4:22 PM Skye Jonke via RT &lt;UMBCHelp@rt.umbc.edu&gt; wro= te:<br /> <br /> &gt;&gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3= 296910 &gt;<br /> <br /> &gt;&gt;&gt; Last Update From Ticket:<br /> <br /> &gt;&gt;&gt; Mohammad,<br /> <br /> &gt;&gt;&gt; So long as you continue to access them through<br /> &gt;&gt;&gt; /umbc/software/scripts, they should remain up to date and you = will<br /> &gt;&gt;&gt; see any new scripts. If you copy them to a local directory on = the<br /> &gt;&gt;&gt; HPC cluster, they will not be updated.<br /> <br /> &gt;&gt;&gt; ---<br /> &gt;&gt;&gt; Skye Jonke<br /> <br /> &gt;&gt;&gt; she/her<br /> <br /> &gt;&gt;&gt; Specialist, Linux System Administrator &amp; Lab Technical Sup= port<br /> <br /> &gt;&gt;&gt; &gt; On Oct 22, 2025, at 16:14, Naghmeh Karimi &lt;nkarimi@umb= c.edu&gt;<br /> &gt;&gt;&gt; wrote:<br /> &gt;&gt;&gt; &gt;<br /> &gt;&gt;&gt; &gt; Hi Max, Roy, Skye,<br /> &gt;&gt;&gt; &gt;<br /> &gt;&gt;&gt; &gt; Thank you all for your help.<br /> &gt;&gt;&gt; &gt;<br /> &gt;&gt;&gt; &gt; I added Skye here as she is our IT exert on adding the ca= pability<br /> &gt;&gt;&gt; of running synopsis tools (Hspice) in servers.<br /> &gt;&gt;&gt; &gt; Skye we want to use the HPC cluster to run hspice an othe= r<br /> &gt;&gt;&gt; synopsys tools. Could you please help on this?<br /> &gt;&gt;&gt; &gt;<br /> &gt;&gt;&gt; &gt; Thanks,<br /> &gt;&gt;&gt; &gt; Naghmeh<br /> &gt;&gt;&gt; &gt;<br /> &gt;&gt;&gt; &gt; On Wed, Oct 22, 2025 at 2:06 PM Mohammad Ebrahimabadi via=  RT<br /> &gt;&gt;&gt; &lt;UMBCHelp@rt.umbc.edu &lt;mailto:UMBCHelp@rt.umbc.edu&gt;&g= t; wrote:<br /> &gt;&gt;&gt; &gt;&gt; Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.ht= ml?id=3D3296910<br /> &gt;&gt;&gt; &lt;https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/= Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D176176889100000= 0&amp;usg=3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW&gt;<br /> &gt;&gt;&gt; &gt;<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; Last Update From Ticket:<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; Hi Max,<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; Thanks for your help! Let&rsquo;s proceed based on yo= ur suggestion,<br /> &gt;&gt;&gt; hopefully it<br /> &gt;&gt;&gt; &gt;&gt; works as expected.<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; I just have one quick question: since the IT team in = the<br /> &gt;&gt;&gt; department can modify<br /> &gt;&gt;&gt; &gt;&gt; or add new shell scripts in that directory, will we a= lso have<br /> &gt;&gt;&gt; access to those<br /> &gt;&gt;&gt; &gt;&gt; updated or newly added scripts from the mounted direc= tory on<br /> &gt;&gt;&gt; HPCF?<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; Thanks again,<br /> &gt;&gt;&gt; &gt;&gt; Mohammad<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; On Wed Oct 22 14:01:32 2025, OL73413 wrote:<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt; Hi Mohammad,<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt; We can get the software mounted for you on chip,=  but it will<br /> &gt;&gt;&gt; have to be<br /> &gt;&gt;&gt; &gt;&gt; &gt; read-only as we won&#39;t be able to manage who = can make changes<br /> &gt;&gt;&gt; to it<br /> &gt;&gt;&gt; &gt;&gt; &gt; otherwise.<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt; As a reminder, the machines on that run on chip = are not the<br /> &gt;&gt;&gt; same as those<br /> &gt;&gt;&gt; &gt;&gt; &gt; run by the CSEE department, so we can&#39;t guar= antee that these<br /> &gt;&gt;&gt; will run the<br /> &gt;&gt;&gt; &gt;&gt; &gt; same way or as effectively.<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt; If that&#39;s all ok with you, please give me so= me time to get the<br /> &gt;&gt;&gt; directory<br /> &gt;&gt;&gt; &gt;&gt; &gt; mounted.<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt; On Wed Oct 22 13:26:05 2025, NQ23652 wrote:<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thanks Tartela for your help to resolve my i= ssue!<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Actually it seems all server in CSEE departm= ent have access<br /> &gt;&gt;&gt; to that<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; path. When I ran the command that you gave m= e I got the<br /> &gt;&gt;&gt; following<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; report:<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; mohammad@alborz:~$ df -h /umbc/software<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Filesystem Size Used Avail Use% Mounted on<b= r /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; nfs.iss.rs.umbc.edu:/ifs/data/csee/software = 2.5T 2.2T 326G<br /> &gt;&gt;&gt; 88%<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; /umbc/software<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; mohammad@alborz:~$ mount | grep /umbc/softwa= re<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; nfs.iss.rs.umbc.edu:/ifs/data/csee/software = on /umbc/software<br /> &gt;&gt;&gt; type nfs<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namlen=3D2= 55,acregmin=3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,proto= =3Dtcp,timeo=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountvers= =3D3,mountport=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.60)<= br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Please let me know if you need other informa= tion.<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Regards,<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; Mohammad<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt; On Wed Oct 22 13:15:12 2025, HK41259 wrote:<= br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Hello,<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Thanks for reaching out. Could you clari= fy which specific<br /> &gt;&gt;&gt; machine<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; or server you&rsquo;re referring to when=  you mention the<br /> &gt;&gt;&gt; &ldquo;departmental<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; server&rdquo;?<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; The /umbc/software/scripts/ directory is= n&rsquo;t accessible from<br /> &gt;&gt;&gt; HPCF<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; (e.g., Chip) by default &mdash; those sy= stems have separate<br /> &gt;&gt;&gt; storage and<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; don&rsquo;t automatically mount departme= ntal shares.<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Could you check where /umbc/software is = mounted on your<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; departmental system (e.g., by running df=  -h /umbc/software<br /> &gt;&gt;&gt; or mount<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; | grep /umbc/software)? That&rsquo;ll te= ll us which server hosts<br /> &gt;&gt;&gt; it. Once<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; we know that, we can confirm whether it&= rsquo;s possible or<br /> &gt;&gt;&gt; appropriate<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; to make it visible from HPCF.<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Best,<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Tartela<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; On Tue Oct 21 15:59:49 2025, ZZ99999 wro= te:<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; First Name: Mohammad<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Last Name: Ebrahimabadi<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Email: e127@umbc.edu &lt;mailto:e127= @umbc.edu&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Campus ID: NQ23652<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Request Type: High Performance Clust= er<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Dear Team,<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; I hope you&rsquo;re doing well.<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; I&rsquo;m a Ph.D. student in Compute= r Engineering and have a<br /> &gt;&gt;&gt; question regarding running a software on the HPCF. On the<br /> &gt;&gt;&gt; department server, I usually run software using shell scripts<= br /> &gt;&gt;&gt; located at /umbc/software/scripts/ which is provided by the IT= <br /> &gt;&gt;&gt; group pf the department (Geoff Weiss Team). However, in case o= f<br /> &gt;&gt;&gt; HPCF, it seems that this path is not accessible from the HPCF<= br /> &gt;&gt;&gt; environment.<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Could you please let me know if ther= e is any specific<br /> &gt;&gt;&gt; configuration or access permission required to reach this dire= ctory<br /> &gt;&gt;&gt; through the HPCF? or direct me to a related person that can he= lp<br /> &gt;&gt;&gt; me.<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Best regards,<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Mohammad<br /> &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt; --<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt; &gt; Best,<br /> &gt;&gt;&gt; &gt;&gt; &gt; Max Breitmeyer<br /> &gt;&gt;&gt; &gt;&gt; &gt; DOIT HPC System Administrator<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt;&gt; &gt;<br /> &gt;&gt;&gt; &gt;<br /> &gt;&gt;&gt; &gt;<br /> &gt;&gt;&gt; &gt; --<br /> &gt;&gt;&gt; &gt; Naghmeh Karimi, Ph.D.<br /> &gt;&gt;&gt; &gt; Associate Professor<br /> &gt;&gt;&gt; &gt; Department of Computer Science and Electrical Engineering= <br /> &gt;&gt;&gt; &gt; University of Maryland, Baltimore County<br /> &gt;&gt;&gt; &gt; Baltimore, MD 21250<br /> &gt;&gt;&gt; &gt; Tel: 410-455-3965 E-mail: nkarimi@umbc.edu<br /> &gt;&gt;&gt; &lt;mailto:nkarimi@umbc.edu&gt;<br /> &gt;&gt;&gt; &gt; Web: http://www.csee.umbc.edu/~nkarimi/<br /> &gt;&gt;&gt; &lt;https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~n= karimi/&amp;source=3Dgmail-imap&amp;ust=3D1761768891000000&amp;usg=3DAOvVaw= 0xLE02sw5-ueWRHyHyCwkN&gt;<br /> <br /> &gt;&gt; -- Naghmeh Karimi, Ph.D.<br /> &gt;&gt; Associate Professor<br /> &gt;&gt; Department of Computer Science and Electrical Engineering<br /> &gt;&gt; University of Maryland, Baltimore County<br /> &gt;&gt; Baltimore, MD 21250<br /> &gt;&gt; Tel: 410-455-3965 E-mail: nkarimi@umbc.edu<br /> &gt;&gt; Web: http://www.csee.umbc.edu/~nkarimi/<br /> <br /> &gt; --<br /> <br /> &gt; Best,<br /> &gt; Max Breitmeyer<br /> &gt; DOIT HPC System Administrator<br /> <br /> &nbsp;</blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --  <div> <div> <div> <div>V/R,</div>  <div>Maxwell Breitmeyer</div>  <div>UMBC HPCF Specialist</div>  <div>UMBC Observatory IT Manager <div>Graduate Student</div>  <div>(443) 835-8250</div> </div> </div> </div> </div> </blockquote> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3296910,72470310,Correspond,DoIT-Research-Computing,2025-10-24 01:32:54.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,NULL,naghmeh.karimi@umbc.edu,"Hi Max, thanks for getting back to us.  Hi Skye, we need your help here to find the file locations. Could you please help to resolve this issue?  Thanks a lot Naghmeh  Naghmeh Karimi, Ph.D. Associate Professor Department of Computer Science and Electrical Engineering University of Maryland, Baltimore County Baltimore, MD 21250 Tel: *410-455-3965* E-mail: *nkarimi@umbc.edu <nkarimi@umbc.edu>* Web: *http://www.csee.umbc.edu/~nkarimi/ <http://www.csee.umbc.edu/~nkarimi/>*  On Thu, Oct 23, 2025, 12:22=E2=80=AFPM Max Breitmeyer via RT <UMBCHelp@rt.u= mbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 > > > Last Update From Ticket: > > Hi Mohammad, > > According to the error message, it's having an issue finding another file > in > the directory. This appears to be an issue due to the mounting locations > being > different. I would point you to the previous email where I said that any > software with hard-coded locations would not work, and that software > compatibility is not guaranteed. I would recommend working with Skye on > porting > what is in the directory to your local research directories on chip. > > On Thu Oct 23 12:09:34 2025, NQ23652 wrote: > > > Yes you are right, now I could see all shell files. However still I have > > problem when I run the sh file. > > > [e127@chip-login2 scripts]$ sh launch_synopsys_hspice.sh > > launch_synopsys_hspice.sh: line 3: > > /umbc/software/scripts/env_synopsys_hspice.sh: No such file or directory > > [e127@chip-login2 scripts]$ > > > Would you please take a look to it. > > > Regards, > > > Mohammad > > > On Thu Oct 23 11:59:17 2025, OL73413 wrote: > > >> Hi Mohammad, > > >> A good point. Chip uses a service called ""autofs"" to mount and unmount > >> directories that aren't being actively used. This helps maintain the > >> login nodes integrity. If you see in my screenshot below, the directory > >> appears to be empty, but when I cd to the csee directory where your > >> software is mounted, it is all available. Backing out of the directory > >> and ls'ing the directory also shows that the directory is now mounted > >> and available. > > >> On Thu, Oct 23, 2025 at 11:41 AM Mohammad Ebrahimabadi via RT > >> <UMBCHelp@rt.umbc.edu> wrote: > > >>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 > > > >>> Comment From Ticket: > >>> Thank you Max! > > >>> Actually we have still problem for accessing that directory. I > >>> uploaded two > >>> screenshots that shows what we can see in the department servers > >>> and what we > >>> can see in CHIP. > > >>> For example by running this shell file we can launch HSPICE > >>> software in the > >>> department server: > >>> /umbc/software/scripts/launch_synopsys_hspice.sh > >>> But it is not still accessible on CHIP. > > >>> Regards, > > >>> Mohammad > > >>> On Thu Oct 23 10:54:59 2025, OL73413 wrote: > > >>> > Hi Nahmeh, > > >>> > Apologies =E2=80=94 I meant to update you yesterday after completing > >>> this, but I > >>> > was pulled into a long meeting. The software is now available on > >>> chip at: > > >>> > /umbc/software/csee > > >>> > Please note that if the software contains any hardcoded file > >>> paths, it may > >>> > not function correctly. Unfortunately, we don=E2=80=99t have contro= l over > >>> those > >>> > cases. > > >>> > Also, just to clarify, Skye and I work in separate departments. > >>> Similar to > >>> > how I don=E2=80=99t have administrative access to the CSEE departme= nt > >>> servers > >>> > (which include the software directory), Skye is not an > >>> administrator on > >>> > chip. > > >>> > Please let me know if you encounter any issues accessing the > >>> software on > >>> > the server. If there are problems running the software, I=E2=80=99ll > >>> coordinate > >>> > with Skye to troubleshoot, but since we don=E2=80=99t maintain this > >>> software, we > >>> > can=E2=80=99t guarantee full compatibility on our systems. > > >>> > On Thu Oct 23 10:36:41 2025, naghmeh.karimi@umbc.edu wrote: > > >>> >> Hi Skye and Max, Could you please help on mounting this today so > >>> we can > >>> >> use them as we have a deadline? Thanks,Naghmeh Karimi On Wed, > >>> Oct 22, > >>> >> 2025 at 4:22 PM Skye Jonke via RT <UMBCHelp@rt.umbc.edu> wrote: > > >>> >>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 > >>> > > > >>> >>> Last Update From Ticket: > > >>> >>> Mohammad, > > >>> >>> So long as you continue to access them through > >>> >>> /umbc/software/scripts, they should remain up to date and you > >>> will > >>> >>> see any new scripts. If you copy them to a local directory on > >>> the > >>> >>> HPC cluster, they will not be updated. > > >>> >>> --- > >>> >>> Skye Jonke > > >>> >>> she/her > > >>> >>> Specialist, Linux System Administrator & Lab Technical Support > > >>> >>> > On Oct 22, 2025, at 16:14, Naghmeh Karimi <nkarimi@umbc.edu> > >>> >>> wrote: > >>> >>> > > >>> >>> > Hi Max, Roy, Skye, > >>> >>> > > >>> >>> > Thank you all for your help. > >>> >>> > > >>> >>> > I added Skye here as she is our IT exert on adding the > >>> capability > >>> >>> of running synopsis tools (Hspice) in servers. > >>> >>> > Skye we want to use the HPC cluster to run hspice an other > >>> >>> synopsys tools. Could you please help on this? > >>> >>> > > >>> >>> > Thanks, > >>> >>> > Naghmeh > >>> >>> > > >>> >>> > On Wed, Oct 22, 2025 at 2:06 PM Mohammad Ebrahimabadi via RT > >>> >>> <UMBCHelp@rt.umbc.edu <mailto:UMBCHelp@rt.umbc.edu>> wrote: > >>> >>> >> Ticket <URL: > >>> https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 > >>> >>> > >>> < > https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id= %3D3296910&source=3Dgmail-imap&ust=3D1761768891000000&usg=3DAOvVaw2jxB3nArH= BD7-ij9ZN8ZsW > > > >>> >>> > > >>> >>> >> > >>> >>> >> Last Update From Ticket: > >>> >>> >> > >>> >>> >> Hi Max, > >>> >>> >> > >>> >>> >> Thanks for your help! Let=E2=80=99s proceed based on your > >>> suggestion, > >>> >>> hopefully it > >>> >>> >> works as expected. > >>> >>> >> > >>> >>> >> I just have one quick question: since the IT team in the > >>> >>> department can modify > >>> >>> >> or add new shell scripts in that directory, will we also > >>> have > >>> >>> access to those > >>> >>> >> updated or newly added scripts from the mounted directory on > >>> >>> HPCF? > >>> >>> >> > >>> >>> >> Thanks again, > >>> >>> >> Mohammad > >>> >>> >> > >>> >>> >> On Wed Oct 22 14:01:32 2025, OL73413 wrote: > >>> >>> >> > >>> >>> >> > Hi Mohammad, > >>> >>> >> > >>> >>> >> > We can get the software mounted for you on chip, but it > >>> will > >>> >>> have to be > >>> >>> >> > read-only as we won't be able to manage who can make > >>> changes > >>> >>> to it > >>> >>> >> > otherwise. > >>> >>> >> > >>> >>> >> > As a reminder, the machines on that run on chip are not > >>> the > >>> >>> same as those > >>> >>> >> > run by the CSEE department, so we can't guarantee that > >>> these > >>> >>> will run the > >>> >>> >> > same way or as effectively. > >>> >>> >> > >>> >>> >> > If that's all ok with you, please give me some time to get > >>> the > >>> >>> directory > >>> >>> >> > mounted. > >>> >>> >> > >>> >>> >> > On Wed Oct 22 13:26:05 2025, NQ23652 wrote: > >>> >>> >> > >>> >>> >> >> Thanks Tartela for your help to resolve my issue! > >>> >>> >> > >>> >>> >> >> Actually it seems all server in CSEE department have > >>> access > >>> >>> to that > >>> >>> >> >> path. When I ran the command that you gave me I got the > >>> >>> following > >>> >>> >> >> report: > >>> >>> >> > >>> >>> >> >> mohammad@alborz:~$ df -h /umbc/software > >>> >>> >> >> Filesystem Size Used Avail Use% Mounted on > >>> >>> >> >> nfs.iss.rs.umbc.edu:/ifs/data/csee/software 2.5T 2.2T > >>> 326G > >>> >>> 88% > >>> >>> >> >> /umbc/software > >>> >>> >> > >>> >>> >> >> mohammad@alborz:~$ mount | grep /umbc/software > >>> >>> >> >> nfs.iss.rs.umbc.edu:/ifs/data/csee/software on > >>> /umbc/software > >>> >>> type nfs > >>> >>> >> >> > >>> >>> > >>> > (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namlen=3D255,acregmin= =3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,proto=3Dtcp,time= o=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountvers=3D3,mountpor= t=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.60) > >>> >>> >> > >>> >>> >> >> Please let me know if you need other information. > >>> >>> >> > >>> >>> >> >> Regards, > >>> >>> >> > >>> >>> >> >> Mohammad > >>> >>> >> > >>> >>> >> >> On Wed Oct 22 13:15:12 2025, HK41259 wrote: > >>> >>> >> > >>> >>> >> >>> Hello, > >>> >>> >> > >>> >>> >> >>> Thanks for reaching out. Could you clarify which > >>> specific > >>> >>> machine > >>> >>> >> >>> or server you=E2=80=99re referring to when you mention the > >>> >>> =E2=80=9Cdepartmental > >>> >>> >> >>> server=E2=80=9D? > >>> >>> >> > >>> >>> >> >>> The /umbc/software/scripts/ directory isn=E2=80=99t access= ible > >>> from > >>> >>> HPCF > >>> >>> >> >>> (e.g., Chip) by default =E2=80=94 those systems have separ= ate > >>> >>> storage and > >>> >>> >> >>> don=E2=80=99t automatically mount departmental shares. > >>> >>> >> > >>> >>> >> >>> Could you check where /umbc/software is mounted on your > >>> >>> >> >>> departmental system (e.g., by running df -h > >>> /umbc/software > >>> >>> or mount > >>> >>> >> >>> | grep /umbc/software)? That=E2=80=99ll tell us which serv= er > >>> hosts > >>> >>> it. Once > >>> >>> >> >>> we know that, we can confirm whether it=E2=80=99s possible=  or > >>> >>> appropriate > >>> >>> >> >>> to make it visible from HPCF. > >>> >>> >> > >>> >>> >> >>> Best, > >>> >>> >> > >>> >>> >> >>> Tartela > >>> >>> >> > >>> >>> >> >>> On Tue Oct 21 15:59:49 2025, ZZ99999 wrote: > >>> >>> >> > >>> >>> >> >>>> First Name: Mohammad > >>> >>> >> >>>> Last Name: Ebrahimabadi > >>> >>> >> >>>> Email: e127@umbc.edu <mailto:e127@umbc.edu> > >>> >>> >> >>>> Campus ID: NQ23652 > >>> >>> >> >>>> > >>> >>> >> >>>> Request Type: High Performance Cluster > >>> >>> >> >>>> > >>> >>> >> >>>> Dear Team, > >>> >>> >> >>>> > >>> >>> >> >>>> I hope you=E2=80=99re doing well. > >>> >>> >> >>>> I=E2=80=99m a Ph.D. student in Computer Engineering and h= ave a > >>> >>> question regarding running a software on the HPCF. On the > >>> >>> department server, I usually run software using shell scripts > >>> >>> located at /umbc/software/scripts/ which is provided by the IT > >>> >>> group pf the department (Geoff Weiss Team). However, in case of > >>> >>> HPCF, it seems that this path is not accessible from the HPCF > >>> >>> environment. > >>> >>> >> >>>> > >>> >>> >> >>>> Could you please let me know if there is any specific > >>> >>> configuration or access permission required to reach this > >>> directory > >>> >>> through the HPCF? or direct me to a related person that can > >>> help > >>> >>> me. > >>> >>> >> >>>> > >>> >>> >> >>>> Best regards, > >>> >>> >> >>>> Mohammad > >>> >>> >> >>> > >>> >>> >> > >>> >>> >> > -- > >>> >>> >> > >>> >>> >> > Best, > >>> >>> >> > Max Breitmeyer > >>> >>> >> > DOIT HPC System Administrator > >>> >>> >> > >>> >>> >> > >>> >>> > > >>> >>> > > >>> >>> > > >>> >>> > -- > >>> >>> > Naghmeh Karimi, Ph.D. > >>> >>> > Associate Professor > >>> >>> > Department of Computer Science and Electrical Engineering > >>> >>> > University of Maryland, Baltimore County > >>> >>> > Baltimore, MD 21250 > >>> >>> > Tel: 410-455-3965 E-mail: nkarimi@umbc.edu > >>> >>> <mailto:nkarimi@umbc.edu> > >>> >>> > Web: http://www.csee.umbc.edu/~nkarimi/ > >>> >>> > >>> < > https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source= =3Dgmail-imap&ust=3D1761768891000000&usg=3DAOvVaw0xLE02sw5-ueWRHyHyCwkN > > > > >>> >> -- Naghmeh Karimi, Ph.D. > >>> >> Associate Professor > >>> >> Department of Computer Science and Electrical Engineering > >>> >> University of Maryland, Baltimore County > >>> >> Baltimore, MD 21250 > >>> >> Tel: 410-455-3965 E-mail: nkarimi@umbc.edu > >>> >> Web: http://www.csee.umbc.edu/~nkarimi/ > > >>> > -- > > >>> > Best, > >>> > Max Breitmeyer > >>> > DOIT HPC System Administrator > > >> -- V/R,Maxwell BreitmeyerUMBC HPCF SpecialistUMBC Observatory IT > >> Manager Graduate Student(443) 835-8250 > > -- > > Best, > Max Breitmeyer > DOIT HPC System Administrator > > > "
3296910,72470310,Correspond,DoIT-Research-Computing,2025-10-24 01:32:54.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,NULL,naghmeh.karimi@umbc.edu,"<div dir=3D""auto""><div>Hi Max, thanks=C2=A0for getting back to us.</div><di= v dir=3D""auto""><br></div><div dir=3D""auto"">Hi Skye, we need your help here = to find the file locations. Could you please help to resolve=C2=A0this issu= e?</div><div dir=3D""auto""><br></div><div dir=3D""auto"">Thanks a lot</div><di= v dir=3D""auto"">Naghmeh=C2=A0</div><div><br></div><div data-smartmail=3D""gma= il_signature""><div dir=3D""ltr""><span style=3D""font-size:9.5pt;line-height:1= 15%;font-family:Arial,sans-serif;color:rgb(136,136,136);background-image:in= itial;background-position:initial;background-repeat:initial"">Naghmeh=C2=A0K= arimi, Ph.D.<br> Associate Professor<br> Department of Computer Science and Electrical Engineering<br> University of Maryland, Baltimore County<br> Baltimore, MD 21250<br> Tel:</span><span style=3D""font-size:11.0pt;line-height:115%;font-family:&qu= ot;Calibri&quot;,&quot;sans-serif&quot;;color:black"">=C2=A0</span><u><span = style=3D""font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,= &quot;sans-serif&quot;;color:#1155cc"">410-455-3965</span></u><span style=3D= ""font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sa= ns-serif&quot;;color:black""> </span><span style=3D""font-size:9.5pt;line-hei= ght:115%;font-family:Arial,sans-serif;color:rgb(136,136,136);background-ima= ge:initial;background-position:initial;background-repeat:initial"">E-mail:</= span><span style=3D""font-size:11.0pt;line-height:115%;font-family:&quot;Cal= ibri&quot;,&quot;sans-serif&quot;;color:black"">=C2=A0</span><u><span style= =3D""font-size:11.0pt;line-height:115%;font-family:&quot;Calibri&quot;,&quot= ;sans-serif&quot;;color:#1155cc""><a href=3D""mailto:nkarimi@umbc.edu"" target= =3D""_blank"">nkarimi@umbc.edu</a></span></u><span style=3D""font-size:9.5pt;l= ine-height:115%;font-family:Arial,sans-serif;color:rgb(136,136,136);backgro= und-image:initial;background-position:initial;background-repeat:initial""><b= r> Web:</span><span style=3D""font-size:9.5pt;line-height:115%;font-family:Aria= l,sans-serif;color:black;background-image:initial;background-position:initi= al;background-repeat:initial"">=C2=A0</span><u><span style=3D""font-size:11pt= ;line-height:115%;font-family:Calibri,sans-serif;color:rgb(17,85,204);backg= round-image:initial;background-position:initial;background-repeat:initial"">= <a href=3D""http://www.csee.umbc.edu/~nkarimi/"" target=3D""_blank"">http://www= .csee.umbc.edu/~nkarimi/</a></span></u></div></div></div><br><div class=3D""= gmail_quote gmail_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On=  Thu, Oct 23, 2025, 12:22=E2=80=AFPM Max Breitmeyer via RT &lt;<a href=3D""m= ailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><b= lockquote class=3D""gmail_quote"" style=3D""margin:0 0 0 .8ex;border-left:1px = #ccc solid;padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu= /Ticket/Display.html?id=3D3296910"" rel=3D""noreferrer noreferrer"" target=3D""= _blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3296910</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Mohammad,<br> <br> According to the error message, it&#39;s having an issue finding another fi= le in<br> the directory. This appears to be an issue due to the mounting locations be= ing<br> different. I would point you to the previous email where I said that any<br> software with hard-coded locations would not work, and that software<br> compatibility is not guaranteed. I would recommend working with Skye on por= ting<br> what is in the directory to your local research directories on chip.<br> <br> On Thu Oct 23 12:09:34 2025, NQ23652 wrote:<br> <br> &gt; Yes you are right, now I could see all shell files. However still I ha= ve<br> &gt; problem when I run the sh file.<br> <br> &gt; [e127@chip-login2 scripts]$ sh launch_synopsys_hspice.sh<br> &gt; launch_synopsys_hspice.sh: line 3:<br> &gt; /umbc/software/scripts/env_synopsys_hspice.sh: No such file or directo= ry<br> &gt; [e127@chip-login2 scripts]$<br> <br> &gt; Would you please take a look to it.<br> <br> &gt; Regards,<br> <br> &gt; Mohammad<br> <br> &gt; On Thu Oct 23 11:59:17 2025, OL73413 wrote:<br> <br> &gt;&gt; Hi Mohammad,<br> <br> &gt;&gt; A good point. Chip uses a service called &quot;autofs&quot; to mou= nt and unmount<br> &gt;&gt; directories that aren&#39;t being actively used. This helps mainta= in the<br> &gt;&gt; login nodes integrity. If you see in my screenshot below, the dire= ctory<br> &gt;&gt; appears to be empty, but when I cd to the csee directory where you= r<br> &gt;&gt; software is mounted, it is all available. Backing out of the direc= tory<br> &gt;&gt; and ls&#39;ing the directory also shows that the directory is now = mounted<br> &gt;&gt; and available.<br> <br> &gt;&gt; On Thu, Oct 23, 2025 at 11:41 AM Mohammad Ebrahimabadi via RT<br> &gt;&gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"" rel= =3D""noreferrer"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.= html?id=3D3296910"" rel=3D""noreferrer noreferrer"" target=3D""_blank"">https://= rt.umbc.edu/Ticket/Display.html?id=3D3296910</a> &gt;<br> <br> &gt;&gt;&gt; Comment From Ticket:<br> &gt;&gt;&gt; Thank you Max!<br> <br> &gt;&gt;&gt; Actually we have still problem for accessing that directory. I= <br> &gt;&gt;&gt; uploaded two<br> &gt;&gt;&gt; screenshots that shows what we can see in the department serve= rs<br> &gt;&gt;&gt; and what we<br> &gt;&gt;&gt; can see in CHIP.<br> <br> &gt;&gt;&gt; For example by running this shell file we can launch HSPICE<br> &gt;&gt;&gt; software in the<br> &gt;&gt;&gt; department server:<br> &gt;&gt;&gt; /umbc/software/scripts/launch_synopsys_hspice.sh<br> &gt;&gt;&gt; But it is not still accessible on CHIP.<br> <br> &gt;&gt;&gt; Regards,<br> <br> &gt;&gt;&gt; Mohammad<br> <br> &gt;&gt;&gt; On Thu Oct 23 10:54:59 2025, OL73413 wrote:<br> <br> &gt;&gt;&gt; &gt; Hi Nahmeh,<br> <br> &gt;&gt;&gt; &gt; Apologies =E2=80=94 I meant to update you yesterday after=  completing<br> &gt;&gt;&gt; this, but I<br> &gt;&gt;&gt; &gt; was pulled into a long meeting. The software is now avail= able on<br> &gt;&gt;&gt; chip at:<br> <br> &gt;&gt;&gt; &gt; /umbc/software/csee<br> <br> &gt;&gt;&gt; &gt; Please note that if the software contains any hardcoded f= ile<br> &gt;&gt;&gt; paths, it may<br> &gt;&gt;&gt; &gt; not function correctly. Unfortunately, we don=E2=80=99t h= ave control over<br> &gt;&gt;&gt; those<br> &gt;&gt;&gt; &gt; cases.<br> <br> &gt;&gt;&gt; &gt; Also, just to clarify, Skye and I work in separate depart= ments.<br> &gt;&gt;&gt; Similar to<br> &gt;&gt;&gt; &gt; how I don=E2=80=99t have administrative access to the CSE= E department<br> &gt;&gt;&gt; servers<br> &gt;&gt;&gt; &gt; (which include the software directory), Skye is not an<br> &gt;&gt;&gt; administrator on<br> &gt;&gt;&gt; &gt; chip.<br> <br> &gt;&gt;&gt; &gt; Please let me know if you encounter any issues accessing = the<br> &gt;&gt;&gt; software on<br> &gt;&gt;&gt; &gt; the server. If there are problems running the software, I= =E2=80=99ll<br> &gt;&gt;&gt; coordinate<br> &gt;&gt;&gt; &gt; with Skye to troubleshoot, but since we don=E2=80=99t mai= ntain this<br> &gt;&gt;&gt; software, we<br> &gt;&gt;&gt; &gt; can=E2=80=99t guarantee full compatibility on our systems= .<br> <br> &gt;&gt;&gt; &gt; On Thu Oct 23 10:36:41 2025, <a href=3D""mailto:naghmeh.ka= rimi@umbc.edu"" target=3D""_blank"" rel=3D""noreferrer"">naghmeh.karimi@umbc.edu= </a> wrote:<br> <br> &gt;&gt;&gt; &gt;&gt; Hi Skye and Max, Could you please help on mounting th= is today so<br> &gt;&gt;&gt; we can<br> &gt;&gt;&gt; &gt;&gt; use them as we have a deadline? Thanks,Naghmeh Karimi=  On Wed,<br> &gt;&gt;&gt; Oct 22,<br> &gt;&gt;&gt; &gt;&gt; 2025 at 4:22 PM Skye Jonke via RT &lt;<a href=3D""mail= to:UMBCHelp@rt.umbc.edu"" target=3D""_blank"" rel=3D""noreferrer"">UMBCHelp@rt.u= mbc.edu</a>&gt; wrote:<br> <br> &gt;&gt;&gt; &gt;&gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ti= cket/Display.html?id=3D3296910"" rel=3D""noreferrer noreferrer"" target=3D""_bl= ank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3296910</a><br> &gt;&gt;&gt; &gt;<br> <br> &gt;&gt;&gt; &gt;&gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt;&gt; &gt;&gt;&gt; Mohammad,<br> <br> &gt;&gt;&gt; &gt;&gt;&gt; So long as you continue to access them through<br> &gt;&gt;&gt; &gt;&gt;&gt; /umbc/software/scripts, they should remain up to = date and you<br> &gt;&gt;&gt; will<br> &gt;&gt;&gt; &gt;&gt;&gt; see any new scripts. If you copy them to a local = directory on<br> &gt;&gt;&gt; the<br> &gt;&gt;&gt; &gt;&gt;&gt; HPC cluster, they will not be updated.<br> <br> &gt;&gt;&gt; &gt;&gt;&gt; ---<br> &gt;&gt;&gt; &gt;&gt;&gt; Skye Jonke<br> <br> &gt;&gt;&gt; &gt;&gt;&gt; she/her<br> <br> &gt;&gt;&gt; &gt;&gt;&gt; Specialist, Linux System Administrator &amp; Lab = Technical Support<br> <br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; On Oct 22, 2025, at 16:14, Naghmeh Karimi &l= t;<a href=3D""mailto:nkarimi@umbc.edu"" target=3D""_blank"" rel=3D""noreferrer"">= nkarimi@umbc.edu</a>&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; wrote:<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Hi Max, Roy, Skye,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Thank you all for your help.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; I added Skye here as she is our IT exert on = adding the<br> &gt;&gt;&gt; capability<br> &gt;&gt;&gt; &gt;&gt;&gt; of running synopsis tools (Hspice) in servers.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Skye we want to use the HPC cluster to run h= spice an other<br> &gt;&gt;&gt; &gt;&gt;&gt; synopsys tools. Could you please help on this?<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Thanks,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Naghmeh<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; On Wed, Oct 22, 2025 at 2:06 PM Mohammad Ebr= ahimabadi via RT<br> &gt;&gt;&gt; &gt;&gt;&gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" targe= t=3D""_blank"" rel=3D""noreferrer"">UMBCHelp@rt.umbc.edu</a> &lt;mailto:<a href= =3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"" rel=3D""noreferrer"">UMBCH= elp@rt.umbc.edu</a>&gt;&gt; wrote:<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Ticket &lt;URL:<br> &gt;&gt;&gt; <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D329691= 0"" rel=3D""noreferrer noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tick= et/Display.html?id=3D3296910</a><br> &gt;&gt;&gt; &gt;&gt;&gt;<br> &gt;&gt;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttps://rt.umbc.= edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761= 768891000000&amp;usg=3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW"" rel=3D""noreferrer nore= ferrer"" target=3D""_blank"">https://www.google.com/url?q=3Dhttps://rt.umbc.ed= u/Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D176176= 8891000000&amp;usg=3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW</a>&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Last Update From Ticket:<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Hi Max,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Thanks for your help! Let=E2=80=99s proc= eed based on your<br> &gt;&gt;&gt; suggestion,<br> &gt;&gt;&gt; &gt;&gt;&gt; hopefully it<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; works as expected.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; I just have one quick question: since th= e IT team in the<br> &gt;&gt;&gt; &gt;&gt;&gt; department can modify<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; or add new shell scripts in that directo= ry, will we also<br> &gt;&gt;&gt; have<br> &gt;&gt;&gt; &gt;&gt;&gt; access to those<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; updated or newly added scripts from the = mounted directory on<br> &gt;&gt;&gt; &gt;&gt;&gt; HPCF?<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Thanks again,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Mohammad<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; On Wed Oct 22 14:01:32 2025, OL73413 wro= te:<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Hi Mohammad,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; We can get the software mounted for=  you on chip, but it<br> &gt;&gt;&gt; will<br> &gt;&gt;&gt; &gt;&gt;&gt; have to be<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; read-only as we won&#39;t be able t= o manage who can make<br> &gt;&gt;&gt; changes<br> &gt;&gt;&gt; &gt;&gt;&gt; to it<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; otherwise.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; As a reminder, the machines on that=  run on chip are not<br> &gt;&gt;&gt; the<br> &gt;&gt;&gt; &gt;&gt;&gt; same as those<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; run by the CSEE department, so we c= an&#39;t guarantee that<br> &gt;&gt;&gt; these<br> &gt;&gt;&gt; &gt;&gt;&gt; will run the<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; same way or as effectively.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; If that&#39;s all ok with you, plea= se give me some time to get<br> &gt;&gt;&gt; the<br> &gt;&gt;&gt; &gt;&gt;&gt; directory<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; mounted.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; On Wed Oct 22 13:26:05 2025, NQ2365= 2 wrote:<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thanks Tartela for your help to=  resolve my issue!<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Actually it seems all server in=  CSEE department have<br> &gt;&gt;&gt; access<br> &gt;&gt;&gt; &gt;&gt;&gt; to that<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; path. When I ran the command th= at you gave me I got the<br> &gt;&gt;&gt; &gt;&gt;&gt; following<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; report:<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; mohammad@alborz:~$ df -h /umbc/= software<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Filesystem Size Used Avail Use%=  Mounted on<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; nfs.iss.rs.umbc.edu:/ifs/data/c= see/software 2.5T 2.2T<br> &gt;&gt;&gt; 326G<br> &gt;&gt;&gt; &gt;&gt;&gt; 88%<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; /umbc/software<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; mohammad@alborz:~$ mount | grep=  /umbc/software<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; nfs.iss.rs.umbc.edu:/ifs/data/c= see/software on<br> &gt;&gt;&gt; /umbc/software<br> &gt;&gt;&gt; &gt;&gt;&gt; type nfs<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt;<br> &gt;&gt;&gt; (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namlen=3D2= 55,acregmin=3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,proto= =3Dtcp,timeo=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountvers= =3D3,mountport=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.60)<= br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Please let me know if you need = other information.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Regards,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Mohammad<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; On Wed Oct 22 13:15:12 2025, HK= 41259 wrote:<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Hello,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Thanks for reaching out. Co= uld you clarify which<br> &gt;&gt;&gt; specific<br> &gt;&gt;&gt; &gt;&gt;&gt; machine<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; or server you=E2=80=99re re= ferring to when you mention the<br> &gt;&gt;&gt; &gt;&gt;&gt; =E2=80=9Cdepartmental<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; server=E2=80=9D?<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; The /umbc/software/scripts/=  directory isn=E2=80=99t accessible<br> &gt;&gt;&gt; from<br> &gt;&gt;&gt; &gt;&gt;&gt; HPCF<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; (e.g., Chip) by default =E2= =80=94 those systems have separate<br> &gt;&gt;&gt; &gt;&gt;&gt; storage and<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; don=E2=80=99t automatically=  mount departmental shares.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Could you check where /umbc= /software is mounted on your<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; departmental system (e.g., = by running df -h<br> &gt;&gt;&gt; /umbc/software<br> &gt;&gt;&gt; &gt;&gt;&gt; or mount<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; | grep /umbc/software)? Tha= t=E2=80=99ll tell us which server<br> &gt;&gt;&gt; hosts<br> &gt;&gt;&gt; &gt;&gt;&gt; it. Once<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; we know that, we can confir= m whether it=E2=80=99s possible or<br> &gt;&gt;&gt; &gt;&gt;&gt; appropriate<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; to make it visible from HPC= F.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Best,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Tartela<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; On Tue Oct 21 15:59:49 2025= , ZZ99999 wrote:<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; First Name: Mohammad<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Last Name: Ebrahimabadi= <br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Email: <a href=3D""mailt= o:e127@umbc.edu"" target=3D""_blank"" rel=3D""noreferrer"">e127@umbc.edu</a> &lt= ;mailto:<a href=3D""mailto:e127@umbc.edu"" target=3D""_blank"" rel=3D""noreferre= r"">e127@umbc.edu</a>&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Campus ID: NQ23652<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Request Type: High Perf= ormance Cluster<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Dear Team,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; I hope you=E2=80=99re d= oing well.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; I=E2=80=99m a Ph.D. stu= dent in Computer Engineering and have a<br> &gt;&gt;&gt; &gt;&gt;&gt; question regarding running a software on the HPCF= . On the<br> &gt;&gt;&gt; &gt;&gt;&gt; department server, I usually run software using s= hell scripts<br> &gt;&gt;&gt; &gt;&gt;&gt; located at /umbc/software/scripts/ which is provi= ded by the IT<br> &gt;&gt;&gt; &gt;&gt;&gt; group pf the department (Geoff Weiss Team). Howev= er, in case of<br> &gt;&gt;&gt; &gt;&gt;&gt; HPCF, it seems that this path is not accessible f= rom the HPCF<br> &gt;&gt;&gt; &gt;&gt;&gt; environment.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Could you please let me=  know if there is any specific<br> &gt;&gt;&gt; &gt;&gt;&gt; configuration or access permission required to re= ach this<br> &gt;&gt;&gt; directory<br> &gt;&gt;&gt; &gt;&gt;&gt; through the HPCF? or direct me to a related perso= n that can<br> &gt;&gt;&gt; help<br> &gt;&gt;&gt; &gt;&gt;&gt; me.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Best regards,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Mohammad<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; --<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Best,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Max Breitmeyer<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; DOIT HPC System Administrator<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; --<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Naghmeh Karimi, Ph.D.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Associate Professor<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Department of Computer Science and Electrica= l Engineering<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; University of Maryland, Baltimore County<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Baltimore, MD 21250<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Tel: 410-455-3965 E-mail: <a href=3D""mailto:= nkarimi@umbc.edu"" target=3D""_blank"" rel=3D""noreferrer"">nkarimi@umbc.edu</a>= <br> &gt;&gt;&gt; &gt;&gt;&gt; &lt;mailto:<a href=3D""mailto:nkarimi@umbc.edu"" ta= rget=3D""_blank"" rel=3D""noreferrer"">nkarimi@umbc.edu</a>&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Web: <a href=3D""http://www.csee.umbc.edu/~nk= arimi/"" rel=3D""noreferrer noreferrer"" target=3D""_blank"">http://www.csee.umb= c.edu/~nkarimi/</a><br> &gt;&gt;&gt; &gt;&gt;&gt;<br> &gt;&gt;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttp://www.csee.= umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761768891000000&amp;u= sg=3DAOvVaw0xLE02sw5-ueWRHyHyCwkN"" rel=3D""noreferrer noreferrer"" target=3D""= _blank"">https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&a= mp;source=3Dgmail-imap&amp;ust=3D1761768891000000&amp;usg=3DAOvVaw0xLE02sw5= -ueWRHyHyCwkN</a>&gt;<br> <br> &gt;&gt;&gt; &gt;&gt; -- Naghmeh Karimi, Ph.D.<br> &gt;&gt;&gt; &gt;&gt; Associate Professor<br> &gt;&gt;&gt; &gt;&gt; Department of Computer Science and Electrical Enginee= ring<br> &gt;&gt;&gt; &gt;&gt; University of Maryland, Baltimore County<br> &gt;&gt;&gt; &gt;&gt; Baltimore, MD 21250<br> &gt;&gt;&gt; &gt;&gt; Tel: 410-455-3965 E-mail: <a href=3D""mailto:nkarimi@u= mbc.edu"" target=3D""_blank"" rel=3D""noreferrer"">nkarimi@umbc.edu</a><br> &gt;&gt;&gt; &gt;&gt; Web: <a href=3D""http://www.csee.umbc.edu/~nkarimi/"" r= el=3D""noreferrer noreferrer"" target=3D""_blank"">http://www.csee.umbc.edu/~nk= arimi/</a><br> <br> &gt;&gt;&gt; &gt; --<br> <br> &gt;&gt;&gt; &gt; Best,<br> &gt;&gt;&gt; &gt; Max Breitmeyer<br> &gt;&gt;&gt; &gt; DOIT HPC System Administrator<br> <br> &gt;&gt; -- V/R,Maxwell BreitmeyerUMBC HPCF SpecialistUMBC Observatory IT<b= r> &gt;&gt; Manager Graduate Student(443) 835-8250<br> <br> --<br> <br> Best,<br> Max Breitmeyer<br> DOIT HPC System Administrator<br> <br> <br> </blockquote></div> "
3296910,72474720,Correspond,DoIT-Research-Computing,2025-10-24 13:39:57.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Skye Jonke,ii69854@umbc.edu,"Its not possible to use the existing scripts in a situation where the files=  aren=E2=80=99t mounted in /umbc/software, but I can write some scripts spe= cific to this setup to this situation with a bit more information. Where ar= e the script files located on the HPC? If you aren=E2=80=99t sure, you can = get the full path by running =E2=80=9Crealpath myfile.sh=E2=80=9D (replacin= g myfile.sh with one of the cadence scripts, of course)=20 --- Skye Jonke  she/her  Specialist, Linux System Administrator & Lab Technical Support  > On Oct 23, 2025, at 21:32, Naghmeh Karimi <nkarimi@umbc.edu> wrote: >=20 > Hi Max, thanks for getting back to us. >=20 > Hi Skye, we need your help here to find the file locations. Could you ple= ase help to resolve this issue? >=20 > Thanks a lot > Naghmeh=20 >=20 > Naghmeh Karimi, Ph.D. > Associate Professor > Department of Computer Science and Electrical Engineering > University of Maryland, Baltimore County > Baltimore, MD 21250 > Tel: 410-455-3965 E-mail: nkarimi@umbc.edu <mailto:nkarimi@umbc.edu> > Web: http://www.csee.umbc.edu/~nkarimi/ <https://www.google.com/url?q=3Dh= ttp://www.csee.umbc.edu/~nkarimi/&source=3Dgmail-imap&ust=3D176187437400000= 0&usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW> > On Thu, Oct 23, 2025, 12:22=E2=80=AFPM Max Breitmeyer via RT <UMBCHelp@rt= .umbc.edu <mailto:UMBCHelp@rt.umbc.edu>> wrote: >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 <https= ://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3296= 910&source=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw3ImziGtKZ-9j9JFW= 8J65w7> > >>=20 >> Last Update From Ticket: >>=20 >> Hi Mohammad, >>=20 >> According to the error message, it's having an issue finding another fil= e in >> the directory. This appears to be an issue due to the mounting locations=  being >> different. I would point you to the previous email where I said that any >> software with hard-coded locations would not work, and that software >> compatibility is not guaranteed. I would recommend working with Skye on = porting >> what is in the directory to your local research directories on chip. >>=20 >> On Thu Oct 23 12:09:34 2025, NQ23652 wrote: >>=20 >> > Yes you are right, now I could see all shell files. However still I ha= ve >> > problem when I run the sh file. >>=20 >> > [e127@chip-login2 scripts]$ sh launch_synopsys_hspice.sh >> > launch_synopsys_hspice.sh: line 3: >> > /umbc/software/scripts/env_synopsys_hspice.sh: No such file or directo= ry >> > [e127@chip-login2 scripts]$ >>=20 >> > Would you please take a look to it. >>=20 >> > Regards, >>=20 >> > Mohammad >>=20 >> > On Thu Oct 23 11:59:17 2025, OL73413 wrote: >>=20 >> >> Hi Mohammad, >>=20 >> >> A good point. Chip uses a service called ""autofs"" to mount and unmount >> >> directories that aren't being actively used. This helps maintain the >> >> login nodes integrity. If you see in my screenshot below, the directo= ry >> >> appears to be empty, but when I cd to the csee directory where your >> >> software is mounted, it is all available. Backing out of the directory >> >> and ls'ing the directory also shows that the directory is now mounted >> >> and available. >>=20 >> >> On Thu, Oct 23, 2025 at 11:41 AM Mohammad Ebrahimabadi via RT >> >> <UMBCHelp@rt.umbc.edu <mailto:UMBCHelp@rt.umbc.edu>> wrote: >>=20 >> >>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 <h= ttps://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D= 3296910&source=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw3ImziGtKZ-9j= 9JFW8J65w7> > >>=20 >> >>> Comment From Ticket: >> >>> Thank you Max! >>=20 >> >>> Actually we have still problem for accessing that directory. I >> >>> uploaded two >> >>> screenshots that shows what we can see in the department servers >> >>> and what we >> >>> can see in CHIP. >>=20 >> >>> For example by running this shell file we can launch HSPICE >> >>> software in the >> >>> department server: >> >>> /umbc/software/scripts/launch_synopsys_hspice.sh >> >>> But it is not still accessible on CHIP. >>=20 >> >>> Regards, >>=20 >> >>> Mohammad >>=20 >> >>> On Thu Oct 23 10:54:59 2025, OL73413 wrote: >>=20 >> >>> > Hi Nahmeh, >>=20 >> >>> > Apologies =E2=80=94 I meant to update you yesterday after completi= ng >> >>> this, but I >> >>> > was pulled into a long meeting. The software is now available on >> >>> chip at: >>=20 >> >>> > /umbc/software/csee >>=20 >> >>> > Please note that if the software contains any hardcoded file >> >>> paths, it may >> >>> > not function correctly. Unfortunately, we don=E2=80=99t have contr= ol over >> >>> those >> >>> > cases. >>=20 >> >>> > Also, just to clarify, Skye and I work in separate departments. >> >>> Similar to >> >>> > how I don=E2=80=99t have administrative access to the CSEE departm= ent >> >>> servers >> >>> > (which include the software directory), Skye is not an >> >>> administrator on >> >>> > chip. >>=20 >> >>> > Please let me know if you encounter any issues accessing the >> >>> software on >> >>> > the server. If there are problems running the software, I=E2=80=99= ll >> >>> coordinate >> >>> > with Skye to troubleshoot, but since we don=E2=80=99t maintain this >> >>> software, we >> >>> > can=E2=80=99t guarantee full compatibility on our systems. >>=20 >> >>> > On Thu Oct 23 10:36:41 2025, naghmeh.karimi@umbc.edu <mailto:naghm= eh.karimi@umbc.edu> wrote: >>=20 >> >>> >> Hi Skye and Max, Could you please help on mounting this today so >> >>> we can >> >>> >> use them as we have a deadline? Thanks,Naghmeh Karimi On Wed, >> >>> Oct 22, >> >>> >> 2025 at 4:22 PM Skye Jonke via RT <UMBCHelp@rt.umbc.edu <mailto:U= MBCHelp@rt.umbc.edu>> wrote: >>=20 >> >>> >>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D329691= 0 <https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?i= d%3D3296910&source=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw3ImziGtK= Z-9j9JFW8J65w7> >> >>> > >>=20 >> >>> >>> Last Update From Ticket: >>=20 >> >>> >>> Mohammad, >>=20 >> >>> >>> So long as you continue to access them through >> >>> >>> /umbc/software/scripts, they should remain up to date and you >> >>> will >> >>> >>> see any new scripts. If you copy them to a local directory on >> >>> the >> >>> >>> HPC cluster, they will not be updated. >>=20 >> >>> >>> --- >> >>> >>> Skye Jonke >>=20 >> >>> >>> she/her >>=20 >> >>> >>> Specialist, Linux System Administrator & Lab Technical Support >>=20 >> >>> >>> > On Oct 22, 2025, at 16:14, Naghmeh Karimi <nkarimi@umbc.edu <m= ailto:nkarimi@umbc.edu>> >> >>> >>> wrote: >> >>> >>> > >> >>> >>> > Hi Max, Roy, Skye, >> >>> >>> > >> >>> >>> > Thank you all for your help. >> >>> >>> > >> >>> >>> > I added Skye here as she is our IT exert on adding the >> >>> capability >> >>> >>> of running synopsis tools (Hspice) in servers. >> >>> >>> > Skye we want to use the HPC cluster to run hspice an other >> >>> >>> synopsys tools. Could you please help on this? >> >>> >>> > >> >>> >>> > Thanks, >> >>> >>> > Naghmeh >> >>> >>> > >> >>> >>> > On Wed, Oct 22, 2025 at 2:06 PM Mohammad Ebrahimabadi via RT >> >>> >>> <UMBCHelp@rt.umbc.edu <mailto:UMBCHelp@rt.umbc.edu> <mailto:UMBC= Help@rt.umbc.edu <mailto:UMBCHelp@rt.umbc.edu>>> wrote: >> >>> >>> >> Ticket <URL: >> >>> https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 <https://www.go= ogle.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3296910&sourc= e=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7> >> >>> >>> >> >>> <https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.h= tml?id%3D3296910&source=3Dgmail-imap&ust=3D1761768891000000&usg=3DAOvVaw2jx= B3nArHBD7-ij9ZN8ZsW <https://www.google.com/url?q=3Dhttps://www.google.com/= url?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dg= mail-imap%26ust%3D1761768891000000%26usg%3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW&sou= rce=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw0fLylAsBAoQ6LgqY79I9-d>> >> >>> >>> > >> >>> >>> >> >> >>> >>> >> Last Update From Ticket: >> >>> >>> >> >> >>> >>> >> Hi Max, >> >>> >>> >> >> >>> >>> >> Thanks for your help! Let=E2=80=99s proceed based on your >> >>> suggestion, >> >>> >>> hopefully it >> >>> >>> >> works as expected. >> >>> >>> >> >> >>> >>> >> I just have one quick question: since the IT team in the >> >>> >>> department can modify >> >>> >>> >> or add new shell scripts in that directory, will we also >> >>> have >> >>> >>> access to those >> >>> >>> >> updated or newly added scripts from the mounted directory on >> >>> >>> HPCF? >> >>> >>> >> >> >>> >>> >> Thanks again, >> >>> >>> >> Mohammad >> >>> >>> >> >> >>> >>> >> On Wed Oct 22 14:01:32 2025, OL73413 wrote: >> >>> >>> >> >> >>> >>> >> > Hi Mohammad, >> >>> >>> >> >> >>> >>> >> > We can get the software mounted for you on chip, but it >> >>> will >> >>> >>> have to be >> >>> >>> >> > read-only as we won't be able to manage who can make >> >>> changes >> >>> >>> to it >> >>> >>> >> > otherwise. >> >>> >>> >> >> >>> >>> >> > As a reminder, the machines on that run on chip are not >> >>> the >> >>> >>> same as those >> >>> >>> >> > run by the CSEE department, so we can't guarantee that >> >>> these >> >>> >>> will run the >> >>> >>> >> > same way or as effectively. >> >>> >>> >> >> >>> >>> >> > If that's all ok with you, please give me some time to get >> >>> the >> >>> >>> directory >> >>> >>> >> > mounted. >> >>> >>> >> >> >>> >>> >> > On Wed Oct 22 13:26:05 2025, NQ23652 wrote: >> >>> >>> >> >> >>> >>> >> >> Thanks Tartela for your help to resolve my issue! >> >>> >>> >> >> >>> >>> >> >> Actually it seems all server in CSEE department have >> >>> access >> >>> >>> to that >> >>> >>> >> >> path. When I ran the command that you gave me I got the >> >>> >>> following >> >>> >>> >> >> report: >> >>> >>> >> >> >>> >>> >> >> mohammad@alborz:~$ df -h /umbc/software >> >>> >>> >> >> Filesystem Size Used Avail Use% Mounted on >> >>> >>> >> >> nfs.iss.rs.umbc.edu:/ifs/data/csee/software 2.5T 2.2T >> >>> 326G >> >>> >>> 88% >> >>> >>> >> >> /umbc/software >> >>> >>> >> >> >>> >>> >> >> mohammad@alborz:~$ mount | grep /umbc/software >> >>> >>> >> >> nfs.iss.rs.umbc.edu:/ifs/data/csee/software on >> >>> /umbc/software >> >>> >>> type nfs >> >>> >>> >> >> >> >>> >>> >> >>> (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namlen=3D255,acr= egmin=3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,proto=3Dtcp= ,timeo=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountvers=3D3,mou= ntport=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.60) >> >>> >>> >> >> >>> >>> >> >> Please let me know if you need other information. >> >>> >>> >> >> >>> >>> >> >> Regards, >> >>> >>> >> >> >>> >>> >> >> Mohammad >> >>> >>> >> >> >>> >>> >> >> On Wed Oct 22 13:15:12 2025, HK41259 wrote: >> >>> >>> >> >> >>> >>> >> >>> Hello, >> >>> >>> >> >> >>> >>> >> >>> Thanks for reaching out. Could you clarify which >> >>> specific >> >>> >>> machine >> >>> >>> >> >>> or server you=E2=80=99re referring to when you mention the >> >>> >>> =E2=80=9Cdepartmental >> >>> >>> >> >>> server=E2=80=9D? >> >>> >>> >> >> >>> >>> >> >>> The /umbc/software/scripts/ directory isn=E2=80=99t acces= sible >> >>> from >> >>> >>> HPCF >> >>> >>> >> >>> (e.g., Chip) by default =E2=80=94 those systems have sepa= rate >> >>> >>> storage and >> >>> >>> >> >>> don=E2=80=99t automatically mount departmental shares. >> >>> >>> >> >> >>> >>> >> >>> Could you check where /umbc/software is mounted on your >> >>> >>> >> >>> departmental system (e.g., by running df -h >> >>> /umbc/software >> >>> >>> or mount >> >>> >>> >> >>> | grep /umbc/software)? That=E2=80=99ll tell us which ser= ver >> >>> hosts >> >>> >>> it. Once >> >>> >>> >> >>> we know that, we can confirm whether it=E2=80=99s possibl= e or >> >>> >>> appropriate >> >>> >>> >> >>> to make it visible from HPCF. >> >>> >>> >> >> >>> >>> >> >>> Best, >> >>> >>> >> >> >>> >>> >> >>> Tartela >> >>> >>> >> >> >>> >>> >> >>> On Tue Oct 21 15:59:49 2025, ZZ99999 wrote: >> >>> >>> >> >> >>> >>> >> >>>> First Name: Mohammad >> >>> >>> >> >>>> Last Name: Ebrahimabadi >> >>> >>> >> >>>> Email: e127@umbc.edu <mailto:e127@umbc.edu> <mailto:e127= @umbc.edu <mailto:e127@umbc.edu>> >> >>> >>> >> >>>> Campus ID: NQ23652 >> >>> >>> >> >>>> >> >>> >>> >> >>>> Request Type: High Performance Cluster >> >>> >>> >> >>>> >> >>> >>> >> >>>> Dear Team, >> >>> >>> >> >>>> >> >>> >>> >> >>>> I hope you=E2=80=99re doing well. >> >>> >>> >> >>>> I=E2=80=99m a Ph.D. student in Computer Engineering and = have a >> >>> >>> question regarding running a software on the HPCF. On the >> >>> >>> department server, I usually run software using shell scripts >> >>> >>> located at /umbc/software/scripts/ which is provided by the IT >> >>> >>> group pf the department (Geoff Weiss Team). However, in case of >> >>> >>> HPCF, it seems that this path is not accessible from the HPCF >> >>> >>> environment. >> >>> >>> >> >>>> >> >>> >>> >> >>>> Could you please let me know if there is any specific >> >>> >>> configuration or access permission required to reach this >> >>> directory >> >>> >>> through the HPCF? or direct me to a related person that can >> >>> help >> >>> >>> me. >> >>> >>> >> >>>> >> >>> >>> >> >>>> Best regards, >> >>> >>> >> >>>> Mohammad >> >>> >>> >> >>> >> >>> >>> >> >> >>> >>> >> > -- >> >>> >>> >> >> >>> >>> >> > Best, >> >>> >>> >> > Max Breitmeyer >> >>> >>> >> > DOIT HPC System Administrator >> >>> >>> >> >> >>> >>> >> >> >>> >>> > >> >>> >>> > >> >>> >>> > >> >>> >>> > -- >> >>> >>> > Naghmeh Karimi, Ph.D. >> >>> >>> > Associate Professor >> >>> >>> > Department of Computer Science and Electrical Engineering >> >>> >>> > University of Maryland, Baltimore County >> >>> >>> > Baltimore, MD 21250 >> >>> >>> > Tel: 410-455-3965 E-mail: nkarimi@umbc.edu <mailto:nkarimi@umb= c.edu> >> >>> >>> <mailto:nkarimi@umbc.edu <mailto:nkarimi@umbc.edu>> >> >>> >>> > Web: http://www.csee.umbc.edu/~nkarimi/ <https://www.google.co= m/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source=3Dgmail-imap&ust=3D1761= 874374000000&usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW> >> >>> >>> >> >>> <https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&s= ource=3Dgmail-imap&ust=3D1761768891000000&usg=3DAOvVaw0xLE02sw5-ueWRHyHyCwk= N <https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://www= .csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D1761768891000000%26u= sg%3DAOvVaw0xLE02sw5-ueWRHyHyCwkN&source=3Dgmail-imap&ust=3D176187437400000= 0&usg=3DAOvVaw3g338pGQqShCgyX8cIt8tV>> >>=20 >> >>> >> -- Naghmeh Karimi, Ph.D. >> >>> >> Associate Professor >> >>> >> Department of Computer Science and Electrical Engineering >> >>> >> University of Maryland, Baltimore County >> >>> >> Baltimore, MD 21250 >> >>> >> Tel: 410-455-3965 E-mail: nkarimi@umbc.edu <mailto:nkarimi@umbc.e= du> >> >>> >> Web: http://www.csee.umbc.edu/~nkarimi/ <https://www.google.com/u= rl?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source=3Dgmail-imap&ust=3D1761874= 374000000&usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW> >>=20 >> >>> > -- >>=20 >> >>> > Best, >> >>> > Max Breitmeyer >> >>> > DOIT HPC System Administrator >>=20 >> >> -- V/R,Maxwell BreitmeyerUMBC HPCF SpecialistUMBC Observatory IT >> >> Manager Graduate Student(443) 835-8250 >>=20 >> -- >>=20 >> Best, >> Max Breitmeyer >> DOIT HPC System Administrator >>=20 >>=20  "
3296910,72474720,Correspond,DoIT-Research-Computing,2025-10-24 13:39:57.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Skye Jonke,ii69854@umbc.edu,"<html aria-label=3D""message body""><head><meta http-equiv=3D""content-type"" c= ontent=3D""text/html; charset=3Dutf-8""></head><body style=3D""overflow-wrap: = break-word; -webkit-nbsp-mode: space; line-break: after-white-space;"">Its n= ot possible to use the existing scripts in a situation where the files aren= =E2=80=99t mounted in /umbc/software, but I can write some scripts specific=  to this setup to this situation with a bit more information. Where are the=  script files located on the HPC? If you aren=E2=80=99t sure, you can get t= he full path by running =E2=80=9Crealpath myfile.sh=E2=80=9D (replacing myf= ile.sh with one of the cadence scripts, of course)&nbsp;<br id=3D""lineBreak= AtBeginningOfMessage""><div> <meta charset=3D""UTF-8""><div dir=3D""auto"" style=3D""caret-color: rgb(0, 0, 0= ); color: rgb(0, 0, 0); letter-spacing: normal; text-align: start; text-ind= ent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; -we= bkit-text-stroke-width: 0px; text-decoration: none; overflow-wrap: break-wo= rd; -webkit-nbsp-mode: space; line-break: after-white-space;"">---</div><div=  dir=3D""auto"" style=3D""caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0); lett= er-spacing: normal; text-align: start; text-indent: 0px; text-transform: no= ne; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px;=  text-decoration: none; overflow-wrap: break-word; -webkit-nbsp-mode: space= ; line-break: after-white-space;"">Skye Jonke<br><br>she/her<br><br>Speciali= st, Linux System Administrator &amp;<span class=3D""Apple-converted-space"">&= nbsp;</span>Lab Technical Support<br></div> </div> <div><br><blockquote type=3D""cite""><div>On Oct 23, 2025, at 21:32, Naghmeh = Karimi &lt;nkarimi@umbc.edu&gt; wrote:</div><br class=3D""Apple-interchange-= newline""><div><div dir=3D""auto""><div>Hi Max, thanks&nbsp;for getting back t= o us.</div><div dir=3D""auto""><br></div><div dir=3D""auto"">Hi Skye, we need y= our help here to find the file locations. Could you please help to resolve&= nbsp;this issue?</div><div dir=3D""auto""><br></div><div dir=3D""auto"">Thanks = a lot</div><div dir=3D""auto"">Naghmeh&nbsp;</div><div><br></div><div data-sm= artmail=3D""gmail_signature""><div dir=3D""ltr""><span style=3D""font-size:9.5pt= ;line-height:115%;font-family:Arial,sans-serif;color:rgb(136,136,136);backg= round-image:initial;background-position:initial;background-repeat:initial"">= Naghmeh&nbsp;Karimi, Ph.D.<br> Associate Professor<br> Department of Computer Science and Electrical Engineering<br> University of Maryland, Baltimore County<br> Baltimore, MD 21250<br> Tel:</span><span style=3D""font-size: 11pt; line-height: 115%; font-family: = Calibri, sans-serif;"">&nbsp;</span><u><span style=3D""font-size:11.0pt;line-= height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1= 155cc"">410-455-3965</span></u><span style=3D""font-size: 11pt; line-height: = 115%; font-family: Calibri, sans-serif;""> </span><span style=3D""font-size:9= .5pt;line-height:115%;font-family:Arial,sans-serif;color:rgb(136,136,136);b= ackground-image:initial;background-position:initial;background-repeat:initi= al"">E-mail:</span><span style=3D""font-size: 11pt; line-height: 115%; font-f= amily: Calibri, sans-serif;"">&nbsp;</span><u><span style=3D""font-size:11.0p= t;line-height:115%;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;c= olor:#1155cc""><a href=3D""mailto:nkarimi@umbc.edu"" target=3D""_blank"">nkarimi= @umbc.edu</a></span></u><span style=3D""font-size:9.5pt;line-height:115%;fon= t-family:Arial,sans-serif;color:rgb(136,136,136);background-image:initial;b= ackground-position:initial;background-repeat:initial""><br> Web:</span><span style=3D""font-size: 9.5pt; line-height: 115%; font-family:=  Arial, sans-serif; background-image: initial; background-position: initial= ; background-repeat: initial;"">&nbsp;</span><u><span style=3D""font-size:11p= t;line-height:115%;font-family:Calibri,sans-serif;color:rgb(17,85,204);back= ground-image:initial;background-position:initial;background-repeat:initial""= ><a href=3D""https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarim= i/&amp;source=3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw23Fk_= KFBnlrXC_uhIbUFIW"" target=3D""_blank"">http://www.csee.umbc.edu/~nkarimi/</a>= </span></u></div></div></div><br><div class=3D""gmail_quote gmail_quote_cont= ainer""><div dir=3D""ltr"" class=3D""gmail_attr"">On Thu, Oct 23, 2025, 12:22=E2= =80=AFPM Max Breitmeyer via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">= UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quo= te"" style=3D""margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex""= >Ticket &lt;URL: <a href=3D""https://www.google.com/url?q=3Dhttps://rt.umbc.= edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761= 874374000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7"" rel=3D""noreferrer nore= ferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3296= 910</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Mohammad,<br> <br> According to the error message, it's having an issue finding another file i= n<br> the directory. This appears to be an issue due to the mounting locations be= ing<br> different. I would point you to the previous email where I said that any<br> software with hard-coded locations would not work, and that software<br> compatibility is not guaranteed. I would recommend working with Skye on por= ting<br> what is in the directory to your local research directories on chip.<br> <br> On Thu Oct 23 12:09:34 2025, NQ23652 wrote:<br> <br> &gt; Yes you are right, now I could see all shell files. However still I ha= ve<br> &gt; problem when I run the sh file.<br> <br> &gt; [e127@chip-login2 scripts]$ sh launch_synopsys_hspice.sh<br> &gt; launch_synopsys_hspice.sh: line 3:<br> &gt; /umbc/software/scripts/env_synopsys_hspice.sh: No such file or directo= ry<br> &gt; [e127@chip-login2 scripts]$<br> <br> &gt; Would you please take a look to it.<br> <br> &gt; Regards,<br> <br> &gt; Mohammad<br> <br> &gt; On Thu Oct 23 11:59:17 2025, OL73413 wrote:<br> <br> &gt;&gt; Hi Mohammad,<br> <br> &gt;&gt; A good point. Chip uses a service called ""autofs"" to mount and unm= ount<br> &gt;&gt; directories that aren't being actively used. This helps maintain t= he<br> &gt;&gt; login nodes integrity. If you see in my screenshot below, the dire= ctory<br> &gt;&gt; appears to be empty, but when I cd to the csee directory where you= r<br> &gt;&gt; software is mounted, it is all available. Backing out of the direc= tory<br> &gt;&gt; and ls'ing the directory also shows that the directory is now moun= ted<br> &gt;&gt; and available.<br> <br> &gt;&gt; On Thu, Oct 23, 2025 at 11:41 AM Mohammad Ebrahimabadi via RT<br> &gt;&gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"" rel= =3D""noreferrer"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt;&gt; Ticket &lt;URL: <a href=3D""https://www.google.com/url?q=3Dhttp= s://rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&am= p;ust=3D1761874374000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7"" rel=3D""nor= eferrer noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.ht= ml?id=3D3296910</a> &gt;<br> <br> &gt;&gt;&gt; Comment From Ticket:<br> &gt;&gt;&gt; Thank you Max!<br> <br> &gt;&gt;&gt; Actually we have still problem for accessing that directory. I= <br> &gt;&gt;&gt; uploaded two<br> &gt;&gt;&gt; screenshots that shows what we can see in the department serve= rs<br> &gt;&gt;&gt; and what we<br> &gt;&gt;&gt; can see in CHIP.<br> <br> &gt;&gt;&gt; For example by running this shell file we can launch HSPICE<br> &gt;&gt;&gt; software in the<br> &gt;&gt;&gt; department server:<br> &gt;&gt;&gt; /umbc/software/scripts/launch_synopsys_hspice.sh<br> &gt;&gt;&gt; But it is not still accessible on CHIP.<br> <br> &gt;&gt;&gt; Regards,<br> <br> &gt;&gt;&gt; Mohammad<br> <br> &gt;&gt;&gt; On Thu Oct 23 10:54:59 2025, OL73413 wrote:<br> <br> &gt;&gt;&gt; &gt; Hi Nahmeh,<br> <br> &gt;&gt;&gt; &gt; Apologies =E2=80=94 I meant to update you yesterday after=  completing<br> &gt;&gt;&gt; this, but I<br> &gt;&gt;&gt; &gt; was pulled into a long meeting. The software is now avail= able on<br> &gt;&gt;&gt; chip at:<br> <br> &gt;&gt;&gt; &gt; /umbc/software/csee<br> <br> &gt;&gt;&gt; &gt; Please note that if the software contains any hardcoded f= ile<br> &gt;&gt;&gt; paths, it may<br> &gt;&gt;&gt; &gt; not function correctly. Unfortunately, we don=E2=80=99t h= ave control over<br> &gt;&gt;&gt; those<br> &gt;&gt;&gt; &gt; cases.<br> <br> &gt;&gt;&gt; &gt; Also, just to clarify, Skye and I work in separate depart= ments.<br> &gt;&gt;&gt; Similar to<br> &gt;&gt;&gt; &gt; how I don=E2=80=99t have administrative access to the CSE= E department<br> &gt;&gt;&gt; servers<br> &gt;&gt;&gt; &gt; (which include the software directory), Skye is not an<br> &gt;&gt;&gt; administrator on<br> &gt;&gt;&gt; &gt; chip.<br> <br> &gt;&gt;&gt; &gt; Please let me know if you encounter any issues accessing = the<br> &gt;&gt;&gt; software on<br> &gt;&gt;&gt; &gt; the server. If there are problems running the software, I= =E2=80=99ll<br> &gt;&gt;&gt; coordinate<br> &gt;&gt;&gt; &gt; with Skye to troubleshoot, but since we don=E2=80=99t mai= ntain this<br> &gt;&gt;&gt; software, we<br> &gt;&gt;&gt; &gt; can=E2=80=99t guarantee full compatibility on our systems= .<br> <br> &gt;&gt;&gt; &gt; On Thu Oct 23 10:36:41 2025, <a href=3D""mailto:naghmeh.ka= rimi@umbc.edu"" target=3D""_blank"" rel=3D""noreferrer"">naghmeh.karimi@umbc.edu= </a> wrote:<br> <br> &gt;&gt;&gt; &gt;&gt; Hi Skye and Max, Could you please help on mounting th= is today so<br> &gt;&gt;&gt; we can<br> &gt;&gt;&gt; &gt;&gt; use them as we have a deadline? Thanks,Naghmeh Karimi=  On Wed,<br> &gt;&gt;&gt; Oct 22,<br> &gt;&gt;&gt; &gt;&gt; 2025 at 4:22 PM Skye Jonke via RT &lt;<a href=3D""mail= to:UMBCHelp@rt.umbc.edu"" target=3D""_blank"" rel=3D""noreferrer"">UMBCHelp@rt.u= mbc.edu</a>&gt; wrote:<br> <br> &gt;&gt;&gt; &gt;&gt;&gt; Ticket &lt;URL: <a href=3D""https://www.google.com= /url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;source=3D= gmail-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w= 7"" rel=3D""noreferrer noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tick= et/Display.html?id=3D3296910</a><br> &gt;&gt;&gt; &gt;<br> <br> &gt;&gt;&gt; &gt;&gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt;&gt; &gt;&gt;&gt; Mohammad,<br> <br> &gt;&gt;&gt; &gt;&gt;&gt; So long as you continue to access them through<br> &gt;&gt;&gt; &gt;&gt;&gt; /umbc/software/scripts, they should remain up to = date and you<br> &gt;&gt;&gt; will<br> &gt;&gt;&gt; &gt;&gt;&gt; see any new scripts. If you copy them to a local = directory on<br> &gt;&gt;&gt; the<br> &gt;&gt;&gt; &gt;&gt;&gt; HPC cluster, they will not be updated.<br> <br> &gt;&gt;&gt; &gt;&gt;&gt; ---<br> &gt;&gt;&gt; &gt;&gt;&gt; Skye Jonke<br> <br> &gt;&gt;&gt; &gt;&gt;&gt; she/her<br> <br> &gt;&gt;&gt; &gt;&gt;&gt; Specialist, Linux System Administrator &amp; Lab = Technical Support<br> <br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; On Oct 22, 2025, at 16:14, Naghmeh Karimi &l= t;<a href=3D""mailto:nkarimi@umbc.edu"" target=3D""_blank"" rel=3D""noreferrer"">= nkarimi@umbc.edu</a>&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; wrote:<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Hi Max, Roy, Skye,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Thank you all for your help.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; I added Skye here as she is our IT exert on = adding the<br> &gt;&gt;&gt; capability<br> &gt;&gt;&gt; &gt;&gt;&gt; of running synopsis tools (Hspice) in servers.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Skye we want to use the HPC cluster to run h= spice an other<br> &gt;&gt;&gt; &gt;&gt;&gt; synopsys tools. Could you please help on this?<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Thanks,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Naghmeh<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; On Wed, Oct 22, 2025 at 2:06 PM Mohammad Ebr= ahimabadi via RT<br> &gt;&gt;&gt; &gt;&gt;&gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" targe= t=3D""_blank"" rel=3D""noreferrer"">UMBCHelp@rt.umbc.edu</a> &lt;mailto:<a href= =3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"" rel=3D""noreferrer"">UMBCH= elp@rt.umbc.edu</a>&gt;&gt; wrote:<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Ticket &lt;URL:<br> &gt;&gt;&gt; <a href=3D""https://www.google.com/url?q=3Dhttps://rt.umbc.edu/= Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D17618743= 74000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7"" rel=3D""noreferrer noreferr= er"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3296910<= /a><br> &gt;&gt;&gt; &gt;&gt;&gt;<br> &gt;&gt;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttps://www.goog= le.com/url?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26sou= rce%3Dgmail-imap%26ust%3D1761768891000000%26usg%3DAOvVaw2jxB3nArHBD7-ij9ZN8= ZsW&amp;source=3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw0fLy= lAsBAoQ6LgqY79I9-d"" rel=3D""noreferrer noreferrer"" target=3D""_blank"">https:/= /www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D329691= 0&amp;source=3Dgmail-imap&amp;ust=3D1761768891000000&amp;usg=3DAOvVaw2jxB3n= ArHBD7-ij9ZN8ZsW</a>&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Last Update From Ticket:<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Hi Max,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Thanks for your help! Let=E2=80=99s proc= eed based on your<br> &gt;&gt;&gt; suggestion,<br> &gt;&gt;&gt; &gt;&gt;&gt; hopefully it<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; works as expected.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; I just have one quick question: since th= e IT team in the<br> &gt;&gt;&gt; &gt;&gt;&gt; department can modify<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; or add new shell scripts in that directo= ry, will we also<br> &gt;&gt;&gt; have<br> &gt;&gt;&gt; &gt;&gt;&gt; access to those<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; updated or newly added scripts from the = mounted directory on<br> &gt;&gt;&gt; &gt;&gt;&gt; HPCF?<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Thanks again,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Mohammad<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; On Wed Oct 22 14:01:32 2025, OL73413 wro= te:<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Hi Mohammad,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; We can get the software mounted for=  you on chip, but it<br> &gt;&gt;&gt; will<br> &gt;&gt;&gt; &gt;&gt;&gt; have to be<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; read-only as we won't be able to ma= nage who can make<br> &gt;&gt;&gt; changes<br> &gt;&gt;&gt; &gt;&gt;&gt; to it<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; otherwise.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; As a reminder, the machines on that=  run on chip are not<br> &gt;&gt;&gt; the<br> &gt;&gt;&gt; &gt;&gt;&gt; same as those<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; run by the CSEE department, so we c= an't guarantee that<br> &gt;&gt;&gt; these<br> &gt;&gt;&gt; &gt;&gt;&gt; will run the<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; same way or as effectively.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; If that's all ok with you, please g= ive me some time to get<br> &gt;&gt;&gt; the<br> &gt;&gt;&gt; &gt;&gt;&gt; directory<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; mounted.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; On Wed Oct 22 13:26:05 2025, NQ2365= 2 wrote:<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thanks Tartela for your help to=  resolve my issue!<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Actually it seems all server in=  CSEE department have<br> &gt;&gt;&gt; access<br> &gt;&gt;&gt; &gt;&gt;&gt; to that<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; path. When I ran the command th= at you gave me I got the<br> &gt;&gt;&gt; &gt;&gt;&gt; following<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; report:<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; mohammad@alborz:~$ df -h /umbc/= software<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Filesystem Size Used Avail Use%=  Mounted on<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; nfs.iss.rs.umbc.edu:/ifs/data/c= see/software 2.5T 2.2T<br> &gt;&gt;&gt; 326G<br> &gt;&gt;&gt; &gt;&gt;&gt; 88%<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; /umbc/software<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; mohammad@alborz:~$ mount | grep=  /umbc/software<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; nfs.iss.rs.umbc.edu:/ifs/data/c= see/software on<br> &gt;&gt;&gt; /umbc/software<br> &gt;&gt;&gt; &gt;&gt;&gt; type nfs<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt;<br> &gt;&gt;&gt; (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namlen=3D2= 55,acregmin=3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,proto= =3Dtcp,timeo=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountvers= =3D3,mountport=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.60)<= br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Please let me know if you need = other information.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Regards,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Mohammad<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; On Wed Oct 22 13:15:12 2025, HK= 41259 wrote:<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Hello,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Thanks for reaching out. Co= uld you clarify which<br> &gt;&gt;&gt; specific<br> &gt;&gt;&gt; &gt;&gt;&gt; machine<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; or server you=E2=80=99re re= ferring to when you mention the<br> &gt;&gt;&gt; &gt;&gt;&gt; =E2=80=9Cdepartmental<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; server=E2=80=9D?<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; The /umbc/software/scripts/=  directory isn=E2=80=99t accessible<br> &gt;&gt;&gt; from<br> &gt;&gt;&gt; &gt;&gt;&gt; HPCF<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; (e.g., Chip) by default =E2= =80=94 those systems have separate<br> &gt;&gt;&gt; &gt;&gt;&gt; storage and<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; don=E2=80=99t automatically=  mount departmental shares.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Could you check where /umbc= /software is mounted on your<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; departmental system (e.g., = by running df -h<br> &gt;&gt;&gt; /umbc/software<br> &gt;&gt;&gt; &gt;&gt;&gt; or mount<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; | grep /umbc/software)? Tha= t=E2=80=99ll tell us which server<br> &gt;&gt;&gt; hosts<br> &gt;&gt;&gt; &gt;&gt;&gt; it. Once<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; we know that, we can confir= m whether it=E2=80=99s possible or<br> &gt;&gt;&gt; &gt;&gt;&gt; appropriate<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; to make it visible from HPC= F.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Best,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Tartela<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; On Tue Oct 21 15:59:49 2025= , ZZ99999 wrote:<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; First Name: Mohammad<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Last Name: Ebrahimabadi= <br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Email: <a href=3D""mailt= o:e127@umbc.edu"" target=3D""_blank"" rel=3D""noreferrer"">e127@umbc.edu</a> &lt= ;mailto:<a href=3D""mailto:e127@umbc.edu"" target=3D""_blank"" rel=3D""noreferre= r"">e127@umbc.edu</a>&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Campus ID: NQ23652<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Request Type: High Perf= ormance Cluster<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Dear Team,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; I hope you=E2=80=99re d= oing well.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; I=E2=80=99m a Ph.D. stu= dent in Computer Engineering and have a<br> &gt;&gt;&gt; &gt;&gt;&gt; question regarding running a software on the HPCF= . On the<br> &gt;&gt;&gt; &gt;&gt;&gt; department server, I usually run software using s= hell scripts<br> &gt;&gt;&gt; &gt;&gt;&gt; located at /umbc/software/scripts/ which is provi= ded by the IT<br> &gt;&gt;&gt; &gt;&gt;&gt; group pf the department (Geoff Weiss Team). Howev= er, in case of<br> &gt;&gt;&gt; &gt;&gt;&gt; HPCF, it seems that this path is not accessible f= rom the HPCF<br> &gt;&gt;&gt; &gt;&gt;&gt; environment.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Could you please let me=  know if there is any specific<br> &gt;&gt;&gt; &gt;&gt;&gt; configuration or access permission required to re= ach this<br> &gt;&gt;&gt; directory<br> &gt;&gt;&gt; &gt;&gt;&gt; through the HPCF? or direct me to a related perso= n that can<br> &gt;&gt;&gt; help<br> &gt;&gt;&gt; &gt;&gt;&gt; me.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Best regards,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Mohammad<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; --<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Best,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Max Breitmeyer<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; DOIT HPC System Administrator<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; --<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Naghmeh Karimi, Ph.D.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Associate Professor<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Department of Computer Science and Electrica= l Engineering<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; University of Maryland, Baltimore County<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Baltimore, MD 21250<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Tel: 410-455-3965 E-mail: <a href=3D""mailto:= nkarimi@umbc.edu"" target=3D""_blank"" rel=3D""noreferrer"">nkarimi@umbc.edu</a>= <br> &gt;&gt;&gt; &gt;&gt;&gt; &lt;mailto:<a href=3D""mailto:nkarimi@umbc.edu"" ta= rget=3D""_blank"" rel=3D""noreferrer"">nkarimi@umbc.edu</a>&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Web: <a href=3D""https://www.google.com/url?q= =3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D176= 1874374000000&amp;usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW"" rel=3D""noreferrer nor= eferrer"" target=3D""_blank"">http://www.csee.umbc.edu/~nkarimi/</a><br> &gt;&gt;&gt; &gt;&gt;&gt;<br> &gt;&gt;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttps://www.goog= le.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26u= st%3D1761768891000000%26usg%3DAOvVaw0xLE02sw5-ueWRHyHyCwkN&amp;source=3Dgma= il-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw3g338pGQqShCgyX8cIt8tV"" = rel=3D""noreferrer noreferrer"" target=3D""_blank"">https://www.google.com/url?= q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D17= 61768891000000&amp;usg=3DAOvVaw0xLE02sw5-ueWRHyHyCwkN</a>&gt;<br> <br> &gt;&gt;&gt; &gt;&gt; -- Naghmeh Karimi, Ph.D.<br> &gt;&gt;&gt; &gt;&gt; Associate Professor<br> &gt;&gt;&gt; &gt;&gt; Department of Computer Science and Electrical Enginee= ring<br> &gt;&gt;&gt; &gt;&gt; University of Maryland, Baltimore County<br> &gt;&gt;&gt; &gt;&gt; Baltimore, MD 21250<br> &gt;&gt;&gt; &gt;&gt; Tel: 410-455-3965 E-mail: <a href=3D""mailto:nkarimi@u= mbc.edu"" target=3D""_blank"" rel=3D""noreferrer"">nkarimi@umbc.edu</a><br> &gt;&gt;&gt; &gt;&gt; Web: <a href=3D""https://www.google.com/url?q=3Dhttp:/= /www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D176187437400= 0000&amp;usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW"" rel=3D""noreferrer noreferrer"" = target=3D""_blank"">http://www.csee.umbc.edu/~nkarimi/</a><br> <br> &gt;&gt;&gt; &gt; --<br> <br> &gt;&gt;&gt; &gt; Best,<br> &gt;&gt;&gt; &gt; Max Breitmeyer<br> &gt;&gt;&gt; &gt; DOIT HPC System Administrator<br> <br> &gt;&gt; -- V/R,Maxwell BreitmeyerUMBC HPCF SpecialistUMBC Observatory IT<b= r> &gt;&gt; Manager Graduate Student(443) 835-8250<br> <br> --<br> <br> Best,<br> Max Breitmeyer<br> DOIT HPC System Administrator<br> <br> <br> </blockquote></div> </div></blockquote></div><br></body></html>= "
3296910,72474850,Correspond,DoIT-Research-Computing,2025-10-24 13:42:30.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,NULL,naghmeh.karimi@umbc.edu,"That's perfect Skye. I highly appreciate it. I ask Mohamamd to respond the ticket with the path.  regards, Naghmeh  On Fri, Oct 24, 2025 at 9:39=E2=80=AFAM Skye Jonke <ii69854@umbc.edu> wrote:  > Its not possible to use the existing scripts in a situation where the > files aren=E2=80=99t mounted in /umbc/software, but I can write some scri= pts > specific to this setup to this situation with a bit more information. Whe= re > are the script files located on the HPC? If you aren=E2=80=99t sure, you = can get > the full path by running =E2=80=9Crealpath myfile.sh=E2=80=9D (replacing = myfile.sh with one > of the cadence scripts, of course) > --- > Skye Jonke > > she/her > > Specialist, Linux System Administrator & Lab Technical Support > > On Oct 23, 2025, at 21:32, Naghmeh Karimi <nkarimi@umbc.edu> wrote: > > Hi Max, thanks for getting back to us. > > Hi Skye, we need your help here to find the file locations. Could you > please help to resolve this issue? > > Thanks a lot > Naghmeh > > Naghmeh Karimi, Ph.D. > Associate Professor > Department of Computer Science and Electrical Engineering > University of Maryland, Baltimore County > Baltimore, MD 21250 > Tel: *410-455-3965* E-mail: *nkarimi@umbc.edu <nkarimi@umbc.edu>* > Web: *http://www.csee.umbc.edu/~nkarimi/ > <https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source= =3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW>* > > On Thu, Oct 23, 2025, 12:22=E2=80=AFPM Max Breitmeyer via RT <UMBCHelp@rt= .umbc.edu> > wrote: > >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 >> <https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?= id%3D3296910&source=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw3ImziGt= KZ-9j9JFW8J65w7> >> > >> >> Last Update From Ticket: >> >> Hi Mohammad, >> >> According to the error message, it's having an issue finding another file >> in >> the directory. This appears to be an issue due to the mounting locations >> being >> different. I would point you to the previous email where I said that any >> software with hard-coded locations would not work, and that software >> compatibility is not guaranteed. I would recommend working with Skye on >> porting >> what is in the directory to your local research directories on chip. >> >> On Thu Oct 23 12:09:34 2025, NQ23652 wrote: >> >> > Yes you are right, now I could see all shell files. However still I ha= ve >> > problem when I run the sh file. >> >> > [e127@chip-login2 scripts]$ sh launch_synopsys_hspice.sh >> > launch_synopsys_hspice.sh: line 3: >> > /umbc/software/scripts/env_synopsys_hspice.sh: No such file or directo= ry >> > [e127@chip-login2 scripts]$ >> >> > Would you please take a look to it. >> >> > Regards, >> >> > Mohammad >> >> > On Thu Oct 23 11:59:17 2025, OL73413 wrote: >> >> >> Hi Mohammad, >> >> >> A good point. Chip uses a service called ""autofs"" to mount and unmount >> >> directories that aren't being actively used. This helps maintain the >> >> login nodes integrity. If you see in my screenshot below, the directo= ry >> >> appears to be empty, but when I cd to the csee directory where your >> >> software is mounted, it is all available. Backing out of the directory >> >> and ls'ing the directory also shows that the directory is now mounted >> >> and available. >> >> >> On Thu, Oct 23, 2025 at 11:41 AM Mohammad Ebrahimabadi via RT >> >> <UMBCHelp@rt.umbc.edu> wrote: >> >> >>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 >> <https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?= id%3D3296910&source=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw3ImziGt= KZ-9j9JFW8J65w7> >> > >> >> >>> Comment From Ticket: >> >>> Thank you Max! >> >> >>> Actually we have still problem for accessing that directory. I >> >>> uploaded two >> >>> screenshots that shows what we can see in the department servers >> >>> and what we >> >>> can see in CHIP. >> >> >>> For example by running this shell file we can launch HSPICE >> >>> software in the >> >>> department server: >> >>> /umbc/software/scripts/launch_synopsys_hspice.sh >> >>> But it is not still accessible on CHIP. >> >> >>> Regards, >> >> >>> Mohammad >> >> >>> On Thu Oct 23 10:54:59 2025, OL73413 wrote: >> >> >>> > Hi Nahmeh, >> >> >>> > Apologies =E2=80=94 I meant to update you yesterday after completi= ng >> >>> this, but I >> >>> > was pulled into a long meeting. The software is now available on >> >>> chip at: >> >> >>> > /umbc/software/csee >> >> >>> > Please note that if the software contains any hardcoded file >> >>> paths, it may >> >>> > not function correctly. Unfortunately, we don=E2=80=99t have contr= ol over >> >>> those >> >>> > cases. >> >> >>> > Also, just to clarify, Skye and I work in separate departments. >> >>> Similar to >> >>> > how I don=E2=80=99t have administrative access to the CSEE departm= ent >> >>> servers >> >>> > (which include the software directory), Skye is not an >> >>> administrator on >> >>> > chip. >> >> >>> > Please let me know if you encounter any issues accessing the >> >>> software on >> >>> > the server. If there are problems running the software, I=E2=80=99= ll >> >>> coordinate >> >>> > with Skye to troubleshoot, but since we don=E2=80=99t maintain this >> >>> software, we >> >>> > can=E2=80=99t guarantee full compatibility on our systems. >> >> >>> > On Thu Oct 23 10:36:41 2025, naghmeh.karimi@umbc.edu wrote: >> >> >>> >> Hi Skye and Max, Could you please help on mounting this today so >> >>> we can >> >>> >> use them as we have a deadline? Thanks,Naghmeh Karimi On Wed, >> >>> Oct 22, >> >>> >> 2025 at 4:22 PM Skye Jonke via RT <UMBCHelp@rt.umbc.edu> wrote: >> >> >>> >>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 >> <https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?= id%3D3296910&source=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw3ImziGt= KZ-9j9JFW8J65w7> >> >>> > >> >> >>> >>> Last Update From Ticket: >> >> >>> >>> Mohammad, >> >> >>> >>> So long as you continue to access them through >> >>> >>> /umbc/software/scripts, they should remain up to date and you >> >>> will >> >>> >>> see any new scripts. If you copy them to a local directory on >> >>> the >> >>> >>> HPC cluster, they will not be updated. >> >> >>> >>> --- >> >>> >>> Skye Jonke >> >> >>> >>> she/her >> >> >>> >>> Specialist, Linux System Administrator & Lab Technical Support >> >> >>> >>> > On Oct 22, 2025, at 16:14, Naghmeh Karimi <nkarimi@umbc.edu> >> >>> >>> wrote: >> >>> >>> > >> >>> >>> > Hi Max, Roy, Skye, >> >>> >>> > >> >>> >>> > Thank you all for your help. >> >>> >>> > >> >>> >>> > I added Skye here as she is our IT exert on adding the >> >>> capability >> >>> >>> of running synopsis tools (Hspice) in servers. >> >>> >>> > Skye we want to use the HPC cluster to run hspice an other >> >>> >>> synopsys tools. Could you please help on this? >> >>> >>> > >> >>> >>> > Thanks, >> >>> >>> > Naghmeh >> >>> >>> > >> >>> >>> > On Wed, Oct 22, 2025 at 2:06 PM Mohammad Ebrahimabadi via RT >> >>> >>> <UMBCHelp@rt.umbc.edu <mailto:UMBCHelp@rt.umbc.edu>> wrote: >> >>> >>> >> Ticket <URL: >> >>> https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 >> <https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?= id%3D3296910&source=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw3ImziGt= KZ-9j9JFW8J65w7> >> >>> >>> >> >>> < >> https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?i= d%3D3296910&source=3Dgmail-imap&ust=3D1761768891000000&usg=3DAOvVaw2jxB3nAr= HBD7-ij9ZN8ZsW >> <https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://r= t.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3= D1761768891000000%26usg%3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW&source=3Dgmail-imap&= ust=3D1761874374000000&usg=3DAOvVaw0fLylAsBAoQ6LgqY79I9-d> >> > >> >>> >>> > >> >>> >>> >> >> >>> >>> >> Last Update From Ticket: >> >>> >>> >> >> >>> >>> >> Hi Max, >> >>> >>> >> >> >>> >>> >> Thanks for your help! Let=E2=80=99s proceed based on your >> >>> suggestion, >> >>> >>> hopefully it >> >>> >>> >> works as expected. >> >>> >>> >> >> >>> >>> >> I just have one quick question: since the IT team in the >> >>> >>> department can modify >> >>> >>> >> or add new shell scripts in that directory, will we also >> >>> have >> >>> >>> access to those >> >>> >>> >> updated or newly added scripts from the mounted directory on >> >>> >>> HPCF? >> >>> >>> >> >> >>> >>> >> Thanks again, >> >>> >>> >> Mohammad >> >>> >>> >> >> >>> >>> >> On Wed Oct 22 14:01:32 2025, OL73413 wrote: >> >>> >>> >> >> >>> >>> >> > Hi Mohammad, >> >>> >>> >> >> >>> >>> >> > We can get the software mounted for you on chip, but it >> >>> will >> >>> >>> have to be >> >>> >>> >> > read-only as we won't be able to manage who can make >> >>> changes >> >>> >>> to it >> >>> >>> >> > otherwise. >> >>> >>> >> >> >>> >>> >> > As a reminder, the machines on that run on chip are not >> >>> the >> >>> >>> same as those >> >>> >>> >> > run by the CSEE department, so we can't guarantee that >> >>> these >> >>> >>> will run the >> >>> >>> >> > same way or as effectively. >> >>> >>> >> >> >>> >>> >> > If that's all ok with you, please give me some time to get >> >>> the >> >>> >>> directory >> >>> >>> >> > mounted. >> >>> >>> >> >> >>> >>> >> > On Wed Oct 22 13:26:05 2025, NQ23652 wrote: >> >>> >>> >> >> >>> >>> >> >> Thanks Tartela for your help to resolve my issue! >> >>> >>> >> >> >>> >>> >> >> Actually it seems all server in CSEE department have >> >>> access >> >>> >>> to that >> >>> >>> >> >> path. When I ran the command that you gave me I got the >> >>> >>> following >> >>> >>> >> >> report: >> >>> >>> >> >> >>> >>> >> >> mohammad@alborz:~$ df -h /umbc/software >> >>> >>> >> >> Filesystem Size Used Avail Use% Mounted on >> >>> >>> >> >> nfs.iss.rs.umbc.edu:/ifs/data/csee/software 2.5T 2.2T >> >>> 326G >> >>> >>> 88% >> >>> >>> >> >> /umbc/software >> >>> >>> >> >> >>> >>> >> >> mohammad@alborz:~$ mount | grep /umbc/software >> >>> >>> >> >> nfs.iss.rs.umbc.edu:/ifs/data/csee/software on >> >>> /umbc/software >> >>> >>> type nfs >> >>> >>> >> >> >> >>> >>> >> >>> >> (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namlen=3D255,acregmi= n=3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,proto=3Dtcp,tim= eo=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountvers=3D3,mountpo= rt=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.60) >> >>> >>> >> >> >>> >>> >> >> Please let me know if you need other information. >> >>> >>> >> >> >>> >>> >> >> Regards, >> >>> >>> >> >> >>> >>> >> >> Mohammad >> >>> >>> >> >> >>> >>> >> >> On Wed Oct 22 13:15:12 2025, HK41259 wrote: >> >>> >>> >> >> >>> >>> >> >>> Hello, >> >>> >>> >> >> >>> >>> >> >>> Thanks for reaching out. Could you clarify which >> >>> specific >> >>> >>> machine >> >>> >>> >> >>> or server you=E2=80=99re referring to when you mention the >> >>> >>> =E2=80=9Cdepartmental >> >>> >>> >> >>> server=E2=80=9D? >> >>> >>> >> >> >>> >>> >> >>> The /umbc/software/scripts/ directory isn=E2=80=99t acces= sible >> >>> from >> >>> >>> HPCF >> >>> >>> >> >>> (e.g., Chip) by default =E2=80=94 those systems have sepa= rate >> >>> >>> storage and >> >>> >>> >> >>> don=E2=80=99t automatically mount departmental shares. >> >>> >>> >> >> >>> >>> >> >>> Could you check where /umbc/software is mounted on your >> >>> >>> >> >>> departmental system (e.g., by running df -h >> >>> /umbc/software >> >>> >>> or mount >> >>> >>> >> >>> | grep /umbc/software)? That=E2=80=99ll tell us which ser= ver >> >>> hosts >> >>> >>> it. Once >> >>> >>> >> >>> we know that, we can confirm whether it=E2=80=99s possibl= e or >> >>> >>> appropriate >> >>> >>> >> >>> to make it visible from HPCF. >> >>> >>> >> >> >>> >>> >> >>> Best, >> >>> >>> >> >> >>> >>> >> >>> Tartela >> >>> >>> >> >> >>> >>> >> >>> On Tue Oct 21 15:59:49 2025, ZZ99999 wrote: >> >>> >>> >> >> >>> >>> >> >>>> First Name: Mohammad >> >>> >>> >> >>>> Last Name: Ebrahimabadi >> >>> >>> >> >>>> Email: e127@umbc.edu <mailto:e127@umbc.edu> >> >>> >>> >> >>>> Campus ID: NQ23652 >> >>> >>> >> >>>> >> >>> >>> >> >>>> Request Type: High Performance Cluster >> >>> >>> >> >>>> >> >>> >>> >> >>>> Dear Team, >> >>> >>> >> >>>> >> >>> >>> >> >>>> I hope you=E2=80=99re doing well. >> >>> >>> >> >>>> I=E2=80=99m a Ph.D. student in Computer Engineering and = have a >> >>> >>> question regarding running a software on the HPCF. On the >> >>> >>> department server, I usually run software using shell scripts >> >>> >>> located at /umbc/software/scripts/ which is provided by the IT >> >>> >>> group pf the department (Geoff Weiss Team). However, in case of >> >>> >>> HPCF, it seems that this path is not accessible from the HPCF >> >>> >>> environment. >> >>> >>> >> >>>> >> >>> >>> >> >>>> Could you please let me know if there is any specific >> >>> >>> configuration or access permission required to reach this >> >>> directory >> >>> >>> through the HPCF? or direct me to a related person that can >> >>> help >> >>> >>> me. >> >>> >>> >> >>>> >> >>> >>> >> >>>> Best regards, >> >>> >>> >> >>>> Mohammad >> >>> >>> >> >>> >> >>> >>> >> >> >>> >>> >> > -- >> >>> >>> >> >> >>> >>> >> > Best, >> >>> >>> >> > Max Breitmeyer >> >>> >>> >> > DOIT HPC System Administrator >> >>> >>> >> >> >>> >>> >> >> >>> >>> > >> >>> >>> > >> >>> >>> > >> >>> >>> > -- >> >>> >>> > Naghmeh Karimi, Ph.D. >> >>> >>> > Associate Professor >> >>> >>> > Department of Computer Science and Electrical Engineering >> >>> >>> > University of Maryland, Baltimore County >> >>> >>> > Baltimore, MD 21250 >> >>> >>> > Tel: 410-455-3965 E-mail: nkarimi@umbc.edu >> >>> >>> <mailto:nkarimi@umbc.edu> >> >>> >>> > Web: http://www.csee.umbc.edu/~nkarimi/ >> <https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&sourc= e=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW> >> >>> >>> >> >>> < >> https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source= =3Dgmail-imap&ust=3D1761768891000000&usg=3DAOvVaw0xLE02sw5-ueWRHyHyCwkN >> <https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://ww= w.csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D1761768891000000%26= usg%3DAOvVaw0xLE02sw5-ueWRHyHyCwkN&source=3Dgmail-imap&ust=3D17618743740000= 00&usg=3DAOvVaw3g338pGQqShCgyX8cIt8tV> >> > >> >> >>> >> -- Naghmeh Karimi, Ph.D. >> >>> >> Associate Professor >> >>> >> Department of Computer Science and Electrical Engineering >> >>> >> University of Maryland, Baltimore County >> >>> >> Baltimore, MD 21250 >> >>> >> Tel: 410-455-3965 E-mail: nkarimi@umbc.edu >> >>> >> Web: http://www.csee.umbc.edu/~nkarimi/ >> <https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&sourc= e=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW> >> >> >>> > -- >> >> >>> > Best, >> >>> > Max Breitmeyer >> >>> > DOIT HPC System Administrator >> >> >> -- V/R,Maxwell BreitmeyerUMBC HPCF SpecialistUMBC Observatory IT >> >> Manager Graduate Student(443) 835-8250 >> >> -- >> >> Best, >> Max Breitmeyer >> DOIT HPC System Administrator >> >> >> >  --=20 Naghmeh Karimi, Ph.D. Associate Professor Department of Computer Science and Electrical Engineering University of Maryland, Baltimore County Baltimore, MD 21250 Tel: *410-455-3965* E-mail: *nkarimi@umbc.edu <nkarimi@umbc.edu>* Web: *http://www.csee.umbc.edu/~nkarimi/ <http://www.csee.umbc.edu/~nkarimi/>* "
3296910,72474850,Correspond,DoIT-Research-Computing,2025-10-24 13:42:30.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,NULL,naghmeh.karimi@umbc.edu,"<div dir=3D""ltr"">That&#39;s perfect Skye. I highly appreciate=C2=A0it. I as= k Mohamamd to respond the=C2=A0ticket with the path.<div><br></div><div>reg= ards,</div><div>Naghmeh</div></div><br><div class=3D""gmail_quote gmail_quot= e_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Fri, Oct 24, 2025 at = 9:39=E2=80=AFAM Skye Jonke &lt;<a href=3D""mailto:ii69854@umbc.edu"">ii69854@= umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D= ""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-le= ft:1ex""><div>Its not possible to use the existing scripts in a situation wh= ere the files aren=E2=80=99t mounted in /umbc/software, but I can write som= e scripts specific to this setup to this situation with a bit more informat= ion. Where are the script files located on the HPC? If you aren=E2=80=99t s= ure, you can get the full path by running =E2=80=9Crealpath myfile.sh=E2=80= =9D (replacing myfile.sh with one of the cadence scripts, of course)=C2=A0<= br id=3D""m_-919549489414609192lineBreakAtBeginningOfMessage""><div> <div dir=3D""auto"" style=3D""color:rgb(0,0,0);letter-spacing:normal;text-alig= n:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing= :0px;text-decoration:none"">---</div><div dir=3D""auto"" style=3D""color:rgb(0,= 0,0);letter-spacing:normal;text-align:start;text-indent:0px;text-transform:= none;white-space:normal;word-spacing:0px;text-decoration:none"">Skye Jonke<b= r><br>she/her<br><br>Specialist, Linux System Administrator &amp;<span>=C2= =A0</span>Lab Technical Support<br></div> </div> <div><br><blockquote type=3D""cite""><div>On Oct 23, 2025, at 21:32, Naghmeh = Karimi &lt;<a href=3D""mailto:nkarimi@umbc.edu"" target=3D""_blank"">nkarimi@um= bc.edu</a>&gt; wrote:</div><br><div><div dir=3D""auto""><div>Hi Max, thanks= =C2=A0for getting back to us.</div><div dir=3D""auto""><br></div><div dir=3D""= auto"">Hi Skye, we need your help here to find the file locations. Could you=  please help to resolve=C2=A0this issue?</div><div dir=3D""auto""><br></div><= div dir=3D""auto"">Thanks a lot</div><div dir=3D""auto"">Naghmeh=C2=A0</div><di= v><br></div><div><div dir=3D""ltr""><span style=3D""font-size:9.5pt;line-heigh= t:115%;font-family:Arial,sans-serif;color:rgb(136,136,136);background-image= :initial;background-position:initial;background-repeat:initial"">Naghmeh=C2= =A0Karimi, Ph.D.<br> Associate Professor<br> Department of Computer Science and Electrical Engineering<br> University of Maryland, Baltimore County<br> Baltimore, MD 21250<br> Tel:</span><span style=3D""font-size:11pt;line-height:115%;font-family:Calib= ri,sans-serif"">=C2=A0</span><u><span style=3D""font-size:11pt;line-height:11= 5%;font-family:Calibri,&quot;sans-serif&quot;;color:rgb(17,85,204)"">410-455= -3965</span></u><span style=3D""font-size:11pt;line-height:115%;font-family:= Calibri,sans-serif""> </span><span style=3D""font-size:9.5pt;line-height:115%= ;font-family:Arial,sans-serif;color:rgb(136,136,136);background-image:initi= al;background-position:initial;background-repeat:initial"">E-mail:</span><sp= an style=3D""font-size:11pt;line-height:115%;font-family:Calibri,sans-serif""= >=C2=A0</span><u><span style=3D""font-size:11pt;line-height:115%;font-family= :Calibri,&quot;sans-serif&quot;;color:rgb(17,85,204)""><a href=3D""mailto:nka= rimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a></span></u><span style= =3D""font-size:9.5pt;line-height:115%;font-family:Arial,sans-serif;color:rgb= (136,136,136);background-image:initial;background-position:initial;backgrou= nd-repeat:initial""><br> Web:</span><span style=3D""font-size:9.5pt;line-height:115%;font-family:Aria= l,sans-serif;background-image:initial;background-position:initial;backgroun= d-repeat:initial"">=C2=A0</span><u><span style=3D""font-size:11pt;line-height= :115%;font-family:Calibri,sans-serif;color:rgb(17,85,204);background-image:= initial;background-position:initial;background-repeat:initial""><a href=3D""h= ttps://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source= =3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw23Fk_KFBnlrXC_uhIb= UFIW"" target=3D""_blank"">http://www.csee.umbc.edu/~nkarimi/</a></span></u></= div></div></div><br><div class=3D""gmail_quote""><div dir=3D""ltr"" class=3D""gm= ail_attr"">On Thu, Oct 23, 2025, 12:22=E2=80=AFPM Max Breitmeyer via RT &lt;= <a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.= edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""marg= in:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1e= x"">Ticket &lt;URL: <a href=3D""https://www.google.com/url?q=3Dhttps://rt.umb= c.edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D17= 61874374000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7"" rel=3D""noreferrer no= referrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D32= 96910</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Mohammad,<br> <br> According to the error message, it&#39;s having an issue finding another fi= le in<br> the directory. This appears to be an issue due to the mounting locations be= ing<br> different. I would point you to the previous email where I said that any<br> software with hard-coded locations would not work, and that software<br> compatibility is not guaranteed. I would recommend working with Skye on por= ting<br> what is in the directory to your local research directories on chip.<br> <br> On Thu Oct 23 12:09:34 2025, NQ23652 wrote:<br> <br> &gt; Yes you are right, now I could see all shell files. However still I ha= ve<br> &gt; problem when I run the sh file.<br> <br> &gt; [e127@chip-login2 scripts]$ sh launch_synopsys_hspice.sh<br> &gt; launch_synopsys_hspice.sh: line 3:<br> &gt; /umbc/software/scripts/env_synopsys_hspice.sh: No such file or directo= ry<br> &gt; [e127@chip-login2 scripts]$<br> <br> &gt; Would you please take a look to it.<br> <br> &gt; Regards,<br> <br> &gt; Mohammad<br> <br> &gt; On Thu Oct 23 11:59:17 2025, OL73413 wrote:<br> <br> &gt;&gt; Hi Mohammad,<br> <br> &gt;&gt; A good point. Chip uses a service called &quot;autofs&quot; to mou= nt and unmount<br> &gt;&gt; directories that aren&#39;t being actively used. This helps mainta= in the<br> &gt;&gt; login nodes integrity. If you see in my screenshot below, the dire= ctory<br> &gt;&gt; appears to be empty, but when I cd to the csee directory where you= r<br> &gt;&gt; software is mounted, it is all available. Backing out of the direc= tory<br> &gt;&gt; and ls&#39;ing the directory also shows that the directory is now = mounted<br> &gt;&gt; and available.<br> <br> &gt;&gt; On Thu, Oct 23, 2025 at 11:41 AM Mohammad Ebrahimabadi via RT<br> &gt;&gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" rel=3D""noreferrer"" tar= get=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt;&gt; Ticket &lt;URL: <a href=3D""https://www.google.com/url?q=3Dhttp= s://rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&am= p;ust=3D1761874374000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7"" rel=3D""nor= eferrer noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.ht= ml?id=3D3296910</a> &gt;<br> <br> &gt;&gt;&gt; Comment From Ticket:<br> &gt;&gt;&gt; Thank you Max!<br> <br> &gt;&gt;&gt; Actually we have still problem for accessing that directory. I= <br> &gt;&gt;&gt; uploaded two<br> &gt;&gt;&gt; screenshots that shows what we can see in the department serve= rs<br> &gt;&gt;&gt; and what we<br> &gt;&gt;&gt; can see in CHIP.<br> <br> &gt;&gt;&gt; For example by running this shell file we can launch HSPICE<br> &gt;&gt;&gt; software in the<br> &gt;&gt;&gt; department server:<br> &gt;&gt;&gt; /umbc/software/scripts/launch_synopsys_hspice.sh<br> &gt;&gt;&gt; But it is not still accessible on CHIP.<br> <br> &gt;&gt;&gt; Regards,<br> <br> &gt;&gt;&gt; Mohammad<br> <br> &gt;&gt;&gt; On Thu Oct 23 10:54:59 2025, OL73413 wrote:<br> <br> &gt;&gt;&gt; &gt; Hi Nahmeh,<br> <br> &gt;&gt;&gt; &gt; Apologies =E2=80=94 I meant to update you yesterday after=  completing<br> &gt;&gt;&gt; this, but I<br> &gt;&gt;&gt; &gt; was pulled into a long meeting. The software is now avail= able on<br> &gt;&gt;&gt; chip at:<br> <br> &gt;&gt;&gt; &gt; /umbc/software/csee<br> <br> &gt;&gt;&gt; &gt; Please note that if the software contains any hardcoded f= ile<br> &gt;&gt;&gt; paths, it may<br> &gt;&gt;&gt; &gt; not function correctly. Unfortunately, we don=E2=80=99t h= ave control over<br> &gt;&gt;&gt; those<br> &gt;&gt;&gt; &gt; cases.<br> <br> &gt;&gt;&gt; &gt; Also, just to clarify, Skye and I work in separate depart= ments.<br> &gt;&gt;&gt; Similar to<br> &gt;&gt;&gt; &gt; how I don=E2=80=99t have administrative access to the CSE= E department<br> &gt;&gt;&gt; servers<br> &gt;&gt;&gt; &gt; (which include the software directory), Skye is not an<br> &gt;&gt;&gt; administrator on<br> &gt;&gt;&gt; &gt; chip.<br> <br> &gt;&gt;&gt; &gt; Please let me know if you encounter any issues accessing = the<br> &gt;&gt;&gt; software on<br> &gt;&gt;&gt; &gt; the server. If there are problems running the software, I= =E2=80=99ll<br> &gt;&gt;&gt; coordinate<br> &gt;&gt;&gt; &gt; with Skye to troubleshoot, but since we don=E2=80=99t mai= ntain this<br> &gt;&gt;&gt; software, we<br> &gt;&gt;&gt; &gt; can=E2=80=99t guarantee full compatibility on our systems= .<br> <br> &gt;&gt;&gt; &gt; On Thu Oct 23 10:36:41 2025, <a href=3D""mailto:naghmeh.ka= rimi@umbc.edu"" rel=3D""noreferrer"" target=3D""_blank"">naghmeh.karimi@umbc.edu= </a> wrote:<br> <br> &gt;&gt;&gt; &gt;&gt; Hi Skye and Max, Could you please help on mounting th= is today so<br> &gt;&gt;&gt; we can<br> &gt;&gt;&gt; &gt;&gt; use them as we have a deadline? Thanks,Naghmeh Karimi=  On Wed,<br> &gt;&gt;&gt; Oct 22,<br> &gt;&gt;&gt; &gt;&gt; 2025 at 4:22 PM Skye Jonke via RT &lt;<a href=3D""mail= to:UMBCHelp@rt.umbc.edu"" rel=3D""noreferrer"" target=3D""_blank"">UMBCHelp@rt.u= mbc.edu</a>&gt; wrote:<br> <br> &gt;&gt;&gt; &gt;&gt;&gt; Ticket &lt;URL: <a href=3D""https://www.google.com= /url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;source=3D= gmail-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w= 7"" rel=3D""noreferrer noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tick= et/Display.html?id=3D3296910</a><br> &gt;&gt;&gt; &gt;<br> <br> &gt;&gt;&gt; &gt;&gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt;&gt; &gt;&gt;&gt; Mohammad,<br> <br> &gt;&gt;&gt; &gt;&gt;&gt; So long as you continue to access them through<br> &gt;&gt;&gt; &gt;&gt;&gt; /umbc/software/scripts, they should remain up to = date and you<br> &gt;&gt;&gt; will<br> &gt;&gt;&gt; &gt;&gt;&gt; see any new scripts. If you copy them to a local = directory on<br> &gt;&gt;&gt; the<br> &gt;&gt;&gt; &gt;&gt;&gt; HPC cluster, they will not be updated.<br> <br> &gt;&gt;&gt; &gt;&gt;&gt; ---<br> &gt;&gt;&gt; &gt;&gt;&gt; Skye Jonke<br> <br> &gt;&gt;&gt; &gt;&gt;&gt; she/her<br> <br> &gt;&gt;&gt; &gt;&gt;&gt; Specialist, Linux System Administrator &amp; Lab = Technical Support<br> <br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; On Oct 22, 2025, at 16:14, Naghmeh Karimi &l= t;<a href=3D""mailto:nkarimi@umbc.edu"" rel=3D""noreferrer"" target=3D""_blank"">= nkarimi@umbc.edu</a>&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; wrote:<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Hi Max, Roy, Skye,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Thank you all for your help.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; I added Skye here as she is our IT exert on = adding the<br> &gt;&gt;&gt; capability<br> &gt;&gt;&gt; &gt;&gt;&gt; of running synopsis tools (Hspice) in servers.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Skye we want to use the HPC cluster to run h= spice an other<br> &gt;&gt;&gt; &gt;&gt;&gt; synopsys tools. Could you please help on this?<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Thanks,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Naghmeh<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; On Wed, Oct 22, 2025 at 2:06 PM Mohammad Ebr= ahimabadi via RT<br> &gt;&gt;&gt; &gt;&gt;&gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" rel= =3D""noreferrer"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a> &lt;mailto:<a hr= ef=3D""mailto:UMBCHelp@rt.umbc.edu"" rel=3D""noreferrer"" target=3D""_blank"">UMB= CHelp@rt.umbc.edu</a>&gt;&gt; wrote:<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Ticket &lt;URL:<br> &gt;&gt;&gt; <a href=3D""https://www.google.com/url?q=3Dhttps://rt.umbc.edu/= Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D17618743= 74000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7"" rel=3D""noreferrer noreferr= er"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3296910<= /a><br> &gt;&gt;&gt; &gt;&gt;&gt;<br> &gt;&gt;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttps://www.goog= le.com/url?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26sou= rce%3Dgmail-imap%26ust%3D1761768891000000%26usg%3DAOvVaw2jxB3nArHBD7-ij9ZN8= ZsW&amp;source=3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw0fLy= lAsBAoQ6LgqY79I9-d"" rel=3D""noreferrer noreferrer"" target=3D""_blank"">https:/= /www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D329691= 0&amp;source=3Dgmail-imap&amp;ust=3D1761768891000000&amp;usg=3DAOvVaw2jxB3n= ArHBD7-ij9ZN8ZsW</a>&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Last Update From Ticket:<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Hi Max,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Thanks for your help! Let=E2=80=99s proc= eed based on your<br> &gt;&gt;&gt; suggestion,<br> &gt;&gt;&gt; &gt;&gt;&gt; hopefully it<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; works as expected.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; I just have one quick question: since th= e IT team in the<br> &gt;&gt;&gt; &gt;&gt;&gt; department can modify<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; or add new shell scripts in that directo= ry, will we also<br> &gt;&gt;&gt; have<br> &gt;&gt;&gt; &gt;&gt;&gt; access to those<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; updated or newly added scripts from the = mounted directory on<br> &gt;&gt;&gt; &gt;&gt;&gt; HPCF?<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Thanks again,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Mohammad<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; On Wed Oct 22 14:01:32 2025, OL73413 wro= te:<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Hi Mohammad,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; We can get the software mounted for=  you on chip, but it<br> &gt;&gt;&gt; will<br> &gt;&gt;&gt; &gt;&gt;&gt; have to be<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; read-only as we won&#39;t be able t= o manage who can make<br> &gt;&gt;&gt; changes<br> &gt;&gt;&gt; &gt;&gt;&gt; to it<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; otherwise.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; As a reminder, the machines on that=  run on chip are not<br> &gt;&gt;&gt; the<br> &gt;&gt;&gt; &gt;&gt;&gt; same as those<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; run by the CSEE department, so we c= an&#39;t guarantee that<br> &gt;&gt;&gt; these<br> &gt;&gt;&gt; &gt;&gt;&gt; will run the<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; same way or as effectively.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; If that&#39;s all ok with you, plea= se give me some time to get<br> &gt;&gt;&gt; the<br> &gt;&gt;&gt; &gt;&gt;&gt; directory<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; mounted.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; On Wed Oct 22 13:26:05 2025, NQ2365= 2 wrote:<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thanks Tartela for your help to=  resolve my issue!<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Actually it seems all server in=  CSEE department have<br> &gt;&gt;&gt; access<br> &gt;&gt;&gt; &gt;&gt;&gt; to that<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; path. When I ran the command th= at you gave me I got the<br> &gt;&gt;&gt; &gt;&gt;&gt; following<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; report:<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; mohammad@alborz:~$ df -h /umbc/= software<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Filesystem Size Used Avail Use%=  Mounted on<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; nfs.iss.rs.umbc.edu:/ifs/data/c= see/software 2.5T 2.2T<br> &gt;&gt;&gt; 326G<br> &gt;&gt;&gt; &gt;&gt;&gt; 88%<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; /umbc/software<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; mohammad@alborz:~$ mount | grep=  /umbc/software<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; nfs.iss.rs.umbc.edu:/ifs/data/c= see/software on<br> &gt;&gt;&gt; /umbc/software<br> &gt;&gt;&gt; &gt;&gt;&gt; type nfs<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt;<br> &gt;&gt;&gt; (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namlen=3D2= 55,acregmin=3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,proto= =3Dtcp,timeo=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountvers= =3D3,mountport=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.60)<= br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Please let me know if you need = other information.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Regards,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Mohammad<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; On Wed Oct 22 13:15:12 2025, HK= 41259 wrote:<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Hello,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Thanks for reaching out. Co= uld you clarify which<br> &gt;&gt;&gt; specific<br> &gt;&gt;&gt; &gt;&gt;&gt; machine<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; or server you=E2=80=99re re= ferring to when you mention the<br> &gt;&gt;&gt; &gt;&gt;&gt; =E2=80=9Cdepartmental<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; server=E2=80=9D?<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; The /umbc/software/scripts/=  directory isn=E2=80=99t accessible<br> &gt;&gt;&gt; from<br> &gt;&gt;&gt; &gt;&gt;&gt; HPCF<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; (e.g., Chip) by default =E2= =80=94 those systems have separate<br> &gt;&gt;&gt; &gt;&gt;&gt; storage and<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; don=E2=80=99t automatically=  mount departmental shares.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Could you check where /umbc= /software is mounted on your<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; departmental system (e.g., = by running df -h<br> &gt;&gt;&gt; /umbc/software<br> &gt;&gt;&gt; &gt;&gt;&gt; or mount<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; | grep /umbc/software)? Tha= t=E2=80=99ll tell us which server<br> &gt;&gt;&gt; hosts<br> &gt;&gt;&gt; &gt;&gt;&gt; it. Once<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; we know that, we can confir= m whether it=E2=80=99s possible or<br> &gt;&gt;&gt; &gt;&gt;&gt; appropriate<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; to make it visible from HPC= F.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Best,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Tartela<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; On Tue Oct 21 15:59:49 2025= , ZZ99999 wrote:<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; First Name: Mohammad<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Last Name: Ebrahimabadi= <br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Email: <a href=3D""mailt= o:e127@umbc.edu"" rel=3D""noreferrer"" target=3D""_blank"">e127@umbc.edu</a> &lt= ;mailto:<a href=3D""mailto:e127@umbc.edu"" rel=3D""noreferrer"" target=3D""_blan= k"">e127@umbc.edu</a>&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Campus ID: NQ23652<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Request Type: High Perf= ormance Cluster<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Dear Team,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; I hope you=E2=80=99re d= oing well.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; I=E2=80=99m a Ph.D. stu= dent in Computer Engineering and have a<br> &gt;&gt;&gt; &gt;&gt;&gt; question regarding running a software on the HPCF= . On the<br> &gt;&gt;&gt; &gt;&gt;&gt; department server, I usually run software using s= hell scripts<br> &gt;&gt;&gt; &gt;&gt;&gt; located at /umbc/software/scripts/ which is provi= ded by the IT<br> &gt;&gt;&gt; &gt;&gt;&gt; group pf the department (Geoff Weiss Team). Howev= er, in case of<br> &gt;&gt;&gt; &gt;&gt;&gt; HPCF, it seems that this path is not accessible f= rom the HPCF<br> &gt;&gt;&gt; &gt;&gt;&gt; environment.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Could you please let me=  know if there is any specific<br> &gt;&gt;&gt; &gt;&gt;&gt; configuration or access permission required to re= ach this<br> &gt;&gt;&gt; directory<br> &gt;&gt;&gt; &gt;&gt;&gt; through the HPCF? or direct me to a related perso= n that can<br> &gt;&gt;&gt; help<br> &gt;&gt;&gt; &gt;&gt;&gt; me.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Best regards,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Mohammad<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; --<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Best,<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Max Breitmeyer<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; DOIT HPC System Administrator<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; --<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Naghmeh Karimi, Ph.D.<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Associate Professor<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Department of Computer Science and Electrica= l Engineering<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; University of Maryland, Baltimore County<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Baltimore, MD 21250<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Tel: 410-455-3965 E-mail: <a href=3D""mailto:= nkarimi@umbc.edu"" rel=3D""noreferrer"" target=3D""_blank"">nkarimi@umbc.edu</a>= <br> &gt;&gt;&gt; &gt;&gt;&gt; &lt;mailto:<a href=3D""mailto:nkarimi@umbc.edu"" re= l=3D""noreferrer"" target=3D""_blank"">nkarimi@umbc.edu</a>&gt;<br> &gt;&gt;&gt; &gt;&gt;&gt; &gt; Web: <a href=3D""https://www.google.com/url?q= =3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D176= 1874374000000&amp;usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW"" rel=3D""noreferrer nor= eferrer"" target=3D""_blank"">http://www.csee.umbc.edu/~nkarimi/</a><br> &gt;&gt;&gt; &gt;&gt;&gt;<br> &gt;&gt;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttps://www.goog= le.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26u= st%3D1761768891000000%26usg%3DAOvVaw0xLE02sw5-ueWRHyHyCwkN&amp;source=3Dgma= il-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw3g338pGQqShCgyX8cIt8tV"" = rel=3D""noreferrer noreferrer"" target=3D""_blank"">https://www.google.com/url?= q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D17= 61768891000000&amp;usg=3DAOvVaw0xLE02sw5-ueWRHyHyCwkN</a>&gt;<br> <br> &gt;&gt;&gt; &gt;&gt; -- Naghmeh Karimi, Ph.D.<br> &gt;&gt;&gt; &gt;&gt; Associate Professor<br> &gt;&gt;&gt; &gt;&gt; Department of Computer Science and Electrical Enginee= ring<br> &gt;&gt;&gt; &gt;&gt; University of Maryland, Baltimore County<br> &gt;&gt;&gt; &gt;&gt; Baltimore, MD 21250<br> &gt;&gt;&gt; &gt;&gt; Tel: 410-455-3965 E-mail: <a href=3D""mailto:nkarimi@u= mbc.edu"" rel=3D""noreferrer"" target=3D""_blank"">nkarimi@umbc.edu</a><br> &gt;&gt;&gt; &gt;&gt; Web: <a href=3D""https://www.google.com/url?q=3Dhttp:/= /www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D176187437400= 0000&amp;usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW"" rel=3D""noreferrer noreferrer"" = target=3D""_blank"">http://www.csee.umbc.edu/~nkarimi/</a><br> <br> &gt;&gt;&gt; &gt; --<br> <br> &gt;&gt;&gt; &gt; Best,<br> &gt;&gt;&gt; &gt; Max Breitmeyer<br> &gt;&gt;&gt; &gt; DOIT HPC System Administrator<br> <br> &gt;&gt; -- V/R,Maxwell BreitmeyerUMBC HPCF SpecialistUMBC Observatory IT<b= r> &gt;&gt; Manager Graduate Student(443) 835-8250<br> <br> --<br> <br> Best,<br> Max Breitmeyer<br> DOIT HPC System Administrator<br> <br> <br> </blockquote></div> </div></blockquote></div><br></div></blockquote></div><div><br clear=3D""all= ""></div><div><br></div><span class=3D""gmail_signature_prefix"">-- </span><br= ><div dir=3D""ltr"" class=3D""gmail_signature""><div dir=3D""ltr""><span style=3D= ""font-size:9.5pt;line-height:115%;font-family:Arial,sans-serif;color:rgb(13= 6,136,136);background-image:initial;background-position:initial;background-= repeat:initial"">Naghmeh=C2=A0Karimi, Ph.D.<br> Associate Professor<br> Department of Computer Science and Electrical Engineering<br> University of Maryland, Baltimore County<br> Baltimore, MD 21250<br> Tel:</span><span style=3D""font-size:11pt;line-height:115%;font-family:Calib= ri,&quot;sans-serif&quot;;color:black"">=C2=A0</span><u><span style=3D""font-= size:11pt;line-height:115%;font-family:Calibri,&quot;sans-serif&quot;;color= :rgb(17,85,204)"">410-455-3965</span></u><span style=3D""font-size:11pt;line-= height:115%;font-family:Calibri,&quot;sans-serif&quot;;color:black""> </span= ><span style=3D""font-size:9.5pt;line-height:115%;font-family:Arial,sans-ser= if;color:rgb(136,136,136);background-image:initial;background-position:init= ial;background-repeat:initial"">E-mail:</span><span style=3D""font-size:11pt;= line-height:115%;font-family:Calibri,&quot;sans-serif&quot;;color:black"">= =C2=A0</span><u><span style=3D""font-size:11pt;line-height:115%;font-family:= Calibri,&quot;sans-serif&quot;;color:rgb(17,85,204)""><a href=3D""mailto:nkar= imi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a></span></u><span style= =3D""font-size:9.5pt;line-height:115%;font-family:Arial,sans-serif;color:rgb= (136,136,136);background-image:initial;background-position:initial;backgrou= nd-repeat:initial""><br> Web:</span><span style=3D""font-size:9.5pt;line-height:115%;font-family:Aria= l,sans-serif;color:black;background-image:initial;background-position:initi= al;background-repeat:initial"">=C2=A0</span><u><span style=3D""font-size:11pt= ;line-height:115%;font-family:Calibri,sans-serif;color:rgb(17,85,204);backg= round-image:initial;background-position:initial;background-repeat:initial"">= <a href=3D""http://www.csee.umbc.edu/~nkarimi/"" target=3D""_blank"">http://www= .csee.umbc.edu/~nkarimi/</a></span></u></div></div> "
3296910,72477484,Correspond,DoIT-Research-Computing,2025-10-24 14:46:39.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Mohammad Ebrahimabadi,e127@umbc.edu,"Hello Skye,  I believe the way that Max told me to run the script was using the same path that we have in the CSEE department as he mounted that path in HPCF. I mean this path:  /umbc/software/scripts/launch_synopsys_hspice.sh   Regards, Mohammad     On Fri, Oct 24, 2025 at 9:42=E2=80=AFAM naghmeh.karimi@umbc.edu via RT < UMBCHelp@rt.umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 > > > Last Update From Ticket: > > That's perfect Skye. I highly appreciate it. I ask Mohamamd to respond > the ticket with the path. > > regards, > Naghmeh > > On Fri, Oct 24, 2025 at 9:39=E2=80=AFAM Skye Jonke <ii69854@umbc.edu> wro= te: > > > Its not possible to use the existing scripts in a situation where the > > files aren=E2=80=99t mounted in /umbc/software, but I can write some sc= ripts > > specific to this setup to this situation with a bit more information. > Where > > are the script files located on the HPC? If you aren=E2=80=99t sure, yo= u can get > > the full path by running =E2=80=9Crealpath myfile.sh=E2=80=9D (replacin= g myfile.sh with > one > > of the cadence scripts, of course) > > --- > > Skye Jonke > > > > she/her > > > > Specialist, Linux System Administrator & Lab Technical Support > > > > On Oct 23, 2025, at 21:32, Naghmeh Karimi <nkarimi@umbc.edu> wrote: > > > > Hi Max, thanks for getting back to us. > > > > Hi Skye, we need your help here to find the file locations. Could you > > please help to resolve this issue? > > > > Thanks a lot > > Naghmeh > > > > Naghmeh Karimi, Ph.D. > > Associate Professor > > Department of Computer Science and Electrical Engineering > > University of Maryland, Baltimore County > > Baltimore, MD 21250 > > Tel: *410-455-3965* E-mail: *nkarimi@umbc.edu <nkarimi@umbc.edu>* > > Web: *http://www.csee.umbc.edu/~nkarimi/ > > < > https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source= =3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW > >* > > > > On Thu, Oct 23, 2025, 12:22=E2=80=AFPM Max Breitmeyer via RT < > UMBCHelp@rt.umbc.edu> > > wrote: > > > >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 > >> < > https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id= %3D3296910&source=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw3ImziGtKZ= -9j9JFW8J65w7 > > > >> > > >> > >> Last Update From Ticket: > >> > >> Hi Mohammad, > >> > >> According to the error message, it's having an issue finding another > file > >> in > >> the directory. This appears to be an issue due to the mounting locatio= ns > >> being > >> different. I would point you to the previous email where I said that a= ny > >> software with hard-coded locations would not work, and that software > >> compatibility is not guaranteed. I would recommend working with Skye on > >> porting > >> what is in the directory to your local research directories on chip. > >> > >> On Thu Oct 23 12:09:34 2025, NQ23652 wrote: > >> > >> > Yes you are right, now I could see all shell files. However still I > have > >> > problem when I run the sh file. > >> > >> > [e127@chip-login2 scripts]$ sh launch_synopsys_hspice.sh > >> > launch_synopsys_hspice.sh: line 3: > >> > /umbc/software/scripts/env_synopsys_hspice.sh: No such file or > directory > >> > [e127@chip-login2 scripts]$ > >> > >> > Would you please take a look to it. > >> > >> > Regards, > >> > >> > Mohammad > >> > >> > On Thu Oct 23 11:59:17 2025, OL73413 wrote: > >> > >> >> Hi Mohammad, > >> > >> >> A good point. Chip uses a service called ""autofs"" to mount and > unmount > >> >> directories that aren't being actively used. This helps maintain the > >> >> login nodes integrity. If you see in my screenshot below, the > directory > >> >> appears to be empty, but when I cd to the csee directory where your > >> >> software is mounted, it is all available. Backing out of the > directory > >> >> and ls'ing the directory also shows that the directory is now mount= ed > >> >> and available. > >> > >> >> On Thu, Oct 23, 2025 at 11:41 AM Mohammad Ebrahimabadi via RT > >> >> <UMBCHelp@rt.umbc.edu> wrote: > >> > >> >>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 > >> < > https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id= %3D3296910&source=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw3ImziGtKZ= -9j9JFW8J65w7 > > > >> > > >> > >> >>> Comment From Ticket: > >> >>> Thank you Max! > >> > >> >>> Actually we have still problem for accessing that directory. I > >> >>> uploaded two > >> >>> screenshots that shows what we can see in the department servers > >> >>> and what we > >> >>> can see in CHIP. > >> > >> >>> For example by running this shell file we can launch HSPICE > >> >>> software in the > >> >>> department server: > >> >>> /umbc/software/scripts/launch_synopsys_hspice.sh > >> >>> But it is not still accessible on CHIP. > >> > >> >>> Regards, > >> > >> >>> Mohammad > >> > >> >>> On Thu Oct 23 10:54:59 2025, OL73413 wrote: > >> > >> >>> > Hi Nahmeh, > >> > >> >>> > Apologies =E2=80=94 I meant to update you yesterday after comple= ting > >> >>> this, but I > >> >>> > was pulled into a long meeting. The software is now available on > >> >>> chip at: > >> > >> >>> > /umbc/software/csee > >> > >> >>> > Please note that if the software contains any hardcoded file > >> >>> paths, it may > >> >>> > not function correctly. Unfortunately, we don=E2=80=99t have con= trol over > >> >>> those > >> >>> > cases. > >> > >> >>> > Also, just to clarify, Skye and I work in separate departments. > >> >>> Similar to > >> >>> > how I don=E2=80=99t have administrative access to the CSEE depar= tment > >> >>> servers > >> >>> > (which include the software directory), Skye is not an > >> >>> administrator on > >> >>> > chip. > >> > >> >>> > Please let me know if you encounter any issues accessing the > >> >>> software on > >> >>> > the server. If there are problems running the software, I=E2=80= =99ll > >> >>> coordinate > >> >>> > with Skye to troubleshoot, but since we don=E2=80=99t maintain t= his > >> >>> software, we > >> >>> > can=E2=80=99t guarantee full compatibility on our systems. > >> > >> >>> > On Thu Oct 23 10:36:41 2025, naghmeh.karimi@umbc.edu wrote: > >> > >> >>> >> Hi Skye and Max, Could you please help on mounting this today so > >> >>> we can > >> >>> >> use them as we have a deadline? Thanks,Naghmeh Karimi On Wed, > >> >>> Oct 22, > >> >>> >> 2025 at 4:22 PM Skye Jonke via RT <UMBCHelp@rt.umbc.edu> wrote: > >> > >> >>> >>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296= 910 > >> < > https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id= %3D3296910&source=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw3ImziGtKZ= -9j9JFW8J65w7 > > > >> >>> > > >> > >> >>> >>> Last Update From Ticket: > >> > >> >>> >>> Mohammad, > >> > >> >>> >>> So long as you continue to access them through > >> >>> >>> /umbc/software/scripts, they should remain up to date and you > >> >>> will > >> >>> >>> see any new scripts. If you copy them to a local directory on > >> >>> the > >> >>> >>> HPC cluster, they will not be updated. > >> > >> >>> >>> --- > >> >>> >>> Skye Jonke > >> > >> >>> >>> she/her > >> > >> >>> >>> Specialist, Linux System Administrator & Lab Technical Support > >> > >> >>> >>> > On Oct 22, 2025, at 16:14, Naghmeh Karimi <nkarimi@umbc.edu> > >> >>> >>> wrote: > >> >>> >>> > > >> >>> >>> > Hi Max, Roy, Skye, > >> >>> >>> > > >> >>> >>> > Thank you all for your help. > >> >>> >>> > > >> >>> >>> > I added Skye here as she is our IT exert on adding the > >> >>> capability > >> >>> >>> of running synopsis tools (Hspice) in servers. > >> >>> >>> > Skye we want to use the HPC cluster to run hspice an other > >> >>> >>> synopsys tools. Could you please help on this? > >> >>> >>> > > >> >>> >>> > Thanks, > >> >>> >>> > Naghmeh > >> >>> >>> > > >> >>> >>> > On Wed, Oct 22, 2025 at 2:06 PM Mohammad Ebrahimabadi via RT > >> >>> >>> <UMBCHelp@rt.umbc.edu <mailto:UMBCHelp@rt.umbc.edu>> wrote: > >> >>> >>> >> Ticket <URL: > >> >>> https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 > >> < > https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id= %3D3296910&source=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw3ImziGtKZ= -9j9JFW8J65w7 > > > >> >>> >>> > >> >>> < > >> > https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id= %3D3296910&source=3Dgmail-imap&ust=3D1761768891000000&usg=3DAOvVaw2jxB3nArH= BD7-ij9ZN8ZsW > >> < > https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.= umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D1= 761768891000000%26usg%3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW&source=3Dgmail-imap&us= t=3D1761874374000000&usg=3DAOvVaw0fLylAsBAoQ6LgqY79I9-d > > > >> > > >> >>> >>> > > >> >>> >>> >> > >> >>> >>> >> Last Update From Ticket: > >> >>> >>> >> > >> >>> >>> >> Hi Max, > >> >>> >>> >> > >> >>> >>> >> Thanks for your help! Let=E2=80=99s proceed based on your > >> >>> suggestion, > >> >>> >>> hopefully it > >> >>> >>> >> works as expected. > >> >>> >>> >> > >> >>> >>> >> I just have one quick question: since the IT team in the > >> >>> >>> department can modify > >> >>> >>> >> or add new shell scripts in that directory, will we also > >> >>> have > >> >>> >>> access to those > >> >>> >>> >> updated or newly added scripts from the mounted directory on > >> >>> >>> HPCF? > >> >>> >>> >> > >> >>> >>> >> Thanks again, > >> >>> >>> >> Mohammad > >> >>> >>> >> > >> >>> >>> >> On Wed Oct 22 14:01:32 2025, OL73413 wrote: > >> >>> >>> >> > >> >>> >>> >> > Hi Mohammad, > >> >>> >>> >> > >> >>> >>> >> > We can get the software mounted for you on chip, but it > >> >>> will > >> >>> >>> have to be > >> >>> >>> >> > read-only as we won't be able to manage who can make > >> >>> changes > >> >>> >>> to it > >> >>> >>> >> > otherwise. > >> >>> >>> >> > >> >>> >>> >> > As a reminder, the machines on that run on chip are not > >> >>> the > >> >>> >>> same as those > >> >>> >>> >> > run by the CSEE department, so we can't guarantee that > >> >>> these > >> >>> >>> will run the > >> >>> >>> >> > same way or as effectively. > >> >>> >>> >> > >> >>> >>> >> > If that's all ok with you, please give me some time to get > >> >>> the > >> >>> >>> directory > >> >>> >>> >> > mounted. > >> >>> >>> >> > >> >>> >>> >> > On Wed Oct 22 13:26:05 2025, NQ23652 wrote: > >> >>> >>> >> > >> >>> >>> >> >> Thanks Tartela for your help to resolve my issue! > >> >>> >>> >> > >> >>> >>> >> >> Actually it seems all server in CSEE department have > >> >>> access > >> >>> >>> to that > >> >>> >>> >> >> path. When I ran the command that you gave me I got the > >> >>> >>> following > >> >>> >>> >> >> report: > >> >>> >>> >> > >> >>> >>> >> >> mohammad@alborz:~$ df -h /umbc/software > >> >>> >>> >> >> Filesystem Size Used Avail Use% Mounted on > >> >>> >>> >> >> nfs.iss.rs.umbc.edu:/ifs/data/csee/software 2.5T 2.2T > >> >>> 326G > >> >>> >>> 88% > >> >>> >>> >> >> /umbc/software > >> >>> >>> >> > >> >>> >>> >> >> mohammad@alborz:~$ mount | grep /umbc/software > >> >>> >>> >> >> nfs.iss.rs.umbc.edu:/ifs/data/csee/software on > >> >>> /umbc/software > >> >>> >>> type nfs > >> >>> >>> >> >> > >> >>> >>> > >> >>> > >> > (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namlen=3D255,acregmin= =3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,proto=3Dtcp,time= o=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountvers=3D3,mountpor= t=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.60) > >> >>> >>> >> > >> >>> >>> >> >> Please let me know if you need other information. > >> >>> >>> >> > >> >>> >>> >> >> Regards, > >> >>> >>> >> > >> >>> >>> >> >> Mohammad > >> >>> >>> >> > >> >>> >>> >> >> On Wed Oct 22 13:15:12 2025, HK41259 wrote: > >> >>> >>> >> > >> >>> >>> >> >>> Hello, > >> >>> >>> >> > >> >>> >>> >> >>> Thanks for reaching out. Could you clarify which > >> >>> specific > >> >>> >>> machine > >> >>> >>> >> >>> or server you=E2=80=99re referring to when you mention = the > >> >>> >>> =E2=80=9Cdepartmental > >> >>> >>> >> >>> server=E2=80=9D? > >> >>> >>> >> > >> >>> >>> >> >>> The /umbc/software/scripts/ directory isn=E2=80=99t acc= essible > >> >>> from > >> >>> >>> HPCF > >> >>> >>> >> >>> (e.g., Chip) by default =E2=80=94 those systems have se= parate > >> >>> >>> storage and > >> >>> >>> >> >>> don=E2=80=99t automatically mount departmental shares. > >> >>> >>> >> > >> >>> >>> >> >>> Could you check where /umbc/software is mounted on your > >> >>> >>> >> >>> departmental system (e.g., by running df -h > >> >>> /umbc/software > >> >>> >>> or mount > >> >>> >>> >> >>> | grep /umbc/software)? That=E2=80=99ll tell us which s= erver > >> >>> hosts > >> >>> >>> it. Once > >> >>> >>> >> >>> we know that, we can confirm whether it=E2=80=99s possi= ble or > >> >>> >>> appropriate > >> >>> >>> >> >>> to make it visible from HPCF. > >> >>> >>> >> > >> >>> >>> >> >>> Best, > >> >>> >>> >> > >> >>> >>> >> >>> Tartela > >> >>> >>> >> > >> >>> >>> >> >>> On Tue Oct 21 15:59:49 2025, ZZ99999 wrote: > >> >>> >>> >> > >> >>> >>> >> >>>> First Name: Mohammad > >> >>> >>> >> >>>> Last Name: Ebrahimabadi > >> >>> >>> >> >>>> Email: e127@umbc.edu <mailto:e127@umbc.edu> > >> >>> >>> >> >>>> Campus ID: NQ23652 > >> >>> >>> >> >>>> > >> >>> >>> >> >>>> Request Type: High Performance Cluster > >> >>> >>> >> >>>> > >> >>> >>> >> >>>> Dear Team, > >> >>> >>> >> >>>> > >> >>> >>> >> >>>> I hope you=E2=80=99re doing well. > >> >>> >>> >> >>>> I=E2=80=99m a Ph.D. student in Computer Engineering an= d have a > >> >>> >>> question regarding running a software on the HPCF. On the > >> >>> >>> department server, I usually run software using shell scripts > >> >>> >>> located at /umbc/software/scripts/ which is provided by the IT > >> >>> >>> group pf the department (Geoff Weiss Team). However, in case of > >> >>> >>> HPCF, it seems that this path is not accessible from the HPCF > >> >>> >>> environment. > >> >>> >>> >> >>>> > >> >>> >>> >> >>>> Could you please let me know if there is any specific > >> >>> >>> configuration or access permission required to reach this > >> >>> directory > >> >>> >>> through the HPCF? or direct me to a related person that can > >> >>> help > >> >>> >>> me. > >> >>> >>> >> >>>> > >> >>> >>> >> >>>> Best regards, > >> >>> >>> >> >>>> Mohammad > >> >>> >>> >> >>> > >> >>> >>> >> > >> >>> >>> >> > -- > >> >>> >>> >> > >> >>> >>> >> > Best, > >> >>> >>> >> > Max Breitmeyer > >> >>> >>> >> > DOIT HPC System Administrator > >> >>> >>> >> > >> >>> >>> >> > >> >>> >>> > > >> >>> >>> > > >> >>> >>> > > >> >>> >>> > -- > >> >>> >>> > Naghmeh Karimi, Ph.D. > >> >>> >>> > Associate Professor > >> >>> >>> > Department of Computer Science and Electrical Engineering > >> >>> >>> > University of Maryland, Baltimore County > >> >>> >>> > Baltimore, MD 21250 > >> >>> >>> > Tel: 410-455-3965 E-mail: nkarimi@umbc.edu > >> >>> >>> <mailto:nkarimi@umbc.edu> > >> >>> >>> > Web: http://www.csee.umbc.edu/~nkarimi/ > >> < > https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source= =3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW > > > >> >>> >>> > >> >>> < > >> > https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source= =3Dgmail-imap&ust=3D1761768891000000&usg=3DAOvVaw0xLE02sw5-ueWRHyHyCwkN > >> < > https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://www.= csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D1761768891000000%26us= g%3DAOvVaw0xLE02sw5-ueWRHyHyCwkN&source=3Dgmail-imap&ust=3D1761874374000000= &usg=3DAOvVaw3g338pGQqShCgyX8cIt8tV > > > >> > > >> > >> >>> >> -- Naghmeh Karimi, Ph.D. > >> >>> >> Associate Professor > >> >>> >> Department of Computer Science and Electrical Engineering > >> >>> >> University of Maryland, Baltimore County > >> >>> >> Baltimore, MD 21250 > >> >>> >> Tel: 410-455-3965 E-mail: nkarimi@umbc.edu > >> >>> >> Web: http://www.csee.umbc.edu/~nkarimi/ > >> < > https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source= =3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW > > > >> > >> >>> > -- > >> > >> >>> > Best, > >> >>> > Max Breitmeyer > >> >>> > DOIT HPC System Administrator > >> > >> >> -- V/R,Maxwell BreitmeyerUMBC HPCF SpecialistUMBC Observatory IT > >> >> Manager Graduate Student(443) 835-8250 > >> > >> -- > >> > >> Best, > >> Max Breitmeyer > >> DOIT HPC System Administrator > >> > >> > >> > > > > -- > Naghmeh Karimi, Ph.D. > Associate Professor > Department of Computer Science and Electrical Engineering > University of Maryland, Baltimore County > Baltimore, MD 21250 > Tel: *410-455-3965* E-mail: *nkarimi@umbc.edu <nkarimi@umbc.edu>* > Web: *http://www.csee.umbc.edu/~nkarimi/ > <http://www.csee.umbc.edu/~nkarimi/>* > "
3296910,72477484,Correspond,DoIT-Research-Computing,2025-10-24 14:46:39.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Mohammad Ebrahimabadi,e127@umbc.edu,"<div dir=3D""ltr""><div dir=3D""ltr"">Hello Skye,<div><br></div><div>I believe= =C2=A0the way that Max told me to run the script was using the same path th= at we have in the CSEE department as he mounted that path in HPCF. I mean t= his path:</div><div><span style=3D""color:rgb(92,98,115);font-family:Inter,s= ans-serif;font-size:13px"">=C2=A0/umbc/software/scripts/</span><span style= =3D""color:rgb(92,98,115);font-family:Inter,sans-serif;font-size:13px"">launc= h_synopsys_hspice.sh</span></div><div><br></div><div><span style=3D""color:r= gb(92,98,115);font-family:Inter,sans-serif;font-size:13px""><br></span></div= ><div><span style=3D""color:rgb(92,98,115);font-family:Inter,sans-serif;font= -size:13px"">Regards,</span></div><div><span style=3D""color:rgb(92,98,115);f= ont-family:Inter,sans-serif;font-size:13px"">Mohammad</span></div><div><span=  style=3D""color:rgb(92,98,115);font-family:Inter,sans-serif;font-size:13px""= ><br></span></div><div><span style=3D""color:rgb(92,98,115);font-family:Inte= r,sans-serif;font-size:13px""><br></span></div><div><span style=3D""color:rgb= (92,98,115);font-family:Inter,sans-serif;font-size:13px""><br></span></div><= /div></div><br><div class=3D""gmail_quote gmail_quote_container""><div dir=3D= ""ltr"" class=3D""gmail_attr"">On Fri, Oct 24, 2025 at 9:42=E2=80=AFAM <a href= =3D""mailto:naghmeh.karimi@umbc.edu"">naghmeh.karimi@umbc.edu</a> via RT &lt;= <a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:= <br></div><blockquote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8= ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"">Ticket &lt;URL:=  <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D3296910"" rel=3D""no= referrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D32= 96910</a> &gt;<br> <br> Last Update From Ticket:<br> <br> That&#39;s perfect Skye. I highly appreciate it. I ask Mohamamd to respond<= br> the ticket with the path.<br> <br> regards,<br> Naghmeh<br> <br> On Fri, Oct 24, 2025 at 9:39=E2=80=AFAM Skye Jonke &lt;<a href=3D""mailto:ii= 69854@umbc.edu"" target=3D""_blank"">ii69854@umbc.edu</a>&gt; wrote:<br> <br> &gt; Its not possible to use the existing scripts in a situation where the<= br> &gt; files aren=E2=80=99t mounted in /umbc/software, but I can write some s= cripts<br> &gt; specific to this setup to this situation with a bit more information. = Where<br> &gt; are the script files located on the HPC? If you aren=E2=80=99t sure, y= ou can get<br> &gt; the full path by running =E2=80=9Crealpath myfile.sh=E2=80=9D (replaci= ng myfile.sh with one<br> &gt; of the cadence scripts, of course)<br> &gt; ---<br> &gt; Skye Jonke<br> &gt;<br> &gt; she/her<br> &gt;<br> &gt; Specialist, Linux System Administrator &amp; Lab Technical Support<br> &gt;<br> &gt; On Oct 23, 2025, at 21:32, Naghmeh Karimi &lt;<a href=3D""mailto:nkarim= i@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a>&gt; wrote:<br> &gt;<br> &gt; Hi Max, thanks for getting back to us.<br> &gt;<br> &gt; Hi Skye, we need your help here to find the file locations. Could you<= br> &gt; please help to resolve this issue?<br> &gt;<br> &gt; Thanks a lot<br> &gt; Naghmeh<br> &gt;<br> &gt; Naghmeh Karimi, Ph.D.<br> &gt; Associate Professor<br> &gt; Department of Computer Science and Electrical Engineering<br> &gt; University of Maryland, Baltimore County<br> &gt; Baltimore, MD 21250<br> &gt; Tel: *410-455-3965* E-mail: *<a href=3D""mailto:nkarimi@umbc.edu"" targe= t=3D""_blank"">nkarimi@umbc.edu</a> &lt;<a href=3D""mailto:nkarimi@umbc.edu"" t= arget=3D""_blank"">nkarimi@umbc.edu</a>&gt;*<br> &gt; Web: *<a href=3D""http://www.csee.umbc.edu/~nkarimi/"" rel=3D""noreferrer= "" target=3D""_blank"">http://www.csee.umbc.edu/~nkarimi/</a><br> &gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu= /~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg=3DAOv= Vaw23Fk_KFBnlrXC_uhIbUFIW"" rel=3D""noreferrer"" target=3D""_blank"">https://www= .google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-i= map&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW</a>&g= t;*<br> &gt;<br> &gt; On Thu, Oct 23, 2025, 12:22=E2=80=AFPM Max Breitmeyer via RT &lt;<a hr= ef=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</= a>&gt;<br> &gt; wrote:<br> &gt;<br> &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html= ?id=3D3296910"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tic= ket/Display.html?id=3D3296910</a><br> &gt;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttps://rt.umbc.edu/= Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D17618743= 74000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7"" rel=3D""noreferrer"" target= =3D""_blank"">https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Displ= ay.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761874374000000&amp= ;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7</a>&gt;<br> &gt;&gt; &gt;<br> &gt;&gt;<br> &gt;&gt; Last Update From Ticket:<br> &gt;&gt;<br> &gt;&gt; Hi Mohammad,<br> &gt;&gt;<br> &gt;&gt; According to the error message, it&#39;s having an issue finding a= nother file<br> &gt;&gt; in<br> &gt;&gt; the directory. This appears to be an issue due to the mounting loc= ations<br> &gt;&gt; being<br> &gt;&gt; different. I would point you to the previous email where I said th= at any<br> &gt;&gt; software with hard-coded locations would not work, and that softwa= re<br> &gt;&gt; compatibility is not guaranteed. I would recommend working with Sk= ye on<br> &gt;&gt; porting<br> &gt;&gt; what is in the directory to your local research directories on chi= p.<br> &gt;&gt;<br> &gt;&gt; On Thu Oct 23 12:09:34 2025, NQ23652 wrote:<br> &gt;&gt;<br> &gt;&gt; &gt; Yes you are right, now I could see all shell files. However s= till I have<br> &gt;&gt; &gt; problem when I run the sh file.<br> &gt;&gt;<br> &gt;&gt; &gt; [e127@chip-login2 scripts]$ sh launch_synopsys_hspice.sh<br> &gt;&gt; &gt; launch_synopsys_hspice.sh: line 3:<br> &gt;&gt; &gt; /umbc/software/scripts/env_synopsys_hspice.sh: No such file o= r directory<br> &gt;&gt; &gt; [e127@chip-login2 scripts]$<br> &gt;&gt;<br> &gt;&gt; &gt; Would you please take a look to it.<br> &gt;&gt;<br> &gt;&gt; &gt; Regards,<br> &gt;&gt;<br> &gt;&gt; &gt; Mohammad<br> &gt;&gt;<br> &gt;&gt; &gt; On Thu Oct 23 11:59:17 2025, OL73413 wrote:<br> &gt;&gt;<br> &gt;&gt; &gt;&gt; Hi Mohammad,<br> &gt;&gt;<br> &gt;&gt; &gt;&gt; A good point. Chip uses a service called &quot;autofs&quo= t; to mount and unmount<br> &gt;&gt; &gt;&gt; directories that aren&#39;t being actively used. This hel= ps maintain the<br> &gt;&gt; &gt;&gt; login nodes integrity. If you see in my screenshot below,=  the directory<br> &gt;&gt; &gt;&gt; appears to be empty, but when I cd to the csee directory = where your<br> &gt;&gt; &gt;&gt; software is mounted, it is all available. Backing out of = the directory<br> &gt;&gt; &gt;&gt; and ls&#39;ing the directory also shows that the director= y is now mounted<br> &gt;&gt; &gt;&gt; and available.<br> &gt;&gt;<br> &gt;&gt; &gt;&gt; On Thu, Oct 23, 2025 at 11:41 AM Mohammad Ebrahimabadi vi= a RT<br> &gt;&gt; &gt;&gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_bl= ank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket= /Display.html?id=3D3296910"" rel=3D""noreferrer"" target=3D""_blank"">https://rt= .umbc.edu/Ticket/Display.html?id=3D3296910</a><br> &gt;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttps://rt.umbc.edu/= Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D17618743= 74000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7"" rel=3D""noreferrer"" target= =3D""_blank"">https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Displ= ay.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761874374000000&amp= ;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7</a>&gt;<br> &gt;&gt; &gt;<br> &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; Comment From Ticket:<br> &gt;&gt; &gt;&gt;&gt; Thank you Max!<br> &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; Actually we have still problem for accessing that dir= ectory. I<br> &gt;&gt; &gt;&gt;&gt; uploaded two<br> &gt;&gt; &gt;&gt;&gt; screenshots that shows what we can see in the departm= ent servers<br> &gt;&gt; &gt;&gt;&gt; and what we<br> &gt;&gt; &gt;&gt;&gt; can see in CHIP.<br> &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; For example by running this shell file we can launch = HSPICE<br> &gt;&gt; &gt;&gt;&gt; software in the<br> &gt;&gt; &gt;&gt;&gt; department server:<br> &gt;&gt; &gt;&gt;&gt; /umbc/software/scripts/launch_synopsys_hspice.sh<br> &gt;&gt; &gt;&gt;&gt; But it is not still accessible on CHIP.<br> &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; Regards,<br> &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; Mohammad<br> &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; On Thu Oct 23 10:54:59 2025, OL73413 wrote:<br> &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt; Hi Nahmeh,<br> &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt; Apologies =E2=80=94 I meant to update you yester= day after completing<br> &gt;&gt; &gt;&gt;&gt; this, but I<br> &gt;&gt; &gt;&gt;&gt; &gt; was pulled into a long meeting. The software is = now available on<br> &gt;&gt; &gt;&gt;&gt; chip at:<br> &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt; /umbc/software/csee<br> &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt; Please note that if the software contains any ha= rdcoded file<br> &gt;&gt; &gt;&gt;&gt; paths, it may<br> &gt;&gt; &gt;&gt;&gt; &gt; not function correctly. Unfortunately, we don=E2= =80=99t have control over<br> &gt;&gt; &gt;&gt;&gt; those<br> &gt;&gt; &gt;&gt;&gt; &gt; cases.<br> &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt; Also, just to clarify, Skye and I work in separa= te departments.<br> &gt;&gt; &gt;&gt;&gt; Similar to<br> &gt;&gt; &gt;&gt;&gt; &gt; how I don=E2=80=99t have administrative access t= o the CSEE department<br> &gt;&gt; &gt;&gt;&gt; servers<br> &gt;&gt; &gt;&gt;&gt; &gt; (which include the software directory), Skye is = not an<br> &gt;&gt; &gt;&gt;&gt; administrator on<br> &gt;&gt; &gt;&gt;&gt; &gt; chip.<br> &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt; Please let me know if you encounter any issues a= ccessing the<br> &gt;&gt; &gt;&gt;&gt; software on<br> &gt;&gt; &gt;&gt;&gt; &gt; the server. If there are problems running the so= ftware, I=E2=80=99ll<br> &gt;&gt; &gt;&gt;&gt; coordinate<br> &gt;&gt; &gt;&gt;&gt; &gt; with Skye to troubleshoot, but since we don=E2= =80=99t maintain this<br> &gt;&gt; &gt;&gt;&gt; software, we<br> &gt;&gt; &gt;&gt;&gt; &gt; can=E2=80=99t guarantee full compatibility on ou= r systems.<br> &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt; On Thu Oct 23 10:36:41 2025, <a href=3D""mailto:n= aghmeh.karimi@umbc.edu"" target=3D""_blank"">naghmeh.karimi@umbc.edu</a> wrote= :<br> &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; Hi Skye and Max, Could you please help on mo= unting this today so<br> &gt;&gt; &gt;&gt;&gt; we can<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; use them as we have a deadline? Thanks,Naghm= eh Karimi On Wed,<br> &gt;&gt; &gt;&gt;&gt; Oct 22,<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; 2025 at 4:22 PM Skye Jonke via RT &lt;<a hre= f=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a= >&gt; wrote:<br> &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.um= bc.edu/Ticket/Display.html?id=3D3296910"" rel=3D""noreferrer"" target=3D""_blan= k"">https://rt.umbc.edu/Ticket/Display.html?id=3D3296910</a><br> &gt;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttps://rt.umbc.edu/= Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D17618743= 74000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7"" rel=3D""noreferrer"" target= =3D""_blank"">https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Displ= ay.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761874374000000&amp= ;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7</a>&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Last Update From Ticket:<br> &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Mohammad,<br> &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; So long as you continue to access them t= hrough<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; /umbc/software/scripts, they should rema= in up to date and you<br> &gt;&gt; &gt;&gt;&gt; will<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; see any new scripts. If you copy them to=  a local directory on<br> &gt;&gt; &gt;&gt;&gt; the<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; HPC cluster, they will not be updated.<b= r> &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; ---<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Skye Jonke<br> &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; she/her<br> &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Specialist, Linux System Administrator &= amp; Lab Technical Support<br> &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; On Oct 22, 2025, at 16:14, Naghmeh = Karimi &lt;<a href=3D""mailto:nkarimi@umbc.edu"" target=3D""_blank"">nkarimi@um= bc.edu</a>&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; wrote:<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Hi Max, Roy, Skye,<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Thank you all for your help.<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; I added Skye here as she is our IT = exert on adding the<br> &gt;&gt; &gt;&gt;&gt; capability<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; of running synopsis tools (Hspice) in se= rvers.<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Skye we want to use the HPC cluster=  to run hspice an other<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; synopsys tools. Could you please help on=  this?<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Thanks,<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Naghmeh<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; On Wed, Oct 22, 2025 at 2:06 PM Moh= ammad Ebrahimabadi via RT<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.e= du"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a> &lt;mailto:<a href=3D""mailto= :UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt;&gt; w= rote:<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Ticket &lt;URL:<br> &gt;&gt; &gt;&gt;&gt; <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id= =3D3296910"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket= /Display.html?id=3D3296910</a><br> &gt;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttps://rt.umbc.edu/= Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D17618743= 74000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7"" rel=3D""noreferrer"" target= =3D""_blank"">https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Displ= ay.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761874374000000&amp= ;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7</a>&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &lt;<br> &gt;&gt; <a href=3D""https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Tick= et/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D176176889100= 0000&amp;usg=3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW"" rel=3D""noreferrer"" target=3D""_= blank"">https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.ht= ml?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761768891000000&amp;usg= =3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW</a><br> &gt;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttps://www.google.c= om/url?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%= 3Dgmail-imap%26ust%3D1761768891000000%26usg%3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW&= amp;source=3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw0fLylAsB= AoQ6LgqY79I9-d"" rel=3D""noreferrer"" target=3D""_blank"">https://www.google.com= /url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.edu/Ticket/Display.= html?id%253D3296910%26source%3Dgmail-imap%26ust%3D1761768891000000%26usg%3D= AOvVaw2jxB3nArHBD7-ij9ZN8ZsW&amp;source=3Dgmail-imap&amp;ust=3D176187437400= 0000&amp;usg=3DAOvVaw0fLylAsBAoQ6LgqY79I9-d</a>&gt;<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Last Update From Ticket:<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Hi Max,<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Thanks for your help! Let=E2=80= =99s proceed based on your<br> &gt;&gt; &gt;&gt;&gt; suggestion,<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; hopefully it<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; works as expected.<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; I just have one quick question:=  since the IT team in the<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; department can modify<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; or add new shell scripts in tha= t directory, will we also<br> &gt;&gt; &gt;&gt;&gt; have<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; access to those<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; updated or newly added scripts = from the mounted directory on<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; HPCF?<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Thanks again,<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Mohammad<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; On Wed Oct 22 14:01:32 2025, OL= 73413 wrote:<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Hi Mohammad,<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; We can get the software mo= unted for you on chip, but it<br> &gt;&gt; &gt;&gt;&gt; will<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; have to be<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; read-only as we won&#39;t = be able to manage who can make<br> &gt;&gt; &gt;&gt;&gt; changes<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; to it<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; otherwise.<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; As a reminder, the machine= s on that run on chip are not<br> &gt;&gt; &gt;&gt;&gt; the<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; same as those<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; run by the CSEE department= , so we can&#39;t guarantee that<br> &gt;&gt; &gt;&gt;&gt; these<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; will run the<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; same way or as effectively= .<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; If that&#39;s all ok with = you, please give me some time to get<br> &gt;&gt; &gt;&gt;&gt; the<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; directory<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; mounted.<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; On Wed Oct 22 13:26:05 202= 5, NQ23652 wrote:<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thanks Tartela for you= r help to resolve my issue!<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Actually it seems all = server in CSEE department have<br> &gt;&gt; &gt;&gt;&gt; access<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; to that<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; path. When I ran the c= ommand that you gave me I got the<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; following<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; report:<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; mohammad@alborz:~$ df = -h /umbc/software<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Filesystem Size Used A= vail Use% Mounted on<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; nfs.iss.rs.umbc.edu:/i= fs/data/csee/software 2.5T 2.2T<br> &gt;&gt; &gt;&gt;&gt; 326G<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; 88%<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; /umbc/software<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; mohammad@alborz:~$ mou= nt | grep /umbc/software<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; nfs.iss.rs.umbc.edu:/i= fs/data/csee/software on<br> &gt;&gt; &gt;&gt;&gt; /umbc/software<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; type nfs<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt;<br> &gt;&gt; &gt;&gt;&gt;<br> &gt;&gt; (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namlen=3D255,a= cregmin=3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,proto=3Dt= cp,timeo=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountvers=3D3,m= ountport=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.60)<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Please let me know if = you need other information.<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Regards,<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Mohammad<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; On Wed Oct 22 13:15:12=  2025, HK41259 wrote:<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Hello,<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Thanks for reachin= g out. Could you clarify which<br> &gt;&gt; &gt;&gt;&gt; specific<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; machine<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; or server you=E2= =80=99re referring to when you mention the<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; =E2=80=9Cdepartmental<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; server=E2=80=9D?<b= r> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; The /umbc/software= /scripts/ directory isn=E2=80=99t accessible<br> &gt;&gt; &gt;&gt;&gt; from<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; HPCF<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; (e.g., Chip) by de= fault =E2=80=94 those systems have separate<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; storage and<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; don=E2=80=99t auto= matically mount departmental shares.<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Could you check wh= ere /umbc/software is mounted on your<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; departmental syste= m (e.g., by running df -h<br> &gt;&gt; &gt;&gt;&gt; /umbc/software<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; or mount<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; | grep /umbc/softw= are)? That=E2=80=99ll tell us which server<br> &gt;&gt; &gt;&gt;&gt; hosts<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; it. Once<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; we know that, we c= an confirm whether it=E2=80=99s possible or<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; appropriate<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; to make it visible=  from HPCF.<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Best,<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Tartela<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; On Tue Oct 21 15:5= 9:49 2025, ZZ99999 wrote:<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; First Name: Mo= hammad<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Last Name: Ebr= ahimabadi<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Email: <a href= =3D""mailto:e127@umbc.edu"" target=3D""_blank"">e127@umbc.edu</a> &lt;mailto:<a=  href=3D""mailto:e127@umbc.edu"" target=3D""_blank"">e127@umbc.edu</a>&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Campus ID: NQ2= 3652<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Request Type: = High Performance Cluster<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Dear Team,<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; I hope you=E2= =80=99re doing well.<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; I=E2=80=99m a = Ph.D. student in Computer Engineering and have a<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; question regarding running a software on=  the HPCF. On the<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; department server, I usually run softwar= e using shell scripts<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; located at /umbc/software/scripts/ which=  is provided by the IT<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; group pf the department (Geoff Weiss Tea= m). However, in case of<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; HPCF, it seems that this path is not acc= essible from the HPCF<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; environment.<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Could you plea= se let me know if there is any specific<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; configuration or access permission requi= red to reach this<br> &gt;&gt; &gt;&gt;&gt; directory<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; through the HPCF? or direct me to a rela= ted person that can<br> &gt;&gt; &gt;&gt;&gt; help<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; me.<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Best regards,<= br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Mohammad<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; --<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Best,<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Max Breitmeyer<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; DOIT HPC System Administra= tor<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; --<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Naghmeh Karimi, Ph.D.<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Associate Professor<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Department of Computer Science and = Electrical Engineering<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; University of Maryland, Baltimore C= ounty<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Baltimore, MD 21250<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Tel: 410-455-3965 E-mail: <a href= =3D""mailto:nkarimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a><br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &lt;mailto:<a href=3D""mailto:nkarimi@umb= c.edu"" target=3D""_blank"">nkarimi@umbc.edu</a>&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Web: <a href=3D""http://www.csee.umb= c.edu/~nkarimi/"" rel=3D""noreferrer"" target=3D""_blank"">http://www.csee.umbc.= edu/~nkarimi/</a><br> &gt;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttp://www.csee.umbc= .edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg= =3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW"" rel=3D""noreferrer"" target=3D""_blank"">https= ://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source=3Dg= mail-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW= </a>&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &lt;<br> &gt;&gt; <a href=3D""https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu= /~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761768891000000&amp;usg=3DAOv= Vaw0xLE02sw5-ueWRHyHyCwkN"" rel=3D""noreferrer"" target=3D""_blank"">https://www= .google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-i= map&amp;ust=3D1761768891000000&amp;usg=3DAOvVaw0xLE02sw5-ueWRHyHyCwkN</a><b= r> &gt;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttps://www.google.c= om/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3= D1761768891000000%26usg%3DAOvVaw0xLE02sw5-ueWRHyHyCwkN&amp;source=3Dgmail-i= map&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw3g338pGQqShCgyX8cIt8tV"" rel= =3D""noreferrer"" target=3D""_blank"">https://www.google.com/url?q=3Dhttps://ww= w.google.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmail-im= ap%26ust%3D1761768891000000%26usg%3DAOvVaw0xLE02sw5-ueWRHyHyCwkN&amp;source= =3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw3g338pGQqShCgyX8cI= t8tV</a>&gt;<br> &gt;&gt; &gt;<br> &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; -- Naghmeh Karimi, Ph.D.<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; Associate Professor<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; Department of Computer Science and Electrica= l Engineering<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; University of Maryland, Baltimore County<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; Baltimore, MD 21250<br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; Tel: 410-455-3965 E-mail: <a href=3D""mailto:= nkarimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a><br> &gt;&gt; &gt;&gt;&gt; &gt;&gt; Web: <a href=3D""http://www.csee.umbc.edu/~nk= arimi/"" rel=3D""noreferrer"" target=3D""_blank"">http://www.csee.umbc.edu/~nkar= imi/</a><br> &gt;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttp://www.csee.umbc= .edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg= =3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW"" rel=3D""noreferrer"" target=3D""_blank"">https= ://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source=3Dg= mail-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW= </a>&gt;<br> &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt; --<br> &gt;&gt;<br> &gt;&gt; &gt;&gt;&gt; &gt; Best,<br> &gt;&gt; &gt;&gt;&gt; &gt; Max Breitmeyer<br> &gt;&gt; &gt;&gt;&gt; &gt; DOIT HPC System Administrator<br> &gt;&gt;<br> &gt;&gt; &gt;&gt; -- V/R,Maxwell BreitmeyerUMBC HPCF SpecialistUMBC Observa= tory IT<br> &gt;&gt; &gt;&gt; Manager Graduate Student(443) 835-8250<br> &gt;&gt;<br> &gt;&gt; --<br> &gt;&gt;<br> &gt;&gt; Best,<br> &gt;&gt; Max Breitmeyer<br> &gt;&gt; DOIT HPC System Administrator<br> &gt;&gt;<br> &gt;&gt;<br> &gt;&gt;<br> &gt;<br> <br> -- <br> Naghmeh Karimi, Ph.D.<br> Associate Professor<br> Department of Computer Science and Electrical Engineering<br> University of Maryland, Baltimore County<br> Baltimore, MD 21250<br> Tel: *410-455-3965* E-mail: *<a href=3D""mailto:nkarimi@umbc.edu"" target=3D""= _blank"">nkarimi@umbc.edu</a> &lt;<a href=3D""mailto:nkarimi@umbc.edu"" target= =3D""_blank"">nkarimi@umbc.edu</a>&gt;*<br> Web: *<a href=3D""http://www.csee.umbc.edu/~nkarimi/"" rel=3D""noreferrer"" tar= get=3D""_blank"">http://www.csee.umbc.edu/~nkarimi/</a><br> &lt;<a href=3D""http://www.csee.umbc.edu/~nkarimi/"" rel=3D""noreferrer"" targe= t=3D""_blank"">http://www.csee.umbc.edu/~nkarimi/</a>&gt;*<br> </blockquote></div> "
3296910,72477635,Correspond,DoIT-Research-Computing,2025-10-24 14:51:33.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Skye Jonke,ii69854@umbc.edu,"<html aria-label=3D""message body""><head><meta http-equiv=3D""content-type"" c= ontent=3D""text/html; charset=3Dutf-8""></head><body style=3D""overflow-wrap: = break-word; -webkit-nbsp-mode: space; line-break: after-white-space;"">That= =E2=80=99s a bit confusing, since hardcoded paths won=E2=80=99t work, pleas= e have Max contact me directly so we can talk about what needs to be done t= o make this work.&nbsp;<br id=3D""lineBreakAtBeginningOfMessage""><div> <meta charset=3D""UTF-8""><div dir=3D""auto"" style=3D""caret-color: rgb(0, 0, 0= ); color: rgb(0, 0, 0); letter-spacing: normal; text-align: start; text-ind= ent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; -we= bkit-text-stroke-width: 0px; text-decoration: none; overflow-wrap: break-wo= rd; -webkit-nbsp-mode: space; line-break: after-white-space;"">---</div><div=  dir=3D""auto"" style=3D""caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0); lett= er-spacing: normal; text-align: start; text-indent: 0px; text-transform: no= ne; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px;=  text-decoration: none; overflow-wrap: break-word; -webkit-nbsp-mode: space= ; line-break: after-white-space;"">Skye Jonke<br><br>she/her<br><br>Speciali= st, Linux System Administrator &amp;<span class=3D""Apple-converted-space"">&= nbsp;</span>Lab Technical Support<br></div> </div> <div><br><blockquote type=3D""cite""><div>On Oct 24, 2025, at 10:46, Mohammad=  Ebrahimabadi &lt;e127@umbc.edu&gt; wrote:</div><br class=3D""Apple-intercha= nge-newline""><div><meta charset=3D""UTF-8""><div dir=3D""ltr"" style=3D""caret-c= olor: rgb(0, 0, 0); font-family: Helvetica; font-size: 14px; font-style: no= rmal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; = text-align: start; text-indent: 0px; text-transform: none; white-space: nor= mal; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration: no= ne;""><div dir=3D""ltr"">Hello Skye,<div><br></div><div>I believe&nbsp;the way=  that Max told me to run the script was using the same path that we have in=  the CSEE department as he mounted that path in HPCF. I mean this path:</di= v><div><span style=3D""color: rgb(92, 98, 115); font-family: Inter, sans-ser= if; font-size: 13px;"">&nbsp;/umbc/software/scripts/</span><span style=3D""co= lor: rgb(92, 98, 115); font-family: Inter, sans-serif; font-size: 13px;"">la= unch_synopsys_hspice.sh</span></div><div><br></div><div><span style=3D""colo= r: rgb(92, 98, 115); font-family: Inter, sans-serif; font-size: 13px;""><br>= </span></div><div><span style=3D""color: rgb(92, 98, 115); font-family: Inte= r, sans-serif; font-size: 13px;"">Regards,</span></div><div><span style=3D""c= olor: rgb(92, 98, 115); font-family: Inter, sans-serif; font-size: 13px;"">M= ohammad</span></div><div><span style=3D""color: rgb(92, 98, 115); font-famil= y: Inter, sans-serif; font-size: 13px;""><br></span></div><div><span style= =3D""color: rgb(92, 98, 115); font-family: Inter, sans-serif; font-size: 13p= x;""><br></span></div><div><span style=3D""color: rgb(92, 98, 115); font-fami= ly: Inter, sans-serif; font-size: 13px;""><br></span></div></div></div><br s= tyle=3D""caret-color: rgb(0, 0, 0); font-family: Helvetica; font-size: 14px;=  font-style: normal; font-variant-caps: normal; font-weight: 400; letter-sp= acing: normal; text-align: start; text-indent: 0px; text-transform: none; w= hite-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; text= -decoration: none;""><div class=3D""gmail_quote gmail_quote_container"" style= =3D""caret-color: rgb(0, 0, 0); font-family: Helvetica; font-size: 14px; fon= t-style: normal; font-variant-caps: normal; font-weight: 400; letter-spacin= g: normal; text-align: start; text-indent: 0px; text-transform: none; white= -space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-dec= oration: none;""><div dir=3D""ltr"" class=3D""gmail_attr"">On Fri, Oct 24, 2025 = at 9:42=E2=80=AFAM<span class=3D""Apple-converted-space"">&nbsp;</span><a hre= f=3D""mailto:naghmeh.karimi@umbc.edu"">naghmeh.karimi@umbc.edu</a><span class= =3D""Apple-converted-space"">&nbsp;</span>via RT &lt;<a href=3D""mailto:UMBCHe= lp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote cl= ass=3D""gmail_quote"" style=3D""margin: 0px 0px 0px 0.8ex; border-left-width: = 1px; border-left-style: solid; border-left-color: rgb(204, 204, 204); paddi= ng-left: 1ex;"">Ticket &lt;URL:<span class=3D""Apple-converted-space"">&nbsp;<= /span><a href=3D""https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/= Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D176192199900000= 0&amp;usg=3DAOvVaw099WO1Ar-MqC9UTifVgSrZ"" rel=3D""noreferrer"" target=3D""_bla= nk"">https://rt.umbc.edu/Ticket/Display.html?id=3D3296910</a><span class=3D""= Apple-converted-space"">&nbsp;</span>&gt;<br><br>Last Update From Ticket:<br= ><br>That's perfect Skye. I highly appreciate it. I ask Mohamamd to respond= <br>the ticket with the path.<br><br>regards,<br>Naghmeh<br><br>On Fri, Oct=  24, 2025 at 9:39=E2=80=AFAM Skye Jonke &lt;<a href=3D""mailto:ii69854@umbc.= edu"" target=3D""_blank"">ii69854@umbc.edu</a>&gt; wrote:<br><br>&gt; Its not = possible to use the existing scripts in a situation where the<br>&gt; files=  aren=E2=80=99t mounted in /umbc/software, but I can write some scripts<br>= &gt; specific to this setup to this situation with a bit more information. = Where<br>&gt; are the script files located on the HPC? If you aren=E2=80=99= t sure, you can get<br>&gt; the full path by running =E2=80=9Crealpath myfi= le.sh=E2=80=9D (replacing myfile.sh with one<br>&gt; of the cadence scripts= , of course)<br>&gt; ---<br>&gt; Skye Jonke<br>&gt;<br>&gt; she/her<br>&gt;= <br>&gt; Specialist, Linux System Administrator &amp; Lab Technical Support= <br>&gt;<br>&gt; On Oct 23, 2025, at 21:32, Naghmeh Karimi &lt;<a href=3D""m= ailto:nkarimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a>&gt; wrote:<b= r>&gt;<br>&gt; Hi Max, thanks for getting back to us.<br>&gt;<br>&gt; Hi Sk= ye, we need your help here to find the file locations. Could you<br>&gt; pl= ease help to resolve this issue?<br>&gt;<br>&gt; Thanks a lot<br>&gt; Naghm= eh<br>&gt;<br>&gt; Naghmeh Karimi, Ph.D.<br>&gt; Associate Professor<br>&gt= ; Department of Computer Science and Electrical Engineering<br>&gt; Univers= ity of Maryland, Baltimore County<br>&gt; Baltimore, MD 21250<br>&gt; Tel: = *410-455-3965* E-mail: *<a href=3D""mailto:nkarimi@umbc.edu"" target=3D""_blan= k"">nkarimi@umbc.edu</a><span class=3D""Apple-converted-space"">&nbsp;</span>&= lt;<a href=3D""mailto:nkarimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</= a>&gt;*<br>&gt; Web: *<a href=3D""https://www.google.com/url?q=3Dhttp://www.= csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&= amp;usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7"" rel=3D""noreferrer"" target=3D""_blank= "">http://www.csee.umbc.edu/~nkarimi/</a><br>&gt; &lt;<a href=3D""https://www= .google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://www.csee.umbc.edu= /~nkarimi/%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw23F= k_KFBnlrXC_uhIbUFIW&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&amp;= usg=3DAOvVaw05OHRJ3ua_KXM9EHoVYfgE"" rel=3D""noreferrer"" target=3D""_blank"">ht= tps://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source= =3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw23Fk_KFBnlrXC_uhIb= UFIW</a>&gt;*<br>&gt;<br>&gt; On Thu, Oct 23, 2025, 12:22=E2=80=AFPM Max Br= eitmeyer via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blan= k"">UMBCHelp@rt.umbc.edu</a>&gt;<br>&gt; wrote:<br>&gt;<br>&gt;&gt; Ticket &= lt;URL:<span class=3D""Apple-converted-space"">&nbsp;</span><a href=3D""https:= //www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D32969= 10&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw099WO= 1Ar-MqC9UTifVgSrZ"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu= /Ticket/Display.html?id=3D3296910</a><br>&gt;&gt; &lt;<a href=3D""https://ww= w.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.edu/Tic= ket/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D1761874374000= 000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&amp;source=3Dgmail-imap&amp;ust=3D= 1761921999000000&amp;usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g"" rel=3D""noreferrer""=  target=3D""_blank"">https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticke= t/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761874374000= 000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7</a>&gt;<br>&gt;&gt; &gt;<br>&gt;= &gt;<br>&gt;&gt; Last Update From Ticket:<br>&gt;&gt;<br>&gt;&gt; Hi Mohamm= ad,<br>&gt;&gt;<br>&gt;&gt; According to the error message, it's having an = issue finding another file<br>&gt;&gt; in<br>&gt;&gt; the directory. This a= ppears to be an issue due to the mounting locations<br>&gt;&gt; being<br>&g= t;&gt; different. I would point you to the previous email where I said that=  any<br>&gt;&gt; software with hard-coded locations would not work, and tha= t software<br>&gt;&gt; compatibility is not guaranteed. I would recommend w= orking with Skye on<br>&gt;&gt; porting<br>&gt;&gt; what is in the director= y to your local research directories on chip.<br>&gt;&gt;<br>&gt;&gt; On Th= u Oct 23 12:09:34 2025, NQ23652 wrote:<br>&gt;&gt;<br>&gt;&gt; &gt; Yes you=  are right, now I could see all shell files. However still I have<br>&gt;&g= t; &gt; problem when I run the sh file.<br>&gt;&gt;<br>&gt;&gt; &gt; [e127@= chip-login2 scripts]$ sh launch_synopsys_hspice.sh<br>&gt;&gt; &gt; launch_= synopsys_hspice.sh: line 3:<br>&gt;&gt; &gt; /umbc/software/scripts/env_syn= opsys_hspice.sh: No such file or directory<br>&gt;&gt; &gt; [e127@chip-logi= n2 scripts]$<br>&gt;&gt;<br>&gt;&gt; &gt; Would you please take a look to i= t.<br>&gt;&gt;<br>&gt;&gt; &gt; Regards,<br>&gt;&gt;<br>&gt;&gt; &gt; Moham= mad<br>&gt;&gt;<br>&gt;&gt; &gt; On Thu Oct 23 11:59:17 2025, OL73413 wrote= :<br>&gt;&gt;<br>&gt;&gt; &gt;&gt; Hi Mohammad,<br>&gt;&gt;<br>&gt;&gt; &gt= ;&gt; A good point. Chip uses a service called ""autofs"" to mount and unmoun= t<br>&gt;&gt; &gt;&gt; directories that aren't being actively used. This he= lps maintain the<br>&gt;&gt; &gt;&gt; login nodes integrity. If you see in = my screenshot below, the directory<br>&gt;&gt; &gt;&gt; appears to be empty= , but when I cd to the csee directory where your<br>&gt;&gt; &gt;&gt; softw= are is mounted, it is all available. Backing out of the directory<br>&gt;&g= t; &gt;&gt; and ls'ing the directory also shows that the directory is now m= ounted<br>&gt;&gt; &gt;&gt; and available.<br>&gt;&gt;<br>&gt;&gt; &gt;&gt;=  On Thu, Oct 23, 2025 at 11:41 AM Mohammad Ebrahimabadi via RT<br>&gt;&gt; = &gt;&gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBC= Help@rt.umbc.edu</a>&gt; wrote:<br>&gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; Ticket=  &lt;URL:<span class=3D""Apple-converted-space"">&nbsp;</span><a href=3D""http= s://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D329= 6910&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw099= WO1Ar-MqC9UTifVgSrZ"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.e= du/Ticket/Display.html?id=3D3296910</a><br>&gt;&gt; &lt;<a href=3D""https://= www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.edu/T= icket/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D17618743740= 00000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&amp;source=3Dgmail-imap&amp;ust= =3D1761921999000000&amp;usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g"" rel=3D""noreferr= er"" target=3D""_blank"">https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ti= cket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761874374= 000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7</a>&gt;<br>&gt;&gt; &gt;<br>&= gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; Comment From Ticket:<br>&gt;&gt; &gt;&gt;&= gt; Thank you Max!<br>&gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; Actually we have st= ill problem for accessing that directory. I<br>&gt;&gt; &gt;&gt;&gt; upload= ed two<br>&gt;&gt; &gt;&gt;&gt; screenshots that shows what we can see in t= he department servers<br>&gt;&gt; &gt;&gt;&gt; and what we<br>&gt;&gt; &gt;= &gt;&gt; can see in CHIP.<br>&gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; For example = by running this shell file we can launch HSPICE<br>&gt;&gt; &gt;&gt;&gt; so= ftware in the<br>&gt;&gt; &gt;&gt;&gt; department server:<br>&gt;&gt; &gt;&= gt;&gt; /umbc/software/scripts/launch_synopsys_hspice.sh<br>&gt;&gt; &gt;&g= t;&gt; But it is not still accessible on CHIP.<br>&gt;&gt;<br>&gt;&gt; &gt;= &gt;&gt; Regards,<br>&gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; Mohammad<br>&gt;&gt;= <br>&gt;&gt; &gt;&gt;&gt; On Thu Oct 23 10:54:59 2025, OL73413 wrote:<br>&g= t;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt; Hi Nahmeh,<br>&gt;&gt;<br>&gt;&gt; &gt= ;&gt;&gt; &gt; Apologies =E2=80=94 I meant to update you yesterday after co= mpleting<br>&gt;&gt; &gt;&gt;&gt; this, but I<br>&gt;&gt; &gt;&gt;&gt; &gt;=  was pulled into a long meeting. The software is now available on<br>&gt;&g= t; &gt;&gt;&gt; chip at:<br>&gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt; /umbc/so= ftware/csee<br>&gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt; Please note that if t= he software contains any hardcoded file<br>&gt;&gt; &gt;&gt;&gt; paths, it = may<br>&gt;&gt; &gt;&gt;&gt; &gt; not function correctly. Unfortunately, we=  don=E2=80=99t have control over<br>&gt;&gt; &gt;&gt;&gt; those<br>&gt;&gt;=  &gt;&gt;&gt; &gt; cases.<br>&gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt; Also, j= ust to clarify, Skye and I work in separate departments.<br>&gt;&gt; &gt;&g= t;&gt; Similar to<br>&gt;&gt; &gt;&gt;&gt; &gt; how I don=E2=80=99t have ad= ministrative access to the CSEE department<br>&gt;&gt; &gt;&gt;&gt; servers= <br>&gt;&gt; &gt;&gt;&gt; &gt; (which include the software directory), Skye=  is not an<br>&gt;&gt; &gt;&gt;&gt; administrator on<br>&gt;&gt; &gt;&gt;&g= t; &gt; chip.<br>&gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt; Please let me know = if you encounter any issues accessing the<br>&gt;&gt; &gt;&gt;&gt; software=  on<br>&gt;&gt; &gt;&gt;&gt; &gt; the server. If there are problems running=  the software, I=E2=80=99ll<br>&gt;&gt; &gt;&gt;&gt; coordinate<br>&gt;&gt;=  &gt;&gt;&gt; &gt; with Skye to troubleshoot, but since we don=E2=80=99t ma= intain this<br>&gt;&gt; &gt;&gt;&gt; software, we<br>&gt;&gt; &gt;&gt;&gt; = &gt; can=E2=80=99t guarantee full compatibility on our systems.<br>&gt;&gt;= <br>&gt;&gt; &gt;&gt;&gt; &gt; On Thu Oct 23 10:36:41 2025,<span class=3D""A= pple-converted-space"">&nbsp;</span><a href=3D""mailto:naghmeh.karimi@umbc.ed= u"" target=3D""_blank"">naghmeh.karimi@umbc.edu</a><span class=3D""Apple-conver= ted-space"">&nbsp;</span>wrote:<br>&gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt= ; Hi Skye and Max, Could you please help on mounting this today so<br>&gt;&= gt; &gt;&gt;&gt; we can<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt; use them as we ha= ve a deadline? Thanks,Naghmeh Karimi On Wed,<br>&gt;&gt; &gt;&gt;&gt; Oct 2= 2,<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt; 2025 at 4:22 PM Skye Jonke via RT &lt;= <a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.= edu</a>&gt; wrote:<br>&gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Ticket=  &lt;URL:<span class=3D""Apple-converted-space"">&nbsp;</span><a href=3D""http= s://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D329= 6910&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw099= WO1Ar-MqC9UTifVgSrZ"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.e= du/Ticket/Display.html?id=3D3296910</a><br>&gt;&gt; &lt;<a href=3D""https://= www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.edu/T= icket/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D17618743740= 00000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&amp;source=3Dgmail-imap&amp;ust= =3D1761921999000000&amp;usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g"" rel=3D""noreferr= er"" target=3D""_blank"">https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ti= cket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761874374= 000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7</a>&gt;<br>&gt;&gt; &gt;&gt;&= gt; &gt;<br>&gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Last Update From=  Ticket:<br>&gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Mohammad,<br>&gt= ;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; So long as you continue to acce= ss them through<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; /umbc/software/script= s, they should remain up to date and you<br>&gt;&gt; &gt;&gt;&gt; will<br>&= gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; see any new scripts. If you copy them to = a local directory on<br>&gt;&gt; &gt;&gt;&gt; the<br>&gt;&gt; &gt;&gt;&gt; = &gt;&gt;&gt; HPC cluster, they will not be updated.<br>&gt;&gt;<br>&gt;&gt;=  &gt;&gt;&gt; &gt;&gt;&gt; ---<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Skye J= onke<br>&gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; she/her<br>&gt;&gt;<= br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Specialist, Linux System Administrato= r &amp; Lab Technical Support<br>&gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;= &gt; &gt; On Oct 22, 2025, at 16:14, Naghmeh Karimi &lt;<a href=3D""mailto:n= karimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a>&gt;<br>&gt;&gt; &gt= ;&gt;&gt; &gt;&gt;&gt; wrote:<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br= >&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Hi Max, Roy, Skye,<br>&gt;&gt; &gt= ;&gt;&gt; &gt;&gt;&gt; &gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Than= k you all for your help.<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br>&gt;= &gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; I added Skye here as she is our IT exer= t on adding the<br>&gt;&gt; &gt;&gt;&gt; capability<br>&gt;&gt; &gt;&gt;&gt= ; &gt;&gt;&gt; of running synopsis tools (Hspice) in servers.<br>&gt;&gt; &= gt;&gt;&gt; &gt;&gt;&gt; &gt; Skye we want to use the HPC cluster to run hs= pice an other<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; synopsys tools. Could y= ou please help on this?<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br>&gt;&= gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Thanks,<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt= ;&gt; &gt; Naghmeh<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br>&gt;&gt; &= gt;&gt;&gt; &gt;&gt;&gt; &gt; On Wed, Oct 22, 2025 at 2:06 PM Mohammad Ebra= himabadi via RT<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &lt;<a href=3D""mailto= :UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a><span clas= s=3D""Apple-converted-space"">&nbsp;</span>&lt;mailto:<a href=3D""mailto:UMBCH= elp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt;&gt; wrote:<= br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Ticket &lt;URL:<br>&gt;&gt; = &gt;&gt;&gt;<span class=3D""Apple-converted-space"">&nbsp;</span><a href=3D""h= ttps://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D= 3296910&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw= 099WO1Ar-MqC9UTifVgSrZ"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umb= c.edu/Ticket/Display.html?id=3D3296910</a><br>&gt;&gt; &lt;<a href=3D""https= ://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.ed= u/Ticket/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D17618743= 74000000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&amp;source=3Dgmail-imap&amp;u= st=3D1761921999000000&amp;usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g"" rel=3D""norefe= rrer"" target=3D""_blank"">https://www.google.com/url?q=3Dhttps://rt.umbc.edu/= Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D17618743= 74000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7</a>&gt;<br>&gt;&gt; &gt;&gt= ;&gt; &gt;&gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &lt;<br>&gt;&gt;<span class=3D""= Apple-converted-space"">&nbsp;</span><a href=3D""https://www.google.com/url?q= =3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?i= d%253D3296910%26source%3Dgmail-imap%26ust%3D1761768891000000%26usg%3DAOvVaw= 2jxB3nArHBD7-ij9ZN8ZsW&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&a= mp;usg=3DAOvVaw04hQC2SRO0y4BoVR0LFNp_"" rel=3D""noreferrer"" target=3D""_blank""= >https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%= 3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761768891000000&amp;usg=3DAOvV= aw2jxB3nArHBD7-ij9ZN8ZsW</a><br>&gt;&gt; &lt;<a href=3D""https://www.google.= com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://www.google.com/url?q%253= Dhttps://rt.umbc.edu/Ticket/Display.html?id%25253D3296910%2526source%253Dgm= ail-imap%2526ust%253D1761768891000000%2526usg%253DAOvVaw2jxB3nArHBD7-ij9ZN8= ZsW%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw0fLylAsBAo= Q6LgqY79I9-d&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DA= OvVaw3pQOXM-6FD6kt0kA6ujsVN"" rel=3D""noreferrer"" target=3D""_blank"">https://w= ww.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.edu/Ti= cket/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D176176889100= 0000%26usg%3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW&amp;source=3Dgmail-imap&amp;ust= =3D1761874374000000&amp;usg=3DAOvVaw0fLylAsBAoQ6LgqY79I9-d</a>&gt;<br>&gt;&= gt; &gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br>&gt;&gt; &gt;&gt;&gt= ; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Last=  Update From Ticket:<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;= &gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Hi Max,<br>&gt;&gt; &gt;&gt;&gt; &g= t;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Thanks f= or your help! Let=E2=80=99s proceed based on your<br>&gt;&gt; &gt;&gt;&gt; = suggestion,<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; hopefully it<br>&gt;&gt; = &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; works as expected.<br>&gt;&gt; &gt;&gt;&= gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; I = just have one quick question: since the IT team in the<br>&gt;&gt; &gt;&gt;= &gt; &gt;&gt;&gt; department can modify<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&g= t; &gt;&gt; or add new shell scripts in that directory, will we also<br>&gt= ;&gt; &gt;&gt;&gt; have<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; access to tho= se<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; updated or newly added sc= ripts from the mounted directory on<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; H= PCF?<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt;&gt= ; &gt;&gt;&gt; &gt;&gt; Thanks again,<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt;=  &gt;&gt; Mohammad<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&g= t; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; On Wed Oct 22 14:01:32 2025, OL73413 = wrote:<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt;&= gt; &gt;&gt;&gt; &gt;&gt; &gt; Hi Mohammad,<br>&gt;&gt; &gt;&gt;&gt; &gt;&g= t;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; We can = get the software mounted for you on chip, but it<br>&gt;&gt; &gt;&gt;&gt; w= ill<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; have to be<br>&gt;&gt; &gt;&gt;&g= t; &gt;&gt;&gt; &gt;&gt; &gt; read-only as we won't be able to manage who c= an make<br>&gt;&gt; &gt;&gt;&gt; changes<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&= gt; to it<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; otherwise.<br= >&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&= gt;&gt; &gt;&gt; &gt; As a reminder, the machines on that run on chip are n= ot<br>&gt;&gt; &gt;&gt;&gt; the<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; same = as those<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; run by the CSE= E department, so we can't guarantee that<br>&gt;&gt; &gt;&gt;&gt; these<br>= &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; will run the<br>&gt;&gt; &gt;&gt;&gt; &g= t;&gt;&gt; &gt;&gt; &gt; same way or as effectively.<br>&gt;&gt; &gt;&gt;&g= t; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt= ; If that's all ok with you, please give me some time to get<br>&gt;&gt; &g= t;&gt;&gt; the<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; directory<br>&gt;&gt; = &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; mounted.<br>&gt;&gt; &gt;&gt;&gt; &= gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; On=  Wed Oct 22 13:26:05 2025, NQ23652 wrote:<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;= &gt; &gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thank= s Tartela for your help to resolve my issue!<br>&gt;&gt; &gt;&gt;&gt; &gt;&= gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Ac= tually it seems all server in CSEE department have<br>&gt;&gt; &gt;&gt;&gt;=  access<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; to that<br>&gt;&gt; &gt;&gt;&= gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; path. When I ran the command that you ga= ve me I got the<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; following<br>&gt;&gt;=  &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; report:<br>&gt;&gt; &gt;&gt;&g= t; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt= ;&gt; mohammad@alborz:~$ df -h /umbc/software<br>&gt;&gt; &gt;&gt;&gt; &gt;= &gt;&gt; &gt;&gt; &gt;&gt; Filesystem Size Used Avail Use% Mounted on<br>&g= t;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;<span class=3D""Apple-conv= erted-space"">&nbsp;</span><a href=3D""http://nfs.iss.rs.umbc.edu/"">nfs.iss.r= s.umbc.edu</a>:/ifs/data/csee/software 2.5T 2.2T<br>&gt;&gt; &gt;&gt;&gt; 3= 26G<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; 88%<br>&gt;&gt; &gt;&gt;&gt; &gt;= &gt;&gt; &gt;&gt; &gt;&gt; /umbc/software<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;= &gt; &gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; moham= mad@alborz:~$ mount | grep /umbc/software<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;= &gt; &gt;&gt; &gt;&gt;<span class=3D""Apple-converted-space"">&nbsp;</span><a=  href=3D""http://nfs.iss.rs.umbc.edu/"">nfs.iss.rs.umbc.edu</a>:/ifs/data/cse= e/software on<br>&gt;&gt; &gt;&gt;&gt; /umbc/software<br>&gt;&gt; &gt;&gt;&= gt; &gt;&gt;&gt; type nfs<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &g= t;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt;<br>&gt;&gt; &gt;&gt;&gt;<br>&g= t;&gt; (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namlen=3D255,acr= egmin=3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,proto=3Dtcp= ,timeo=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountvers=3D3,mou= ntport=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.60)<br>&gt;&= gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt= ; &gt;&gt; &gt;&gt; Please let me know if you need other information.<br>&g= t;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;= &gt; &gt;&gt; &gt;&gt; Regards,<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&= gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Mohammad<br>&gt= ;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&= gt; &gt;&gt; &gt;&gt; On Wed Oct 22 13:15:12 2025, HK41259 wrote:<br>&gt;&g= t; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt;=  &gt;&gt; &gt;&gt;&gt; Hello,<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt= ;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Thanks for re= aching out. Could you clarify which<br>&gt;&gt; &gt;&gt;&gt; specific<br>&g= t;&gt; &gt;&gt;&gt; &gt;&gt;&gt; machine<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&= gt; &gt;&gt; &gt;&gt;&gt; or server you=E2=80=99re referring to when you me= ntion the<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; =E2=80=9Cdepartmental<br>&g= t;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; server=E2=80=9D?<br>= &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&g= t;&gt; &gt;&gt; &gt;&gt;&gt; The /umbc/software/scripts/ directory isn=E2= =80=99t accessible<br>&gt;&gt; &gt;&gt;&gt; from<br>&gt;&gt; &gt;&gt;&gt; &= gt;&gt;&gt; HPCF<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt= ; (e.g., Chip) by default =E2=80=94 those systems have separate<br>&gt;&gt;=  &gt;&gt;&gt; &gt;&gt;&gt; storage and<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt= ; &gt;&gt; &gt;&gt;&gt; don=E2=80=99t automatically mount departmental shar= es.<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt;&gt;=  &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Could you check where /umbc/software is=  mounted on your<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt= ; departmental system (e.g., by running df -h<br>&gt;&gt; &gt;&gt;&gt; /umb= c/software<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; or mount<br>&gt;&gt; &gt;&= gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; | grep /umbc/software)? That=E2= =80=99ll tell us which server<br>&gt;&gt; &gt;&gt;&gt; hosts<br>&gt;&gt; &g= t;&gt;&gt; &gt;&gt;&gt; it. Once<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;= &gt; &gt;&gt;&gt; we know that, we can confirm whether it=E2=80=99s possibl= e or<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; appropriate<br>&gt;&gt; &gt;&gt;= &gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; to make it visible from HPCF.<br>&g= t;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;= &gt; &gt;&gt; &gt;&gt;&gt; Best,<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;= &gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Tartela<br= >&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&= gt;&gt; &gt;&gt; &gt;&gt;&gt; On Tue Oct 21 15:59:49 2025, ZZ99999 wrote:<b= r>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;= &gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; First Name: Mohammad<br>&gt;&gt; &gt;&gt= ;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Last Name: Ebrahimabadi<br>&gt= ;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Email:<span class= =3D""Apple-converted-space"">&nbsp;</span><a href=3D""mailto:e127@umbc.edu"" ta= rget=3D""_blank"">e127@umbc.edu</a><span class=3D""Apple-converted-space"">&nbs= p;</span>&lt;mailto:<a href=3D""mailto:e127@umbc.edu"" target=3D""_blank"">e127= @umbc.edu</a>&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&g= t;&gt; Campus ID: NQ23652<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &g= t;&gt;&gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&= gt; Request Type: High Performance Cluster<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt= ;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&= gt; &gt;&gt;&gt;&gt; Dear Team,<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&= gt; &gt;&gt;&gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt= ;&gt;&gt; I hope you=E2=80=99re doing well.<br>&gt;&gt; &gt;&gt;&gt; &gt;&g= t;&gt; &gt;&gt; &gt;&gt;&gt;&gt; I=E2=80=99m a Ph.D. student in Computer En= gineering and have a<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; question regardi= ng running a software on the HPCF. On the<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;= &gt; department server, I usually run software using shell scripts<br>&gt;&= gt; &gt;&gt;&gt; &gt;&gt;&gt; located at /umbc/software/scripts/ which is p= rovided by the IT<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; group pf the depart= ment (Geoff Weiss Team). However, in case of<br>&gt;&gt; &gt;&gt;&gt; &gt;&= gt;&gt; HPCF, it seems that this path is not accessible from the HPCF<br>&g= t;&gt; &gt;&gt;&gt; &gt;&gt;&gt; environment.<br>&gt;&gt; &gt;&gt;&gt; &gt;= &gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &g= t;&gt; &gt;&gt;&gt;&gt; Could you please let me know if there is any specif= ic<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; configuration or access permission=  required to reach this<br>&gt;&gt; &gt;&gt;&gt; directory<br>&gt;&gt; &gt;= &gt;&gt; &gt;&gt;&gt; through the HPCF? or direct me to a related person th= at can<br>&gt;&gt; &gt;&gt;&gt; help<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; = me.<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br>&gt;= &gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Best regards,<br>&= gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Mohammad<br>&gt= ;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;<br>&gt;&gt; &gt;&gt;&= gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &g= t; --<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt;&g= t; &gt;&gt;&gt; &gt;&gt; &gt; Best,<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &= gt;&gt; &gt; Max Breitmeyer<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; = &gt; DOIT HPC System Administrator<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &g= t;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt;&= gt; &gt;&gt;&gt; &gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br>&gt;&gt= ; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;=  --<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Naghmeh Karimi, Ph.D.<br>&gt= ;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Associate Professor<br>&gt;&gt; &gt;&g= t;&gt; &gt;&gt;&gt; &gt; Department of Computer Science and Electrical Engi= neering<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; University of Maryland, = Baltimore County<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Baltimore, MD 2= 1250<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Tel: 410-455-3965 E-mail:<s= pan class=3D""Apple-converted-space"">&nbsp;</span><a href=3D""mailto:nkarimi@= umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a><br>&gt;&gt; &gt;&gt;&gt; &= gt;&gt;&gt; &lt;mailto:<a href=3D""mailto:nkarimi@umbc.edu"" target=3D""_blank= "">nkarimi@umbc.edu</a>&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Web:<= span class=3D""Apple-converted-space"">&nbsp;</span><a href=3D""https://www.go= ogle.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap= &amp;ust=3D1761921999000000&amp;usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7"" rel=3D""= noreferrer"" target=3D""_blank"">http://www.csee.umbc.edu/~nkarimi/</a><br>&gt= ;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttps://www.google.com/= url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D17= 61874374000000%26usg%3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&amp;source=3Dgmail-imap= &amp;ust=3D1761921999000000&amp;usg=3DAOvVaw05OHRJ3ua_KXM9EHoVYfgE"" rel=3D""= noreferrer"" target=3D""_blank"">https://www.google.com/url?q=3Dhttp://www.cse= e.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761874374000000&amp= ;usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW</a>&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt;&g= t;&gt;<br>&gt;&gt; &gt;&gt;&gt; &lt;<br>&gt;&gt;<span class=3D""Apple-conver= ted-space"">&nbsp;</span><a href=3D""https://www.google.com/url?q=3Dhttps://w= ww.google.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmail-i= map%26ust%3D1761768891000000%26usg%3DAOvVaw0xLE02sw5-ueWRHyHyCwkN&amp;sourc= e=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw06Mkt_vSjpo8lnqxE= qL6dt"" rel=3D""noreferrer"" target=3D""_blank"">https://www.google.com/url?q=3D= http://www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D176176= 8891000000&amp;usg=3DAOvVaw0xLE02sw5-ueWRHyHyCwkN</a><br>&gt;&gt; &lt;<a hr= ef=3D""https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps:/= /www.google.com/url?q%253Dhttp://www.csee.umbc.edu/~nkarimi/%2526source%253= Dgmail-imap%2526ust%253D1761768891000000%2526usg%253DAOvVaw0xLE02sw5-ueWRHy= HyCwkN%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw3g338pG= QqShCgyX8cIt8tV&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg= =3DAOvVaw3ktt5GEiV5miHD1TY9Dut7"" rel=3D""noreferrer"" target=3D""_blank"">https= ://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://www.csee.um= bc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D1761768891000000%26usg%3DAOv= Vaw0xLE02sw5-ueWRHyHyCwkN&amp;source=3Dgmail-imap&amp;ust=3D176187437400000= 0&amp;usg=3DAOvVaw3g338pGQqShCgyX8cIt8tV</a>&gt;<br>&gt;&gt; &gt;<br>&gt;&g= t;<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt; -- Naghmeh Karimi, Ph.D.<br>&gt;&gt; &= gt;&gt;&gt; &gt;&gt; Associate Professor<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt; = Department of Computer Science and Electrical Engineering<br>&gt;&gt; &gt;&= gt;&gt; &gt;&gt; University of Maryland, Baltimore County<br>&gt;&gt; &gt;&= gt;&gt; &gt;&gt; Baltimore, MD 21250<br>&gt;&gt; &gt;&gt;&gt; &gt;&gt; Tel:=  410-455-3965 E-mail:<span class=3D""Apple-converted-space"">&nbsp;</span><a = href=3D""mailto:nkarimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a><br>= &gt;&gt; &gt;&gt;&gt; &gt;&gt; Web:<span class=3D""Apple-converted-space"">&n= bsp;</span><a href=3D""https://www.google.com/url?q=3Dhttp://www.csee.umbc.e= du/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DA= OvVaw35D5VfZwU_gTyQhCtkbUZ7"" rel=3D""noreferrer"" target=3D""_blank"">http://ww= w.csee.umbc.edu/~nkarimi/</a><br>&gt;&gt; &lt;<a href=3D""https://www.google= .com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://www.csee.umbc.edu/~nkari= mi/%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw23Fk_KFBnl= rXC_uhIbUFIW&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DA= OvVaw05OHRJ3ua_KXM9EHoVYfgE"" rel=3D""noreferrer"" target=3D""_blank"">https://w= ww.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail= -imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW</a>= &gt;<br>&gt;&gt;<br>&gt;&gt; &gt;&gt;&gt; &gt; --<br>&gt;&gt;<br>&gt;&gt; &= gt;&gt;&gt; &gt; Best,<br>&gt;&gt; &gt;&gt;&gt; &gt; Max Breitmeyer<br>&gt;= &gt; &gt;&gt;&gt; &gt; DOIT HPC System Administrator<br>&gt;&gt;<br>&gt;&gt= ; &gt;&gt; -- V/R,Maxwell BreitmeyerUMBC HPCF SpecialistUMBC Observatory IT= <br>&gt;&gt; &gt;&gt; Manager Graduate Student(443) 835-8250<br>&gt;&gt;<br= >&gt;&gt; --<br>&gt;&gt;<br>&gt;&gt; Best,<br>&gt;&gt; Max Breitmeyer<br>&g= t;&gt; DOIT HPC System Administrator<br>&gt;&gt;<br>&gt;&gt;<br>&gt;&gt;<br= >&gt;<br><br>--<span class=3D""Apple-converted-space"">&nbsp;</span><br>Naghm= eh Karimi, Ph.D.<br>Associate Professor<br>Department of Computer Science a= nd Electrical Engineering<br>University of Maryland, Baltimore County<br>Ba= ltimore, MD 21250<br>Tel: *410-455-3965* E-mail: *<a href=3D""mailto:nkarimi= @umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a><span class=3D""Apple-conve= rted-space"">&nbsp;</span>&lt;<a href=3D""mailto:nkarimi@umbc.edu"" target=3D""= _blank"">nkarimi@umbc.edu</a>&gt;*<br>Web: *<a href=3D""https://www.google.co= m/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;us= t=3D1761921999000000&amp;usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7"" rel=3D""norefer= rer"" target=3D""_blank"">http://www.csee.umbc.edu/~nkarimi/</a><br>&lt;<a hre= f=3D""https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;= source=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw35D5VfZwU_gT= yQhCtkbUZ7"" rel=3D""noreferrer"" target=3D""_blank"">http://www.csee.umbc.edu/~= nkarimi/</a>&gt;*</blockquote></div></div></blockquote></div><br></body></h= tml>= "
3296910,72477635,Correspond,DoIT-Research-Computing,2025-10-24 14:51:33.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Skye Jonke,ii69854@umbc.edu,"That=E2=80=99s a bit confusing, since hardcoded paths won=E2=80=99t work, p= lease have Max contact me directly so we can talk about what needs to be do= ne to make this work.=20 --- Skye Jonke  she/her  Specialist, Linux System Administrator & Lab Technical Support  > On Oct 24, 2025, at 10:46, Mohammad Ebrahimabadi <e127@umbc.edu> wrote: >=20 > Hello Skye, >=20 > I believe the way that Max told me to run the script was using the same p= ath that we have in the CSEE department as he mounted that path in HPCF. I = mean this path: >  /umbc/software/scripts/launch_synopsys_hspice.sh >=20 >=20 > Regards, > Mohammad >=20 >=20 >=20 >=20 > On Fri, Oct 24, 2025 at 9:42=E2=80=AFAM naghmeh.karimi@umbc.edu <mailto:n= aghmeh.karimi@umbc.edu> via RT <UMBCHelp@rt.umbc.edu <mailto:UMBCHelp@rt.um= bc.edu>> wrote: >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 <https= ://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3296= 910&source=3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw099WO1Ar-MqC9UTi= fVgSrZ> > >>=20 >> Last Update From Ticket: >>=20 >> That's perfect Skye. I highly appreciate it. I ask Mohamamd to respond >> the ticket with the path. >>=20 >> regards, >> Naghmeh >>=20 >> On Fri, Oct 24, 2025 at 9:39=E2=80=AFAM Skye Jonke <ii69854@umbc.edu <ma= ilto:ii69854@umbc.edu>> wrote: >>=20 >> > Its not possible to use the existing scripts in a situation where the >> > files aren=E2=80=99t mounted in /umbc/software, but I can write some s= cripts >> > specific to this setup to this situation with a bit more information. = Where >> > are the script files located on the HPC? If you aren=E2=80=99t sure, y= ou can get >> > the full path by running =E2=80=9Crealpath myfile.sh=E2=80=9D (replaci= ng myfile.sh with one >> > of the cadence scripts, of course) >> > --- >> > Skye Jonke >> > >> > she/her >> > >> > Specialist, Linux System Administrator & Lab Technical Support >> > >> > On Oct 23, 2025, at 21:32, Naghmeh Karimi <nkarimi@umbc.edu <mailto:nk= arimi@umbc.edu>> wrote: >> > >> > Hi Max, thanks for getting back to us. >> > >> > Hi Skye, we need your help here to find the file locations. Could you >> > please help to resolve this issue? >> > >> > Thanks a lot >> > Naghmeh >> > >> > Naghmeh Karimi, Ph.D. >> > Associate Professor >> > Department of Computer Science and Electrical Engineering >> > University of Maryland, Baltimore County >> > Baltimore, MD 21250 >> > Tel: *410-455-3965* E-mail: *nkarimi@umbc.edu <mailto:nkarimi@umbc.edu= > <nkarimi@umbc.edu <mailto:nkarimi@umbc.edu>>* >> > Web: *http://www.csee.umbc.edu/~nkarimi/ <https://www.google.com/url?q= =3Dhttp://www.csee.umbc.edu/~nkarimi/&source=3Dgmail-imap&ust=3D17619219990= 00000&usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7> >> > <https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&sou= rce=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW = <https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://www.c= see.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg= %3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&source=3Dgmail-imap&ust=3D1761921999000000&= usg=3DAOvVaw05OHRJ3ua_KXM9EHoVYfgE>>* >> > >> > On Thu, Oct 23, 2025, 12:22=E2=80=AFPM Max Breitmeyer via RT <UMBCHelp= @rt.umbc.edu <mailto:UMBCHelp@rt.umbc.edu>> >> > wrote: >> > >> >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 <ht= tps://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3= 296910&source=3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw099WO1Ar-MqC9= UTifVgSrZ> >> >> <https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.ht= ml?id%3D3296910&source=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw3Imz= iGtKZ-9j9JFW8J65w7 <https://www.google.com/url?q=3Dhttps://www.google.com/u= rl?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgm= ail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&sour= ce=3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g>> >> >> > >> >> >> >> Last Update From Ticket: >> >> >> >> Hi Mohammad, >> >> >> >> According to the error message, it's having an issue finding another = file >> >> in >> >> the directory. This appears to be an issue due to the mounting locati= ons >> >> being >> >> different. I would point you to the previous email where I said that = any >> >> software with hard-coded locations would not work, and that software >> >> compatibility is not guaranteed. I would recommend working with Skye = on >> >> porting >> >> what is in the directory to your local research directories on chip. >> >> >> >> On Thu Oct 23 12:09:34 2025, NQ23652 wrote: >> >> >> >> > Yes you are right, now I could see all shell files. However still I=  have >> >> > problem when I run the sh file. >> >> >> >> > [e127@chip-login2 scripts]$ sh launch_synopsys_hspice.sh >> >> > launch_synopsys_hspice.sh: line 3: >> >> > /umbc/software/scripts/env_synopsys_hspice.sh: No such file or dire= ctory >> >> > [e127@chip-login2 scripts]$ >> >> >> >> > Would you please take a look to it. >> >> >> >> > Regards, >> >> >> >> > Mohammad >> >> >> >> > On Thu Oct 23 11:59:17 2025, OL73413 wrote: >> >> >> >> >> Hi Mohammad, >> >> >> >> >> A good point. Chip uses a service called ""autofs"" to mount and unm= ount >> >> >> directories that aren't being actively used. This helps maintain t= he >> >> >> login nodes integrity. If you see in my screenshot below, the dire= ctory >> >> >> appears to be empty, but when I cd to the csee directory where your >> >> >> software is mounted, it is all available. Backing out of the direc= tory >> >> >> and ls'ing the directory also shows that the directory is now moun= ted >> >> >> and available. >> >> >> >> >> On Thu, Oct 23, 2025 at 11:41 AM Mohammad Ebrahimabadi via RT >> >> >> <UMBCHelp@rt.umbc.edu <mailto:UMBCHelp@rt.umbc.edu>> wrote: >> >> >> >> >>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910=  <https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id= %3D3296910&source=3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw099WO1Ar-= MqC9UTifVgSrZ> >> >> <https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.ht= ml?id%3D3296910&source=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw3Imz= iGtKZ-9j9JFW8J65w7 <https://www.google.com/url?q=3Dhttps://www.google.com/u= rl?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgm= ail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&sour= ce=3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g>> >> >> > >> >> >> >> >>> Comment From Ticket: >> >> >>> Thank you Max! >> >> >> >> >>> Actually we have still problem for accessing that directory. I >> >> >>> uploaded two >> >> >>> screenshots that shows what we can see in the department servers >> >> >>> and what we >> >> >>> can see in CHIP. >> >> >> >> >>> For example by running this shell file we can launch HSPICE >> >> >>> software in the >> >> >>> department server: >> >> >>> /umbc/software/scripts/launch_synopsys_hspice.sh >> >> >>> But it is not still accessible on CHIP. >> >> >> >> >>> Regards, >> >> >> >> >>> Mohammad >> >> >> >> >>> On Thu Oct 23 10:54:59 2025, OL73413 wrote: >> >> >> >> >>> > Hi Nahmeh, >> >> >> >> >>> > Apologies =E2=80=94 I meant to update you yesterday after compl= eting >> >> >>> this, but I >> >> >>> > was pulled into a long meeting. The software is now available on >> >> >>> chip at: >> >> >> >> >>> > /umbc/software/csee >> >> >> >> >>> > Please note that if the software contains any hardcoded file >> >> >>> paths, it may >> >> >>> > not function correctly. Unfortunately, we don=E2=80=99t have co= ntrol over >> >> >>> those >> >> >>> > cases. >> >> >> >> >>> > Also, just to clarify, Skye and I work in separate departments. >> >> >>> Similar to >> >> >>> > how I don=E2=80=99t have administrative access to the CSEE depa= rtment >> >> >>> servers >> >> >>> > (which include the software directory), Skye is not an >> >> >>> administrator on >> >> >>> > chip. >> >> >> >> >>> > Please let me know if you encounter any issues accessing the >> >> >>> software on >> >> >>> > the server. If there are problems running the software, I=E2=80= =99ll >> >> >>> coordinate >> >> >>> > with Skye to troubleshoot, but since we don=E2=80=99t maintain = this >> >> >>> software, we >> >> >>> > can=E2=80=99t guarantee full compatibility on our systems. >> >> >> >> >>> > On Thu Oct 23 10:36:41 2025, naghmeh.karimi@umbc.edu <mailto:na= ghmeh.karimi@umbc.edu> wrote: >> >> >> >> >>> >> Hi Skye and Max, Could you please help on mounting this today = so >> >> >>> we can >> >> >>> >> use them as we have a deadline? Thanks,Naghmeh Karimi On Wed, >> >> >>> Oct 22, >> >> >>> >> 2025 at 4:22 PM Skye Jonke via RT <UMBCHelp@rt.umbc.edu <mailt= o:UMBCHelp@rt.umbc.edu>> wrote: >> >> >> >> >>> >>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D329= 6910 <https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.htm= l?id%3D3296910&source=3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw099WO= 1Ar-MqC9UTifVgSrZ> >> >> <https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.ht= ml?id%3D3296910&source=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw3Imz= iGtKZ-9j9JFW8J65w7 <https://www.google.com/url?q=3Dhttps://www.google.com/u= rl?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgm= ail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&sour= ce=3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g>> >> >> >>> > >> >> >> >> >>> >>> Last Update From Ticket: >> >> >> >> >>> >>> Mohammad, >> >> >> >> >>> >>> So long as you continue to access them through >> >> >>> >>> /umbc/software/scripts, they should remain up to date and you >> >> >>> will >> >> >>> >>> see any new scripts. If you copy them to a local directory on >> >> >>> the >> >> >>> >>> HPC cluster, they will not be updated. >> >> >> >> >>> >>> --- >> >> >>> >>> Skye Jonke >> >> >> >> >>> >>> she/her >> >> >> >> >>> >>> Specialist, Linux System Administrator & Lab Technical Support >> >> >> >> >>> >>> > On Oct 22, 2025, at 16:14, Naghmeh Karimi <nkarimi@umbc.edu=  <mailto:nkarimi@umbc.edu>> >> >> >>> >>> wrote: >> >> >>> >>> > >> >> >>> >>> > Hi Max, Roy, Skye, >> >> >>> >>> > >> >> >>> >>> > Thank you all for your help. >> >> >>> >>> > >> >> >>> >>> > I added Skye here as she is our IT exert on adding the >> >> >>> capability >> >> >>> >>> of running synopsis tools (Hspice) in servers. >> >> >>> >>> > Skye we want to use the HPC cluster to run hspice an other >> >> >>> >>> synopsys tools. Could you please help on this? >> >> >>> >>> > >> >> >>> >>> > Thanks, >> >> >>> >>> > Naghmeh >> >> >>> >>> > >> >> >>> >>> > On Wed, Oct 22, 2025 at 2:06 PM Mohammad Ebrahimabadi via RT >> >> >>> >>> <UMBCHelp@rt.umbc.edu <mailto:UMBCHelp@rt.umbc.edu> <mailto:U= MBCHelp@rt.umbc.edu <mailto:UMBCHelp@rt.umbc.edu>>> wrote: >> >> >>> >>> >> Ticket <URL: >> >> >>> https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 <https://www= .google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3296910&so= urce=3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw099WO1Ar-MqC9UTifVgSrZ> >> >> <https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.ht= ml?id%3D3296910&source=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw3Imz= iGtKZ-9j9JFW8J65w7 <https://www.google.com/url?q=3Dhttps://www.google.com/u= rl?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgm= ail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&sour= ce=3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g>> >> >> >>> >>> >> >> >>> < >> >> https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.htm= l?id%3D3296910&source=3Dgmail-imap&ust=3D1761768891000000&usg=3DAOvVaw2jxB3= nArHBD7-ij9ZN8ZsW <https://www.google.com/url?q=3Dhttps://www.google.com/ur= l?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgma= il-imap%26ust%3D1761768891000000%26usg%3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW&sourc= e=3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw04hQC2SRO0y4BoVR0LFNp_> >> >> <https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps:= //rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgmail-imap%26us= t%3D1761768891000000%26usg%3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW&source=3Dgmail-im= ap&ust=3D1761874374000000&usg=3DAOvVaw0fLylAsBAoQ6LgqY79I9-d <https://www.g= oogle.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://www.google.com/url= ?q%253Dhttps://rt.umbc.edu/Ticket/Display.html?id%25253D3296910%2526source%= 253Dgmail-imap%2526ust%253D1761768891000000%2526usg%253DAOvVaw2jxB3nArHBD7-= ij9ZN8ZsW%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw0fLy= lAsBAoQ6LgqY79I9-d&source=3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw3= pQOXM-6FD6kt0kA6ujsVN>> >> >> > >> >> >>> >>> > >> >> >>> >>> >> >> >> >>> >>> >> Last Update From Ticket: >> >> >>> >>> >> >> >> >>> >>> >> Hi Max, >> >> >>> >>> >> >> >> >>> >>> >> Thanks for your help! Let=E2=80=99s proceed based on your >> >> >>> suggestion, >> >> >>> >>> hopefully it >> >> >>> >>> >> works as expected. >> >> >>> >>> >> >> >> >>> >>> >> I just have one quick question: since the IT team in the >> >> >>> >>> department can modify >> >> >>> >>> >> or add new shell scripts in that directory, will we also >> >> >>> have >> >> >>> >>> access to those >> >> >>> >>> >> updated or newly added scripts from the mounted directory = on >> >> >>> >>> HPCF? >> >> >>> >>> >> >> >> >>> >>> >> Thanks again, >> >> >>> >>> >> Mohammad >> >> >>> >>> >> >> >> >>> >>> >> On Wed Oct 22 14:01:32 2025, OL73413 wrote: >> >> >>> >>> >> >> >> >>> >>> >> > Hi Mohammad, >> >> >>> >>> >> >> >> >>> >>> >> > We can get the software mounted for you on chip, but it >> >> >>> will >> >> >>> >>> have to be >> >> >>> >>> >> > read-only as we won't be able to manage who can make >> >> >>> changes >> >> >>> >>> to it >> >> >>> >>> >> > otherwise. >> >> >>> >>> >> >> >> >>> >>> >> > As a reminder, the machines on that run on chip are not >> >> >>> the >> >> >>> >>> same as those >> >> >>> >>> >> > run by the CSEE department, so we can't guarantee that >> >> >>> these >> >> >>> >>> will run the >> >> >>> >>> >> > same way or as effectively. >> >> >>> >>> >> >> >> >>> >>> >> > If that's all ok with you, please give me some time to g= et >> >> >>> the >> >> >>> >>> directory >> >> >>> >>> >> > mounted. >> >> >>> >>> >> >> >> >>> >>> >> > On Wed Oct 22 13:26:05 2025, NQ23652 wrote: >> >> >>> >>> >> >> >> >>> >>> >> >> Thanks Tartela for your help to resolve my issue! >> >> >>> >>> >> >> >> >>> >>> >> >> Actually it seems all server in CSEE department have >> >> >>> access >> >> >>> >>> to that >> >> >>> >>> >> >> path. When I ran the command that you gave me I got the >> >> >>> >>> following >> >> >>> >>> >> >> report: >> >> >>> >>> >> >> >> >>> >>> >> >> mohammad@alborz:~$ df -h /umbc/software >> >> >>> >>> >> >> Filesystem Size Used Avail Use% Mounted on >> >> >>> >>> >> >> nfs.iss.rs.umbc.edu <http://nfs.iss.rs.umbc.edu/>:/ifs/= data/csee/software 2.5T 2.2T >> >> >>> 326G >> >> >>> >>> 88% >> >> >>> >>> >> >> /umbc/software >> >> >>> >>> >> >> >> >>> >>> >> >> mohammad@alborz:~$ mount | grep /umbc/software >> >> >>> >>> >> >> nfs.iss.rs.umbc.edu <http://nfs.iss.rs.umbc.edu/>:/ifs/= data/csee/software on >> >> >>> /umbc/software >> >> >>> >>> type nfs >> >> >>> >>> >> >> >> >> >>> >>> >> >> >>> >> >> (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namlen=3D255,acre= gmin=3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,proto=3Dtcp,= timeo=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountvers=3D3,moun= tport=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.60) >> >> >>> >>> >> >> >> >>> >>> >> >> Please let me know if you need other information. >> >> >>> >>> >> >> >> >>> >>> >> >> Regards, >> >> >>> >>> >> >> >> >>> >>> >> >> Mohammad >> >> >>> >>> >> >> >> >>> >>> >> >> On Wed Oct 22 13:15:12 2025, HK41259 wrote: >> >> >>> >>> >> >> >> >>> >>> >> >>> Hello, >> >> >>> >>> >> >> >> >>> >>> >> >>> Thanks for reaching out. Could you clarify which >> >> >>> specific >> >> >>> >>> machine >> >> >>> >>> >> >>> or server you=E2=80=99re referring to when you mention=  the >> >> >>> >>> =E2=80=9Cdepartmental >> >> >>> >>> >> >>> server=E2=80=9D? >> >> >>> >>> >> >> >> >>> >>> >> >>> The /umbc/software/scripts/ directory isn=E2=80=99t ac= cessible >> >> >>> from >> >> >>> >>> HPCF >> >> >>> >>> >> >>> (e.g., Chip) by default =E2=80=94 those systems have s= eparate >> >> >>> >>> storage and >> >> >>> >>> >> >>> don=E2=80=99t automatically mount departmental shares. >> >> >>> >>> >> >> >> >>> >>> >> >>> Could you check where /umbc/software is mounted on your >> >> >>> >>> >> >>> departmental system (e.g., by running df -h >> >> >>> /umbc/software >> >> >>> >>> or mount >> >> >>> >>> >> >>> | grep /umbc/software)? That=E2=80=99ll tell us which = server >> >> >>> hosts >> >> >>> >>> it. Once >> >> >>> >>> >> >>> we know that, we can confirm whether it=E2=80=99s poss= ible or >> >> >>> >>> appropriate >> >> >>> >>> >> >>> to make it visible from HPCF. >> >> >>> >>> >> >> >> >>> >>> >> >>> Best, >> >> >>> >>> >> >> >> >>> >>> >> >>> Tartela >> >> >>> >>> >> >> >> >>> >>> >> >>> On Tue Oct 21 15:59:49 2025, ZZ99999 wrote: >> >> >>> >>> >> >> >> >>> >>> >> >>>> First Name: Mohammad >> >> >>> >>> >> >>>> Last Name: Ebrahimabadi >> >> >>> >>> >> >>>> Email: e127@umbc.edu <mailto:e127@umbc.edu> <mailto:e= 127@umbc.edu <mailto:e127@umbc.edu>> >> >> >>> >>> >> >>>> Campus ID: NQ23652 >> >> >>> >>> >> >>>> >> >> >>> >>> >> >>>> Request Type: High Performance Cluster >> >> >>> >>> >> >>>> >> >> >>> >>> >> >>>> Dear Team, >> >> >>> >>> >> >>>> >> >> >>> >>> >> >>>> I hope you=E2=80=99re doing well. >> >> >>> >>> >> >>>> I=E2=80=99m a Ph.D. student in Computer Engineering a= nd have a >> >> >>> >>> question regarding running a software on the HPCF. On the >> >> >>> >>> department server, I usually run software using shell scripts >> >> >>> >>> located at /umbc/software/scripts/ which is provided by the IT >> >> >>> >>> group pf the department (Geoff Weiss Team). However, in case = of >> >> >>> >>> HPCF, it seems that this path is not accessible from the HPCF >> >> >>> >>> environment. >> >> >>> >>> >> >>>> >> >> >>> >>> >> >>>> Could you please let me know if there is any specific >> >> >>> >>> configuration or access permission required to reach this >> >> >>> directory >> >> >>> >>> through the HPCF? or direct me to a related person that can >> >> >>> help >> >> >>> >>> me. >> >> >>> >>> >> >>>> >> >> >>> >>> >> >>>> Best regards, >> >> >>> >>> >> >>>> Mohammad >> >> >>> >>> >> >>> >> >> >>> >>> >> >> >> >>> >>> >> > -- >> >> >>> >>> >> >> >> >>> >>> >> > Best, >> >> >>> >>> >> > Max Breitmeyer >> >> >>> >>> >> > DOIT HPC System Administrator >> >> >>> >>> >> >> >> >>> >>> >> >> >> >>> >>> > >> >> >>> >>> > >> >> >>> >>> > >> >> >>> >>> > -- >> >> >>> >>> > Naghmeh Karimi, Ph.D. >> >> >>> >>> > Associate Professor >> >> >>> >>> > Department of Computer Science and Electrical Engineering >> >> >>> >>> > University of Maryland, Baltimore County >> >> >>> >>> > Baltimore, MD 21250 >> >> >>> >>> > Tel: 410-455-3965 E-mail: nkarimi@umbc.edu <mailto:nkarimi@= umbc.edu> >> >> >>> >>> <mailto:nkarimi@umbc.edu <mailto:nkarimi@umbc.edu>> >> >> >>> >>> > Web: http://www.csee.umbc.edu/~nkarimi/ <https://www.google= .com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source=3Dgmail-imap&ust=3D1= 761921999000000&usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7> >> >> <https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&so= urce=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW=  <https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://www.= csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D1761874374000000%26us= g%3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&source=3Dgmail-imap&ust=3D1761921999000000= &usg=3DAOvVaw05OHRJ3ua_KXM9EHoVYfgE>> >> >> >>> >>> >> >> >>> < >> >> https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&sou= rce=3Dgmail-imap&ust=3D1761768891000000&usg=3DAOvVaw0xLE02sw5-ueWRHyHyCwkN = <https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://www.c= see.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D1761768891000000%26usg= %3DAOvVaw0xLE02sw5-ueWRHyHyCwkN&source=3Dgmail-imap&ust=3D1761921999000000&= usg=3DAOvVaw06Mkt_vSjpo8lnqxEqL6dt> >> >> <https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp:/= /www.csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D1761768891000000= %26usg%3DAOvVaw0xLE02sw5-ueWRHyHyCwkN&source=3Dgmail-imap&ust=3D17618743740= 00000&usg=3DAOvVaw3g338pGQqShCgyX8cIt8tV <https://www.google.com/url?q=3Dht= tps://www.google.com/url?q%3Dhttps://www.google.com/url?q%253Dhttp://www.cs= ee.umbc.edu/~nkarimi/%2526source%253Dgmail-imap%2526ust%253D176176889100000= 0%2526usg%253DAOvVaw0xLE02sw5-ueWRHyHyCwkN%26source%3Dgmail-imap%26ust%3D17= 61874374000000%26usg%3DAOvVaw3g338pGQqShCgyX8cIt8tV&source=3Dgmail-imap&ust= =3D1761921999000000&usg=3DAOvVaw3ktt5GEiV5miHD1TY9Dut7>> >> >> > >> >> >> >> >>> >> -- Naghmeh Karimi, Ph.D. >> >> >>> >> Associate Professor >> >> >>> >> Department of Computer Science and Electrical Engineering >> >> >>> >> University of Maryland, Baltimore County >> >> >>> >> Baltimore, MD 21250 >> >> >>> >> Tel: 410-455-3965 E-mail: nkarimi@umbc.edu <mailto:nkarimi@umb= c.edu> >> >> >>> >> Web: http://www.csee.umbc.edu/~nkarimi/ <https://www.google.co= m/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source=3Dgmail-imap&ust=3D1761= 921999000000&usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7> >> >> <https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&so= urce=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW=  <https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://www.= csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D1761874374000000%26us= g%3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&source=3Dgmail-imap&ust=3D1761921999000000= &usg=3DAOvVaw05OHRJ3ua_KXM9EHoVYfgE>> >> >> >> >> >>> > -- >> >> >> >> >>> > Best, >> >> >>> > Max Breitmeyer >> >> >>> > DOIT HPC System Administrator >> >> >> >> >> -- V/R,Maxwell BreitmeyerUMBC HPCF SpecialistUMBC Observatory IT >> >> >> Manager Graduate Student(443) 835-8250 >> >> >> >> -- >> >> >> >> Best, >> >> Max Breitmeyer >> >> DOIT HPC System Administrator >> >> >> >> >> >> >> > >>=20 >> --=20 >> Naghmeh Karimi, Ph.D. >> Associate Professor >> Department of Computer Science and Electrical Engineering >> University of Maryland, Baltimore County >> Baltimore, MD 21250 >> Tel: *410-455-3965* E-mail: *nkarimi@umbc.edu <mailto:nkarimi@umbc.edu> = <nkarimi@umbc.edu <mailto:nkarimi@umbc.edu>>* >> Web: *http://www.csee.umbc.edu/~nkarimi/ <https://www.google.com/url?q= =3Dhttp://www.csee.umbc.edu/~nkarimi/&source=3Dgmail-imap&ust=3D17619219990= 00000&usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7> >> <http://www.csee.umbc.edu/~nkarimi/ <https://www.google.com/url?q=3Dhttp= ://www.csee.umbc.edu/~nkarimi/&source=3Dgmail-imap&ust=3D1761921999000000&u= sg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7>>*  "
3296910,72477694,Correspond,DoIT-Research-Computing,2025-10-24 14:54:30.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,NULL,naghmeh.karimi@umbc.edu,"Hi Max,  We need you to please talk to Skye. She is willing to help and rewrite the scripts but she needs more information from you. She is in this email.  Regards, Naghmeh  On Wed, Oct 22, 2025 at 2:06=E2=80=AFPM Mohammad Ebrahimabadi via RT < UMBCHelp@rt.umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 > > > Last Update From Ticket: > > Hi Max, > > Thanks for your help! Let=E2=80=99s proceed based on your suggestion, hop= efully it > works as expected. > > I just have one quick question: since the IT team in the department can > modify > or add new shell scripts in that directory, will we also have access to > those > updated or newly added scripts from the mounted directory on HPCF? > > Thanks again, > Mohammad > > On Wed Oct 22 14:01:32 2025, OL73413 wrote: > > > Hi Mohammad, > > > We can get the software mounted for you on chip, but it will have to be > > read-only as we won't be able to manage who can make changes to it > > otherwise. > > > As a reminder, the machines on that run on chip are not the same as tho= se > > run by the CSEE department, so we can't guarantee that these will run t= he > > same way or as effectively. > > > If that's all ok with you, please give me some time to get the directory > > mounted. > > > On Wed Oct 22 13:26:05 2025, NQ23652 wrote: > > >> Thanks Tartela for your help to resolve my issue! > > >> Actually it seems all server in CSEE department have access to that > >> path. When I ran the command that you gave me I got the following > >> report: > > >> mohammad@alborz:~$ df -h /umbc/software > >> Filesystem Size Used Avail Use% Mounted on > >> nfs.iss.rs.umbc.edu:/ifs/data/csee/software 2.5T 2.2T 326G 88% > >> /umbc/software > > >> mohammad@alborz:~$ mount | grep /umbc/software > >> nfs.iss.rs.umbc.edu:/ifs/data/csee/software on /umbc/software type nfs > >> > (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namlen=3D255,acregmin= =3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,proto=3Dtcp,time= o=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountvers=3D3,mountpor= t=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.60) > > >> Please let me know if you need other information. > > >> Regards, > > >> Mohammad > > >> On Wed Oct 22 13:15:12 2025, HK41259 wrote: > > >>> Hello, > > >>> Thanks for reaching out. Could you clarify which specific machine > >>> or server you=E2=80=99re referring to when you mention the =E2=80=9Cd= epartmental > >>> server=E2=80=9D? > > >>> The /umbc/software/scripts/ directory isn=E2=80=99t accessible from H= PCF > >>> (e.g., Chip) by default =E2=80=94 those systems have separate storage=  and > >>> don=E2=80=99t automatically mount departmental shares. > > >>> Could you check where /umbc/software is mounted on your > >>> departmental system (e.g., by running df -h /umbc/software or mount > >>> | grep /umbc/software)? That=E2=80=99ll tell us which server hosts it= . Once > >>> we know that, we can confirm whether it=E2=80=99s possible or appropr= iate > >>> to make it visible from HPCF. > > >>> Best, > > >>> Tartela > > >>> On Tue Oct 21 15:59:49 2025, ZZ99999 wrote: > > >>>> First Name:                Mohammad > >>>> Last Name:                 Ebrahimabadi > >>>> Email:                     e127@umbc.edu > >>>> Campus ID:                 NQ23652 > >>>> > >>>> Request Type:              High Performance Cluster > >>>> > >>>> Dear Team, > >>>> > >>>> I hope you=E2=80=99re doing well. > >>>> I=E2=80=99m a Ph.D. student in Computer Engineering and have a quest= ion > regarding running a software on the HPCF. On the department server, I > usually run software using shell scripts located at /umbc/software/script= s/ > which is provided by the IT group pf the department (Geoff Weiss Team). > However, in case of HPCF, it seems that this path is not accessible from > the HPCF environment. > >>>> > >>>> Could you please let me know if there is any specific configuration > or access permission required to reach this directory through the HPCF? or > direct me to a related person that can help me. > >>>> > >>>> Best regards, > >>>> Mohammad > >>> > > > -- > > > Best, > > Max Breitmeyer > > DOIT HPC System Administrator > > >  --=20 Naghmeh Karimi, Ph.D. Associate Professor Department of Computer Science and Electrical Engineering University of Maryland, Baltimore County Baltimore, MD 21250 Tel: *410-455-3965* E-mail: *nkarimi@umbc.edu <nkarimi@umbc.edu>* Web: *http://www.csee.umbc.edu/~nkarimi/ <http://www.csee.umbc.edu/~nkarimi/>* "
3296910,72477694,Correspond,DoIT-Research-Computing,2025-10-24 14:54:30.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,NULL,naghmeh.karimi@umbc.edu,"<div dir=3D""ltr"">Hi Max,<div><br></div><div>We need you to please talk to S= kye.</div><div>She is willing to help and rewrite the=C2=A0scripts but she = needs more information from you.</div><div>She is in this email.</div><div>= <br></div><div>Regards,</div><div>Naghmeh</div></div><br><div class=3D""gmai= l_quote gmail_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Wed= , Oct 22, 2025 at 2:06=E2=80=AFPM Mohammad Ebrahimabadi via RT &lt;<a href= =3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></d= iv><blockquote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;bord= er-left:1px solid rgb(204,204,204);padding-left:1ex"">Ticket &lt;URL: <a hre= f=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D3296910"" rel=3D""noreferre= r"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3296910</= a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Max,<br> <br> Thanks for your help! Let=E2=80=99s proceed based on your suggestion, hopef= ully it<br> works as expected.<br> <br> I just have one quick question: since the IT team in the department can mod= ify<br> or add new shell scripts in that directory, will we also have access to tho= se<br> updated or newly added scripts from the mounted directory on HPCF?<br> <br> Thanks again,<br> Mohammad<br> <br> On Wed Oct 22 14:01:32 2025, OL73413 wrote:<br> <br> &gt; Hi Mohammad,<br> <br> &gt; We can get the software mounted for you on chip, but it will have to b= e<br> &gt; read-only as we won&#39;t be able to manage who can make changes to it= <br> &gt; otherwise.<br> <br> &gt; As a reminder, the machines on that run on chip are not the same as th= ose<br> &gt; run by the CSEE department, so we can&#39;t guarantee that these will = run the<br> &gt; same way or as effectively.<br> <br> &gt; If that&#39;s all ok with you, please give me some time to get the dir= ectory<br> &gt; mounted.<br> <br> &gt; On Wed Oct 22 13:26:05 2025, NQ23652 wrote:<br> <br> &gt;&gt; Thanks Tartela for your help to resolve my issue!<br> <br> &gt;&gt; Actually it seems all server in CSEE department have access to tha= t<br> &gt;&gt; path. When I ran the command that you gave me I got the following<= br> &gt;&gt; report:<br> <br> &gt;&gt; mohammad@alborz:~$ df -h /umbc/software<br> &gt;&gt; Filesystem Size Used Avail Use% Mounted on<br> &gt;&gt; nfs.iss.rs.umbc.edu:/ifs/data/csee/software 2.5T 2.2T 326G 88%<br> &gt;&gt; /umbc/software<br> <br> &gt;&gt; mohammad@alborz:~$ mount | grep /umbc/software<br> &gt;&gt; nfs.iss.rs.umbc.edu:/ifs/data/csee/software on /umbc/software type=  nfs<br> &gt;&gt; (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namlen=3D255,a= cregmin=3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,proto=3Dt= cp,timeo=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountvers=3D3,m= ountport=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.60)<br> <br> &gt;&gt; Please let me know if you need other information.<br> <br> &gt;&gt; Regards,<br> <br> &gt;&gt; Mohammad<br> <br> &gt;&gt; On Wed Oct 22 13:15:12 2025, HK41259 wrote:<br> <br> &gt;&gt;&gt; Hello,<br> <br> &gt;&gt;&gt; Thanks for reaching out. Could you clarify which specific mach= ine<br> &gt;&gt;&gt; or server you=E2=80=99re referring to when you mention the =E2= =80=9Cdepartmental<br> &gt;&gt;&gt; server=E2=80=9D?<br> <br> &gt;&gt;&gt; The /umbc/software/scripts/ directory isn=E2=80=99t accessible=  from HPCF<br> &gt;&gt;&gt; (e.g., Chip) by default =E2=80=94 those systems have separate = storage and<br> &gt;&gt;&gt; don=E2=80=99t automatically mount departmental shares.<br> <br> &gt;&gt;&gt; Could you check where /umbc/software is mounted on your<br> &gt;&gt;&gt; departmental system (e.g., by running df -h /umbc/software or = mount<br> &gt;&gt;&gt; | grep /umbc/software)? That=E2=80=99ll tell us which server h= osts it. Once<br> &gt;&gt;&gt; we know that, we can confirm whether it=E2=80=99s possible or = appropriate<br> &gt;&gt;&gt; to make it visible from HPCF.<br> <br> &gt;&gt;&gt; Best,<br> <br> &gt;&gt;&gt; Tartela<br> <br> &gt;&gt;&gt; On Tue Oct 21 15:59:49 2025, ZZ99999 wrote:<br> <br> &gt;&gt;&gt;&gt; First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 Mohammad<br> &gt;&gt;&gt;&gt; Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=  =C2=A0 =C2=A0Ebrahimabadi<br> &gt;&gt;&gt;&gt; Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 =C2=A0 =C2=A0<a href=3D""mailto:e127@umbc.edu"" target=3D""_blank"">= e127@umbc.edu</a><br> &gt;&gt;&gt;&gt; Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=  =C2=A0 =C2=A0NQ23652<br> &gt;&gt;&gt;&gt; <br> &gt;&gt;&gt;&gt; Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 High Performance Cluster<br> &gt;&gt;&gt;&gt; <br> &gt;&gt;&gt;&gt; Dear Team,<br> &gt;&gt;&gt;&gt; <br> &gt;&gt;&gt;&gt; I hope you=E2=80=99re doing well.<br> &gt;&gt;&gt;&gt; I=E2=80=99m a Ph.D. student in Computer Engineering and ha= ve a question regarding running a software on the HPCF. On the department s= erver, I usually run software using shell scripts located at /umbc/software= /scripts/ which is provided by the IT group pf the department (Geoff Weiss = Team). However, in case of HPCF, it seems that this path is not accessible = from the HPCF environment.<br> &gt;&gt;&gt;&gt; <br> &gt;&gt;&gt;&gt; Could you please let me know if there is any specific conf= iguration or access permission required to reach this directory through the=  HPCF? or direct me to a related person that can help me.<br> &gt;&gt;&gt;&gt; <br> &gt;&gt;&gt;&gt; Best regards,<br> &gt;&gt;&gt;&gt; Mohammad<br> &gt;&gt;&gt; <br> <br> &gt; --<br> <br> &gt; Best,<br> &gt; Max Breitmeyer<br> &gt; DOIT HPC System Administrator<br> <br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><span style=3D""font-size:9.5pt;line-height:115%;= font-family:Arial,sans-serif;color:rgb(136,136,136);background-image:initia= l;background-position:initial;background-repeat:initial"">Naghmeh=C2=A0Karim= i, Ph.D.<br> Associate Professor<br> Department of Computer Science and Electrical Engineering<br> University of Maryland, Baltimore County<br> Baltimore, MD 21250<br> Tel:</span><span style=3D""font-size:11pt;line-height:115%;font-family:Calib= ri,&quot;sans-serif&quot;;color:black"">=C2=A0</span><u><span style=3D""font-= size:11pt;line-height:115%;font-family:Calibri,&quot;sans-serif&quot;;color= :rgb(17,85,204)"">410-455-3965</span></u><span style=3D""font-size:11pt;line-= height:115%;font-family:Calibri,&quot;sans-serif&quot;;color:black""> </span= ><span style=3D""font-size:9.5pt;line-height:115%;font-family:Arial,sans-ser= if;color:rgb(136,136,136);background-image:initial;background-position:init= ial;background-repeat:initial"">E-mail:</span><span style=3D""font-size:11pt;= line-height:115%;font-family:Calibri,&quot;sans-serif&quot;;color:black"">= =C2=A0</span><u><span style=3D""font-size:11pt;line-height:115%;font-family:= Calibri,&quot;sans-serif&quot;;color:rgb(17,85,204)""><a href=3D""mailto:nkar= imi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a></span></u><span style= =3D""font-size:9.5pt;line-height:115%;font-family:Arial,sans-serif;color:rgb= (136,136,136);background-image:initial;background-position:initial;backgrou= nd-repeat:initial""><br> Web:</span><span style=3D""font-size:9.5pt;line-height:115%;font-family:Aria= l,sans-serif;color:black;background-image:initial;background-position:initi= al;background-repeat:initial"">=C2=A0</span><u><span style=3D""font-size:11pt= ;line-height:115%;font-family:Calibri,sans-serif;color:rgb(17,85,204);backg= round-image:initial;background-position:initial;background-repeat:initial"">= <a href=3D""http://www.csee.umbc.edu/~nkarimi/"" target=3D""_blank"">http://www= .csee.umbc.edu/~nkarimi/</a></span></u></div></div> "
3296910,72477826,Correspond,DoIT-Research-Computing,2025-10-24 14:57:06.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Mohammad Ebrahimabadi,e127@umbc.edu,"Thanks Skye,  @ Dear Max, I just cced Skye from the IT of csee department to help us run software on the chip server.  Regards, Mohammad   On Fri, Oct 24, 2025 at 10:51=E2=80=AFAM Skye Jonke via RT <UMBCHelp@rt.umb= c.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 > > > Last Update From Ticket: > > That=E2=80=99s a bit confusing, since hardcoded paths won=E2=80=99t work,=  please have Max > contact me directly so we can talk about what needs to be done to make th= is > work. > --- > Skye Jonke > > she/her > > Specialist, Linux System Administrator & Lab Technical Support > > > On Oct 24, 2025, at 10:46, Mohammad Ebrahimabadi <e127@umbc.edu> wrote: > > > > Hello Skye, > > > > I believe the way that Max told me to run the script was using the same > path that we have in the CSEE department as he mounted that path in HPCF.=  I > mean this path: > >  /umbc/software/scripts/launch_synopsys_hspice.sh > > > > > > Regards, > > Mohammad > > > > > > > > > > On Fri, Oct 24, 2025 at 9:42=E2=80=AFAM naghmeh.karimi@umbc.edu <mailto: > naghmeh.karimi@umbc.edu> via RT <UMBCHelp@rt.umbc.edu <mailto: > UMBCHelp@rt.umbc.edu>> wrote: > >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 < > https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id= %3D3296910&source=3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw099WO1Ar-= MqC9UTifVgSrZ> > > > >> > >> Last Update From Ticket: > >> > >> That's perfect Skye. I highly appreciate it. I ask Mohamamd to respond > >> the ticket with the path. > >> > >> regards, > >> Naghmeh > >> > >> On Fri, Oct 24, 2025 at 9:39=E2=80=AFAM Skye Jonke <ii69854@umbc.edu <= mailto: > ii69854@umbc.edu>> wrote: > >> > >> > Its not possible to use the existing scripts in a situation where the > >> > files aren=E2=80=99t mounted in /umbc/software, but I can write some=  scripts > >> > specific to this setup to this situation with a bit more information. > Where > >> > are the script files located on the HPC? If you aren=E2=80=99t sure,=  you can > get > >> > the full path by running =E2=80=9Crealpath myfile.sh=E2=80=9D (repla= cing myfile.sh > with one > >> > of the cadence scripts, of course) > >> > --- > >> > Skye Jonke > >> > > >> > she/her > >> > > >> > Specialist, Linux System Administrator & Lab Technical Support > >> > > >> > On Oct 23, 2025, at 21:32, Naghmeh Karimi <nkarimi@umbc.edu <mailto: > nkarimi@umbc.edu>> wrote: > >> > > >> > Hi Max, thanks for getting back to us. > >> > > >> > Hi Skye, we need your help here to find the file locations. Could you > >> > please help to resolve this issue? > >> > > >> > Thanks a lot > >> > Naghmeh > >> > > >> > Naghmeh Karimi, Ph.D. > >> > Associate Professor > >> > Department of Computer Science and Electrical Engineering > >> > University of Maryland, Baltimore County > >> > Baltimore, MD 21250 > >> > Tel: *410-455-3965* E-mail: *nkarimi@umbc.edu <mailto: > nkarimi@umbc.edu> <nkarimi@umbc.edu <mailto:nkarimi@umbc.edu>>* > >> > Web: *http://www.csee.umbc.edu/~nkarimi/ < > https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source= =3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7 > > > >> > < > https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source= =3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW > < > https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://www.= csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D1761874374000000%26us= g%3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&source=3Dgmail-imap&ust=3D1761921999000000= &usg=3DAOvVaw05OHRJ3ua_KXM9EHoVYfgE > >>* > >> > > >> > On Thu, Oct 23, 2025, 12:22=E2=80=AFPM Max Breitmeyer via RT < > UMBCHelp@rt.umbc.edu <mailto:UMBCHelp@rt.umbc.edu>> > >> > wrote: > >> > > >> >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 < > https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id= %3D3296910&source=3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw099WO1Ar-= MqC9UTifVgSrZ > > > >> >> < > https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id= %3D3296910&source=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw3ImziGtKZ= -9j9JFW8J65w7 > < > https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.= umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D1= 761874374000000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&source=3Dgmail-imap&us= t=3D1761921999000000&usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g > >> > >> >> > > >> >> > >> >> Last Update From Ticket: > >> >> > >> >> Hi Mohammad, > >> >> > >> >> According to the error message, it's having an issue finding another > file > >> >> in > >> >> the directory. This appears to be an issue due to the mounting > locations > >> >> being > >> >> different. I would point you to the previous email where I said that > any > >> >> software with hard-coded locations would not work, and that software > >> >> compatibility is not guaranteed. I would recommend working with Skye > on > >> >> porting > >> >> what is in the directory to your local research directories on chip. > >> >> > >> >> On Thu Oct 23 12:09:34 2025, NQ23652 wrote: > >> >> > >> >> > Yes you are right, now I could see all shell files. However still > I have > >> >> > problem when I run the sh file. > >> >> > >> >> > [e127@chip-login2 scripts]$ sh launch_synopsys_hspice.sh > >> >> > launch_synopsys_hspice.sh: line 3: > >> >> > /umbc/software/scripts/env_synopsys_hspice.sh: No such file or > directory > >> >> > [e127@chip-login2 scripts]$ > >> >> > >> >> > Would you please take a look to it. > >> >> > >> >> > Regards, > >> >> > >> >> > Mohammad > >> >> > >> >> > On Thu Oct 23 11:59:17 2025, OL73413 wrote: > >> >> > >> >> >> Hi Mohammad, > >> >> > >> >> >> A good point. Chip uses a service called ""autofs"" to mount and > unmount > >> >> >> directories that aren't being actively used. This helps maintain > the > >> >> >> login nodes integrity. If you see in my screenshot below, the > directory > >> >> >> appears to be empty, but when I cd to the csee directory where > your > >> >> >> software is mounted, it is all available. Backing out of the > directory > >> >> >> and ls'ing the directory also shows that the directory is now > mounted > >> >> >> and available. > >> >> > >> >> >> On Thu, Oct 23, 2025 at 11:41 AM Mohammad Ebrahimabadi via RT > >> >> >> <UMBCHelp@rt.umbc.edu <mailto:UMBCHelp@rt.umbc.edu>> wrote: > >> >> > >> >> >>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D32969= 10 > < > https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id= %3D3296910&source=3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw099WO1Ar-= MqC9UTifVgSrZ > > > >> >> < > https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id= %3D3296910&source=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw3ImziGtKZ= -9j9JFW8J65w7 > < > https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.= umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D1= 761874374000000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&source=3Dgmail-imap&us= t=3D1761921999000000&usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g > >> > >> >> > > >> >> > >> >> >>> Comment From Ticket: > >> >> >>> Thank you Max! > >> >> > >> >> >>> Actually we have still problem for accessing that directory. I > >> >> >>> uploaded two > >> >> >>> screenshots that shows what we can see in the department servers > >> >> >>> and what we > >> >> >>> can see in CHIP. > >> >> > >> >> >>> For example by running this shell file we can launch HSPICE > >> >> >>> software in the > >> >> >>> department server: > >> >> >>> /umbc/software/scripts/launch_synopsys_hspice.sh > >> >> >>> But it is not still accessible on CHIP. > >> >> > >> >> >>> Regards, > >> >> > >> >> >>> Mohammad > >> >> > >> >> >>> On Thu Oct 23 10:54:59 2025, OL73413 wrote: > >> >> > >> >> >>> > Hi Nahmeh, > >> >> > >> >> >>> > Apologies =E2=80=94 I meant to update you yesterday after com= pleting > >> >> >>> this, but I > >> >> >>> > was pulled into a long meeting. The software is now available > on > >> >> >>> chip at: > >> >> > >> >> >>> > /umbc/software/csee > >> >> > >> >> >>> > Please note that if the software contains any hardcoded file > >> >> >>> paths, it may > >> >> >>> > not function correctly. Unfortunately, we don=E2=80=99t have = control > over > >> >> >>> those > >> >> >>> > cases. > >> >> > >> >> >>> > Also, just to clarify, Skye and I work in separate department= s. > >> >> >>> Similar to > >> >> >>> > how I don=E2=80=99t have administrative access to the CSEE de= partment > >> >> >>> servers > >> >> >>> > (which include the software directory), Skye is not an > >> >> >>> administrator on > >> >> >>> > chip. > >> >> > >> >> >>> > Please let me know if you encounter any issues accessing the > >> >> >>> software on > >> >> >>> > the server. If there are problems running the software, I=E2= =80=99ll > >> >> >>> coordinate > >> >> >>> > with Skye to troubleshoot, but since we don=E2=80=99t maintai= n this > >> >> >>> software, we > >> >> >>> > can=E2=80=99t guarantee full compatibility on our systems. > >> >> > >> >> >>> > On Thu Oct 23 10:36:41 2025, naghmeh.karimi@umbc.edu <mailto: > naghmeh.karimi@umbc.edu> wrote: > >> >> > >> >> >>> >> Hi Skye and Max, Could you please help on mounting this today > so > >> >> >>> we can > >> >> >>> >> use them as we have a deadline? Thanks,Naghmeh Karimi On Wed, > >> >> >>> Oct 22, > >> >> >>> >> 2025 at 4:22 PM Skye Jonke via RT <UMBCHelp@rt.umbc.edu > <mailto:UMBCHelp@rt.umbc.edu>> wrote: > >> >> > >> >> >>> >>> Ticket <URL: > https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 < > https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id= %3D3296910&source=3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw099WO1Ar-= MqC9UTifVgSrZ > > > >> >> < > https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id= %3D3296910&source=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw3ImziGtKZ= -9j9JFW8J65w7 > < > https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.= umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D1= 761874374000000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&source=3Dgmail-imap&us= t=3D1761921999000000&usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g > >> > >> >> >>> > > >> >> > >> >> >>> >>> Last Update From Ticket: > >> >> > >> >> >>> >>> Mohammad, > >> >> > >> >> >>> >>> So long as you continue to access them through > >> >> >>> >>> /umbc/software/scripts, they should remain up to date and y= ou > >> >> >>> will > >> >> >>> >>> see any new scripts. If you copy them to a local directory = on > >> >> >>> the > >> >> >>> >>> HPC cluster, they will not be updated. > >> >> > >> >> >>> >>> --- > >> >> >>> >>> Skye Jonke > >> >> > >> >> >>> >>> she/her > >> >> > >> >> >>> >>> Specialist, Linux System Administrator & Lab Technical > Support > >> >> > >> >> >>> >>> > On Oct 22, 2025, at 16:14, Naghmeh Karimi < > nkarimi@umbc.edu <mailto:nkarimi@umbc.edu>> > >> >> >>> >>> wrote: > >> >> >>> >>> > > >> >> >>> >>> > Hi Max, Roy, Skye, > >> >> >>> >>> > > >> >> >>> >>> > Thank you all for your help. > >> >> >>> >>> > > >> >> >>> >>> > I added Skye here as she is our IT exert on adding the > >> >> >>> capability > >> >> >>> >>> of running synopsis tools (Hspice) in servers. > >> >> >>> >>> > Skye we want to use the HPC cluster to run hspice an other > >> >> >>> >>> synopsys tools. Could you please help on this? > >> >> >>> >>> > > >> >> >>> >>> > Thanks, > >> >> >>> >>> > Naghmeh > >> >> >>> >>> > > >> >> >>> >>> > On Wed, Oct 22, 2025 at 2:06 PM Mohammad Ebrahimabadi via > RT > >> >> >>> >>> <UMBCHelp@rt.umbc.edu <mailto:UMBCHelp@rt.umbc.edu> <mailto: > UMBCHelp@rt.umbc.edu <mailto:UMBCHelp@rt.umbc.edu>>> wrote: > >> >> >>> >>> >> Ticket <URL: > >> >> >>> https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 < > https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id= %3D3296910&source=3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw099WO1Ar-= MqC9UTifVgSrZ > > > >> >> < > https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id= %3D3296910&source=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw3ImziGtKZ= -9j9JFW8J65w7 > < > https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.= umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D1= 761874374000000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&source=3Dgmail-imap&us= t=3D1761921999000000&usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g > >> > >> >> >>> >>> > >> >> >>> < > >> >> > https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id= %3D3296910&source=3Dgmail-imap&ust=3D1761768891000000&usg=3DAOvVaw2jxB3nArH= BD7-ij9ZN8ZsW > < > https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.= umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D1= 761768891000000%26usg%3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW&source=3Dgmail-imap&us= t=3D1761921999000000&usg=3DAOvVaw04hQC2SRO0y4BoVR0LFNp_ > > > >> >> < > https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.= umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D1= 761768891000000%26usg%3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW&source=3Dgmail-imap&us= t=3D1761874374000000&usg=3DAOvVaw0fLylAsBAoQ6LgqY79I9-d > < > https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://www= .google.com/url?q%253Dhttps://rt.umbc.edu/Ticket/Display.html?id%25253D3296= 910%2526source%253Dgmail-imap%2526ust%253D1761768891000000%2526usg%253DAOvV= aw2jxB3nArHBD7-ij9ZN8ZsW%26source%3Dgmail-imap%26ust%3D1761874374000000%26u= sg%3DAOvVaw0fLylAsBAoQ6LgqY79I9-d&source=3Dgmail-imap&ust=3D176192199900000= 0&usg=3DAOvVaw3pQOXM-6FD6kt0kA6ujsVN > >> > >> >> > > >> >> >>> >>> > > >> >> >>> >>> >> > >> >> >>> >>> >> Last Update From Ticket: > >> >> >>> >>> >> > >> >> >>> >>> >> Hi Max, > >> >> >>> >>> >> > >> >> >>> >>> >> Thanks for your help! Let=E2=80=99s proceed based on your > >> >> >>> suggestion, > >> >> >>> >>> hopefully it > >> >> >>> >>> >> works as expected. > >> >> >>> >>> >> > >> >> >>> >>> >> I just have one quick question: since the IT team in the > >> >> >>> >>> department can modify > >> >> >>> >>> >> or add new shell scripts in that directory, will we also > >> >> >>> have > >> >> >>> >>> access to those > >> >> >>> >>> >> updated or newly added scripts from the mounted directory > on > >> >> >>> >>> HPCF? > >> >> >>> >>> >> > >> >> >>> >>> >> Thanks again, > >> >> >>> >>> >> Mohammad > >> >> >>> >>> >> > >> >> >>> >>> >> On Wed Oct 22 14:01:32 2025, OL73413 wrote: > >> >> >>> >>> >> > >> >> >>> >>> >> > Hi Mohammad, > >> >> >>> >>> >> > >> >> >>> >>> >> > We can get the software mounted for you on chip, but it > >> >> >>> will > >> >> >>> >>> have to be > >> >> >>> >>> >> > read-only as we won't be able to manage who can make > >> >> >>> changes > >> >> >>> >>> to it > >> >> >>> >>> >> > otherwise. > >> >> >>> >>> >> > >> >> >>> >>> >> > As a reminder, the machines on that run on chip are not > >> >> >>> the > >> >> >>> >>> same as those > >> >> >>> >>> >> > run by the CSEE department, so we can't guarantee that > >> >> >>> these > >> >> >>> >>> will run the > >> >> >>> >>> >> > same way or as effectively. > >> >> >>> >>> >> > >> >> >>> >>> >> > If that's all ok with you, please give me some time to > get > >> >> >>> the > >> >> >>> >>> directory > >> >> >>> >>> >> > mounted. > >> >> >>> >>> >> > >> >> >>> >>> >> > On Wed Oct 22 13:26:05 2025, NQ23652 wrote: > >> >> >>> >>> >> > >> >> >>> >>> >> >> Thanks Tartela for your help to resolve my issue! > >> >> >>> >>> >> > >> >> >>> >>> >> >> Actually it seems all server in CSEE department have > >> >> >>> access > >> >> >>> >>> to that > >> >> >>> >>> >> >> path. When I ran the command that you gave me I got t= he > >> >> >>> >>> following > >> >> >>> >>> >> >> report: > >> >> >>> >>> >> > >> >> >>> >>> >> >> mohammad@alborz:~$ df -h /umbc/software > >> >> >>> >>> >> >> Filesystem Size Used Avail Use% Mounted on > >> >> >>> >>> >> >> nfs.iss.rs.umbc.edu <http://nfs.iss.rs.umbc.edu/>:/if= s/data/csee/software > 2.5T 2.2T > >> >> >>> 326G > >> >> >>> >>> 88% > >> >> >>> >>> >> >> /umbc/software > >> >> >>> >>> >> > >> >> >>> >>> >> >> mohammad@alborz:~$ mount | grep /umbc/software > >> >> >>> >>> >> >> nfs.iss.rs.umbc.edu <http://nfs.iss.rs.umbc.edu/>:/if= s/data/csee/software > on > >> >> >>> /umbc/software > >> >> >>> >>> type nfs > >> >> >>> >>> >> >> > >> >> >>> >>> > >> >> >>> > >> >> > (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namlen=3D255,acregmin= =3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,proto=3Dtcp,time= o=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountvers=3D3,mountpor= t=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.60) > >> >> >>> >>> >> > >> >> >>> >>> >> >> Please let me know if you need other information. > >> >> >>> >>> >> > >> >> >>> >>> >> >> Regards, > >> >> >>> >>> >> > >> >> >>> >>> >> >> Mohammad > >> >> >>> >>> >> > >> >> >>> >>> >> >> On Wed Oct 22 13:15:12 2025, HK41259 wrote: > >> >> >>> >>> >> > >> >> >>> >>> >> >>> Hello, > >> >> >>> >>> >> > >> >> >>> >>> >> >>> Thanks for reaching out. Could you clarify which > >> >> >>> specific > >> >> >>> >>> machine > >> >> >>> >>> >> >>> or server you=E2=80=99re referring to when you menti= on the > >> >> >>> >>> =E2=80=9Cdepartmental > >> >> >>> >>> >> >>> server=E2=80=9D? > >> >> >>> >>> >> > >> >> >>> >>> >> >>> The /umbc/software/scripts/ directory isn=E2=80=99t = accessible > >> >> >>> from > >> >> >>> >>> HPCF > >> >> >>> >>> >> >>> (e.g., Chip) by default =E2=80=94 those systems have=  separate > >> >> >>> >>> storage and > >> >> >>> >>> >> >>> don=E2=80=99t automatically mount departmental share= s. > >> >> >>> >>> >> > >> >> >>> >>> >> >>> Could you check where /umbc/software is mounted on > your > >> >> >>> >>> >> >>> departmental system (e.g., by running df -h > >> >> >>> /umbc/software > >> >> >>> >>> or mount > >> >> >>> >>> >> >>> | grep /umbc/software)? That=E2=80=99ll tell us whic= h server > >> >> >>> hosts > >> >> >>> >>> it. Once > >> >> >>> >>> >> >>> we know that, we can confirm whether it=E2=80=99s po= ssible or > >> >> >>> >>> appropriate > >> >> >>> >>> >> >>> to make it visible from HPCF. > >> >> >>> >>> >> > >> >> >>> >>> >> >>> Best, > >> >> >>> >>> >> > >> >> >>> >>> >> >>> Tartela > >> >> >>> >>> >> > >> >> >>> >>> >> >>> On Tue Oct 21 15:59:49 2025, ZZ99999 wrote: > >> >> >>> >>> >> > >> >> >>> >>> >> >>>> First Name: Mohammad > >> >> >>> >>> >> >>>> Last Name: Ebrahimabadi > >> >> >>> >>> >> >>>> Email: e127@umbc.edu <mailto:e127@umbc.edu> <mailto: > e127@umbc.edu <mailto:e127@umbc.edu>> > >> >> >>> >>> >> >>>> Campus ID: NQ23652 > >> >> >>> >>> >> >>>> > >> >> >>> >>> >> >>>> Request Type: High Performance Cluster > >> >> >>> >>> >> >>>> > >> >> >>> >>> >> >>>> Dear Team, > >> >> >>> >>> >> >>>> > >> >> >>> >>> >> >>>> I hope you=E2=80=99re doing well. > >> >> >>> >>> >> >>>> I=E2=80=99m a Ph.D. student in Computer Engineering=  and have > a > >> >> >>> >>> question regarding running a software on the HPCF. On the > >> >> >>> >>> department server, I usually run software using shell scrip= ts > >> >> >>> >>> located at /umbc/software/scripts/ which is provided by the > IT > >> >> >>> >>> group pf the department (Geoff Weiss Team). However, in case > of > >> >> >>> >>> HPCF, it seems that this path is not accessible from the HP= CF > >> >> >>> >>> environment. > >> >> >>> >>> >> >>>> > >> >> >>> >>> >> >>>> Could you please let me know if there is any specif= ic > >> >> >>> >>> configuration or access permission required to reach this > >> >> >>> directory > >> >> >>> >>> through the HPCF? or direct me to a related person that can > >> >> >>> help > >> >> >>> >>> me. > >> >> >>> >>> >> >>>> > >> >> >>> >>> >> >>>> Best regards, > >> >> >>> >>> >> >>>> Mohammad > >> >> >>> >>> >> >>> > >> >> >>> >>> >> > >> >> >>> >>> >> > -- > >> >> >>> >>> >> > >> >> >>> >>> >> > Best, > >> >> >>> >>> >> > Max Breitmeyer > >> >> >>> >>> >> > DOIT HPC System Administrator > >> >> >>> >>> >> > >> >> >>> >>> >> > >> >> >>> >>> > > >> >> >>> >>> > > >> >> >>> >>> > > >> >> >>> >>> > -- > >> >> >>> >>> > Naghmeh Karimi, Ph.D. > >> >> >>> >>> > Associate Professor > >> >> >>> >>> > Department of Computer Science and Electrical Engineering > >> >> >>> >>> > University of Maryland, Baltimore County > >> >> >>> >>> > Baltimore, MD 21250 > >> >> >>> >>> > Tel: 410-455-3965 E-mail: nkarimi@umbc.edu <mailto: > nkarimi@umbc.edu> > >> >> >>> >>> <mailto:nkarimi@umbc.edu <mailto:nkarimi@umbc.edu>> > >> >> >>> >>> > Web: http://www.csee.umbc.edu/~nkarimi/ < > https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source= =3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7 > > > >> >> < > https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source= =3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW > < > https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://www.= csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D1761874374000000%26us= g%3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&source=3Dgmail-imap&ust=3D1761921999000000= &usg=3DAOvVaw05OHRJ3ua_KXM9EHoVYfgE > >> > >> >> >>> >>> > >> >> >>> < > >> >> > https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source= =3Dgmail-imap&ust=3D1761768891000000&usg=3DAOvVaw0xLE02sw5-ueWRHyHyCwkN > < > https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://www.= csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D1761768891000000%26us= g%3DAOvVaw0xLE02sw5-ueWRHyHyCwkN&source=3Dgmail-imap&ust=3D1761921999000000= &usg=3DAOvVaw06Mkt_vSjpo8lnqxEqL6dt > > > >> >> < > https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://www.= csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D1761768891000000%26us= g%3DAOvVaw0xLE02sw5-ueWRHyHyCwkN&source=3Dgmail-imap&ust=3D1761874374000000= &usg=3DAOvVaw3g338pGQqShCgyX8cIt8tV > < > https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://www= .google.com/url?q%253Dhttp://www.csee.umbc.edu/~nkarimi/%2526source%253Dgma= il-imap%2526ust%253D1761768891000000%2526usg%253DAOvVaw0xLE02sw5-ueWRHyHyCw= kN%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw3g338pGQqSh= CgyX8cIt8tV&source=3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw3ktt5GEi= V5miHD1TY9Dut7 > >> > >> >> > > >> >> > >> >> >>> >> -- Naghmeh Karimi, Ph.D. > >> >> >>> >> Associate Professor > >> >> >>> >> Department of Computer Science and Electrical Engineering > >> >> >>> >> University of Maryland, Baltimore County > >> >> >>> >> Baltimore, MD 21250 > >> >> >>> >> Tel: 410-455-3965 E-mail: nkarimi@umbc.edu <mailto: > nkarimi@umbc.edu> > >> >> >>> >> Web: http://www.csee.umbc.edu/~nkarimi/ < > https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source= =3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7 > > > >> >> < > https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source= =3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW > < > https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://www.= csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D1761874374000000%26us= g%3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&source=3Dgmail-imap&ust=3D1761921999000000= &usg=3DAOvVaw05OHRJ3ua_KXM9EHoVYfgE > >> > >> >> > >> >> >>> > -- > >> >> > >> >> >>> > Best, > >> >> >>> > Max Breitmeyer > >> >> >>> > DOIT HPC System Administrator > >> >> > >> >> >> -- V/R,Maxwell BreitmeyerUMBC HPCF SpecialistUMBC Observatory IT > >> >> >> Manager Graduate Student(443) 835-8250 > >> >> > >> >> -- > >> >> > >> >> Best, > >> >> Max Breitmeyer > >> >> DOIT HPC System Administrator > >> >> > >> >> > >> >> > >> > > >> > >> -- > >> Naghmeh Karimi, Ph.D. > >> Associate Professor > >> Department of Computer Science and Electrical Engineering > >> University of Maryland, Baltimore County > >> Baltimore, MD 21250 > >> Tel: *410-455-3965* E-mail: *nkarimi@umbc.edu <mailto:nkarimi@umbc.edu> > <nkarimi@umbc.edu <mailto:nkarimi@umbc.edu>>* > >> Web: *http://www.csee.umbc.edu/~nkarimi/ < > https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source= =3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7 > > > >> <http://www.csee.umbc.edu/~nkarimi/ < > https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source= =3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7 > >>* > > "
3296910,72477826,Correspond,DoIT-Research-Computing,2025-10-24 14:57:06.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Mohammad Ebrahimabadi,e127@umbc.edu,"<div dir=3D""ltr"">Thanks Skye,<div><br></div><div>@ Dear Max,<br></div><div>= I just cced Skye from the IT of csee department to help us run software on = the chip server.</div><div><br></div><div>Regards,</div><div>Mohammad</div>= <div><br></div></div><br><div class=3D""gmail_quote gmail_quote_container""><= div dir=3D""ltr"" class=3D""gmail_attr"">On Fri, Oct 24, 2025 at 10:51=E2=80=AF= AM Skye Jonke via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@r= t.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" style= =3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding= -left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.ht= ml?id=3D3296910"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/T= icket/Display.html?id=3D3296910</a> &gt;<br> <br> Last Update From Ticket:<br> <br> That=E2=80=99s a bit confusing, since hardcoded paths won=E2=80=99t work, p= lease have Max contact me directly so we can talk about what needs to be do= ne to make this work. <br> ---<br> Skye Jonke<br> <br> she/her<br> <br> Specialist, Linux System Administrator &amp; Lab Technical Support<br> <br> &gt; On Oct 24, 2025, at 10:46, Mohammad Ebrahimabadi &lt;<a href=3D""mailto= :e127@umbc.edu"" target=3D""_blank"">e127@umbc.edu</a>&gt; wrote:<br> &gt; <br> &gt; Hello Skye,<br> &gt; <br> &gt; I believe the way that Max told me to run the script was using the sam= e path that we have in the CSEE department as he mounted that path in HPCF.=  I mean this path:<br> &gt;=C2=A0 /umbc/software/scripts/launch_synopsys_hspice.sh<br> &gt; <br> &gt; <br> &gt; Regards,<br> &gt; Mohammad<br> &gt; <br> &gt; <br> &gt; <br> &gt; <br> &gt; On Fri, Oct 24, 2025 at 9:42=E2=80=AFAM <a href=3D""mailto:naghmeh.kari= mi@umbc.edu"" target=3D""_blank"">naghmeh.karimi@umbc.edu</a> &lt;mailto:<a hr= ef=3D""mailto:naghmeh.karimi@umbc.edu"" target=3D""_blank"">naghmeh.karimi@umbc= .edu</a>&gt; via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_= blank"">UMBCHelp@rt.umbc.edu</a> &lt;mailto:<a href=3D""mailto:UMBCHelp@rt.um= bc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt;&gt; wrote:<br> &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html= ?id=3D3296910"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tic= ket/Display.html?id=3D3296910</a> &lt;<a href=3D""https://www.google.com/url= ?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgmai= l-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw099WO1Ar-MqC9UTifVgSrZ"" r= el=3D""noreferrer"" target=3D""_blank"">https://www.google.com/url?q=3Dhttps://= rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;us= t=3D1761921999000000&amp;usg=3DAOvVaw099WO1Ar-MqC9UTifVgSrZ</a>&gt; &gt;<br> &gt;&gt; <br> &gt;&gt; Last Update From Ticket:<br> &gt;&gt; <br> &gt;&gt; That&#39;s perfect Skye. I highly appreciate it. I ask Mohamamd to=  respond<br> &gt;&gt; the ticket with the path.<br> &gt;&gt; <br> &gt;&gt; regards,<br> &gt;&gt; Naghmeh<br> &gt;&gt; <br> &gt;&gt; On Fri, Oct 24, 2025 at 9:39=E2=80=AFAM Skye Jonke &lt;<a href=3D""= mailto:ii69854@umbc.edu"" target=3D""_blank"">ii69854@umbc.edu</a> &lt;mailto:= <a href=3D""mailto:ii69854@umbc.edu"" target=3D""_blank"">ii69854@umbc.edu</a>&= gt;&gt; wrote:<br> &gt;&gt; <br> &gt;&gt; &gt; Its not possible to use the existing scripts in a situation w= here the<br> &gt;&gt; &gt; files aren=E2=80=99t mounted in /umbc/software, but I can wri= te some scripts<br> &gt;&gt; &gt; specific to this setup to this situation with a bit more info= rmation. Where<br> &gt;&gt; &gt; are the script files located on the HPC? If you aren=E2=80=99= t sure, you can get<br> &gt;&gt; &gt; the full path by running =E2=80=9Crealpath myfile.sh=E2=80=9D=  (replacing myfile.sh with one<br> &gt;&gt; &gt; of the cadence scripts, of course)<br> &gt;&gt; &gt; ---<br> &gt;&gt; &gt; Skye Jonke<br> &gt;&gt; &gt;<br> &gt;&gt; &gt; she/her<br> &gt;&gt; &gt;<br> &gt;&gt; &gt; Specialist, Linux System Administrator &amp; Lab Technical Su= pport<br> &gt;&gt; &gt;<br> &gt;&gt; &gt; On Oct 23, 2025, at 21:32, Naghmeh Karimi &lt;<a href=3D""mail= to:nkarimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a> &lt;mailto:<a h= ref=3D""mailto:nkarimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a>&gt;&= gt; wrote:<br> &gt;&gt; &gt;<br> &gt;&gt; &gt; Hi Max, thanks for getting back to us.<br> &gt;&gt; &gt;<br> &gt;&gt; &gt; Hi Skye, we need your help here to find the file locations. C= ould you<br> &gt;&gt; &gt; please help to resolve this issue?<br> &gt;&gt; &gt;<br> &gt;&gt; &gt; Thanks a lot<br> &gt;&gt; &gt; Naghmeh<br> &gt;&gt; &gt;<br> &gt;&gt; &gt; Naghmeh Karimi, Ph.D.<br> &gt;&gt; &gt; Associate Professor<br> &gt;&gt; &gt; Department of Computer Science and Electrical Engineering<br> &gt;&gt; &gt; University of Maryland, Baltimore County<br> &gt;&gt; &gt; Baltimore, MD 21250<br> &gt;&gt; &gt; Tel: *410-455-3965* E-mail: *<a href=3D""mailto:nkarimi@umbc.e= du"" target=3D""_blank"">nkarimi@umbc.edu</a> &lt;mailto:<a href=3D""mailto:nka= rimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a>&gt; &lt;<a href=3D""ma= ilto:nkarimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a> &lt;mailto:<a=  href=3D""mailto:nkarimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a>&gt= ;&gt;*<br> &gt;&gt; &gt; Web: *<a href=3D""http://www.csee.umbc.edu/~nkarimi/"" rel=3D""n= oreferrer"" target=3D""_blank"">http://www.csee.umbc.edu/~nkarimi/</a> &lt;<a = href=3D""https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&a= mp;source=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw35D5VfZwU= _gTyQhCtkbUZ7"" rel=3D""noreferrer"" target=3D""_blank"">https://www.google.com/= url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust= =3D1761921999000000&amp;usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7</a>&gt;<br> &gt;&gt; &gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttp://www.csee= .umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761874374000000&amp;= usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW"" rel=3D""noreferrer"" target=3D""_blank"">ht= tps://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source= =3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw23Fk_KFBnlrXC_uhIb= UFIW</a> &lt;<a href=3D""https://www.google.com/url?q=3Dhttps://www.google.c= om/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3= D1761874374000000%26usg%3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&amp;source=3Dgmail-i= map&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw05OHRJ3ua_KXM9EHoVYfgE"" rel= =3D""noreferrer"" target=3D""_blank"">https://www.google.com/url?q=3Dhttps://ww= w.google.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmail-im= ap%26ust%3D1761874374000000%26usg%3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&amp;source= =3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw05OHRJ3ua_KXM9EHoV= YfgE</a>&gt;&gt;*<br> &gt;&gt; &gt;<br> &gt;&gt; &gt; On Thu, Oct 23, 2025, 12:22=E2=80=AFPM Max Breitmeyer via RT = &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.u= mbc.edu</a> &lt;mailto:<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_b= lank"">UMBCHelp@rt.umbc.edu</a>&gt;&gt;<br> &gt;&gt; &gt; wrote:<br> &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Dis= play.html?id=3D3296910"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umb= c.edu/Ticket/Display.html?id=3D3296910</a> &lt;<a href=3D""https://www.googl= e.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;sour= ce=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw099WO1Ar-MqC9UTi= fVgSrZ"" rel=3D""noreferrer"" target=3D""_blank"">https://www.google.com/url?q= =3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-= imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw099WO1Ar-MqC9UTifVgSrZ</a>&= gt;<br> &gt;&gt; &gt;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttps://rt.= umbc.edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust= =3D1761874374000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7"" rel=3D""noreferr= er"" target=3D""_blank"">https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ti= cket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761874374= 000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7</a> &lt;<a href=3D""https://ww= w.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.edu/Tic= ket/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D1761874374000= 000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&amp;source=3Dgmail-imap&amp;ust=3D= 1761921999000000&amp;usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g"" rel=3D""noreferrer""=  target=3D""_blank"">https://www.google.com/url?q=3Dhttps://www.google.com/ur= l?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgma= il-imap%26ust%3D1761874374000000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&amp;s= ource=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw3WPs04yO5gQkO= FpjU6my-g</a>&gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; Last Update From Ticket:<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; Hi Mohammad,<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; According to the error message, it&#39;s having an issue = finding another file<br> &gt;&gt; &gt;&gt; in<br> &gt;&gt; &gt;&gt; the directory. This appears to be an issue due to the mou= nting locations<br> &gt;&gt; &gt;&gt; being<br> &gt;&gt; &gt;&gt; different. I would point you to the previous email where = I said that any<br> &gt;&gt; &gt;&gt; software with hard-coded locations would not work, and th= at software<br> &gt;&gt; &gt;&gt; compatibility is not guaranteed. I would recommend workin= g with Skye on<br> &gt;&gt; &gt;&gt; porting<br> &gt;&gt; &gt;&gt; what is in the directory to your local research directori= es on chip.<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; On Thu Oct 23 12:09:34 2025, NQ23652 wrote:<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt; Yes you are right, now I could see all shell files. = However still I have<br> &gt;&gt; &gt;&gt; &gt; problem when I run the sh file.<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt; [e127@chip-login2 scripts]$ sh launch_synopsys_hspic= e.sh<br> &gt;&gt; &gt;&gt; &gt; launch_synopsys_hspice.sh: line 3:<br> &gt;&gt; &gt;&gt; &gt; /umbc/software/scripts/env_synopsys_hspice.sh: No su= ch file or directory<br> &gt;&gt; &gt;&gt; &gt; [e127@chip-login2 scripts]$<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt; Would you please take a look to it.<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt; Regards,<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt; Mohammad<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt; On Thu Oct 23 11:59:17 2025, OL73413 wrote:<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt; Hi Mohammad,<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt; A good point. Chip uses a service called &quot;a= utofs&quot; to mount and unmount<br> &gt;&gt; &gt;&gt; &gt;&gt; directories that aren&#39;t being actively used.=  This helps maintain the<br> &gt;&gt; &gt;&gt; &gt;&gt; login nodes integrity. If you see in my screensh= ot below, the directory<br> &gt;&gt; &gt;&gt; &gt;&gt; appears to be empty, but when I cd to the csee d= irectory where your<br> &gt;&gt; &gt;&gt; &gt;&gt; software is mounted, it is all available. Backin= g out of the directory<br> &gt;&gt; &gt;&gt; &gt;&gt; and ls&#39;ing the directory also shows that the=  directory is now mounted<br> &gt;&gt; &gt;&gt; &gt;&gt; and available.<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt; On Thu, Oct 23, 2025 at 11:41 AM Mohammad Ebrahi= mabadi via RT<br> &gt;&gt; &gt;&gt; &gt;&gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" targ= et=3D""_blank"">UMBCHelp@rt.umbc.edu</a> &lt;mailto:<a href=3D""mailto:UMBCHel= p@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt;&gt; wrote:<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.e= du/Ticket/Display.html?id=3D3296910"" rel=3D""noreferrer"" target=3D""_blank"">h= ttps://rt.umbc.edu/Ticket/Display.html?id=3D3296910</a> &lt;<a href=3D""http= s://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D329= 6910&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw099= WO1Ar-MqC9UTifVgSrZ"" rel=3D""noreferrer"" target=3D""_blank"">https://www.googl= e.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;sour= ce=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw099WO1Ar-MqC9UTi= fVgSrZ</a>&gt;<br> &gt;&gt; &gt;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttps://rt.= umbc.edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust= =3D1761874374000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7"" rel=3D""noreferr= er"" target=3D""_blank"">https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ti= cket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761874374= 000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7</a> &lt;<a href=3D""https://ww= w.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.edu/Tic= ket/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D1761874374000= 000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&amp;source=3Dgmail-imap&amp;ust=3D= 1761921999000000&amp;usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g"" rel=3D""noreferrer""=  target=3D""_blank"">https://www.google.com/url?q=3Dhttps://www.google.com/ur= l?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgma= il-imap%26ust%3D1761874374000000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&amp;s= ource=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw3WPs04yO5gQkO= FpjU6my-g</a>&gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Comment From Ticket:<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Thank you Max!<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Actually we have still problem for accessing=  that directory. I<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; uploaded two<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; screenshots that shows what we can see in th= e department servers<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; and what we<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; can see in CHIP.<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; For example by running this shell file we ca= n launch HSPICE<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; software in the<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; department server:<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; /umbc/software/scripts/launch_synopsys_hspic= e.sh<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; But it is not still accessible on CHIP.<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Regards,<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Mohammad<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; On Thu Oct 23 10:54:59 2025, OL73413 wrote:<= br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Hi Nahmeh,<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Apologies =E2=80=94 I meant to update y= ou yesterday after completing<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; this, but I<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; was pulled into a long meeting. The sof= tware is now available on<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; chip at:<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; /umbc/software/csee<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Please note that if the software contai= ns any hardcoded file<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; paths, it may<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; not function correctly. Unfortunately, = we don=E2=80=99t have control over<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; those<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; cases.<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Also, just to clarify, Skye and I work = in separate departments.<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Similar to<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; how I don=E2=80=99t have administrative=  access to the CSEE department<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; servers<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; (which include the software directory),=  Skye is not an<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; administrator on<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; chip.<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Please let me know if you encounter any=  issues accessing the<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; software on<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; the server. If there are problems runni= ng the software, I=E2=80=99ll<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; coordinate<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; with Skye to troubleshoot, but since we=  don=E2=80=99t maintain this<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; software, we<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; can=E2=80=99t guarantee full compatibil= ity on our systems.<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; On Thu Oct 23 10:36:41 2025, <a href=3D= ""mailto:naghmeh.karimi@umbc.edu"" target=3D""_blank"">naghmeh.karimi@umbc.edu<= /a> &lt;mailto:<a href=3D""mailto:naghmeh.karimi@umbc.edu"" target=3D""_blank""= >naghmeh.karimi@umbc.edu</a>&gt; wrote:<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Hi Skye and Max, Could you please h= elp on mounting this today so<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; we can<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; use them as we have a deadline? Tha= nks,Naghmeh Karimi On Wed,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Oct 22,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; 2025 at 4:22 PM Skye Jonke via RT &= lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.um= bc.edu</a> &lt;mailto:<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_bl= ank"">UMBCHelp@rt.umbc.edu</a>&gt;&gt; wrote:<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Ticket &lt;URL: <a href=3D""http= s://rt.umbc.edu/Ticket/Display.html?id=3D3296910"" rel=3D""noreferrer"" target= =3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3296910</a> &lt;<a=  href=3D""https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.= html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&amp;us= g=3DAOvVaw099WO1Ar-MqC9UTifVgSrZ"" rel=3D""noreferrer"" target=3D""_blank"">http= s://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D329= 6910&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw099= WO1Ar-MqC9UTifVgSrZ</a>&gt;<br> &gt;&gt; &gt;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttps://rt.= umbc.edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust= =3D1761874374000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7"" rel=3D""noreferr= er"" target=3D""_blank"">https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ti= cket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761874374= 000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7</a> &lt;<a href=3D""https://ww= w.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.edu/Tic= ket/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D1761874374000= 000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&amp;source=3Dgmail-imap&amp;ust=3D= 1761921999000000&amp;usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g"" rel=3D""noreferrer""=  target=3D""_blank"">https://www.google.com/url?q=3Dhttps://www.google.com/ur= l?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgma= il-imap%26ust%3D1761874374000000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&amp;s= ource=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw3WPs04yO5gQkO= FpjU6my-g</a>&gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Last Update From Ticket:<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Mohammad,<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; So long as you continue to acce= ss them through<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; /umbc/software/scripts, they sh= ould remain up to date and you<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; will<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; see any new scripts. If you cop= y them to a local directory on<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; the<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; HPC cluster, they will not be u= pdated.<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; ---<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Skye Jonke<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; she/her<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Specialist, Linux System Admini= strator &amp; Lab Technical Support<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; On Oct 22, 2025, at 16:14,=  Naghmeh Karimi &lt;<a href=3D""mailto:nkarimi@umbc.edu"" target=3D""_blank"">n= karimi@umbc.edu</a> &lt;mailto:<a href=3D""mailto:nkarimi@umbc.edu"" target= =3D""_blank"">nkarimi@umbc.edu</a>&gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; wrote:<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Hi Max, Roy, Skye,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Thank you all for your hel= p.<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; I added Skye here as she i= s our IT exert on adding the<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; capability<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; of running synopsis tools (Hspi= ce) in servers.<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Skye we want to use the HP= C cluster to run hspice an other<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; synopsys tools. Could you pleas= e help on this?<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Thanks,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Naghmeh<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; On Wed, Oct 22, 2025 at 2:= 06 PM Mohammad Ebrahimabadi via RT<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &lt;<a href=3D""mailto:UMBCHelp@= rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a> &lt;mailto:<a href= =3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>= &gt; &lt;mailto:<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">U= MBCHelp@rt.umbc.edu</a> &lt;mailto:<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" = target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt;&gt;&gt; wrote:<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Ticket &lt;URL:<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; <a href=3D""https://rt.umbc.edu/Ticket/Displa= y.html?id=3D3296910"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.e= du/Ticket/Display.html?id=3D3296910</a> &lt;<a href=3D""https://www.google.c= om/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;source= =3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw099WO1Ar-MqC9UTifV= gSrZ"" rel=3D""noreferrer"" target=3D""_blank"">https://www.google.com/url?q=3Dh= ttps://rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap= &amp;ust=3D1761921999000000&amp;usg=3DAOvVaw099WO1Ar-MqC9UTifVgSrZ</a>&gt;<= br> &gt;&gt; &gt;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttps://rt.= umbc.edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust= =3D1761874374000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7"" rel=3D""noreferr= er"" target=3D""_blank"">https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ti= cket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761874374= 000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7</a> &lt;<a href=3D""https://ww= w.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.edu/Tic= ket/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D1761874374000= 000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&amp;source=3Dgmail-imap&amp;ust=3D= 1761921999000000&amp;usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g"" rel=3D""noreferrer""=  target=3D""_blank"">https://www.google.com/url?q=3Dhttps://www.google.com/ur= l?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgma= il-imap%26ust%3D1761874374000000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&amp;s= ource=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw3WPs04yO5gQkO= FpjU6my-g</a>&gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &lt;<br> &gt;&gt; &gt;&gt; <a href=3D""https://www.google.com/url?q=3Dhttps://rt.umbc= .edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D176= 1768891000000&amp;usg=3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW"" rel=3D""noreferrer"" ta= rget=3D""_blank"">https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/D= isplay.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761768891000000= &amp;usg=3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW</a> &lt;<a href=3D""https://www.goog= le.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.edu/Ticket/Di= splay.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D1761768891000000%26= usg%3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW&amp;source=3Dgmail-imap&amp;ust=3D176192= 1999000000&amp;usg=3DAOvVaw04hQC2SRO0y4BoVR0LFNp_"" rel=3D""noreferrer"" targe= t=3D""_blank"">https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3D= https://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgmail-ima= p%26ust%3D1761768891000000%26usg%3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW&amp;source= =3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw04hQC2SRO0y4BoVR0L= FNp_</a>&gt;<br> &gt;&gt; &gt;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttps://www= .google.com/url?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%= 26source%3Dgmail-imap%26ust%3D1761768891000000%26usg%3DAOvVaw2jxB3nArHBD7-i= j9ZN8ZsW&amp;source=3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVa= w0fLylAsBAoQ6LgqY79I9-d"" rel=3D""noreferrer"" target=3D""_blank"">https://www.g= oogle.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.edu/Ticket= /Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D1761768891000000= %26usg%3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW&amp;source=3Dgmail-imap&amp;ust=3D176= 1874374000000&amp;usg=3DAOvVaw0fLylAsBAoQ6LgqY79I9-d</a> &lt;<a href=3D""htt= ps://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://www.goog= le.com/url?q%253Dhttps://rt.umbc.edu/Ticket/Display.html?id%25253D3296910%2= 526source%253Dgmail-imap%2526ust%253D1761768891000000%2526usg%253DAOvVaw2jx= B3nArHBD7-ij9ZN8ZsW%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg%3D= AOvVaw0fLylAsBAoQ6LgqY79I9-d&amp;source=3Dgmail-imap&amp;ust=3D176192199900= 0000&amp;usg=3DAOvVaw3pQOXM-6FD6kt0kA6ujsVN"" rel=3D""noreferrer"" target=3D""_= blank"">https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps:= //www.google.com/url?q%253Dhttps://rt.umbc.edu/Ticket/Display.html?id%25253= D3296910%2526source%253Dgmail-imap%2526ust%253D1761768891000000%2526usg%253= DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW%26source%3Dgmail-imap%26ust%3D176187437400000= 0%26usg%3DAOvVaw0fLylAsBAoQ6LgqY79I9-d&amp;source=3Dgmail-imap&amp;ust=3D17= 61921999000000&amp;usg=3DAOvVaw3pQOXM-6FD6kt0kA6ujsVN</a>&gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Last Update From Ticke= t:<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Hi Max,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Thanks for your help! = Let=E2=80=99s proceed based on your<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; suggestion,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; hopefully it<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; works as expected.<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; I just have one quick = question: since the IT team in the<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; department can modify<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; or add new shell scrip= ts in that directory, will we also<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; have<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; access to those<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; updated or newly added=  scripts from the mounted directory on<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; HPCF?<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Thanks again,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Mohammad<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; On Wed Oct 22 14:01:32=  2025, OL73413 wrote:<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Hi Mohammad,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; We can get the so= ftware mounted for you on chip, but it<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; will<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; have to be<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; read-only as we w= on&#39;t be able to manage who can make<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; changes<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; to it<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; otherwise.<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; As a reminder, th= e machines on that run on chip are not<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; the<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; same as those<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; run by the CSEE d= epartment, so we can&#39;t guarantee that<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; these<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; will run the<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; same way or as ef= fectively.<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; If that&#39;s all=  ok with you, please give me some time to get<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; the<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; directory<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; mounted.<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; On Wed Oct 22 13:= 26:05 2025, NQ23652 wrote:<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thanks Tartel= a for your help to resolve my issue!<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Actually it s= eems all server in CSEE department have<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; access<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; to that<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; path. When I = ran the command that you gave me I got the<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; following<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; report:<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; mohammad@albo= rz:~$ df -h /umbc/software<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Filesystem Si= ze Used Avail Use% Mounted on<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; <a href=3D""ht= tp://nfs.iss.rs.umbc.edu"" rel=3D""noreferrer"" target=3D""_blank"">nfs.iss.rs.u= mbc.edu</a> &lt;<a href=3D""http://nfs.iss.rs.umbc.edu/"" rel=3D""noreferrer"" = target=3D""_blank"">http://nfs.iss.rs.umbc.edu/</a>&gt;:/ifs/data/csee/softwa= re 2.5T 2.2T<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; 326G<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; 88%<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; /umbc/softwar= e<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; mohammad@albo= rz:~$ mount | grep /umbc/software<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; <a href=3D""ht= tp://nfs.iss.rs.umbc.edu"" rel=3D""noreferrer"" target=3D""_blank"">nfs.iss.rs.u= mbc.edu</a> &lt;<a href=3D""http://nfs.iss.rs.umbc.edu/"" rel=3D""noreferrer"" = target=3D""_blank"">http://nfs.iss.rs.umbc.edu/</a>&gt;:/ifs/data/csee/softwa= re on<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; /umbc/software<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; type nfs<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt;<br> &gt;&gt; &gt;&gt; (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namle= n=3D255,acregmin=3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,= proto=3Dtcp,timeo=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountv= ers=3D3,mountport=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.6= 0)<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Please let me=  know if you need other information.<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Regards,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Mohammad<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; On Wed Oct 22=  13:15:12 2025, HK41259 wrote:<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Hello,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Thanks fo= r reaching out. Could you clarify which<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; specific<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; machine<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; or server=  you=E2=80=99re referring to when you mention the<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; =E2=80=9Cdepartmental<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; server=E2= =80=9D?<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; The /umbc= /software/scripts/ directory isn=E2=80=99t accessible<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; from<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; HPCF<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; (e.g., Ch= ip) by default =E2=80=94 those systems have separate<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; storage and<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; don=E2=80= =99t automatically mount departmental shares.<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Could you=  check where /umbc/software is mounted on your<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; departmen= tal system (e.g., by running df -h<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; /umbc/software<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; or mount<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; | grep /u= mbc/software)? That=E2=80=99ll tell us which server<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; hosts<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; it. Once<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; we know t= hat, we can confirm whether it=E2=80=99s possible or<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; appropriate<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; to make i= t visible from HPCF.<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Best,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Tartela<b= r> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; On Tue Oc= t 21 15:59:49 2025, ZZ99999 wrote:<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; First=  Name: Mohammad<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Last = Name: Ebrahimabadi<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Email= : <a href=3D""mailto:e127@umbc.edu"" target=3D""_blank"">e127@umbc.edu</a> &lt;= mailto:<a href=3D""mailto:e127@umbc.edu"" target=3D""_blank"">e127@umbc.edu</a>= &gt; &lt;mailto:<a href=3D""mailto:e127@umbc.edu"" target=3D""_blank"">e127@umb= c.edu</a> &lt;mailto:<a href=3D""mailto:e127@umbc.edu"" target=3D""_blank"">e12= 7@umbc.edu</a>&gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Campu= s ID: NQ23652<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Reque= st Type: High Performance Cluster<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Dear = Team,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; I hop= e you=E2=80=99re doing well.<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; I=E2= =80=99m a Ph.D. student in Computer Engineering and have a<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; question regarding running a so= ftware on the HPCF. On the<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; department server, I usually ru= n software using shell scripts<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; located at /umbc/software/scrip= ts/ which is provided by the IT<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; group pf the department (Geoff = Weiss Team). However, in case of<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; HPCF, it seems that this path i= s not accessible from the HPCF<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; environment.<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Could=  you please let me know if there is any specific<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; configuration or access permiss= ion required to reach this<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; directory<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; through the HPCF? or direct me = to a related person that can<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; help<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; me.<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Best = regards,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Moham= mad<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; --<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Best,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Max Breitmeyer<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; DOIT HPC System A= dministrator<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; --<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Naghmeh Karimi, Ph.D.<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Associate Professor<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Department of Computer Sci= ence and Electrical Engineering<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; University of Maryland, Ba= ltimore County<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Baltimore, MD 21250<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Tel: 410-455-3965 E-mail: = <a href=3D""mailto:nkarimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a> = &lt;mailto:<a href=3D""mailto:nkarimi@umbc.edu"" target=3D""_blank"">nkarimi@um= bc.edu</a>&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &lt;mailto:<a href=3D""mailto:nk= arimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a> &lt;mailto:<a href= =3D""mailto:nkarimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a>&gt;&gt;= <br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Web: <a href=3D""http://www= .csee.umbc.edu/~nkarimi/"" rel=3D""noreferrer"" target=3D""_blank"">http://www.c= see.umbc.edu/~nkarimi/</a> &lt;<a href=3D""https://www.google.com/url?q=3Dht= tp://www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D17619219= 99000000&amp;usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7"" rel=3D""noreferrer"" target= =3D""_blank"">https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarim= i/&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw35D5V= fZwU_gTyQhCtkbUZ7</a>&gt;<br> &gt;&gt; &gt;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttp://www.= csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761874374000000&= amp;usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW"" rel=3D""noreferrer"" target=3D""_blank= "">https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;sou= rce=3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw23Fk_KFBnlrXC_u= hIbUFIW</a> &lt;<a href=3D""https://www.google.com/url?q=3Dhttps://www.googl= e.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26us= t%3D1761874374000000%26usg%3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&amp;source=3Dgmai= l-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw05OHRJ3ua_KXM9EHoVYfgE"" r= el=3D""noreferrer"" target=3D""_blank"">https://www.google.com/url?q=3Dhttps://= www.google.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmail-= imap%26ust%3D1761874374000000%26usg%3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&amp;sour= ce=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw05OHRJ3ua_KXM9EH= oVYfgE</a>&gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &lt;<br> &gt;&gt; &gt;&gt; <a href=3D""https://www.google.com/url?q=3Dhttp://www.csee= .umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761768891000000&amp;= usg=3DAOvVaw0xLE02sw5-ueWRHyHyCwkN"" rel=3D""noreferrer"" target=3D""_blank"">ht= tps://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source= =3Dgmail-imap&amp;ust=3D1761768891000000&amp;usg=3DAOvVaw0xLE02sw5-ueWRHyHy= CwkN</a> &lt;<a href=3D""https://www.google.com/url?q=3Dhttps://www.google.c= om/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3= D1761768891000000%26usg%3DAOvVaw0xLE02sw5-ueWRHyHyCwkN&amp;source=3Dgmail-i= map&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw06Mkt_vSjpo8lnqxEqL6dt"" rel= =3D""noreferrer"" target=3D""_blank"">https://www.google.com/url?q=3Dhttps://ww= w.google.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmail-im= ap%26ust%3D1761768891000000%26usg%3DAOvVaw0xLE02sw5-ueWRHyHyCwkN&amp;source= =3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw06Mkt_vSjpo8lnqxEq= L6dt</a>&gt;<br> &gt;&gt; &gt;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttps://www= .google.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmail-ima= p%26ust%3D1761768891000000%26usg%3DAOvVaw0xLE02sw5-ueWRHyHyCwkN&amp;source= =3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw3g338pGQqShCgyX8cI= t8tV"" rel=3D""noreferrer"" target=3D""_blank"">https://www.google.com/url?q=3Dh= ttps://www.google.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3= Dgmail-imap%26ust%3D1761768891000000%26usg%3DAOvVaw0xLE02sw5-ueWRHyHyCwkN&a= mp;source=3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw3g338pGQq= ShCgyX8cIt8tV</a> &lt;<a href=3D""https://www.google.com/url?q=3Dhttps://www= .google.com/url?q%3Dhttps://www.google.com/url?q%253Dhttp://www.csee.umbc.e= du/~nkarimi/%2526source%253Dgmail-imap%2526ust%253D1761768891000000%2526usg= %253DAOvVaw0xLE02sw5-ueWRHyHyCwkN%26source%3Dgmail-imap%26ust%3D17618743740= 00000%26usg%3DAOvVaw3g338pGQqShCgyX8cIt8tV&amp;source=3Dgmail-imap&amp;ust= =3D1761921999000000&amp;usg=3DAOvVaw3ktt5GEiV5miHD1TY9Dut7"" rel=3D""noreferr= er"" target=3D""_blank"">https://www.google.com/url?q=3Dhttps://www.google.com= /url?q%3Dhttps://www.google.com/url?q%253Dhttp://www.csee.umbc.edu/~nkarimi= /%2526source%253Dgmail-imap%2526ust%253D1761768891000000%2526usg%253DAOvVaw= 0xLE02sw5-ueWRHyHyCwkN%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg= %3DAOvVaw3g338pGQqShCgyX8cIt8tV&amp;source=3Dgmail-imap&amp;ust=3D176192199= 9000000&amp;usg=3DAOvVaw3ktt5GEiV5miHD1TY9Dut7</a>&gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; -- Naghmeh Karimi, Ph.D.<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Associate Professor<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Department of Computer Science and = Electrical Engineering<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; University of Maryland, Baltimore C= ounty<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Baltimore, MD 21250<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Tel: 410-455-3965 E-mail: <a href= =3D""mailto:nkarimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a> &lt;mai= lto:<a href=3D""mailto:nkarimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu<= /a>&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Web: <a href=3D""http://www.csee.umb= c.edu/~nkarimi/"" rel=3D""noreferrer"" target=3D""_blank"">http://www.csee.umbc.= edu/~nkarimi/</a> &lt;<a href=3D""https://www.google.com/url?q=3Dhttp://www.= csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&= amp;usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7"" rel=3D""noreferrer"" target=3D""_blank= "">https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;sou= rce=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw35D5VfZwU_gTyQh= CtkbUZ7</a>&gt;<br> &gt;&gt; &gt;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttp://www.= csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761874374000000&= amp;usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW"" rel=3D""noreferrer"" target=3D""_blank= "">https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;sou= rce=3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw23Fk_KFBnlrXC_u= hIbUFIW</a> &lt;<a href=3D""https://www.google.com/url?q=3Dhttps://www.googl= e.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26us= t%3D1761874374000000%26usg%3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&amp;source=3Dgmai= l-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw05OHRJ3ua_KXM9EHoVYfgE"" r= el=3D""noreferrer"" target=3D""_blank"">https://www.google.com/url?q=3Dhttps://= www.google.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmail-= imap%26ust%3D1761874374000000%26usg%3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&amp;sour= ce=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw05OHRJ3ua_KXM9EH= oVYfgE</a>&gt;&gt;<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; --<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Best,<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Max Breitmeyer<br> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; DOIT HPC System Administrator<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; &gt;&gt; -- V/R,Maxwell BreitmeyerUMBC HPCF SpecialistUMB= C Observatory IT<br> &gt;&gt; &gt;&gt; &gt;&gt; Manager Graduate Student(443) 835-8250<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; --<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; Best,<br> &gt;&gt; &gt;&gt; Max Breitmeyer<br> &gt;&gt; &gt;&gt; DOIT HPC System Administrator<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;<br> &gt;&gt; <br> &gt;&gt; -- <br> &gt;&gt; Naghmeh Karimi, Ph.D.<br> &gt;&gt; Associate Professor<br> &gt;&gt; Department of Computer Science and Electrical Engineering<br> &gt;&gt; University of Maryland, Baltimore County<br> &gt;&gt; Baltimore, MD 21250<br> &gt;&gt; Tel: *410-455-3965* E-mail: *<a href=3D""mailto:nkarimi@umbc.edu"" t= arget=3D""_blank"">nkarimi@umbc.edu</a> &lt;mailto:<a href=3D""mailto:nkarimi@= umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a>&gt; &lt;<a href=3D""mailto:= nkarimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a> &lt;mailto:<a href= =3D""mailto:nkarimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a>&gt;&gt;= *<br> &gt;&gt; Web: *<a href=3D""http://www.csee.umbc.edu/~nkarimi/"" rel=3D""norefe= rrer"" target=3D""_blank"">http://www.csee.umbc.edu/~nkarimi/</a> &lt;<a href= =3D""https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;s= ource=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw35D5VfZwU_gTy= QhCtkbUZ7"" rel=3D""noreferrer"" target=3D""_blank"">https://www.google.com/url?= q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D17= 61921999000000&amp;usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7</a>&gt;<br> &gt;&gt; &lt;<a href=3D""http://www.csee.umbc.edu/~nkarimi/"" rel=3D""noreferr= er"" target=3D""_blank"">http://www.csee.umbc.edu/~nkarimi/</a> &lt;<a href=3D= ""https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;sour= ce=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw35D5VfZwU_gTyQhC= tkbUZ7"" rel=3D""noreferrer"" target=3D""_blank"">https://www.google.com/url?q= =3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D176= 1921999000000&amp;usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7</a>&gt;&gt;*<br> <br> </blockquote></div> "
3296910,72506931,Correspond,DoIT-Research-Computing,2025-10-27 16:27:12.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Skye Jonke,ii69854@umbc.edu,"Max,  I need the help of you (or an expert you can point me to) in order to rewri= te the scripts, because Cadence and Synopsys tools require a lot of additio= ns to your PATH and LD_PATH, which are usually handled with absolute paths.=  I=E2=80=99m unsure what the best way to handle that would be on the HPC cl= uster. In addition, I need to know the HPC cluster=E2=80=99s IP range so I = can add it to an NFS share. --- Skye Jonke  she/her  Specialist, Linux System Administrator & Lab Technical Support  > On Oct 24, 2025, at 10:56, Mohammad Ebrahimabadi <e127@umbc.edu> wrote: >=20 > Thanks Skye, >=20 > @ Dear Max, > I just cced Skye from the IT of csee department to help us run software o= n the chip server. >=20 > Regards, > Mohammad >=20 >=20 > On Fri, Oct 24, 2025 at 10:51=E2=80=AFAM Skye Jonke via RT <UMBCHelp@rt.u= mbc.edu <mailto:UMBCHelp@rt.umbc.edu>> wrote: >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 <https= ://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3296= 910&source=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw3nNv4uwna-Cy-WTp= OccXCH> > >>=20 >> Last Update From Ticket: >>=20 >> That=E2=80=99s a bit confusing, since hardcoded paths won=E2=80=99t work= , please have Max contact me directly so we can talk about what needs to be=  done to make this work.=20 >> --- >> Skye Jonke >>=20 >> she/her >>=20 >> Specialist, Linux System Administrator & Lab Technical Support >>=20 >> > On Oct 24, 2025, at 10:46, Mohammad Ebrahimabadi <e127@umbc.edu <mailt= o:e127@umbc.edu>> wrote: >> >=20 >> > Hello Skye, >> >=20 >> > I believe the way that Max told me to run the script was using the sam= e path that we have in the CSEE department as he mounted that path in HPCF.=  I mean this path: >> >  /umbc/software/scripts/launch_synopsys_hspice.sh >> >=20 >> >=20 >> > Regards, >> > Mohammad >> >=20 >> >=20 >> >=20 >> >=20 >> > On Fri, Oct 24, 2025 at 9:42=E2=80=AFAM naghmeh.karimi@umbc.edu <mailt= o:naghmeh.karimi@umbc.edu> <mailto:naghmeh.karimi@umbc.edu <mailto:naghmeh.= karimi@umbc.edu>> via RT <UMBCHelp@rt.umbc.edu <mailto:UMBCHelp@rt.umbc.edu= > <mailto:UMBCHelp@rt.umbc.edu <mailto:UMBCHelp@rt.umbc.edu>>> wrote: >> >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 <ht= tps://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3= 296910&source=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw3nNv4uwna-Cy-= WTpOccXCH> <https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Displ= ay.html?id%3D3296910&source=3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVa= w099WO1Ar-MqC9UTifVgSrZ <https://www.google.com/url?q=3Dhttps://www.google.= com/url?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source= %3Dgmail-imap%26ust%3D1761921999000000%26usg%3DAOvVaw099WO1Ar-MqC9UTifVgSrZ= &source=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw0d5KU99CbDvu6lur7P0= Tnh>> > >> >>=20 >> >> Last Update From Ticket: >> >>=20 >> >> That's perfect Skye. I highly appreciate it. I ask Mohamamd to respond >> >> the ticket with the path. >> >>=20 >> >> regards, >> >> Naghmeh >> >>=20 >> >> On Fri, Oct 24, 2025 at 9:39=E2=80=AFAM Skye Jonke <ii69854@umbc.edu = <mailto:ii69854@umbc.edu> <mailto:ii69854@umbc.edu <mailto:ii69854@umbc.edu= >>> wrote: >> >>=20 >> >> > Its not possible to use the existing scripts in a situation where t= he >> >> > files aren=E2=80=99t mounted in /umbc/software, but I can write som= e scripts >> >> > specific to this setup to this situation with a bit more informatio= n. Where >> >> > are the script files located on the HPC? If you aren=E2=80=99t sure= , you can get >> >> > the full path by running =E2=80=9Crealpath myfile.sh=E2=80=9D (repl= acing myfile.sh with one >> >> > of the cadence scripts, of course) >> >> > --- >> >> > Skye Jonke >> >> > >> >> > she/her >> >> > >> >> > Specialist, Linux System Administrator & Lab Technical Support >> >> > >> >> > On Oct 23, 2025, at 21:32, Naghmeh Karimi <nkarimi@umbc.edu <mailto= :nkarimi@umbc.edu> <mailto:nkarimi@umbc.edu <mailto:nkarimi@umbc.edu>>> wro= te: >> >> > >> >> > Hi Max, thanks for getting back to us. >> >> > >> >> > Hi Skye, we need your help here to find the file locations. Could y= ou >> >> > please help to resolve this issue? >> >> > >> >> > Thanks a lot >> >> > Naghmeh >> >> > >> >> > Naghmeh Karimi, Ph.D. >> >> > Associate Professor >> >> > Department of Computer Science and Electrical Engineering >> >> > University of Maryland, Baltimore County >> >> > Baltimore, MD 21250 >> >> > Tel: *410-455-3965* E-mail: *nkarimi@umbc.edu <mailto:nkarimi@umbc.= edu> <mailto:nkarimi@umbc.edu <mailto:nkarimi@umbc.edu>> <nkarimi@umbc.edu = <mailto:nkarimi@umbc.edu><mailto:nkarimi@umbc.edu <mailto:nkarimi@umbc.edu>= >>* >> >> > Web: *http://www.csee.umbc.edu/~nkarimi/ <https://www.google.com/ur= l?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source=3Dgmail-imap&ust=3D17619226= 26000000&usg=3DAOvVaw3z3DnkSSm-krms88JtwQY_> <https://www.google.com/url?q= =3Dhttp://www.csee.umbc.edu/~nkarimi/&source=3Dgmail-imap&ust=3D17619219990= 00000&usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7 <https://www.google.com/url?q=3Dht= tps://www.google.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3D= gmail-imap%26ust%3D1761921999000000%26usg%3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7&so= urce=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw0X7l2TgWCgMekNlzKnPsJl= >> >> >> > <https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&= source=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw23Fk_KFBnlrXC_uhIbUF= IW <https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://ww= w.csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D1761874374000000%26= usg%3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&source=3Dgmail-imap&ust=3D17619226260000= 00&usg=3DAOvVaw3wW2UaHLr-IAa0KsbyDZYx> <https://www.google.com/url?q=3Dhttp= s://www.google.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgm= ail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&sour= ce=3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw05OHRJ3ua_KXM9EHoVYfgE <= https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://www.g= oogle.com/url?q%253Dhttp://www.csee.umbc.edu/~nkarimi/%2526source%253Dgmail= -imap%2526ust%253D1761874374000000%2526usg%253DAOvVaw23Fk_KFBnlrXC_uhIbUFIW= %26source%3Dgmail-imap%26ust%3D1761921999000000%26usg%3DAOvVaw05OHRJ3ua_KXM= 9EHoVYfgE&source=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw17L4xVKtFJ= KMIHbtKBKHb5>>>* >> >> > >> >> > On Thu, Oct 23, 2025, 12:22=E2=80=AFPM Max Breitmeyer via RT <UMBCH= elp@rt.umbc.edu <mailto:UMBCHelp@rt.umbc.edu> <mailto:UMBCHelp@rt.umbc.edu = <mailto:UMBCHelp@rt.umbc.edu>>> >> >> > wrote: >> >> > >> >> >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 = <https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%= 3D3296910&source=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw3nNv4uwna-= Cy-WTpOccXCH> <https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Di= splay.html?id%3D3296910&source=3Dgmail-imap&ust=3D1761921999000000&usg=3DAO= vVaw099WO1Ar-MqC9UTifVgSrZ <https://www.google.com/url?q=3Dhttps://www.goog= le.com/url?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26sou= rce%3Dgmail-imap%26ust%3D1761921999000000%26usg%3DAOvVaw099WO1Ar-MqC9UTifVg= SrZ&source=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw0d5KU99CbDvu6lur= 7P0Tnh>> >> >> >> <https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display= .html?id%3D3296910&source=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw3= ImziGtKZ-9j9JFW8J65w7 <https://www.google.com/url?q=3Dhttps://www.google.co= m/url?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3= Dgmail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&s= ource=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw1DVHQWapzV68VEop_7Hjj= v> <https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://r= t.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3= D1761874374000000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&source=3Dgmail-imap&= ust=3D1761921999000000&usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g <https://www.goog= le.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://www.google.com/url?q%= 253Dhttps://rt.umbc.edu/Ticket/Display.html?id%25253D3296910%2526source%253= Dgmail-imap%2526ust%253D1761874374000000%2526usg%253DAOvVaw3ImziGtKZ-9j9JFW= 8J65w7%26source%3Dgmail-imap%26ust%3D1761921999000000%26usg%3DAOvVaw3WPs04y= O5gQkOFpjU6my-g&source=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw3cp5= M8Pk5ufwr6CL9J1XNN>>> >> >> >> > >> >> >> >> >> >> Last Update From Ticket: >> >> >> >> >> >> Hi Mohammad, >> >> >> >> >> >> According to the error message, it's having an issue finding anoth= er file >> >> >> in >> >> >> the directory. This appears to be an issue due to the mounting loc= ations >> >> >> being >> >> >> different. I would point you to the previous email where I said th= at any >> >> >> software with hard-coded locations would not work, and that softwa= re >> >> >> compatibility is not guaranteed. I would recommend working with Sk= ye on >> >> >> porting >> >> >> what is in the directory to your local research directories on chi= p. >> >> >> >> >> >> On Thu Oct 23 12:09:34 2025, NQ23652 wrote: >> >> >> >> >> >> > Yes you are right, now I could see all shell files. However stil= l I have >> >> >> > problem when I run the sh file. >> >> >> >> >> >> > [e127@chip-login2 scripts]$ sh launch_synopsys_hspice.sh >> >> >> > launch_synopsys_hspice.sh: line 3: >> >> >> > /umbc/software/scripts/env_synopsys_hspice.sh: No such file or d= irectory >> >> >> > [e127@chip-login2 scripts]$ >> >> >> >> >> >> > Would you please take a look to it. >> >> >> >> >> >> > Regards, >> >> >> >> >> >> > Mohammad >> >> >> >> >> >> > On Thu Oct 23 11:59:17 2025, OL73413 wrote: >> >> >> >> >> >> >> Hi Mohammad, >> >> >> >> >> >> >> A good point. Chip uses a service called ""autofs"" to mount and = unmount >> >> >> >> directories that aren't being actively used. This helps maintai= n the >> >> >> >> login nodes integrity. If you see in my screenshot below, the d= irectory >> >> >> >> appears to be empty, but when I cd to the csee directory where = your >> >> >> >> software is mounted, it is all available. Backing out of the di= rectory >> >> >> >> and ls'ing the directory also shows that the directory is now m= ounted >> >> >> >> and available. >> >> >> >> >> >> >> On Thu, Oct 23, 2025 at 11:41 AM Mohammad Ebrahimabadi via RT >> >> >> >> <UMBCHelp@rt.umbc.edu <mailto:UMBCHelp@rt.umbc.edu> <mailto:UMB= CHelp@rt.umbc.edu <mailto:UMBCHelp@rt.umbc.edu>>> wrote: >> >> >> >> >> >> >>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296= 910 <https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html= ?id%3D3296910&source=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw3nNv4u= wna-Cy-WTpOccXCH> <https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticke= t/Display.html?id%3D3296910&source=3Dgmail-imap&ust=3D1761921999000000&usg= =3DAOvVaw099WO1Ar-MqC9UTifVgSrZ <https://www.google.com/url?q=3Dhttps://www= .google.com/url?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%= 26source%3Dgmail-imap%26ust%3D1761921999000000%26usg%3DAOvVaw099WO1Ar-MqC9U= TifVgSrZ&source=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw0d5KU99CbDv= u6lur7P0Tnh>> >> >> >> <https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display= .html?id%3D3296910&source=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw3= ImziGtKZ-9j9JFW8J65w7 <https://www.google.com/url?q=3Dhttps://www.google.co= m/url?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3= Dgmail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&s= ource=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw1DVHQWapzV68VEop_7Hjj= v> <https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://r= t.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3= D1761874374000000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&source=3Dgmail-imap&= ust=3D1761921999000000&usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g <https://www.goog= le.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://www.google.com/url?q%= 253Dhttps://rt.umbc.edu/Ticket/Display.html?id%25253D3296910%2526source%253= Dgmail-imap%2526ust%253D1761874374000000%2526usg%253DAOvVaw3ImziGtKZ-9j9JFW= 8J65w7%26source%3Dgmail-imap%26ust%3D1761921999000000%26usg%3DAOvVaw3WPs04y= O5gQkOFpjU6my-g&source=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw3cp5= M8Pk5ufwr6CL9J1XNN>>> >> >> >> > >> >> >> >> >> >> >>> Comment From Ticket: >> >> >> >>> Thank you Max! >> >> >> >> >> >> >>> Actually we have still problem for accessing that directory. I >> >> >> >>> uploaded two >> >> >> >>> screenshots that shows what we can see in the department serve= rs >> >> >> >>> and what we >> >> >> >>> can see in CHIP. >> >> >> >> >> >> >>> For example by running this shell file we can launch HSPICE >> >> >> >>> software in the >> >> >> >>> department server: >> >> >> >>> /umbc/software/scripts/launch_synopsys_hspice.sh >> >> >> >>> But it is not still accessible on CHIP. >> >> >> >> >> >> >>> Regards, >> >> >> >> >> >> >>> Mohammad >> >> >> >> >> >> >>> On Thu Oct 23 10:54:59 2025, OL73413 wrote: >> >> >> >> >> >> >>> > Hi Nahmeh, >> >> >> >> >> >> >>> > Apologies =E2=80=94 I meant to update you yesterday after co= mpleting >> >> >> >>> this, but I >> >> >> >>> > was pulled into a long meeting. The software is now availabl= e on >> >> >> >>> chip at: >> >> >> >> >> >> >>> > /umbc/software/csee >> >> >> >> >> >> >>> > Please note that if the software contains any hardcoded file >> >> >> >>> paths, it may >> >> >> >>> > not function correctly. Unfortunately, we don=E2=80=99t have=  control over >> >> >> >>> those >> >> >> >>> > cases. >> >> >> >> >> >> >>> > Also, just to clarify, Skye and I work in separate departmen= ts. >> >> >> >>> Similar to >> >> >> >>> > how I don=E2=80=99t have administrative access to the CSEE d= epartment >> >> >> >>> servers >> >> >> >>> > (which include the software directory), Skye is not an >> >> >> >>> administrator on >> >> >> >>> > chip. >> >> >> >> >> >> >>> > Please let me know if you encounter any issues accessing the >> >> >> >>> software on >> >> >> >>> > the server. If there are problems running the software, I=E2= =80=99ll >> >> >> >>> coordinate >> >> >> >>> > with Skye to troubleshoot, but since we don=E2=80=99t mainta= in this >> >> >> >>> software, we >> >> >> >>> > can=E2=80=99t guarantee full compatibility on our systems. >> >> >> >> >> >> >>> > On Thu Oct 23 10:36:41 2025, naghmeh.karimi@umbc.edu <mailto= :naghmeh.karimi@umbc.edu> <mailto:naghmeh.karimi@umbc.edu <mailto:naghmeh.k= arimi@umbc.edu>> wrote: >> >> >> >> >> >> >>> >> Hi Skye and Max, Could you please help on mounting this tod= ay so >> >> >> >>> we can >> >> >> >>> >> use them as we have a deadline? Thanks,Naghmeh Karimi On We= d, >> >> >> >>> Oct 22, >> >> >> >>> >> 2025 at 4:22 PM Skye Jonke via RT <UMBCHelp@rt.umbc.edu <ma= ilto:UMBCHelp@rt.umbc.edu> <mailto:UMBCHelp@rt.umbc.edu <mailto:UMBCHelp@rt= .umbc.edu>>> wrote: >> >> >> >> >> >> >>> >>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D= 3296910 <https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.= html?id%3D3296910&source=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw3n= Nv4uwna-Cy-WTpOccXCH> <https://www.google.com/url?q=3Dhttps://rt.umbc.edu/T= icket/Display.html?id%3D3296910&source=3Dgmail-imap&ust=3D1761921999000000&= usg=3DAOvVaw099WO1Ar-MqC9UTifVgSrZ <https://www.google.com/url?q=3Dhttps://= www.google.com/url?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D32969= 10%26source%3Dgmail-imap%26ust%3D1761921999000000%26usg%3DAOvVaw099WO1Ar-Mq= C9UTifVgSrZ&source=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw0d5KU99C= bDvu6lur7P0Tnh>> >> >> >> <https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display= .html?id%3D3296910&source=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw3= ImziGtKZ-9j9JFW8J65w7 <https://www.google.com/url?q=3Dhttps://www.google.co= m/url?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3= Dgmail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&s= ource=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw1DVHQWapzV68VEop_7Hjj= v> <https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://r= t.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3= D1761874374000000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&source=3Dgmail-imap&= ust=3D1761921999000000&usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g <https://www.goog= le.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://www.google.com/url?q%= 253Dhttps://rt.umbc.edu/Ticket/Display.html?id%25253D3296910%2526source%253= Dgmail-imap%2526ust%253D1761874374000000%2526usg%253DAOvVaw3ImziGtKZ-9j9JFW= 8J65w7%26source%3Dgmail-imap%26ust%3D1761921999000000%26usg%3DAOvVaw3WPs04y= O5gQkOFpjU6my-g&source=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw3cp5= M8Pk5ufwr6CL9J1XNN>>> >> >> >> >>> > >> >> >> >> >> >> >>> >>> Last Update From Ticket: >> >> >> >> >> >> >>> >>> Mohammad, >> >> >> >> >> >> >>> >>> So long as you continue to access them through >> >> >> >>> >>> /umbc/software/scripts, they should remain up to date and = you >> >> >> >>> will >> >> >> >>> >>> see any new scripts. If you copy them to a local directory=  on >> >> >> >>> the >> >> >> >>> >>> HPC cluster, they will not be updated. >> >> >> >> >> >> >>> >>> --- >> >> >> >>> >>> Skye Jonke >> >> >> >> >> >> >>> >>> she/her >> >> >> >> >> >> >>> >>> Specialist, Linux System Administrator & Lab Technical Sup= port >> >> >> >> >> >> >>> >>> > On Oct 22, 2025, at 16:14, Naghmeh Karimi <nkarimi@umbc.= edu <mailto:nkarimi@umbc.edu> <mailto:nkarimi@umbc.edu <mailto:nkarimi@umbc= .edu>>> >> >> >> >>> >>> wrote: >> >> >> >>> >>> > >> >> >> >>> >>> > Hi Max, Roy, Skye, >> >> >> >>> >>> > >> >> >> >>> >>> > Thank you all for your help. >> >> >> >>> >>> > >> >> >> >>> >>> > I added Skye here as she is our IT exert on adding the >> >> >> >>> capability >> >> >> >>> >>> of running synopsis tools (Hspice) in servers. >> >> >> >>> >>> > Skye we want to use the HPC cluster to run hspice an oth= er >> >> >> >>> >>> synopsys tools. Could you please help on this? >> >> >> >>> >>> > >> >> >> >>> >>> > Thanks, >> >> >> >>> >>> > Naghmeh >> >> >> >>> >>> > >> >> >> >>> >>> > On Wed, Oct 22, 2025 at 2:06 PM Mohammad Ebrahimabadi vi= a RT >> >> >> >>> >>> <UMBCHelp@rt.umbc.edu <mailto:UMBCHelp@rt.umbc.edu> <mailt= o:UMBCHelp@rt.umbc.edu <mailto:UMBCHelp@rt.umbc.edu>> <mailto:UMBCHelp@rt.u= mbc.edu <mailto:UMBCHelp@rt.umbc.edu><mailto:UMBCHelp@rt.umbc.edu <mailto:U= MBCHelp@rt.umbc.edu>>>> wrote: >> >> >> >>> >>> >> Ticket <URL: >> >> >> >>> https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 <https://= www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3296910= &source=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw3nNv4uwna-Cy-WTpOcc= XCH> <https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.htm= l?id%3D3296910&source=3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw099WO= 1Ar-MqC9UTifVgSrZ <https://www.google.com/url?q=3Dhttps://www.google.com/ur= l?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgma= il-imap%26ust%3D1761921999000000%26usg%3DAOvVaw099WO1Ar-MqC9UTifVgSrZ&sourc= e=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw0d5KU99CbDvu6lur7P0Tnh>> >> >> >> <https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display= .html?id%3D3296910&source=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw3= ImziGtKZ-9j9JFW8J65w7 <https://www.google.com/url?q=3Dhttps://www.google.co= m/url?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3= Dgmail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&s= ource=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw1DVHQWapzV68VEop_7Hjj= v> <https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://r= t.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3= D1761874374000000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&source=3Dgmail-imap&= ust=3D1761921999000000&usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g <https://www.goog= le.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://www.google.com/url?q%= 253Dhttps://rt.umbc.edu/Ticket/Display.html?id%25253D3296910%2526source%253= Dgmail-imap%2526ust%253D1761874374000000%2526usg%253DAOvVaw3ImziGtKZ-9j9JFW= 8J65w7%26source%3Dgmail-imap%26ust%3D1761921999000000%26usg%3DAOvVaw3WPs04y= O5gQkOFpjU6my-g&source=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw3cp5= M8Pk5ufwr6CL9J1XNN>>> >> >> >> >>> >>> >> >> >> >>> < >> >> >> https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.= html?id%3D3296910&source=3Dgmail-imap&ust=3D1761768891000000&usg=3DAOvVaw2j= xB3nArHBD7-ij9ZN8ZsW <https://www.google.com/url?q=3Dhttps://www.google.com= /url?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3D= gmail-imap%26ust%3D1761768891000000%26usg%3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW&so= urce=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw2ycbat305sOpELNsl0jFQS= > <https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt= .umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D= 1761768891000000%26usg%3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW&source=3Dgmail-imap&u= st=3D1761921999000000&usg=3DAOvVaw04hQC2SRO0y4BoVR0LFNp_ <https://www.googl= e.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://www.google.com/url?q%2= 53Dhttps://rt.umbc.edu/Ticket/Display.html?id%25253D3296910%2526source%253D= gmail-imap%2526ust%253D1761768891000000%2526usg%253DAOvVaw2jxB3nArHBD7-ij9Z= N8ZsW%26source%3Dgmail-imap%26ust%3D1761921999000000%26usg%3DAOvVaw04hQC2SR= O0y4BoVR0LFNp_&source=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw3dlpN= nBtzA6koeM2xXYQa2>> >> >> >> <https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhtt= ps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgmail-imap%2= 6ust%3D1761768891000000%26usg%3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW&source=3Dgmail= -imap&ust=3D1761874374000000&usg=3DAOvVaw0fLylAsBAoQ6LgqY79I9-d <https://ww= w.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://www.google.com/= url?q%253Dhttps://rt.umbc.edu/Ticket/Display.html?id%25253D3296910%2526sour= ce%253Dgmail-imap%2526ust%253D1761768891000000%2526usg%253DAOvVaw2jxB3nArHB= D7-ij9ZN8ZsW%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw0= fLylAsBAoQ6LgqY79I9-d&source=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvV= aw3ukvs4os21eQNpVKHlHspB><https://www.google.com/url?q=3Dhttps://www.google= .com/url?q%3Dhttps://www.google.com/url?q%253Dhttps://rt.umbc.edu/Ticket/Di= splay.html?id%25253D3296910%2526source%253Dgmail-imap%2526ust%253D176176889= 1000000%2526usg%253DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW%26source%3Dgmail-imap%26us= t%3D1761874374000000%26usg%3DAOvVaw0fLylAsBAoQ6LgqY79I9-d&source=3Dgmail-im= ap&ust=3D1761921999000000&usg=3DAOvVaw3pQOXM-6FD6kt0kA6ujsVN <https://www.g= oogle.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://www.google.com/url= ?q%253Dhttps://www.google.com/url?q%25253Dhttps://rt.umbc.edu/Ticket/Displa= y.html?id%2525253D3296910%252526source%25253Dgmail-imap%252526ust%25253D176= 1768891000000%252526usg%25253DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW%2526source%253Dg= mail-imap%2526ust%253D1761874374000000%2526usg%253DAOvVaw0fLylAsBAoQ6LgqY79= I9-d%26source%3Dgmail-imap%26ust%3D1761921999000000%26usg%3DAOvVaw3pQOXM-6F= D6kt0kA6ujsVN&source=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw1Emr2V= JBRhkK6VbbDl54fR>>> >> >> >> > >> >> >> >>> >>> > >> >> >> >>> >>> >> >> >> >> >>> >>> >> Last Update From Ticket: >> >> >> >>> >>> >> >> >> >> >>> >>> >> Hi Max, >> >> >> >>> >>> >> >> >> >> >>> >>> >> Thanks for your help! Let=E2=80=99s proceed based on yo= ur >> >> >> >>> suggestion, >> >> >> >>> >>> hopefully it >> >> >> >>> >>> >> works as expected. >> >> >> >>> >>> >> >> >> >> >>> >>> >> I just have one quick question: since the IT team in the >> >> >> >>> >>> department can modify >> >> >> >>> >>> >> or add new shell scripts in that directory, will we also >> >> >> >>> have >> >> >> >>> >>> access to those >> >> >> >>> >>> >> updated or newly added scripts from the mounted directo= ry on >> >> >> >>> >>> HPCF? >> >> >> >>> >>> >> >> >> >> >>> >>> >> Thanks again, >> >> >> >>> >>> >> Mohammad >> >> >> >>> >>> >> >> >> >> >>> >>> >> On Wed Oct 22 14:01:32 2025, OL73413 wrote: >> >> >> >>> >>> >> >> >> >> >>> >>> >> > Hi Mohammad, >> >> >> >>> >>> >> >> >> >> >>> >>> >> > We can get the software mounted for you on chip, but = it >> >> >> >>> will >> >> >> >>> >>> have to be >> >> >> >>> >>> >> > read-only as we won't be able to manage who can make >> >> >> >>> changes >> >> >> >>> >>> to it >> >> >> >>> >>> >> > otherwise. >> >> >> >>> >>> >> >> >> >> >>> >>> >> > As a reminder, the machines on that run on chip are n= ot >> >> >> >>> the >> >> >> >>> >>> same as those >> >> >> >>> >>> >> > run by the CSEE department, so we can't guarantee that >> >> >> >>> these >> >> >> >>> >>> will run the >> >> >> >>> >>> >> > same way or as effectively. >> >> >> >>> >>> >> >> >> >> >>> >>> >> > If that's all ok with you, please give me some time t= o get >> >> >> >>> the >> >> >> >>> >>> directory >> >> >> >>> >>> >> > mounted. >> >> >> >>> >>> >> >> >> >> >>> >>> >> > On Wed Oct 22 13:26:05 2025, NQ23652 wrote: >> >> >> >>> >>> >> >> >> >> >>> >>> >> >> Thanks Tartela for your help to resolve my issue! >> >> >> >>> >>> >> >> >> >> >>> >>> >> >> Actually it seems all server in CSEE department have >> >> >> >>> access >> >> >> >>> >>> to that >> >> >> >>> >>> >> >> path. When I ran the command that you gave me I got = the >> >> >> >>> >>> following >> >> >> >>> >>> >> >> report: >> >> >> >>> >>> >> >> >> >> >>> >>> >> >> mohammad@alborz:~$ df -h /umbc/software >> >> >> >>> >>> >> >> Filesystem Size Used Avail Use% Mounted on >> >> >> >>> >>> >> >> nfs.iss.rs.umbc.edu <https://www.google.com/url?q=3D= http://nfs.iss.rs.umbc.edu&source=3Dgmail-imap&ust=3D1761922626000000&usg= =3DAOvVaw15vrKX_7H4XPbaxCxBRx12> <http://nfs.iss.rs.umbc.edu/ <https://www.= google.com/url?q=3Dhttp://nfs.iss.rs.umbc.edu/&source=3Dgmail-imap&ust=3D17= 61922626000000&usg=3DAOvVaw2-3-JimnQAB975lph5arXG>>:/ifs/data/csee/software=  2.5T 2.2T >> >> >> >>> 326G >> >> >> >>> >>> 88% >> >> >> >>> >>> >> >> /umbc/software >> >> >> >>> >>> >> >> >> >> >>> >>> >> >> mohammad@alborz:~$ mount | grep /umbc/software >> >> >> >>> >>> >> >> nfs.iss.rs.umbc.edu <https://www.google.com/url?q=3D= http://nfs.iss.rs.umbc.edu&source=3Dgmail-imap&ust=3D1761922626000000&usg= =3DAOvVaw15vrKX_7H4XPbaxCxBRx12> <http://nfs.iss.rs.umbc.edu/ <https://www.= google.com/url?q=3Dhttp://nfs.iss.rs.umbc.edu/&source=3Dgmail-imap&ust=3D17= 61922626000000&usg=3DAOvVaw2-3-JimnQAB975lph5arXG>>:/ifs/data/csee/software=  on >> >> >> >>> /umbc/software >> >> >> >>> >>> type nfs >> >> >> >>> >>> >> >> >> >> >> >>> >>> >> >> >> >>> >> >> >> (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namlen=3D255,a= cregmin=3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,proto=3Dt= cp,timeo=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountvers=3D3,m= ountport=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.60) >> >> >> >>> >>> >> >> >> >> >>> >>> >> >> Please let me know if you need other information. >> >> >> >>> >>> >> >> >> >> >>> >>> >> >> Regards, >> >> >> >>> >>> >> >> >> >> >>> >>> >> >> Mohammad >> >> >> >>> >>> >> >> >> >> >>> >>> >> >> On Wed Oct 22 13:15:12 2025, HK41259 wrote: >> >> >> >>> >>> >> >> >> >> >>> >>> >> >>> Hello, >> >> >> >>> >>> >> >> >> >> >>> >>> >> >>> Thanks for reaching out. Could you clarify which >> >> >> >>> specific >> >> >> >>> >>> machine >> >> >> >>> >>> >> >>> or server you=E2=80=99re referring to when you ment= ion the >> >> >> >>> >>> =E2=80=9Cdepartmental >> >> >> >>> >>> >> >>> server=E2=80=9D? >> >> >> >>> >>> >> >> >> >> >>> >>> >> >>> The /umbc/software/scripts/ directory isn=E2=80=99t=  accessible >> >> >> >>> from >> >> >> >>> >>> HPCF >> >> >> >>> >>> >> >>> (e.g., Chip) by default =E2=80=94 those systems hav= e separate >> >> >> >>> >>> storage and >> >> >> >>> >>> >> >>> don=E2=80=99t automatically mount departmental shar= es. >> >> >> >>> >>> >> >> >> >> >>> >>> >> >>> Could you check where /umbc/software is mounted on = your >> >> >> >>> >>> >> >>> departmental system (e.g., by running df -h >> >> >> >>> /umbc/software >> >> >> >>> >>> or mount >> >> >> >>> >>> >> >>> | grep /umbc/software)? That=E2=80=99ll tell us whi= ch server >> >> >> >>> hosts >> >> >> >>> >>> it. Once >> >> >> >>> >>> >> >>> we know that, we can confirm whether it=E2=80=99s p= ossible or >> >> >> >>> >>> appropriate >> >> >> >>> >>> >> >>> to make it visible from HPCF. >> >> >> >>> >>> >> >> >> >> >>> >>> >> >>> Best, >> >> >> >>> >>> >> >> >> >> >>> >>> >> >>> Tartela >> >> >> >>> >>> >> >> >> >> >>> >>> >> >>> On Tue Oct 21 15:59:49 2025, ZZ99999 wrote: >> >> >> >>> >>> >> >> >> >> >>> >>> >> >>>> First Name: Mohammad >> >> >> >>> >>> >> >>>> Last Name: Ebrahimabadi >> >> >> >>> >>> >> >>>> Email: e127@umbc.edu <mailto:e127@umbc.edu> <mailt= o:e127@umbc.edu <mailto:e127@umbc.edu>> <mailto:e127@umbc.edu <mailto:e127@= umbc.edu><mailto:e127@umbc.edu <mailto:e127@umbc.edu>>> >> >> >> >>> >>> >> >>>> Campus ID: NQ23652 >> >> >> >>> >>> >> >>>> >> >> >> >>> >>> >> >>>> Request Type: High Performance Cluster >> >> >> >>> >>> >> >>>> >> >> >> >>> >>> >> >>>> Dear Team, >> >> >> >>> >>> >> >>>> >> >> >> >>> >>> >> >>>> I hope you=E2=80=99re doing well. >> >> >> >>> >>> >> >>>> I=E2=80=99m a Ph.D. student in Computer Engineerin= g and have a >> >> >> >>> >>> question regarding running a software on the HPCF. On the >> >> >> >>> >>> department server, I usually run software using shell scri= pts >> >> >> >>> >>> located at /umbc/software/scripts/ which is provided by th= e IT >> >> >> >>> >>> group pf the department (Geoff Weiss Team). However, in ca= se of >> >> >> >>> >>> HPCF, it seems that this path is not accessible from the H= PCF >> >> >> >>> >>> environment. >> >> >> >>> >>> >> >>>> >> >> >> >>> >>> >> >>>> Could you please let me know if there is any speci= fic >> >> >> >>> >>> configuration or access permission required to reach this >> >> >> >>> directory >> >> >> >>> >>> through the HPCF? or direct me to a related person that can >> >> >> >>> help >> >> >> >>> >>> me. >> >> >> >>> >>> >> >>>> >> >> >> >>> >>> >> >>>> Best regards, >> >> >> >>> >>> >> >>>> Mohammad >> >> >> >>> >>> >> >>> >> >> >> >>> >>> >> >> >> >> >>> >>> >> > -- >> >> >> >>> >>> >> >> >> >> >>> >>> >> > Best, >> >> >> >>> >>> >> > Max Breitmeyer >> >> >> >>> >>> >> > DOIT HPC System Administrator >> >> >> >>> >>> >> >> >> >> >>> >>> >> >> >> >> >>> >>> > >> >> >> >>> >>> > >> >> >> >>> >>> > >> >> >> >>> >>> > -- >> >> >> >>> >>> > Naghmeh Karimi, Ph.D. >> >> >> >>> >>> > Associate Professor >> >> >> >>> >>> > Department of Computer Science and Electrical Engineering >> >> >> >>> >>> > University of Maryland, Baltimore County >> >> >> >>> >>> > Baltimore, MD 21250 >> >> >> >>> >>> > Tel: 410-455-3965 E-mail: nkarimi@umbc.edu <mailto:nkari= mi@umbc.edu> <mailto:nkarimi@umbc.edu <mailto:nkarimi@umbc.edu>> >> >> >> >>> >>> <mailto:nkarimi@umbc.edu <mailto:nkarimi@umbc.edu> <mailto= :nkarimi@umbc.edu <mailto:nkarimi@umbc.edu>>> >> >> >> >>> >>> > Web: http://www.csee.umbc.edu/~nkarimi/ <https://www.goo= gle.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source=3Dgmail-imap&ust= =3D1761922626000000&usg=3DAOvVaw3z3DnkSSm-krms88JtwQY_> <https://www.google= .com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source=3Dgmail-imap&ust=3D1= 761921999000000&usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7 <https://www.google.com/= url?q=3Dhttps://www.google.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%2= 6source%3Dgmail-imap%26ust%3D1761921999000000%26usg%3DAOvVaw35D5VfZwU_gTyQh= CtkbUZ7&source=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw0X7l2TgWCgMe= kNlzKnPsJl>> >> >> >> <https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/= &source=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw23Fk_KFBnlrXC_uhIbU= FIW <https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://w= ww.csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D1761874374000000%2= 6usg%3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&source=3Dgmail-imap&ust=3D1761922626000= 000&usg=3DAOvVaw3wW2UaHLr-IAa0KsbyDZYx> <https://www.google.com/url?q=3Dhtt= ps://www.google.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dg= mail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&sou= rce=3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw05OHRJ3ua_KXM9EHoVYfgE = <https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://www.= google.com/url?q%253Dhttp://www.csee.umbc.edu/~nkarimi/%2526source%253Dgmai= l-imap%2526ust%253D1761874374000000%2526usg%253DAOvVaw23Fk_KFBnlrXC_uhIbUFI= W%26source%3Dgmail-imap%26ust%3D1761921999000000%26usg%3DAOvVaw05OHRJ3ua_KX= M9EHoVYfgE&source=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw17L4xVKtF= JKMIHbtKBKHb5>>> >> >> >> >>> >>> >> >> >> >>> < >> >> >> https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&= source=3Dgmail-imap&ust=3D1761768891000000&usg=3DAOvVaw0xLE02sw5-ueWRHyHyCw= kN <https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://ww= w.csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D1761768891000000%26= usg%3DAOvVaw0xLE02sw5-ueWRHyHyCwkN&source=3Dgmail-imap&ust=3D17619226260000= 00&usg=3DAOvVaw3dBzDvUX_2_EdHlI33ESiV> <https://www.google.com/url?q=3Dhttp= s://www.google.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgm= ail-imap%26ust%3D1761768891000000%26usg%3DAOvVaw0xLE02sw5-ueWRHyHyCwkN&sour= ce=3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw06Mkt_vSjpo8lnqxEqL6dt <= https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://www.g= oogle.com/url?q%253Dhttp://www.csee.umbc.edu/~nkarimi/%2526source%253Dgmail= -imap%2526ust%253D1761768891000000%2526usg%253DAOvVaw0xLE02sw5-ueWRHyHyCwkN= %26source%3Dgmail-imap%26ust%3D1761921999000000%26usg%3DAOvVaw06Mkt_vSjpo8l= nqxEqL6dt&source=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw1U6C0itGlA= PYjVL4qE0kbk>> >> >> >> <https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhtt= p://www.csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D1761768891000= 000%26usg%3DAOvVaw0xLE02sw5-ueWRHyHyCwkN&source=3Dgmail-imap&ust=3D17618743= 74000000&usg=3DAOvVaw3g338pGQqShCgyX8cIt8tV <https://www.google.com/url?q= =3Dhttps://www.google.com/url?q%3Dhttps://www.google.com/url?q%253Dhttp://w= ww.csee.umbc.edu/~nkarimi/%2526source%253Dgmail-imap%2526ust%253D1761768891= 000000%2526usg%253DAOvVaw0xLE02sw5-ueWRHyHyCwkN%26source%3Dgmail-imap%26ust= %3D1761874374000000%26usg%3DAOvVaw3g338pGQqShCgyX8cIt8tV&source=3Dgmail-ima= p&ust=3D1761922626000000&usg=3DAOvVaw0o9hUVJgGzn57VaBi4hS5j> <https://www.g= oogle.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://www.google.com/url= ?q%253Dhttp://www.csee.umbc.edu/~nkarimi/%2526source%253Dgmail-imap%2526ust= %253D1761768891000000%2526usg%253DAOvVaw0xLE02sw5-ueWRHyHyCwkN%26source%3Dg= mail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw3g338pGQqShCgyX8cIt8tV&sou= rce=3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw3ktt5GEiV5miHD1TY9Dut7 = <https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://www.= google.com/url?q%253Dhttps://www.google.com/url?q%25253Dhttp://www.csee.umb= c.edu/~nkarimi/%252526source%25253Dgmail-imap%252526ust%25253D1761768891000= 000%252526usg%25253DAOvVaw0xLE02sw5-ueWRHyHyCwkN%2526source%253Dgmail-imap%= 2526ust%253D1761874374000000%2526usg%253DAOvVaw3g338pGQqShCgyX8cIt8tV%26sou= rce%3Dgmail-imap%26ust%3D1761921999000000%26usg%3DAOvVaw3ktt5GEiV5miHD1TY9D= ut7&source=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw3Zg_27w7hdSjlQ1R= bDrQxL>>> >> >> >> > >> >> >> >> >> >> >>> >> -- Naghmeh Karimi, Ph.D. >> >> >> >>> >> Associate Professor >> >> >> >>> >> Department of Computer Science and Electrical Engineering >> >> >> >>> >> University of Maryland, Baltimore County >> >> >> >>> >> Baltimore, MD 21250 >> >> >> >>> >> Tel: 410-455-3965 E-mail: nkarimi@umbc.edu <mailto:nkarimi@= umbc.edu> <mailto:nkarimi@umbc.edu <mailto:nkarimi@umbc.edu>> >> >> >> >>> >> Web: http://www.csee.umbc.edu/~nkarimi/ <https://www.google= .com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source=3Dgmail-imap&ust=3D1= 761922626000000&usg=3DAOvVaw3z3DnkSSm-krms88JtwQY_> <https://www.google.com= /url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source=3Dgmail-imap&ust=3D17619= 21999000000&usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7 <https://www.google.com/url?= q=3Dhttps://www.google.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26sou= rce%3Dgmail-imap%26ust%3D1761921999000000%26usg%3DAOvVaw35D5VfZwU_gTyQhCtkb= UZ7&source=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw0X7l2TgWCgMekNlz= KnPsJl>> >> >> >> <https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/= &source=3Dgmail-imap&ust=3D1761874374000000&usg=3DAOvVaw23Fk_KFBnlrXC_uhIbU= FIW <https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://w= ww.csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D1761874374000000%2= 6usg%3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&source=3Dgmail-imap&ust=3D1761922626000= 000&usg=3DAOvVaw3wW2UaHLr-IAa0KsbyDZYx> <https://www.google.com/url?q=3Dhtt= ps://www.google.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dg= mail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&sou= rce=3Dgmail-imap&ust=3D1761921999000000&usg=3DAOvVaw05OHRJ3ua_KXM9EHoVYfgE = <https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://www.= google.com/url?q%253Dhttp://www.csee.umbc.edu/~nkarimi/%2526source%253Dgmai= l-imap%2526ust%253D1761874374000000%2526usg%253DAOvVaw23Fk_KFBnlrXC_uhIbUFI= W%26source%3Dgmail-imap%26ust%3D1761921999000000%26usg%3DAOvVaw05OHRJ3ua_KX= M9EHoVYfgE&source=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw17L4xVKtF= JKMIHbtKBKHb5>>> >> >> >> >> >> >> >>> > -- >> >> >> >> >> >> >>> > Best, >> >> >> >>> > Max Breitmeyer >> >> >> >>> > DOIT HPC System Administrator >> >> >> >> >> >> >> -- V/R,Maxwell BreitmeyerUMBC HPCF SpecialistUMBC Observatory IT >> >> >> >> Manager Graduate Student(443) 835-8250 >> >> >> >> >> >> -- >> >> >> >> >> >> Best, >> >> >> Max Breitmeyer >> >> >> DOIT HPC System Administrator >> >> >> >> >> >> >> >> >> >> >> > >> >>=20 >> >> --=20 >> >> Naghmeh Karimi, Ph.D. >> >> Associate Professor >> >> Department of Computer Science and Electrical Engineering >> >> University of Maryland, Baltimore County >> >> Baltimore, MD 21250 >> >> Tel: *410-455-3965* E-mail: *nkarimi@umbc.edu <mailto:nkarimi@umbc.ed= u> <mailto:nkarimi@umbc.edu <mailto:nkarimi@umbc.edu>> <nkarimi@umbc.edu <m= ailto:nkarimi@umbc.edu><mailto:nkarimi@umbc.edu <mailto:nkarimi@umbc.edu>>>* >> >> Web: *http://www.csee.umbc.edu/~nkarimi/ <https://www.google.com/url?= q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source=3Dgmail-imap&ust=3D1761922626= 000000&usg=3DAOvVaw3z3DnkSSm-krms88JtwQY_> <https://www.google.com/url?q=3D= http://www.csee.umbc.edu/~nkarimi/&source=3Dgmail-imap&ust=3D17619219990000= 00&usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7 <https://www.google.com/url?q=3Dhttps= ://www.google.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgma= il-imap%26ust%3D1761921999000000%26usg%3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7&sourc= e=3Dgmail-imap&ust=3D1761922626000000&usg=3DAOvVaw0X7l2TgWCgMekNlzKnPsJl>> >> >> <http://www.csee.umbc.edu/~nkarimi/ <https://www.google.com/url?q=3Dh= ttp://www.csee.umbc.edu/~nkarimi/&source=3Dgmail-imap&ust=3D176192262600000= 0&usg=3DAOvVaw3z3DnkSSm-krms88JtwQY_> <https://www.google.com/url?q=3Dhttp:= //www.csee.umbc.edu/~nkarimi/&source=3Dgmail-imap&ust=3D1761921999000000&us= g=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7 <https://www.google.com/url?q=3Dhttps://ww= w.google.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmail-im= ap%26ust%3D1761921999000000%26usg%3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7&source=3Dg= mail-imap&ust=3D1761922626000000&usg=3DAOvVaw0X7l2TgWCgMekNlzKnPsJl>>>*  "
3296910,72506931,Correspond,DoIT-Research-Computing,2025-10-27 16:27:12.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Skye Jonke,ii69854@umbc.edu,"<html aria-label=3D""message body""><head><meta http-equiv=3D""content-type"" c= ontent=3D""text/html; charset=3Dutf-8""></head><body style=3D""overflow-wrap: = break-word; -webkit-nbsp-mode: space; line-break: after-white-space;"">Max,<= div><br></div><div>I need the help of you (or an expert you can point me to= ) in order to rewrite the scripts, because Cadence and Synopsys tools requi= re a lot of additions to your PATH and LD_PATH, which are usually handled w= ith absolute paths. I=E2=80=99m unsure what the best way to handle that wou= ld be on the HPC cluster. In addition, I need to know the HPC cluster=E2=80= =99s IP range so I can add it to an NFS share.<br id=3D""lineBreakAtBeginnin= gOfMessage""><div> <meta charset=3D""UTF-8""><div dir=3D""auto"" style=3D""caret-color: rgb(0, 0, 0= ); color: rgb(0, 0, 0); letter-spacing: normal; text-align: start; text-ind= ent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; -we= bkit-text-stroke-width: 0px; text-decoration: none; overflow-wrap: break-wo= rd; -webkit-nbsp-mode: space; line-break: after-white-space;"">---</div><div=  dir=3D""auto"" style=3D""caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0); lett= er-spacing: normal; text-align: start; text-indent: 0px; text-transform: no= ne; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px;=  text-decoration: none; overflow-wrap: break-word; -webkit-nbsp-mode: space= ; line-break: after-white-space;"">Skye Jonke<br><br>she/her<br><br>Speciali= st, Linux System Administrator &amp;<span class=3D""Apple-converted-space"">&= nbsp;</span>Lab Technical Support<br></div> </div> <div><br><blockquote type=3D""cite""><div>On Oct 24, 2025, at 10:56, Mohammad=  Ebrahimabadi &lt;e127@umbc.edu&gt; wrote:</div><br class=3D""Apple-intercha= nge-newline""><div><meta charset=3D""UTF-8""><div dir=3D""ltr"" style=3D""caret-c= olor: rgb(0, 0, 0); font-family: Helvetica; font-size: 14px; font-style: no= rmal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; = text-align: start; text-indent: 0px; text-transform: none; white-space: nor= mal; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration: no= ne;"">Thanks Skye,<div><br></div><div>@ Dear Max,<br></div><div>I just cced = Skye from the IT of csee department to help us run software on the chip ser= ver.</div><div><br></div><div>Regards,</div><div>Mohammad</div><div><br></d= iv></div><br style=3D""caret-color: rgb(0, 0, 0); font-family: Helvetica; fo= nt-size: 14px; font-style: normal; font-variant-caps: normal; font-weight: = 400; letter-spacing: normal; text-align: start; text-indent: 0px; text-tran= sform: none; white-space: normal; word-spacing: 0px; -webkit-text-stroke-wi= dth: 0px; text-decoration: none;""><div class=3D""gmail_quote gmail_quote_con= tainer"" style=3D""caret-color: rgb(0, 0, 0); font-family: Helvetica; font-si= ze: 14px; font-style: normal; font-variant-caps: normal; font-weight: 400; = letter-spacing: normal; text-align: start; text-indent: 0px; text-transform= : none; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: = 0px; text-decoration: none;""><div dir=3D""ltr"" class=3D""gmail_attr"">On Fri, = Oct 24, 2025 at 10:51=E2=80=AFAM Skye Jonke via RT &lt;<a href=3D""mailto:UM= BCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquot= e class=3D""gmail_quote"" style=3D""margin: 0px 0px 0px 0.8ex; border-left-wid= th: 1px; border-left-style: solid; border-left-color: rgb(204, 204, 204); p= adding-left: 1ex;"">Ticket &lt;URL:<span class=3D""Apple-converted-space"">&nb= sp;</span><a href=3D""https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Tic= ket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D17619226260= 00000&amp;usg=3DAOvVaw3nNv4uwna-Cy-WTpOccXCH"" rel=3D""noreferrer"" target=3D""= _blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3296910</a><span class= =3D""Apple-converted-space"">&nbsp;</span>&gt;<br><br>Last Update From Ticket= :<br><br>That=E2=80=99s a bit confusing, since hardcoded paths won=E2=80=99= t work, please have Max contact me directly so we can talk about what needs=  to be done to make this work.<span class=3D""Apple-converted-space"">&nbsp;<= /span><br>---<br>Skye Jonke<br><br>she/her<br><br>Specialist, Linux System = Administrator &amp; Lab Technical Support<br><br>&gt; On Oct 24, 2025, at 1= 0:46, Mohammad Ebrahimabadi &lt;<a href=3D""mailto:e127@umbc.edu"" target=3D""= _blank"">e127@umbc.edu</a>&gt; wrote:<br>&gt;<span class=3D""Apple-converted-= space"">&nbsp;</span><br>&gt; Hello Skye,<br>&gt;<span class=3D""Apple-conver= ted-space"">&nbsp;</span><br>&gt; I believe the way that Max told me to run = the script was using the same path that we have in the CSEE department as h= e mounted that path in HPCF. I mean this path:<br>&gt;&nbsp; /umbc/software= /scripts/launch_synopsys_hspice.sh<br>&gt;<span class=3D""Apple-converted-sp= ace"">&nbsp;</span><br>&gt;<span class=3D""Apple-converted-space"">&nbsp;</spa= n><br>&gt; Regards,<br>&gt; Mohammad<br>&gt;<span class=3D""Apple-converted-= space"">&nbsp;</span><br>&gt;<span class=3D""Apple-converted-space"">&nbsp;</s= pan><br>&gt;<span class=3D""Apple-converted-space"">&nbsp;</span><br>&gt;<spa= n class=3D""Apple-converted-space"">&nbsp;</span><br>&gt; On Fri, Oct 24, 202= 5 at 9:42=E2=80=AFAM<span class=3D""Apple-converted-space"">&nbsp;</span><a h= ref=3D""mailto:naghmeh.karimi@umbc.edu"" target=3D""_blank"">naghmeh.karimi@umb= c.edu</a><span class=3D""Apple-converted-space"">&nbsp;</span>&lt;mailto:<a h= ref=3D""mailto:naghmeh.karimi@umbc.edu"" target=3D""_blank"">naghmeh.karimi@umb= c.edu</a>&gt; via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""= _blank"">UMBCHelp@rt.umbc.edu</a><span class=3D""Apple-converted-space"">&nbsp= ;</span>&lt;mailto:<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank= "">UMBCHelp@rt.umbc.edu</a>&gt;&gt; wrote:<br>&gt;&gt; Ticket &lt;URL:<span = class=3D""Apple-converted-space"">&nbsp;</span><a href=3D""https://www.google.= com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;source= =3Dgmail-imap&amp;ust=3D1761922626000000&amp;usg=3DAOvVaw3nNv4uwna-Cy-WTpOc= cXCH"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3296910</a><span class=3D""Apple-converted-space"">&nbsp;</span>= &lt;<a href=3D""https://www.google.com/url?q=3Dhttps://www.google.com/url?q%= 3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgmail-i= map%26ust%3D1761921999000000%26usg%3DAOvVaw099WO1Ar-MqC9UTifVgSrZ&amp;sourc= e=3Dgmail-imap&amp;ust=3D1761922626000000&amp;usg=3DAOvVaw0d5KU99CbDvu6lur7= P0Tnh"" rel=3D""noreferrer"" target=3D""_blank"">https://www.google.com/url?q=3D= https://rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-ima= p&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw099WO1Ar-MqC9UTifVgSrZ</a>&gt;=  &gt;<br>&gt;&gt;<span class=3D""Apple-converted-space"">&nbsp;</span><br>&gt= ;&gt; Last Update From Ticket:<br>&gt;&gt;<span class=3D""Apple-converted-sp= ace"">&nbsp;</span><br>&gt;&gt; That's perfect Skye. I highly appreciate it.=  I ask Mohamamd to respond<br>&gt;&gt; the ticket with the path.<br>&gt;&gt= ;<span class=3D""Apple-converted-space"">&nbsp;</span><br>&gt;&gt; regards,<b= r>&gt;&gt; Naghmeh<br>&gt;&gt;<span class=3D""Apple-converted-space"">&nbsp;<= /span><br>&gt;&gt; On Fri, Oct 24, 2025 at 9:39=E2=80=AFAM Skye Jonke &lt;<= a href=3D""mailto:ii69854@umbc.edu"" target=3D""_blank"">ii69854@umbc.edu</a><s= pan class=3D""Apple-converted-space"">&nbsp;</span>&lt;mailto:<a href=3D""mail= to:ii69854@umbc.edu"" target=3D""_blank"">ii69854@umbc.edu</a>&gt;&gt; wrote:<= br>&gt;&gt;<span class=3D""Apple-converted-space"">&nbsp;</span><br>&gt;&gt; = &gt; Its not possible to use the existing scripts in a situation where the<= br>&gt;&gt; &gt; files aren=E2=80=99t mounted in /umbc/software, but I can = write some scripts<br>&gt;&gt; &gt; specific to this setup to this situatio= n with a bit more information. Where<br>&gt;&gt; &gt; are the script files = located on the HPC? If you aren=E2=80=99t sure, you can get<br>&gt;&gt; &gt= ; the full path by running =E2=80=9Crealpath myfile.sh=E2=80=9D (replacing = myfile.sh with one<br>&gt;&gt; &gt; of the cadence scripts, of course)<br>&= gt;&gt; &gt; ---<br>&gt;&gt; &gt; Skye Jonke<br>&gt;&gt; &gt;<br>&gt;&gt; &= gt; she/her<br>&gt;&gt; &gt;<br>&gt;&gt; &gt; Specialist, Linux System Admi= nistrator &amp; Lab Technical Support<br>&gt;&gt; &gt;<br>&gt;&gt; &gt; On = Oct 23, 2025, at 21:32, Naghmeh Karimi &lt;<a href=3D""mailto:nkarimi@umbc.e= du"" target=3D""_blank"">nkarimi@umbc.edu</a><span class=3D""Apple-converted-sp= ace"">&nbsp;</span>&lt;mailto:<a href=3D""mailto:nkarimi@umbc.edu"" target=3D""= _blank"">nkarimi@umbc.edu</a>&gt;&gt; wrote:<br>&gt;&gt; &gt;<br>&gt;&gt; &g= t; Hi Max, thanks for getting back to us.<br>&gt;&gt; &gt;<br>&gt;&gt; &gt;=  Hi Skye, we need your help here to find the file locations. Could you<br>&= gt;&gt; &gt; please help to resolve this issue?<br>&gt;&gt; &gt;<br>&gt;&gt= ; &gt; Thanks a lot<br>&gt;&gt; &gt; Naghmeh<br>&gt;&gt; &gt;<br>&gt;&gt; &= gt; Naghmeh Karimi, Ph.D.<br>&gt;&gt; &gt; Associate Professor<br>&gt;&gt; = &gt; Department of Computer Science and Electrical Engineering<br>&gt;&gt; = &gt; University of Maryland, Baltimore County<br>&gt;&gt; &gt; Baltimore, M= D 21250<br>&gt;&gt; &gt; Tel: *410-455-3965* E-mail: *<a href=3D""mailto:nka= rimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a><span class=3D""Apple-c= onverted-space"">&nbsp;</span>&lt;mailto:<a href=3D""mailto:nkarimi@umbc.edu""=  target=3D""_blank"">nkarimi@umbc.edu</a>&gt; &lt;<a href=3D""mailto:nkarimi@u= mbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a>&lt;mailto:<a href=3D""mailto= :nkarimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a>&gt;&gt;*<br>&gt;&= gt; &gt; Web: *<a href=3D""https://www.google.com/url?q=3Dhttp://www.csee.um= bc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761922626000000&amp;usg= =3DAOvVaw3z3DnkSSm-krms88JtwQY_"" rel=3D""noreferrer"" target=3D""_blank"">http:= //www.csee.umbc.edu/~nkarimi/</a><span class=3D""Apple-converted-space"">&nbs= p;</span>&lt;<a href=3D""https://www.google.com/url?q=3Dhttps://www.google.c= om/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3= D1761921999000000%26usg%3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7&amp;source=3Dgmail-i= map&amp;ust=3D1761922626000000&amp;usg=3DAOvVaw0X7l2TgWCgMekNlzKnPsJl"" rel= =3D""noreferrer"" target=3D""_blank"">https://www.google.com/url?q=3Dhttp://www= .csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000= &amp;usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7</a>&gt;<br>&gt;&gt; &gt; &lt;<a hre= f=3D""https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://w= ww.csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D1761874374000000%2= 6usg%3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&amp;source=3Dgmail-imap&amp;ust=3D17619= 22626000000&amp;usg=3DAOvVaw3wW2UaHLr-IAa0KsbyDZYx"" rel=3D""noreferrer"" targ= et=3D""_blank"">https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkar= imi/&amp;source=3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw23F= k_KFBnlrXC_uhIbUFIW</a><span class=3D""Apple-converted-space"">&nbsp;</span>&= lt;<a href=3D""https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3= Dhttps://www.google.com/url?q%253Dhttp://www.csee.umbc.edu/~nkarimi/%2526so= urce%253Dgmail-imap%2526ust%253D1761874374000000%2526usg%253DAOvVaw23Fk_KFB= nlrXC_uhIbUFIW%26source%3Dgmail-imap%26ust%3D1761921999000000%26usg%3DAOvVa= w05OHRJ3ua_KXM9EHoVYfgE&amp;source=3Dgmail-imap&amp;ust=3D1761922626000000&= amp;usg=3DAOvVaw17L4xVKtFJKMIHbtKBKHb5"" rel=3D""noreferrer"" target=3D""_blank= "">https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://www.= csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D1761874374000000%26us= g%3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&amp;source=3Dgmail-imap&amp;ust=3D17619219= 99000000&amp;usg=3DAOvVaw05OHRJ3ua_KXM9EHoVYfgE</a>&gt;&gt;*<br>&gt;&gt; &g= t;<br>&gt;&gt; &gt; On Thu, Oct 23, 2025, 12:22=E2=80=AFPM Max Breitmeyer v= ia RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHel= p@rt.umbc.edu</a><span class=3D""Apple-converted-space"">&nbsp;</span>&lt;mai= lto:<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.u= mbc.edu</a>&gt;&gt;<br>&gt;&gt; &gt; wrote:<br>&gt;&gt; &gt;<br>&gt;&gt; &g= t;&gt; Ticket &lt;URL:<span class=3D""Apple-converted-space"">&nbsp;</span><a=  href=3D""https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.= html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761922626000000&amp;us= g=3DAOvVaw3nNv4uwna-Cy-WTpOccXCH"" rel=3D""noreferrer"" target=3D""_blank"">http= s://rt.umbc.edu/Ticket/Display.html?id=3D3296910</a><span class=3D""Apple-co= nverted-space"">&nbsp;</span>&lt;<a href=3D""https://www.google.com/url?q=3Dh= ttps://www.google.com/url?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%25= 3D3296910%26source%3Dgmail-imap%26ust%3D1761921999000000%26usg%3DAOvVaw099W= O1Ar-MqC9UTifVgSrZ&amp;source=3Dgmail-imap&amp;ust=3D1761922626000000&amp;u= sg=3DAOvVaw0d5KU99CbDvu6lur7P0Tnh"" rel=3D""noreferrer"" target=3D""_blank"">htt= ps://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D32= 96910&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw09= 9WO1Ar-MqC9UTifVgSrZ</a>&gt;<br>&gt;&gt; &gt;&gt; &lt;<a href=3D""https://ww= w.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.edu/Tic= ket/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D1761874374000= 000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&amp;source=3Dgmail-imap&amp;ust=3D= 1761922626000000&amp;usg=3DAOvVaw1DVHQWapzV68VEop_7Hjjv"" rel=3D""noreferrer""=  target=3D""_blank"">https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticke= t/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761874374000= 000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7</a><span class=3D""Apple-converte= d-space"">&nbsp;</span>&lt;<a href=3D""https://www.google.com/url?q=3Dhttps:/= /www.google.com/url?q%3Dhttps://www.google.com/url?q%253Dhttps://rt.umbc.ed= u/Ticket/Display.html?id%25253D3296910%2526source%253Dgmail-imap%2526ust%25= 3D1761874374000000%2526usg%253DAOvVaw3ImziGtKZ-9j9JFW8J65w7%26source%3Dgmai= l-imap%26ust%3D1761921999000000%26usg%3DAOvVaw3WPs04yO5gQkOFpjU6my-g&amp;so= urce=3Dgmail-imap&amp;ust=3D1761922626000000&amp;usg=3DAOvVaw3cp5M8Pk5ufwr6= CL9J1XNN"" rel=3D""noreferrer"" target=3D""_blank"">https://www.google.com/url?q= =3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?i= d%253D3296910%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw= 3ImziGtKZ-9j9JFW8J65w7&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&a= mp;usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g</a>&gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;= <br>&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; Last Update From Ticket:<br>&gt;= &gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; Hi Mohammad,<br>&gt;&gt; &gt;&gt;<br>&gt= ;&gt; &gt;&gt; According to the error message, it's having an issue finding=  another file<br>&gt;&gt; &gt;&gt; in<br>&gt;&gt; &gt;&gt; the directory. T= his appears to be an issue due to the mounting locations<br>&gt;&gt; &gt;&g= t; being<br>&gt;&gt; &gt;&gt; different. I would point you to the previous = email where I said that any<br>&gt;&gt; &gt;&gt; software with hard-coded l= ocations would not work, and that software<br>&gt;&gt; &gt;&gt; compatibili= ty is not guaranteed. I would recommend working with Skye on<br>&gt;&gt; &g= t;&gt; porting<br>&gt;&gt; &gt;&gt; what is in the directory to your local = research directories on chip.<br>&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; On = Thu Oct 23 12:09:34 2025, NQ23652 wrote:<br>&gt;&gt; &gt;&gt;<br>&gt;&gt; &= gt;&gt; &gt; Yes you are right, now I could see all shell files. However st= ill I have<br>&gt;&gt; &gt;&gt; &gt; problem when I run the sh file.<br>&gt= ;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt; [e127@chip-login2 scripts]$ sh lau= nch_synopsys_hspice.sh<br>&gt;&gt; &gt;&gt; &gt; launch_synopsys_hspice.sh:=  line 3:<br>&gt;&gt; &gt;&gt; &gt; /umbc/software/scripts/env_synopsys_hspi= ce.sh: No such file or directory<br>&gt;&gt; &gt;&gt; &gt; [e127@chip-login= 2 scripts]$<br>&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt; Would you please=  take a look to it.<br>&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt; Regards,= <br>&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt; Mohammad<br>&gt;&gt; &gt;&g= t;<br>&gt;&gt; &gt;&gt; &gt; On Thu Oct 23 11:59:17 2025, OL73413 wrote:<br= >&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt; Hi Mohammad,<br>&gt;&gt; &= gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt; A good point. Chip uses a service cal= led ""autofs"" to mount and unmount<br>&gt;&gt; &gt;&gt; &gt;&gt; directories=  that aren't being actively used. This helps maintain the<br>&gt;&gt; &gt;&= gt; &gt;&gt; login nodes integrity. If you see in my screenshot below, the = directory<br>&gt;&gt; &gt;&gt; &gt;&gt; appears to be empty, but when I cd = to the csee directory where your<br>&gt;&gt; &gt;&gt; &gt;&gt; software is = mounted, it is all available. Backing out of the directory<br>&gt;&gt; &gt;= &gt; &gt;&gt; and ls'ing the directory also shows that the directory is now=  mounted<br>&gt;&gt; &gt;&gt; &gt;&gt; and available.<br>&gt;&gt; &gt;&gt;<= br>&gt;&gt; &gt;&gt; &gt;&gt; On Thu, Oct 23, 2025 at 11:41 AM Mohammad Ebr= ahimabadi via RT<br>&gt;&gt; &gt;&gt; &gt;&gt; &lt;<a href=3D""mailto:UMBCHe= lp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a><span class=3D""Ap= ple-converted-space"">&nbsp;</span>&lt;mailto:<a href=3D""mailto:UMBCHelp@rt.= umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt;&gt; wrote:<br>&gt;= &gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; Ticket &lt;URL:<span class= =3D""Apple-converted-space"">&nbsp;</span><a href=3D""https://www.google.com/u= rl?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgm= ail-imap&amp;ust=3D1761922626000000&amp;usg=3DAOvVaw3nNv4uwna-Cy-WTpOccXCH""=  rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.ht= ml?id=3D3296910</a><span class=3D""Apple-converted-space"">&nbsp;</span>&lt;<= a href=3D""https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhtt= ps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgmail-imap%2= 6ust%3D1761921999000000%26usg%3DAOvVaw099WO1Ar-MqC9UTifVgSrZ&amp;source=3Dg= mail-imap&amp;ust=3D1761922626000000&amp;usg=3DAOvVaw0d5KU99CbDvu6lur7P0Tnh= "" rel=3D""noreferrer"" target=3D""_blank"">https://www.google.com/url?q=3Dhttps= ://rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp= ;ust=3D1761921999000000&amp;usg=3DAOvVaw099WO1Ar-MqC9UTifVgSrZ</a>&gt;<br>&= gt;&gt; &gt;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttps://www.= google.com/url?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%2= 6source%3Dgmail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw3ImziGtKZ-9j9JF= W8J65w7&amp;source=3Dgmail-imap&amp;ust=3D1761922626000000&amp;usg=3DAOvVaw= 1DVHQWapzV68VEop_7Hjjv"" rel=3D""noreferrer"" target=3D""_blank"">https://www.go= ogle.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;s= ource=3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9= JFW8J65w7</a><span class=3D""Apple-converted-space"">&nbsp;</span>&lt;<a href= =3D""https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://w= ww.google.com/url?q%253Dhttps://rt.umbc.edu/Ticket/Display.html?id%25253D32= 96910%2526source%253Dgmail-imap%2526ust%253D1761874374000000%2526usg%253DAO= vVaw3ImziGtKZ-9j9JFW8J65w7%26source%3Dgmail-imap%26ust%3D1761921999000000%2= 6usg%3DAOvVaw3WPs04yO5gQkOFpjU6my-g&amp;source=3Dgmail-imap&amp;ust=3D17619= 22626000000&amp;usg=3DAOvVaw3cp5M8Pk5ufwr6CL9J1XNN"" rel=3D""noreferrer"" targ= et=3D""_blank"">https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3= Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgmail-im= ap%26ust%3D1761874374000000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&amp;source= =3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw3WPs04yO5gQkOFpjU6= my-g</a>&gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;<br>&gt;&gt; &gt;&gt;<br>&gt;&gt;=  &gt;&gt; &gt;&gt;&gt; Comment From Ticket:<br>&gt;&gt; &gt;&gt; &gt;&gt;&g= t; Thank you Max!<br>&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; Ac= tually we have still problem for accessing that directory. I<br>&gt;&gt; &g= t;&gt; &gt;&gt;&gt; uploaded two<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; screensh= ots that shows what we can see in the department servers<br>&gt;&gt; &gt;&g= t; &gt;&gt;&gt; and what we<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; can see in CH= IP.<br>&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; For example by r= unning this shell file we can launch HSPICE<br>&gt;&gt; &gt;&gt; &gt;&gt;&g= t; software in the<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; department server:<br>= &gt;&gt; &gt;&gt; &gt;&gt;&gt; /umbc/software/scripts/launch_synopsys_hspic= e.sh<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; But it is not still accessible on CH= IP.<br>&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; Regards,<br>&gt;= &gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; Mohammad<br>&gt;&gt; &gt;&g= t;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; On Thu Oct 23 10:54:59 2025, OL73413 w= rote:<br>&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Hi Nahmeh= ,<br>&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Apologies =E2= =80=94 I meant to update you yesterday after completing<br>&gt;&gt; &gt;&gt= ; &gt;&gt;&gt; this, but I<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; was pulle= d into a long meeting. The software is now available on<br>&gt;&gt; &gt;&gt= ; &gt;&gt;&gt; chip at:<br>&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&= gt; &gt; /umbc/software/csee<br>&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;= &gt;&gt; &gt; Please note that if the software contains any hardcoded file<= br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; paths, it may<br>&gt;&gt; &gt;&gt; &gt;&g= t;&gt; &gt; not function correctly. Unfortunately, we don=E2=80=99t have co= ntrol over<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; those<br>&gt;&gt; &gt;&gt; &gt= ;&gt;&gt; &gt; cases.<br>&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt= ; &gt; Also, just to clarify, Skye and I work in separate departments.<br>&= gt;&gt; &gt;&gt; &gt;&gt;&gt; Similar to<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; = &gt; how I don=E2=80=99t have administrative access to the CSEE department<= br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; servers<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt;=  &gt; (which include the software directory), Skye is not an<br>&gt;&gt; &g= t;&gt; &gt;&gt;&gt; administrator on<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;=  chip.<br>&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Please l= et me know if you encounter any issues accessing the<br>&gt;&gt; &gt;&gt; &= gt;&gt;&gt; software on<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; the server. = If there are problems running the software, I=E2=80=99ll<br>&gt;&gt; &gt;&g= t; &gt;&gt;&gt; coordinate<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; with Skye=  to troubleshoot, but since we don=E2=80=99t maintain this<br>&gt;&gt; &gt;= &gt; &gt;&gt;&gt; software, we<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; can= =E2=80=99t guarantee full compatibility on our systems.<br>&gt;&gt; &gt;&gt= ;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; On Thu Oct 23 10:36:41 2025,<span = class=3D""Apple-converted-space"">&nbsp;</span><a href=3D""mailto:naghmeh.kari= mi@umbc.edu"" target=3D""_blank"">naghmeh.karimi@umbc.edu</a><span class=3D""Ap= ple-converted-space"">&nbsp;</span>&lt;mailto:<a href=3D""mailto:naghmeh.kari= mi@umbc.edu"" target=3D""_blank"">naghmeh.karimi@umbc.edu</a>&gt; wrote:<br>&g= t;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Hi Skye and Max,=  Could you please help on mounting this today so<br>&gt;&gt; &gt;&gt; &gt;&= gt;&gt; we can<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; use them as we ha= ve a deadline? Thanks,Naghmeh Karimi On Wed,<br>&gt;&gt; &gt;&gt; &gt;&gt;&= gt; Oct 22,<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; 2025 at 4:22 PM Skye=  Jonke via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank""= >UMBCHelp@rt.umbc.edu</a><span class=3D""Apple-converted-space"">&nbsp;</span= >&lt;mailto:<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCH= elp@rt.umbc.edu</a>&gt;&gt; wrote:<br>&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt= ; &gt;&gt;&gt; &gt;&gt;&gt; Ticket &lt;URL:<span class=3D""Apple-converted-s= pace"">&nbsp;</span><a href=3D""https://www.google.com/url?q=3Dhttps://rt.umb= c.edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D17= 61922626000000&amp;usg=3DAOvVaw3nNv4uwna-Cy-WTpOccXCH"" rel=3D""noreferrer"" t= arget=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3296910</a><s= pan class=3D""Apple-converted-space"">&nbsp;</span>&lt;<a href=3D""https://www= .google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.edu/Tick= et/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D17619219990000= 00%26usg%3DAOvVaw099WO1Ar-MqC9UTifVgSrZ&amp;source=3Dgmail-imap&amp;ust=3D1= 761922626000000&amp;usg=3DAOvVaw0d5KU99CbDvu6lur7P0Tnh"" rel=3D""noreferrer"" = target=3D""_blank"">https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket= /Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D17619219990000= 00&amp;usg=3DAOvVaw099WO1Ar-MqC9UTifVgSrZ</a>&gt;<br>&gt;&gt; &gt;&gt; &lt;= <a href=3D""https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dht= tps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgmail-imap%= 26ust%3D1761874374000000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&amp;source=3D= gmail-imap&amp;ust=3D1761922626000000&amp;usg=3DAOvVaw1DVHQWapzV68VEop_7Hjj= v"" rel=3D""noreferrer"" target=3D""_blank"">https://www.google.com/url?q=3Dhttp= s://rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&am= p;ust=3D1761874374000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7</a><span cl= ass=3D""Apple-converted-space"">&nbsp;</span>&lt;<a href=3D""https://www.googl= e.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://www.google.com/url?q%2= 53Dhttps://rt.umbc.edu/Ticket/Display.html?id%25253D3296910%2526source%253D= gmail-imap%2526ust%253D1761874374000000%2526usg%253DAOvVaw3ImziGtKZ-9j9JFW8= J65w7%26source%3Dgmail-imap%26ust%3D1761921999000000%26usg%3DAOvVaw3WPs04yO= 5gQkOFpjU6my-g&amp;source=3Dgmail-imap&amp;ust=3D1761922626000000&amp;usg= =3DAOvVaw3cp5M8Pk5ufwr6CL9J1XNN"" rel=3D""noreferrer"" target=3D""_blank"">https= ://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.ed= u/Ticket/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D17618743= 74000000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&amp;source=3Dgmail-imap&amp;u= st=3D1761921999000000&amp;usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g</a>&gt;&gt;<br= >&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;<br>&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&= gt; &gt;&gt;&gt; &gt;&gt;&gt; Last Update From Ticket:<br>&gt;&gt; &gt;&gt;= <br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Mohammad,<br>&gt;&gt; &gt;&= gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; So long as you continue = to access them through<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; /umbc= /software/scripts, they should remain up to date and you<br>&gt;&gt; &gt;&g= t; &gt;&gt;&gt; will<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; see any=  new scripts. If you copy them to a local directory on<br>&gt;&gt; &gt;&gt;=  &gt;&gt;&gt; the<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; HPC cluste= r, they will not be updated.<br>&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;= &gt;&gt; &gt;&gt;&gt; ---<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Sk= ye Jonke<br>&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt= ; she/her<br>&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&g= t; Specialist, Linux System Administrator &amp; Lab Technical Support<br>&g= t;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; On Oct = 22, 2025, at 16:14, Naghmeh Karimi &lt;<a href=3D""mailto:nkarimi@umbc.edu"" = target=3D""_blank"">nkarimi@umbc.edu</a><span class=3D""Apple-converted-space""= >&nbsp;</span>&lt;mailto:<a href=3D""mailto:nkarimi@umbc.edu"" target=3D""_bla= nk"">nkarimi@umbc.edu</a>&gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;= &gt; wrote:<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br>&gt;&gt;=  &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Hi Max, Roy, Skye,<br>&gt;&gt; &gt= ;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;= &gt;&gt; &gt; Thank you all for your help.<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt= ; &gt;&gt;&gt; &gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; I a= dded Skye here as she is our IT exert on adding the<br>&gt;&gt; &gt;&gt; &g= t;&gt;&gt; capability<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; of run= ning synopsis tools (Hspice) in servers.<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; = &gt;&gt;&gt; &gt; Skye we want to use the HPC cluster to run hspice an othe= r<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; synopsys tools. Could you = please help on this?<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br= >&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Thanks,<br>&gt;&gt; &gt;&= gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Naghmeh<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt= ; &gt;&gt;&gt; &gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; On = Wed, Oct 22, 2025 at 2:06 PM Mohammad Ebrahimabadi via RT<br>&gt;&gt; &gt;&= gt; &gt;&gt;&gt; &gt;&gt;&gt; &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" t= arget=3D""_blank"">UMBCHelp@rt.umbc.edu</a><span class=3D""Apple-converted-spa= ce"">&nbsp;</span>&lt;mailto:<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target= =3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt; &lt;mailto:<a href=3D""mailto:UMBCH= elp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&lt;mailto:<a hr= ef=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</= a>&gt;&gt;&gt; wrote:<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&g= t; Ticket &lt;URL:<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt;<span class=3D""Apple-co= nverted-space"">&nbsp;</span><a href=3D""https://www.google.com/url?q=3Dhttps= ://rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp= ;ust=3D1761922626000000&amp;usg=3DAOvVaw3nNv4uwna-Cy-WTpOccXCH"" rel=3D""nore= ferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3296= 910</a><span class=3D""Apple-converted-space"">&nbsp;</span>&lt;<a href=3D""ht= tps://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc= .edu/Ticket/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D17619= 21999000000%26usg%3DAOvVaw099WO1Ar-MqC9UTifVgSrZ&amp;source=3Dgmail-imap&am= p;ust=3D1761922626000000&amp;usg=3DAOvVaw0d5KU99CbDvu6lur7P0Tnh"" rel=3D""nor= eferrer"" target=3D""_blank"">https://www.google.com/url?q=3Dhttps://rt.umbc.e= du/Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D17619= 21999000000&amp;usg=3DAOvVaw099WO1Ar-MqC9UTifVgSrZ</a>&gt;<br>&gt;&gt; &gt;= &gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttps://www.google.com/u= rl?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgm= ail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&amp;= source=3Dgmail-imap&amp;ust=3D1761922626000000&amp;usg=3DAOvVaw1DVHQWapzV68= VEop_7Hjjv"" rel=3D""noreferrer"" target=3D""_blank"">https://www.google.com/url= ?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgmai= l-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7</a= ><span class=3D""Apple-converted-space"">&nbsp;</span>&lt;<a href=3D""https://= www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://www.google.co= m/url?q%253Dhttps://rt.umbc.edu/Ticket/Display.html?id%25253D3296910%2526so= urce%253Dgmail-imap%2526ust%253D1761874374000000%2526usg%253DAOvVaw3ImziGtK= Z-9j9JFW8J65w7%26source%3Dgmail-imap%26ust%3D1761921999000000%26usg%3DAOvVa= w3WPs04yO5gQkOFpjU6my-g&amp;source=3Dgmail-imap&amp;ust=3D1761922626000000&= amp;usg=3DAOvVaw3cp5M8Pk5ufwr6CL9J1XNN"" rel=3D""noreferrer"" target=3D""_blank= "">https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.= umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D1= 761874374000000%26usg%3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&amp;source=3Dgmail-ima= p&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g</a>&gt;= &gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt;<br>&gt;&gt; &gt;&gt; &g= t;&gt;&gt; &lt;<br>&gt;&gt; &gt;&gt;<span class=3D""Apple-converted-space"">&= nbsp;</span><a href=3D""https://www.google.com/url?q=3Dhttps://www.google.co= m/url?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3= Dgmail-imap%26ust%3D1761768891000000%26usg%3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW&a= mp;source=3Dgmail-imap&amp;ust=3D1761922626000000&amp;usg=3DAOvVaw2ycbat305= sOpELNsl0jFQS"" rel=3D""noreferrer"" target=3D""_blank"">https://www.google.com/= url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;source=3Dg= mail-imap&amp;ust=3D1761768891000000&amp;usg=3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW= </a><span class=3D""Apple-converted-space"">&nbsp;</span>&lt;<a href=3D""https= ://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://www.google= .com/url?q%253Dhttps://rt.umbc.edu/Ticket/Display.html?id%25253D3296910%252= 6source%253Dgmail-imap%2526ust%253D1761768891000000%2526usg%253DAOvVaw2jxB3= nArHBD7-ij9ZN8ZsW%26source%3Dgmail-imap%26ust%3D1761921999000000%26usg%3DAO= vVaw04hQC2SRO0y4BoVR0LFNp_&amp;source=3Dgmail-imap&amp;ust=3D17619226260000= 00&amp;usg=3DAOvVaw3dlpNnBtzA6koeM2xXYQa2"" rel=3D""noreferrer"" target=3D""_bl= ank"">https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://= rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3Dgmail-imap%26ust%= 3D1761768891000000%26usg%3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW&amp;source=3Dgmail-= imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw04hQC2SRO0y4BoVR0LFNp_</a>&= gt;<br>&gt;&gt; &gt;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttp= s://www.google.com/url?q%3Dhttps://www.google.com/url?q%253Dhttps://rt.umbc= .edu/Ticket/Display.html?id%25253D3296910%2526source%253Dgmail-imap%2526ust= %253D1761768891000000%2526usg%253DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW%26source%3Dg= mail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw0fLylAsBAoQ6LgqY79I9-d&amp= ;source=3Dgmail-imap&amp;ust=3D1761922626000000&amp;usg=3DAOvVaw3ukvs4os21e= QNpVKHlHspB"" rel=3D""noreferrer"" target=3D""_blank"">https://www.google.com/ur= l?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.edu/Ticket/Display.htm= l?id%253D3296910%26source%3Dgmail-imap%26ust%3D1761768891000000%26usg%3DAOv= Vaw2jxB3nArHBD7-ij9ZN8ZsW&amp;source=3Dgmail-imap&amp;ust=3D176187437400000= 0&amp;usg=3DAOvVaw0fLylAsBAoQ6LgqY79I9-d</a>&lt;<a href=3D""https://www.goog= le.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://www.google.com/url?q%= 253Dhttps://www.google.com/url?q%25253Dhttps://rt.umbc.edu/Ticket/Display.h= tml?id%2525253D3296910%252526source%25253Dgmail-imap%252526ust%25253D176176= 8891000000%252526usg%25253DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW%2526source%253Dgmai= l-imap%2526ust%253D1761874374000000%2526usg%253DAOvVaw0fLylAsBAoQ6LgqY79I9-= d%26source%3Dgmail-imap%26ust%3D1761921999000000%26usg%3DAOvVaw3pQOXM-6FD6k= t0kA6ujsVN&amp;source=3Dgmail-imap&amp;ust=3D1761922626000000&amp;usg=3DAOv= Vaw1Emr2VJBRhkK6VbbDl54fR"" rel=3D""noreferrer"" target=3D""_blank"">https://www= .google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://www.google.com/u= rl?q%253Dhttps://rt.umbc.edu/Ticket/Display.html?id%25253D3296910%2526sourc= e%253Dgmail-imap%2526ust%253D1761768891000000%2526usg%253DAOvVaw2jxB3nArHBD= 7-ij9ZN8ZsW%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw0f= LylAsBAoQ6LgqY79I9-d&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&amp= ;usg=3DAOvVaw3pQOXM-6FD6kt0kA6ujsVN</a>&gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;<b= r>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br>&gt;&gt; &gt;&gt; &gt= ;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&= gt; &gt;&gt; Last Update From Ticket:<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt= ;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; = Hi Max,<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt;=  &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Thanks for your help! Let=E2= =80=99s proceed based on your<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; suggestion,= <br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; hopefully it<br>&gt;&gt; &g= t;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; works as expected.<br>&gt;&gt; &g= t;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt;=  &gt;&gt;&gt; &gt;&gt; I just have one quick question: since the IT team in=  the<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; department can modify<b= r>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; or add new shell scr= ipts in that directory, will we also<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; have= <br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; access to those<br>&gt;&gt;=  &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; updated or newly added scripts=  from the mounted directory on<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&g= t; HPCF?<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt= ; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Thanks again,<br>&gt;&gt; &gt= ;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Mohammad<br>&gt;&gt; &gt;&gt; &gt;= &gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&g= t; &gt;&gt; On Wed Oct 22 14:01:32 2025, OL73413 wrote:<br>&gt;&gt; &gt;&gt= ; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;= &gt;&gt; &gt;&gt; &gt; Hi Mohammad,<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&= gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &g= t; We can get the software mounted for you on chip, but it<br>&gt;&gt; &gt;= &gt; &gt;&gt;&gt; will<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; have = to be<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; read-onl= y as we won't be able to manage who can make<br>&gt;&gt; &gt;&gt; &gt;&gt;&= gt; changes<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; to it<br>&gt;&gt= ; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; otherwise.<br>&gt;&gt; &= gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt= ; &gt;&gt;&gt; &gt;&gt; &gt; As a reminder, the machines on that run on chi= p are not<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; the<br>&gt;&gt; &gt;&gt; &gt;&g= t;&gt; &gt;&gt;&gt; same as those<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt= ;&gt; &gt;&gt; &gt; run by the CSEE department, so we can't guarantee that<= br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; these<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &= gt;&gt;&gt; will run the<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt= ;&gt; &gt; same way or as effectively.<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &g= t;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;=  &gt; If that's all ok with you, please give me some time to get<br>&gt;&gt= ; &gt;&gt; &gt;&gt;&gt; the<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; = directory<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; moun= ted.<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &g= t;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; On Wed Oct 22 13:26:05 2025,=  NQ23652 wrote:<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>= &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thanks Tartel= a for your help to resolve my issue!<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;= &gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &= gt;&gt; Actually it seems all server in CSEE department have<br>&gt;&gt; &g= t;&gt; &gt;&gt;&gt; access<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; t= o that<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; pat= h. When I ran the command that you gave me I got the<br>&gt;&gt; &gt;&gt; &= gt;&gt;&gt; &gt;&gt;&gt; following<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&g= t;&gt; &gt;&gt; &gt;&gt; report:<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;= &gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&= gt; mohammad@alborz:~$ df -h /umbc/software<br>&gt;&gt; &gt;&gt; &gt;&gt;&g= t; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Filesystem Size Used Avail Use% Mounted o= n<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;<span cla= ss=3D""Apple-converted-space"">&nbsp;</span><a href=3D""https://www.google.com= /url?q=3Dhttp://nfs.iss.rs.umbc.edu&amp;source=3Dgmail-imap&amp;ust=3D17619= 22626000000&amp;usg=3DAOvVaw15vrKX_7H4XPbaxCxBRx12"" rel=3D""noreferrer"" targ= et=3D""_blank"">nfs.iss.rs.umbc.edu</a><span class=3D""Apple-converted-space"">= &nbsp;</span>&lt;<a href=3D""https://www.google.com/url?q=3Dhttp://nfs.iss.r= s.umbc.edu/&amp;source=3Dgmail-imap&amp;ust=3D1761922626000000&amp;usg=3DAO= vVaw2-3-JimnQAB975lph5arXG"" rel=3D""noreferrer"" target=3D""_blank"">http://nfs= .iss.rs.umbc.edu/</a>&gt;:/ifs/data/csee/software 2.5T 2.2T<br>&gt;&gt; &gt= ;&gt; &gt;&gt;&gt; 326G<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; 88%<= br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; /umbc/soft= ware<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &g= t;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; mohammad@alborz:~$ mount=  | grep /umbc/software<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&= gt; &gt;&gt;<span class=3D""Apple-converted-space"">&nbsp;</span><a href=3D""h= ttps://www.google.com/url?q=3Dhttp://nfs.iss.rs.umbc.edu&amp;source=3Dgmail= -imap&amp;ust=3D1761922626000000&amp;usg=3DAOvVaw15vrKX_7H4XPbaxCxBRx12"" re= l=3D""noreferrer"" target=3D""_blank"">nfs.iss.rs.umbc.edu</a><span class=3D""Ap= ple-converted-space"">&nbsp;</span>&lt;<a href=3D""https://www.google.com/url= ?q=3Dhttp://nfs.iss.rs.umbc.edu/&amp;source=3Dgmail-imap&amp;ust=3D17619226= 26000000&amp;usg=3DAOvVaw2-3-JimnQAB975lph5arXG"" rel=3D""noreferrer"" target= =3D""_blank"">http://nfs.iss.rs.umbc.edu/</a>&gt;:/ifs/data/csee/software on<= br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; /umbc/software<br>&gt;&gt; &gt;&gt; &gt;&= gt;&gt; &gt;&gt;&gt; type nfs<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt= ; &gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt;<br>&gt;&= gt; &gt;&gt; &gt;&gt;&gt;<br>&gt;&gt; &gt;&gt; (rw,relatime,vers=3D3,rsize= =3D131072,wsize=3D524288,namlen=3D255,acregmin=3D15,acregmax=3D15,acdirmin= =3D15,acdirmax=3D15,hard,noacl,proto=3Dtcp,timeo=3D600,retrans=3D2,sec=3Dsy= s,mountaddr=3D10.2.44.60,mountvers=3D3,mountport=3D300,mountproto=3Dtcp,loc= al_lock=3Dnone,addr=3D10.2.44.60)<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt= ;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;= &gt; Please let me know if you need other information.<br>&gt;&gt; &gt;&gt;=  &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&= gt;&gt; &gt;&gt; &gt;&gt; Regards,<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&g= t;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt= ;&gt; Mohammad<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&= gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; On Wed Oct 22 = 13:15:12 2025, HK41259 wrote:<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt= ; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;= &gt; Hello,<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;= &gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Thanks for re= aching out. Could you clarify which<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; speci= fic<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; machine<br>&gt;&gt; &gt;= &gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; or server you=E2=80=99= re referring to when you mention the<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;= &gt;&gt; =E2=80=9Cdepartmental<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&g= t; &gt;&gt; &gt;&gt;&gt; server=E2=80=9D?<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt;=  &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&= gt; &gt;&gt;&gt; The /umbc/software/scripts/ directory isn=E2=80=99t access= ible<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; from<br>&gt;&gt; &gt;&gt; &gt;&gt;&g= t; &gt;&gt;&gt; HPCF<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt= ; &gt;&gt;&gt; (e.g., Chip) by default =E2=80=94 those systems have separat= e<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; storage and<br>&gt;&gt; &g= t;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; don=E2=80=99t automa= tically mount departmental shares.<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&g= t;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt= ;&gt;&gt; Could you check where /umbc/software is mounted on your<br>&gt;&g= t; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; departmental sy= stem (e.g., by running df -h<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; /umbc/softwa= re<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; or mount<br>&gt;&gt; &gt;= &gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; | grep /umbc/software)= ? That=E2=80=99ll tell us which server<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; ho= sts<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; it. Once<br>&gt;&gt; &gt= ;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; we know that, we can = confirm whether it=E2=80=99s possible or<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; = &gt;&gt;&gt; appropriate<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt= ;&gt; &gt;&gt;&gt; to make it visible from HPCF.<br>&gt;&gt; &gt;&gt; &gt;&= gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt= ; &gt;&gt; &gt;&gt;&gt; Best,<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt= ; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;= &gt; Tartela<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt= ;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; On Tue Oct 2= 1 15:59:49 2025, ZZ99999 wrote:<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&= gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&g= t;&gt;&gt; First Name: Mohammad<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&= gt; &gt;&gt; &gt;&gt;&gt;&gt; Last Name: Ebrahimabadi<br>&gt;&gt; &gt;&gt; = &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Email:<span class=3D""Ap= ple-converted-space"">&nbsp;</span><a href=3D""mailto:e127@umbc.edu"" target= =3D""_blank"">e127@umbc.edu</a><span class=3D""Apple-converted-space"">&nbsp;</= span>&lt;mailto:<a href=3D""mailto:e127@umbc.edu"" target=3D""_blank"">e127@umb= c.edu</a>&gt; &lt;mailto:<a href=3D""mailto:e127@umbc.edu"" target=3D""_blank""= >e127@umbc.edu</a>&lt;mailto:<a href=3D""mailto:e127@umbc.edu"" target=3D""_bl= ank"">e127@umbc.edu</a>&gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&g= t; &gt;&gt; &gt;&gt;&gt;&gt; Campus ID: NQ23652<br>&gt;&gt; &gt;&gt; &gt;&g= t;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;= &gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Request Type: High Performance = Cluster<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt= ;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&= gt; Dear Team,<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;= &gt;&gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&g= t;&gt;&gt; I hope you=E2=80=99re doing well.<br>&gt;&gt; &gt;&gt; &gt;&gt;&= gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; I=E2=80=99m a Ph.D. student in C= omputer Engineering and have a<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&g= t; question regarding running a software on the HPCF. On the<br>&gt;&gt; &g= t;&gt; &gt;&gt;&gt; &gt;&gt;&gt; department server, I usually run software = using shell scripts<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; located = at /umbc/software/scripts/ which is provided by the IT<br>&gt;&gt; &gt;&gt;=  &gt;&gt;&gt; &gt;&gt;&gt; group pf the department (Geoff Weiss Team). Howe= ver, in case of<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; HPCF, it see= ms that this path is not accessible from the HPCF<br>&gt;&gt; &gt;&gt; &gt;= &gt;&gt; &gt;&gt;&gt; environment.<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&g= t;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;= &gt; &gt;&gt; &gt;&gt;&gt;&gt; Could you please let me know if there is any=  specific<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; configuration or a= ccess permission required to reach this<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; d= irectory<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; through the HPCF? o= r direct me to a related person that can<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; = help<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; me.<br>&gt;&gt; &gt;&gt= ; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br>&gt;&gt; &gt;&gt; = &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Best regards,<br>&gt;&g= t; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Mohammad<br= >&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;<br>&gt;&= gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&g= t;&gt; &gt;&gt;&gt; &gt;&gt; &gt; --<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;= &gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &= gt; Best,<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Max = Breitmeyer<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; DOI= T HPC System Administrator<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &= gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br>&gt;&gt;=  &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; = &gt;&gt;&gt; &gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br>&g= t;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; --<br>&gt;&gt; &gt;&gt; &gt;= &gt;&gt; &gt;&gt;&gt; &gt; Naghmeh Karimi, Ph.D.<br>&gt;&gt; &gt;&gt; &gt;&= gt;&gt; &gt;&gt;&gt; &gt; Associate Professor<br>&gt;&gt; &gt;&gt; &gt;&gt;= &gt; &gt;&gt;&gt; &gt; Department of Computer Science and Electrical Engine= ering<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; University of Mar= yland, Baltimore County<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;=  Baltimore, MD 21250<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Te= l: 410-455-3965 E-mail:<span class=3D""Apple-converted-space"">&nbsp;</span><= a href=3D""mailto:nkarimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a><s= pan class=3D""Apple-converted-space"">&nbsp;</span>&lt;mailto:<a href=3D""mail= to:nkarimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a>&gt;<br>&gt;&gt;=  &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &lt;mailto:<a href=3D""mailto:nkarimi@um= bc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a><span class=3D""Apple-converte= d-space"">&nbsp;</span>&lt;mailto:<a href=3D""mailto:nkarimi@umbc.edu"" target= =3D""_blank"">nkarimi@umbc.edu</a>&gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; = &gt;&gt;&gt; &gt; Web:<span class=3D""Apple-converted-space"">&nbsp;</span><a=  href=3D""https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&= amp;source=3Dgmail-imap&amp;ust=3D1761922626000000&amp;usg=3DAOvVaw3z3DnkSS= m-krms88JtwQY_"" rel=3D""noreferrer"" target=3D""_blank"">http://www.csee.umbc.e= du/~nkarimi/</a><span class=3D""Apple-converted-space"">&nbsp;</span>&lt;<a h= ref=3D""https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp:/= /www.csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D1761921999000000= %26usg%3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7&amp;source=3Dgmail-imap&amp;ust=3D176= 1922626000000&amp;usg=3DAOvVaw0X7l2TgWCgMekNlzKnPsJl"" rel=3D""noreferrer"" ta= rget=3D""_blank"">https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nk= arimi/&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw3= 5D5VfZwU_gTyQhCtkbUZ7</a>&gt;<br>&gt;&gt; &gt;&gt; &lt;<a href=3D""https://w= ww.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://www.csee.umbc.e= du/~nkarimi/%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw2= 3Fk_KFBnlrXC_uhIbUFIW&amp;source=3Dgmail-imap&amp;ust=3D1761922626000000&am= p;usg=3DAOvVaw3wW2UaHLr-IAa0KsbyDZYx"" rel=3D""noreferrer"" target=3D""_blank"">= https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;sourc= e=3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw23Fk_KFBnlrXC_uhI= bUFIW</a><span class=3D""Apple-converted-space"">&nbsp;</span>&lt;<a href=3D""= https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://www.g= oogle.com/url?q%253Dhttp://www.csee.umbc.edu/~nkarimi/%2526source%253Dgmail= -imap%2526ust%253D1761874374000000%2526usg%253DAOvVaw23Fk_KFBnlrXC_uhIbUFIW= %26source%3Dgmail-imap%26ust%3D1761921999000000%26usg%3DAOvVaw05OHRJ3ua_KXM= 9EHoVYfgE&amp;source=3Dgmail-imap&amp;ust=3D1761922626000000&amp;usg=3DAOvV= aw17L4xVKtFJKMIHbtKBKHb5"" rel=3D""noreferrer"" target=3D""_blank"">https://www.= google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://www.csee.umbc.edu/= ~nkarimi/%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw23Fk= _KFBnlrXC_uhIbUFIW&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&amp;u= sg=3DAOvVaw05OHRJ3ua_KXM9EHoVYfgE</a>&gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;= &gt; &gt;&gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &lt;<br>&gt;&gt; &gt;&g= t;<span class=3D""Apple-converted-space"">&nbsp;</span><a href=3D""https://www= .google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://www.csee.umbc.edu= /~nkarimi/%26source%3Dgmail-imap%26ust%3D1761768891000000%26usg%3DAOvVaw0xL= E02sw5-ueWRHyHyCwkN&amp;source=3Dgmail-imap&amp;ust=3D1761922626000000&amp;= usg=3DAOvVaw3dBzDvUX_2_EdHlI33ESiV"" rel=3D""noreferrer"" target=3D""_blank"">ht= tps://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source= =3Dgmail-imap&amp;ust=3D1761768891000000&amp;usg=3DAOvVaw0xLE02sw5-ueWRHyHy= CwkN</a><span class=3D""Apple-converted-space"">&nbsp;</span>&lt;<a href=3D""h= ttps://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://www.go= ogle.com/url?q%253Dhttp://www.csee.umbc.edu/~nkarimi/%2526source%253Dgmail-= imap%2526ust%253D1761768891000000%2526usg%253DAOvVaw0xLE02sw5-ueWRHyHyCwkN%= 26source%3Dgmail-imap%26ust%3D1761921999000000%26usg%3DAOvVaw06Mkt_vSjpo8ln= qxEqL6dt&amp;source=3Dgmail-imap&amp;ust=3D1761922626000000&amp;usg=3DAOvVa= w1U6C0itGlAPYjVL4qE0kbk"" rel=3D""noreferrer"" target=3D""_blank"">https://www.g= oogle.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://www.csee.umbc.edu/~= nkarimi/%26source%3Dgmail-imap%26ust%3D1761768891000000%26usg%3DAOvVaw0xLE0= 2sw5-ueWRHyHyCwkN&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&amp;us= g=3DAOvVaw06Mkt_vSjpo8lnqxEqL6dt</a>&gt;<br>&gt;&gt; &gt;&gt; &lt;<a href= =3D""https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://w= ww.google.com/url?q%253Dhttp://www.csee.umbc.edu/~nkarimi/%2526source%253Dg= mail-imap%2526ust%253D1761768891000000%2526usg%253DAOvVaw0xLE02sw5-ueWRHyHy= CwkN%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw3g338pGQq= ShCgyX8cIt8tV&amp;source=3Dgmail-imap&amp;ust=3D1761922626000000&amp;usg=3D= AOvVaw0o9hUVJgGzn57VaBi4hS5j"" rel=3D""noreferrer"" target=3D""_blank"">https://= www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://www.csee.umbc.= edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D1761768891000000%26usg%3DAOvVaw= 0xLE02sw5-ueWRHyHyCwkN&amp;source=3Dgmail-imap&amp;ust=3D1761874374000000&a= mp;usg=3DAOvVaw3g338pGQqShCgyX8cIt8tV</a><span class=3D""Apple-converted-spa= ce"">&nbsp;</span>&lt;<a href=3D""https://www.google.com/url?q=3Dhttps://www.= google.com/url?q%3Dhttps://www.google.com/url?q%253Dhttps://www.google.com/= url?q%25253Dhttp://www.csee.umbc.edu/~nkarimi/%252526source%25253Dgmail-ima= p%252526ust%25253D1761768891000000%252526usg%25253DAOvVaw0xLE02sw5-ueWRHyHy= CwkN%2526source%253Dgmail-imap%2526ust%253D1761874374000000%2526usg%253DAOv= Vaw3g338pGQqShCgyX8cIt8tV%26source%3Dgmail-imap%26ust%3D1761921999000000%26= usg%3DAOvVaw3ktt5GEiV5miHD1TY9Dut7&amp;source=3Dgmail-imap&amp;ust=3D176192= 2626000000&amp;usg=3DAOvVaw3Zg_27w7hdSjlQ1RbDrQxL"" rel=3D""noreferrer"" targe= t=3D""_blank"">https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3D= https://www.google.com/url?q%253Dhttp://www.csee.umbc.edu/~nkarimi/%2526sou= rce%253Dgmail-imap%2526ust%253D1761768891000000%2526usg%253DAOvVaw0xLE02sw5= -ueWRHyHyCwkN%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw= 3g338pGQqShCgyX8cIt8tV&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&a= mp;usg=3DAOvVaw3ktt5GEiV5miHD1TY9Dut7</a>&gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;= <br>&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; -- Naghmeh=  Karimi, Ph.D.<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Associate Profess= or<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Department of Computer Scienc= e and Electrical Engineering<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Uni= versity of Maryland, Baltimore County<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt= ;&gt; Baltimore, MD 21250<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Tel: 4= 10-455-3965 E-mail:<span class=3D""Apple-converted-space"">&nbsp;</span><a hr= ef=3D""mailto:nkarimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a><span = class=3D""Apple-converted-space"">&nbsp;</span>&lt;mailto:<a href=3D""mailto:n= karimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a>&gt;<br>&gt;&gt; &gt= ;&gt; &gt;&gt;&gt; &gt;&gt; Web:<span class=3D""Apple-converted-space"">&nbsp= ;</span><a href=3D""https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/= ~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761922626000000&amp;usg=3DAOvV= aw3z3DnkSSm-krms88JtwQY_"" rel=3D""noreferrer"" target=3D""_blank"">http://www.c= see.umbc.edu/~nkarimi/</a><span class=3D""Apple-converted-space"">&nbsp;</spa= n>&lt;<a href=3D""https://www.google.com/url?q=3Dhttps://www.google.com/url?= q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D176192= 1999000000%26usg%3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7&amp;source=3Dgmail-imap&amp= ;ust=3D1761922626000000&amp;usg=3DAOvVaw0X7l2TgWCgMekNlzKnPsJl"" rel=3D""nore= ferrer"" target=3D""_blank"">https://www.google.com/url?q=3Dhttp://www.csee.um= bc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg= =3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7</a>&gt;<br>&gt;&gt; &gt;&gt; &lt;<a href=3D= ""https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://www.c= see.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg= %3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&amp;source=3Dgmail-imap&amp;ust=3D176192262= 6000000&amp;usg=3DAOvVaw3wW2UaHLr-IAa0KsbyDZYx"" rel=3D""noreferrer"" target= =3D""_blank"">https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarim= i/&amp;source=3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw23Fk_= KFBnlrXC_uhIbUFIW</a><span class=3D""Apple-converted-space"">&nbsp;</span>&lt= ;<a href=3D""https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dh= ttps://www.google.com/url?q%253Dhttp://www.csee.umbc.edu/~nkarimi/%2526sour= ce%253Dgmail-imap%2526ust%253D1761874374000000%2526usg%253DAOvVaw23Fk_KFBnl= rXC_uhIbUFIW%26source%3Dgmail-imap%26ust%3D1761921999000000%26usg%3DAOvVaw0= 5OHRJ3ua_KXM9EHoVYfgE&amp;source=3Dgmail-imap&amp;ust=3D1761922626000000&am= p;usg=3DAOvVaw17L4xVKtFJKMIHbtKBKHb5"" rel=3D""noreferrer"" target=3D""_blank"">= https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://www.cs= ee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg%= 3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&amp;source=3Dgmail-imap&amp;ust=3D1761921999= 000000&amp;usg=3DAOvVaw05OHRJ3ua_KXM9EHoVYfgE</a>&gt;&gt;<br>&gt;&gt; &gt;&= gt;<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; --<br>&gt;&gt; &gt;&gt;<br>&gt;&= gt; &gt;&gt; &gt;&gt;&gt; &gt; Best,<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;=  Max Breitmeyer<br>&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; DOIT HPC System Admi= nistrator<br>&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; &gt;&gt; -- V/R,Maxwell=  BreitmeyerUMBC HPCF SpecialistUMBC Observatory IT<br>&gt;&gt; &gt;&gt; &gt= ;&gt; Manager Graduate Student(443) 835-8250<br>&gt;&gt; &gt;&gt;<br>&gt;&g= t; &gt;&gt; --<br>&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt; Best,<br>&gt;&gt; = &gt;&gt; Max Breitmeyer<br>&gt;&gt; &gt;&gt; DOIT HPC System Administrator<= br>&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt;<br>&gt;&gt; &gt;&gt;<br>&gt;&gt; = &gt;<br>&gt;&gt;<span class=3D""Apple-converted-space"">&nbsp;</span><br>&gt;= &gt; --<span class=3D""Apple-converted-space"">&nbsp;</span><br>&gt;&gt; Nagh= meh Karimi, Ph.D.<br>&gt;&gt; Associate Professor<br>&gt;&gt; Department of=  Computer Science and Electrical Engineering<br>&gt;&gt; University of Mary= land, Baltimore County<br>&gt;&gt; Baltimore, MD 21250<br>&gt;&gt; Tel: *41= 0-455-3965* E-mail: *<a href=3D""mailto:nkarimi@umbc.edu"" target=3D""_blank"">= nkarimi@umbc.edu</a><span class=3D""Apple-converted-space"">&nbsp;</span>&lt;= mailto:<a href=3D""mailto:nkarimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.e= du</a>&gt; &lt;<a href=3D""mailto:nkarimi@umbc.edu"" target=3D""_blank"">nkarim= i@umbc.edu</a>&lt;mailto:<a href=3D""mailto:nkarimi@umbc.edu"" target=3D""_bla= nk"">nkarimi@umbc.edu</a>&gt;&gt;*<br>&gt;&gt; Web: *<a href=3D""https://www.= google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-im= ap&amp;ust=3D1761922626000000&amp;usg=3DAOvVaw3z3DnkSSm-krms88JtwQY_"" rel= =3D""noreferrer"" target=3D""_blank"">http://www.csee.umbc.edu/~nkarimi/</a><sp= an class=3D""Apple-converted-space"">&nbsp;</span>&lt;<a href=3D""https://www.= google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttp://www.csee.umbc.edu/= ~nkarimi/%26source%3Dgmail-imap%26ust%3D1761921999000000%26usg%3DAOvVaw35D5= VfZwU_gTyQhCtkbUZ7&amp;source=3Dgmail-imap&amp;ust=3D1761922626000000&amp;u= sg=3DAOvVaw0X7l2TgWCgMekNlzKnPsJl"" rel=3D""noreferrer"" target=3D""_blank"">htt= ps://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source= =3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw35D5VfZwU_gTyQhCtk= bUZ7</a>&gt;<br>&gt;&gt; &lt;<a href=3D""https://www.google.com/url?q=3Dhttp= ://www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761922626= 000000&amp;usg=3DAOvVaw3z3DnkSSm-krms88JtwQY_"" rel=3D""noreferrer"" target=3D= ""_blank"">http://www.csee.umbc.edu/~nkarimi/</a><span class=3D""Apple-convert= ed-space"">&nbsp;</span>&lt;<a href=3D""https://www.google.com/url?q=3Dhttps:= //www.google.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmai= l-imap%26ust%3D1761921999000000%26usg%3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7&amp;so= urce=3Dgmail-imap&amp;ust=3D1761922626000000&amp;usg=3DAOvVaw0X7l2TgWCgMekN= lzKnPsJl"" rel=3D""noreferrer"" target=3D""_blank"">https://www.google.com/url?q= =3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D176= 1921999000000&amp;usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7</a>&gt;&gt;*</blockquo= te></div></div></blockquote></div><br></div></body></html>= "
3296910,72521827,Correspond,DoIT-Research-Computing,2025-10-28 12:37:39.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Skye,</p>  <p>The HPC cluster still allows you to use absolute paths in your own path.=  You should be able to just export the absolute path which won&#39;t genera= lly change from one node to another. Additionally, PATH does work with rela= tive paths.</p>  <p>The cluster uses an internal IP range. The worker nodes are only availab= le from inside the cluster. What are you trying to add to an NFS share exac= tly?</p>  <p>On Mon Oct 27 12:27:12 2025, II69854 wrote:</p>  <blockquote>Max, <div>&nbsp;</div>  <div>I need the help of you (or an expert you can point me to) in order to = rewrite the scripts, because Cadence and Synopsys tools require a lot of ad= ditions to your PATH and LD_PATH, which are usually handled with absolute p= aths. I&rsquo;m unsure what the best way to handle that would be on the HPC=  cluster. In addition, I need to know the HPC cluster&rsquo;s IP range so I=  can add it to an NFS share. <div> <div>---</div>  <div>Skye Jonke<br /> <br /> she/her<br /> <br /> Specialist, Linux System Administrator &amp;&nbsp;Lab Technical Support</di= v> </div>  <div>&nbsp; <blockquote> <div>On Oct 24, 2025, at 10:56, Mohammad Ebrahimabadi &lt;e127@umbc.edu&gt;=  wrote:</div> &nbsp;  <div> <div>Thanks Skye, <div>&nbsp;</div>  <div>@ Dear Max,</div>  <div>I just cced Skye from the IT of csee department to help us run softwar= e on the chip server.</div>  <div>&nbsp;</div>  <div>Regards,</div>  <div>Mohammad</div>  <div>&nbsp;</div> </div> &nbsp;  <div> <div>On Fri, Oct 24, 2025 at 10:51=E2=80=AFAM Skye Jonke via RT &lt;UMBCHel= p@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL:&nbsp;https://rt.umbc.edu/Ticket/Display.html?id= =3D3296910&nbsp;&gt;<br /> <br /> Last Update From Ticket:<br /> <br /> That&rsquo;s a bit confusing, since hardcoded paths won&rsquo;t work, pleas= e have Max contact me directly so we can talk about what needs to be done t= o make this work.&nbsp;<br /> ---<br /> Skye Jonke<br /> <br /> she/her<br /> <br /> Specialist, Linux System Administrator &amp; Lab Technical Support<br /> <br /> &gt; On Oct 24, 2025, at 10:46, Mohammad Ebrahimabadi &lt;e127@umbc.edu&gt;=  wrote:<br /> &gt;&nbsp;<br /> &gt; Hello Skye,<br /> &gt;&nbsp;<br /> &gt; I believe the way that Max told me to run the script was using the sam= e path that we have in the CSEE department as he mounted that path in HPCF.=  I mean this path:<br /> &gt;&nbsp; /umbc/software/scripts/launch_synopsys_hspice.sh<br /> &gt;&nbsp;<br /> &gt;&nbsp;<br /> &gt; Regards,<br /> &gt; Mohammad<br /> &gt;&nbsp;<br /> &gt;&nbsp;<br /> &gt;&nbsp;<br /> &gt;&nbsp;<br /> &gt; On Fri, Oct 24, 2025 at 9:42=E2=80=AFAM&nbsp;naghmeh.karimi@umbc.edu&n= bsp;&lt;mailto:naghmeh.karimi@umbc.edu&gt; via RT &lt;UMBCHelp@rt.umbc.edu&= nbsp;&lt;mailto:UMBCHelp@rt.umbc.edu&gt;&gt; wrote:<br /> &gt;&gt; Ticket &lt;URL:&nbsp;https://rt.umbc.edu/Ticket/Display.html?id=3D= 3296910&nbsp;&lt;https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/= Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D176192199900000= 0&amp;usg=3DAOvVaw099WO1Ar-MqC9UTifVgSrZ&gt; &gt;<br /> &gt;&gt;&nbsp;<br /> &gt;&gt; Last Update From Ticket:<br /> &gt;&gt;&nbsp;<br /> &gt;&gt; That&#39;s perfect Skye. I highly appreciate it. I ask Mohamamd to=  respond<br /> &gt;&gt; the ticket with the path.<br /> &gt;&gt;&nbsp;<br /> &gt;&gt; regards,<br /> &gt;&gt; Naghmeh<br /> &gt;&gt;&nbsp;<br /> &gt;&gt; On Fri, Oct 24, 2025 at 9:39=E2=80=AFAM Skye Jonke &lt;ii69854@umb= c.edu&nbsp;&lt;mailto:ii69854@umbc.edu&gt;&gt; wrote:<br /> &gt;&gt;&nbsp;<br /> &gt;&gt; &gt; Its not possible to use the existing scripts in a situation w= here the<br /> &gt;&gt; &gt; files aren&rsquo;t mounted in /umbc/software, but I can write=  some scripts<br /> &gt;&gt; &gt; specific to this setup to this situation with a bit more info= rmation. Where<br /> &gt;&gt; &gt; are the script files located on the HPC? If you aren&rsquo;t = sure, you can get<br /> &gt;&gt; &gt; the full path by running &ldquo;realpath myfile.sh&rdquo; (re= placing myfile.sh with one<br /> &gt;&gt; &gt; of the cadence scripts, of course)<br /> &gt;&gt; &gt; ---<br /> &gt;&gt; &gt; Skye Jonke<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt; she/her<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt; Specialist, Linux System Administrator &amp; Lab Technical Su= pport<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt; On Oct 23, 2025, at 21:32, Naghmeh Karimi &lt;nkarimi@umbc.ed= u&nbsp;&lt;mailto:nkarimi@umbc.edu&gt;&gt; wrote:<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt; Hi Max, thanks for getting back to us.<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt; Hi Skye, we need your help here to find the file locations. C= ould you<br /> &gt;&gt; &gt; please help to resolve this issue?<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt; Thanks a lot<br /> &gt;&gt; &gt; Naghmeh<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt; Naghmeh Karimi, Ph.D.<br /> &gt;&gt; &gt; Associate Professor<br /> &gt;&gt; &gt; Department of Computer Science and Electrical Engineering<br = /> &gt;&gt; &gt; University of Maryland, Baltimore County<br /> &gt;&gt; &gt; Baltimore, MD 21250<br /> &gt;&gt; &gt; Tel: *410-455-3965* E-mail: *nkarimi@umbc.edu&nbsp;&lt;mailto= :nkarimi@umbc.edu&gt; &lt;nkarimi@umbc.edu&lt;mailto:nkarimi@umbc.edu&gt;&g= t;*<br /> &gt;&gt; &gt; Web: *http://www.csee.umbc.edu/~nkarimi/&nbsp;&lt;https://www= .google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-i= map&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7&gt;<b= r /> &gt;&gt; &gt; &lt;https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~= nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVa= w23Fk_KFBnlrXC_uhIbUFIW&nbsp;&lt;https://www.google.com/url?q=3Dhttps://www= .google.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmail-ima= p%26ust%3D1761874374000000%26usg%3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&amp;source= =3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw05OHRJ3ua_KXM9EHoV= YfgE&gt;&gt;*<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt; On Thu, Oct 23, 2025, 12:22=E2=80=AFPM Max Breitmeyer via RT = &lt;UMBCHelp@rt.umbc.edu&nbsp;&lt;mailto:UMBCHelp@rt.umbc.edu&gt;&gt;<br /> &gt;&gt; &gt; wrote:<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt; Ticket &lt;URL:&nbsp;https://rt.umbc.edu/Ticket/Display.h= tml?id=3D3296910&nbsp;&lt;https://www.google.com/url?q=3Dhttps://rt.umbc.ed= u/Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D176192= 1999000000&amp;usg=3DAOvVaw099WO1Ar-MqC9UTifVgSrZ&gt;<br /> &gt;&gt; &gt;&gt; &lt;https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ti= cket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761874374= 000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&nbsp;&lt;https://www.google.c= om/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.edu/Ticket/Displa= y.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg%= 3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&amp;source=3Dgmail-imap&amp;ust=3D1761921999= 000000&amp;usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; Last Update From Ticket:<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; Hi Mohammad,<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; According to the error message, it&#39;s having an issue = finding another file<br /> &gt;&gt; &gt;&gt; in<br /> &gt;&gt; &gt;&gt; the directory. This appears to be an issue due to the mou= nting locations<br /> &gt;&gt; &gt;&gt; being<br /> &gt;&gt; &gt;&gt; different. I would point you to the previous email where = I said that any<br /> &gt;&gt; &gt;&gt; software with hard-coded locations would not work, and th= at software<br /> &gt;&gt; &gt;&gt; compatibility is not guaranteed. I would recommend workin= g with Skye on<br /> &gt;&gt; &gt;&gt; porting<br /> &gt;&gt; &gt;&gt; what is in the directory to your local research directori= es on chip.<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; On Thu Oct 23 12:09:34 2025, NQ23652 wrote:<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt; Yes you are right, now I could see all shell files. = However still I have<br /> &gt;&gt; &gt;&gt; &gt; problem when I run the sh file.<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt; [e127@chip-login2 scripts]$ sh launch_synopsys_hspic= e.sh<br /> &gt;&gt; &gt;&gt; &gt; launch_synopsys_hspice.sh: line 3:<br /> &gt;&gt; &gt;&gt; &gt; /umbc/software/scripts/env_synopsys_hspice.sh: No su= ch file or directory<br /> &gt;&gt; &gt;&gt; &gt; [e127@chip-login2 scripts]$<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt; Would you please take a look to it.<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt; Regards,<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt; Mohammad<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt; On Thu Oct 23 11:59:17 2025, OL73413 wrote:<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt; Hi Mohammad,<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt; A good point. Chip uses a service called &quot;a= utofs&quot; to mount and unmount<br /> &gt;&gt; &gt;&gt; &gt;&gt; directories that aren&#39;t being actively used.=  This helps maintain the<br /> &gt;&gt; &gt;&gt; &gt;&gt; login nodes integrity. If you see in my screensh= ot below, the directory<br /> &gt;&gt; &gt;&gt; &gt;&gt; appears to be empty, but when I cd to the csee d= irectory where your<br /> &gt;&gt; &gt;&gt; &gt;&gt; software is mounted, it is all available. Backin= g out of the directory<br /> &gt;&gt; &gt;&gt; &gt;&gt; and ls&#39;ing the directory also shows that the=  directory is now mounted<br /> &gt;&gt; &gt;&gt; &gt;&gt; and available.<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt; On Thu, Oct 23, 2025 at 11:41 AM Mohammad Ebrahi= mabadi via RT<br /> &gt;&gt; &gt;&gt; &gt;&gt; &lt;UMBCHelp@rt.umbc.edu&nbsp;&lt;mailto:UMBCHel= p@rt.umbc.edu&gt;&gt; wrote:<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Ticket &lt;URL:&nbsp;https://rt.umbc.edu/Tic= ket/Display.html?id=3D3296910&nbsp;&lt;https://www.google.com/url?q=3Dhttps= ://rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp= ;ust=3D1761921999000000&amp;usg=3DAOvVaw099WO1Ar-MqC9UTifVgSrZ&gt;<br /> &gt;&gt; &gt;&gt; &lt;https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ti= cket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761874374= 000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&nbsp;&lt;https://www.google.c= om/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.edu/Ticket/Displa= y.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg%= 3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&amp;source=3Dgmail-imap&amp;ust=3D1761921999= 000000&amp;usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Comment From Ticket:<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Thank you Max!<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Actually we have still problem for accessing=  that directory. I<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; uploaded two<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; screenshots that shows what we can see in th= e department servers<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; and what we<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; can see in CHIP.<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; For example by running this shell file we ca= n launch HSPICE<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; software in the<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; department server:<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; /umbc/software/scripts/launch_synopsys_hspic= e.sh<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; But it is not still accessible on CHIP.<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Regards,<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Mohammad<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; On Thu Oct 23 10:54:59 2025, OL73413 wrote:<= br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Hi Nahmeh,<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Apologies &mdash; I meant to update you=  yesterday after completing<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; this, but I<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; was pulled into a long meeting. The sof= tware is now available on<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; chip at:<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; /umbc/software/csee<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Please note that if the software contai= ns any hardcoded file<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; paths, it may<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; not function correctly. Unfortunately, = we don&rsquo;t have control over<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; those<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; cases.<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Also, just to clarify, Skye and I work = in separate departments.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Similar to<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; how I don&rsquo;t have administrative a= ccess to the CSEE department<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; servers<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; (which include the software directory),=  Skye is not an<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; administrator on<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; chip.<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Please let me know if you encounter any=  issues accessing the<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; software on<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; the server. If there are problems runni= ng the software, I&rsquo;ll<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; coordinate<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; with Skye to troubleshoot, but since we=  don&rsquo;t maintain this<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; software, we<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; can&rsquo;t guarantee full compatibilit= y on our systems.<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; On Thu Oct 23 10:36:41 2025,&nbsp;naghm= eh.karimi@umbc.edu&nbsp;&lt;mailto:naghmeh.karimi@umbc.edu&gt; wrote:<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Hi Skye and Max, Could you please h= elp on mounting this today so<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; we can<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; use them as we have a deadline? Tha= nks,Naghmeh Karimi On Wed,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Oct 22,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; 2025 at 4:22 PM Skye Jonke via RT &= lt;UMBCHelp@rt.umbc.edu&nbsp;&lt;mailto:UMBCHelp@rt.umbc.edu&gt;&gt; wrote:= <br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Ticket &lt;URL:&nbsp;https://rt= .umbc.edu/Ticket/Display.html?id=3D3296910&nbsp;&lt;https://www.google.com/= url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;source=3Dg= mail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw099WO1Ar-MqC9UTifVgSrZ= &gt;<br /> &gt;&gt; &gt;&gt; &lt;https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ti= cket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761874374= 000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&nbsp;&lt;https://www.google.c= om/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.edu/Ticket/Displa= y.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg%= 3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&amp;source=3Dgmail-imap&amp;ust=3D1761921999= 000000&amp;usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Last Update From Ticket:<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Mohammad,<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; So long as you continue to acce= ss them through<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; /umbc/software/scripts, they sh= ould remain up to date and you<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; will<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; see any new scripts. If you cop= y them to a local directory on<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; the<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; HPC cluster, they will not be u= pdated.<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; ---<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Skye Jonke<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; she/her<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Specialist, Linux System Admini= strator &amp; Lab Technical Support<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; On Oct 22, 2025, at 16:14,=  Naghmeh Karimi &lt;nkarimi@umbc.edu&nbsp;&lt;mailto:nkarimi@umbc.edu&gt;&g= t;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; wrote:<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Hi Max, Roy, Skye,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Thank you all for your hel= p.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; I added Skye here as she i= s our IT exert on adding the<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; capability<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; of running synopsis tools (Hspi= ce) in servers.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Skye we want to use the HP= C cluster to run hspice an other<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; synopsys tools. Could you pleas= e help on this?<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Thanks,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Naghmeh<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; On Wed, Oct 22, 2025 at 2:= 06 PM Mohammad Ebrahimabadi via RT<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &lt;UMBCHelp@rt.umbc.edu&nbsp;&= lt;mailto:UMBCHelp@rt.umbc.edu&gt; &lt;mailto:UMBCHelp@rt.umbc.edu&lt;mailt= o:UMBCHelp@rt.umbc.edu&gt;&gt;&gt; wrote:<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Ticket &lt;URL:<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt;&nbsp;https://rt.umbc.edu/Ticket/Display.html= ?id=3D3296910&nbsp;&lt;https://www.google.com/url?q=3Dhttps://rt.umbc.edu/T= icket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D176192199= 9000000&amp;usg=3DAOvVaw099WO1Ar-MqC9UTifVgSrZ&gt;<br /> &gt;&gt; &gt;&gt; &lt;https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ti= cket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761874374= 000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&nbsp;&lt;https://www.google.c= om/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.edu/Ticket/Displa= y.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg%= 3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&amp;source=3Dgmail-imap&amp;ust=3D1761921999= 000000&amp;usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &lt;<br /> &gt;&gt; &gt;&gt;&nbsp;https://www.google.com/url?q=3Dhttps://rt.umbc.edu/T= icket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D176176889= 1000000&amp;usg=3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW&nbsp;&lt;https://www.google.= com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.edu/Ticket/Displ= ay.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D1761768891000000%26usg= %3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW&amp;source=3Dgmail-imap&amp;ust=3D176192199= 9000000&amp;usg=3DAOvVaw04hQC2SRO0y4BoVR0LFNp_&gt;<br /> &gt;&gt; &gt;&gt; &lt;https://www.google.com/url?q=3Dhttps://www.google.com= /url?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3D= gmail-imap%26ust%3D1761768891000000%26usg%3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW&am= p;source=3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw0fLylAsBAo= Q6LgqY79I9-d&lt;https://www.google.com/url?q=3Dhttps://www.google.com/url?q= %3Dhttps://www.google.com/url?q%253Dhttps://rt.umbc.edu/Ticket/Display.html= ?id%25253D3296910%2526source%253Dgmail-imap%2526ust%253D1761768891000000%25= 26usg%253DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW%26source%3Dgmail-imap%26ust%3D176187= 4374000000%26usg%3DAOvVaw0fLylAsBAoQ6LgqY79I9-d&amp;source=3Dgmail-imap&amp= ;ust=3D1761921999000000&amp;usg=3DAOvVaw3pQOXM-6FD6kt0kA6ujsVN&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Last Update From Ticke= t:<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Hi Max,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Thanks for your help! = Let&rsquo;s proceed based on your<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; suggestion,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; hopefully it<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; works as expected.<br = /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; I just have one quick = question: since the IT team in the<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; department can modify<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; or add new shell scrip= ts in that directory, will we also<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; have<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; access to those<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; updated or newly added=  scripts from the mounted directory on<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; HPCF?<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Thanks again,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Mohammad<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; On Wed Oct 22 14:01:32=  2025, OL73413 wrote:<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Hi Mohammad,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; We can get the so= ftware mounted for you on chip, but it<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; will<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; have to be<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; read-only as we w= on&#39;t be able to manage who can make<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; changes<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; to it<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; otherwise.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; As a reminder, th= e machines on that run on chip are not<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; the<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; same as those<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; run by the CSEE d= epartment, so we can&#39;t guarantee that<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; these<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; will run the<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; same way or as ef= fectively.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; If that&#39;s all=  ok with you, please give me some time to get<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; the<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; directory<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; mounted.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; On Wed Oct 22 13:= 26:05 2025, NQ23652 wrote:<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thanks Tartel= a for your help to resolve my issue!<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Actually it s= eems all server in CSEE department have<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; access<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; to that<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; path. When I = ran the command that you gave me I got the<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; following<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; report:<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; mohammad@albo= rz:~$ df -h /umbc/software<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Filesystem Si= ze Used Avail Use% Mounted on<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&nbsp;nfs.iss.= rs.umbc.edu&nbsp;&lt;http://nfs.iss.rs.umbc.edu/&gt;:/ifs/data/csee/softwar= e 2.5T 2.2T<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; 326G<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; 88%<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; /umbc/softwar= e<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; mohammad@albo= rz:~$ mount | grep /umbc/software<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&nbsp;nfs.iss.= rs.umbc.edu&nbsp;&lt;http://nfs.iss.rs.umbc.edu/&gt;:/ifs/data/csee/softwar= e on<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; /umbc/software<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; type nfs<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt; (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namle= n=3D255,acregmin=3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,= proto=3Dtcp,timeo=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountv= ers=3D3,mountport=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.6= 0)<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Please let me=  know if you need other information.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Regards,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Mohammad<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; On Wed Oct 22=  13:15:12 2025, HK41259 wrote:<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Hello,<br=  /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Thanks fo= r reaching out. Could you clarify which<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; specific<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; machine<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; or server=  you&rsquo;re referring to when you mention the<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &ldquo;departmental<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; server&rd= quo;?<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; The /umbc= /software/scripts/ directory isn&rsquo;t accessible<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; from<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; HPCF<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; (e.g., Ch= ip) by default &mdash; those systems have separate<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; storage and<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; don&rsquo= ;t automatically mount departmental shares.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Could you=  check where /umbc/software is mounted on your<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; departmen= tal system (e.g., by running df -h<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; /umbc/software<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; or mount<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; | grep /u= mbc/software)? That&rsquo;ll tell us which server<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; hosts<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; it. Once<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; we know t= hat, we can confirm whether it&rsquo;s possible or<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; appropriate<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; to make i= t visible from HPCF.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Best,<br = /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Tartela<b= r /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; On Tue Oc= t 21 15:59:49 2025, ZZ99999 wrote:<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; First=  Name: Mohammad<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Last = Name: Ebrahimabadi<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Email= :&nbsp;e127@umbc.edu&nbsp;&lt;mailto:e127@umbc.edu&gt; &lt;mailto:e127@umbc= .edu&lt;mailto:e127@umbc.edu&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Campu= s ID: NQ23652<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Reque= st Type: High Performance Cluster<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Dear = Team,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; I hop= e you&rsquo;re doing well.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; I&rsq= uo;m a Ph.D. student in Computer Engineering and have a<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; question regarding running a so= ftware on the HPCF. On the<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; department server, I usually ru= n software using shell scripts<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; located at /umbc/software/scrip= ts/ which is provided by the IT<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; group pf the department (Geoff = Weiss Team). However, in case of<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; HPCF, it seems that this path i= s not accessible from the HPCF<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; environment.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Could=  you please let me know if there is any specific<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; configuration or access permiss= ion required to reach this<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; directory<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; through the HPCF? or direct me = to a related person that can<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; help<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; me.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Best = regards,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Moham= mad<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; --<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Best,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Max Breitmeyer<br=  /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; DOIT HPC System A= dministrator<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; --<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Naghmeh Karimi, Ph.D.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Associate Professor<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Department of Computer Sci= ence and Electrical Engineering<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; University of Maryland, Ba= ltimore County<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Baltimore, MD 21250<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Tel: 410-455-3965 E-mail:&= nbsp;nkarimi@umbc.edu&nbsp;&lt;mailto:nkarimi@umbc.edu&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &lt;mailto:nkarimi@umbc.edu&nbs= p;&lt;mailto:nkarimi@umbc.edu&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Web:&nbsp;http://www.csee.= umbc.edu/~nkarimi/&nbsp;&lt;https://www.google.com/url?q=3Dhttp://www.csee.= umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&amp;u= sg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7&gt;<br /> &gt;&gt; &gt;&gt; &lt;https://www.google.com/url?q=3Dhttp://www.csee.umbc.e= du/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg=3DA= OvVaw23Fk_KFBnlrXC_uhIbUFIW&nbsp;&lt;https://www.google.com/url?q=3Dhttps:/= /www.google.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmail= -imap%26ust%3D1761874374000000%26usg%3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&amp;sou= rce=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw05OHRJ3ua_KXM9E= HoVYfgE&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &lt;<br /> &gt;&gt; &gt;&gt;&nbsp;https://www.google.com/url?q=3Dhttp://www.csee.umbc.= edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761768891000000&amp;usg=3D= AOvVaw0xLE02sw5-ueWRHyHyCwkN&nbsp;&lt;https://www.google.com/url?q=3Dhttps:= //www.google.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmai= l-imap%26ust%3D1761768891000000%26usg%3DAOvVaw0xLE02sw5-ueWRHyHyCwkN&amp;so= urce=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw06Mkt_vSjpo8ln= qxEqL6dt&gt;<br /> &gt;&gt; &gt;&gt; &lt;https://www.google.com/url?q=3Dhttps://www.google.com= /url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D1= 761768891000000%26usg%3DAOvVaw0xLE02sw5-ueWRHyHyCwkN&amp;source=3Dgmail-ima= p&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw3g338pGQqShCgyX8cIt8tV&nbsp;&l= t;https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://www= .google.com/url?q%253Dhttp://www.csee.umbc.edu/~nkarimi/%2526source%253Dgma= il-imap%2526ust%253D1761768891000000%2526usg%253DAOvVaw0xLE02sw5-ueWRHyHyCw= kN%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw3g338pGQqSh= CgyX8cIt8tV&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAO= vVaw3ktt5GEiV5miHD1TY9Dut7&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; -- Naghmeh Karimi, Ph.D.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Associate Professor<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Department of Computer Science and = Electrical Engineering<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; University of Maryland, Baltimore C= ounty<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Baltimore, MD 21250<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Tel: 410-455-3965 E-mail:&nbsp;nkar= imi@umbc.edu&nbsp;&lt;mailto:nkarimi@umbc.edu&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Web:&nbsp;http://www.csee.umbc.edu/= ~nkarimi/&nbsp;&lt;https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/= ~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvV= aw35D5VfZwU_gTyQhCtkbUZ7&gt;<br /> &gt;&gt; &gt;&gt; &lt;https://www.google.com/url?q=3Dhttp://www.csee.umbc.e= du/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg=3DA= OvVaw23Fk_KFBnlrXC_uhIbUFIW&nbsp;&lt;https://www.google.com/url?q=3Dhttps:/= /www.google.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmail= -imap%26ust%3D1761874374000000%26usg%3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&amp;sou= rce=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw05OHRJ3ua_KXM9E= HoVYfgE&gt;&gt;<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; --<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Best,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Max Breitmeyer<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; DOIT HPC System Administrator<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt; -- V/R,Maxwell BreitmeyerUMBC HPCF SpecialistUMB= C Observatory IT<br /> &gt;&gt; &gt;&gt; &gt;&gt; Manager Graduate Student(443) 835-8250<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; --<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; Best,<br /> &gt;&gt; &gt;&gt; Max Breitmeyer<br /> &gt;&gt; &gt;&gt; DOIT HPC System Administrator<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;<br /> &gt;&gt;&nbsp;<br /> &gt;&gt; --&nbsp;<br /> &gt;&gt; Naghmeh Karimi, Ph.D.<br /> &gt;&gt; Associate Professor<br /> &gt;&gt; Department of Computer Science and Electrical Engineering<br /> &gt;&gt; University of Maryland, Baltimore County<br /> &gt;&gt; Baltimore, MD 21250<br /> &gt;&gt; Tel: *410-455-3965* E-mail: *nkarimi@umbc.edu&nbsp;&lt;mailto:nkar= imi@umbc.edu&gt; &lt;nkarimi@umbc.edu&lt;mailto:nkarimi@umbc.edu&gt;&gt;*<b= r /> &gt;&gt; Web: *http://www.csee.umbc.edu/~nkarimi/&nbsp;&lt;https://www.goog= le.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&a= mp;ust=3D1761921999000000&amp;usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7&gt;<br /> &gt;&gt; &lt;http://www.csee.umbc.edu/~nkarimi/&nbsp;&lt;https://www.google= .com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp= ;ust=3D1761921999000000&amp;usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7&gt;&gt;*</bl= ockquote> </div> </div> </blockquote> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3296910,72521974,Correspond,DoIT-Research-Computing,2025-10-28 12:42:32.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Skye, apologies if you didn&#39;t see my above response!</p>  <p>On Tue Oct 28 08:37:39 2025, OL73413 wrote:</p>  <blockquote> <div> <p>Hi Skye,</p>  <p>The HPC cluster still allows you to use absolute paths in your own path.=  You should be able to just export the absolute path which won&#39;t genera= lly change from one node to another. Additionally, PATH does work with rela= tive paths.</p>  <p>The cluster uses an internal IP range. The worker nodes are only availab= le from inside the cluster. What are you trying to add to an NFS share exac= tly?</p>  <p>On Mon Oct 27 12:27:12 2025, II69854 wrote:</p>  <blockquote>Max, <div>&nbsp;</div>  <div>I need the help of you (or an expert you can point me to) in order to = rewrite the scripts, because Cadence and Synopsys tools require a lot of ad= ditions to your PATH and LD_PATH, which are usually handled with absolute p= aths. I&rsquo;m unsure what the best way to handle that would be on the HPC=  cluster. In addition, I need to know the HPC cluster&rsquo;s IP range so I=  can add it to an NFS share. <div> <div>---</div>  <div>Skye Jonke<br /> <br /> she/her<br /> <br /> Specialist, Linux System Administrator &amp;&nbsp;Lab Technical Support</di= v> </div>  <div>&nbsp; <blockquote> <div>On Oct 24, 2025, at 10:56, Mohammad Ebrahimabadi &lt;e127@umbc.edu&gt;=  wrote:</div> &nbsp;  <div> <div>Thanks Skye, <div>&nbsp;</div>  <div>@ Dear Max,</div>  <div>I just cced Skye from the IT of csee department to help us run softwar= e on the chip server.</div>  <div>&nbsp;</div>  <div>Regards,</div>  <div>Mohammad</div>  <div>&nbsp;</div> </div> &nbsp;  <div> <div>On Fri, Oct 24, 2025 at 10:51=E2=80=AFAM Skye Jonke via RT &lt;UMBCHel= p@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL:&nbsp;https://rt.umbc.edu/Ticket/Display.html?id= =3D3296910&nbsp;&gt;<br /> <br /> Last Update From Ticket:<br /> <br /> That&rsquo;s a bit confusing, since hardcoded paths won&rsquo;t work, pleas= e have Max contact me directly so we can talk about what needs to be done t= o make this work.&nbsp;<br /> ---<br /> Skye Jonke<br /> <br /> she/her<br /> <br /> Specialist, Linux System Administrator &amp; Lab Technical Support<br /> <br /> &gt; On Oct 24, 2025, at 10:46, Mohammad Ebrahimabadi &lt;e127@umbc.edu&gt;=  wrote:<br /> &gt;&nbsp;<br /> &gt; Hello Skye,<br /> &gt;&nbsp;<br /> &gt; I believe the way that Max told me to run the script was using the sam= e path that we have in the CSEE department as he mounted that path in HPCF.=  I mean this path:<br /> &gt;&nbsp; /umbc/software/scripts/launch_synopsys_hspice.sh<br /> &gt;&nbsp;<br /> &gt;&nbsp;<br /> &gt; Regards,<br /> &gt; Mohammad<br /> &gt;&nbsp;<br /> &gt;&nbsp;<br /> &gt;&nbsp;<br /> &gt;&nbsp;<br /> &gt; On Fri, Oct 24, 2025 at 9:42=E2=80=AFAM&nbsp;naghmeh.karimi@umbc.edu&n= bsp;&lt;mailto:naghmeh.karimi@umbc.edu&gt; via RT &lt;UMBCHelp@rt.umbc.edu&= nbsp;&lt;mailto:UMBCHelp@rt.umbc.edu&gt;&gt; wrote:<br /> &gt;&gt; Ticket &lt;URL:&nbsp;https://rt.umbc.edu/Ticket/Display.html?id=3D= 3296910&nbsp;&lt;https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/= Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D176192199900000= 0&amp;usg=3DAOvVaw099WO1Ar-MqC9UTifVgSrZ&gt; &gt;<br /> &gt;&gt;&nbsp;<br /> &gt;&gt; Last Update From Ticket:<br /> &gt;&gt;&nbsp;<br /> &gt;&gt; That&#39;s perfect Skye. I highly appreciate it. I ask Mohamamd to=  respond<br /> &gt;&gt; the ticket with the path.<br /> &gt;&gt;&nbsp;<br /> &gt;&gt; regards,<br /> &gt;&gt; Naghmeh<br /> &gt;&gt;&nbsp;<br /> &gt;&gt; On Fri, Oct 24, 2025 at 9:39=E2=80=AFAM Skye Jonke &lt;ii69854@umb= c.edu&nbsp;&lt;mailto:ii69854@umbc.edu&gt;&gt; wrote:<br /> &gt;&gt;&nbsp;<br /> &gt;&gt; &gt; Its not possible to use the existing scripts in a situation w= here the<br /> &gt;&gt; &gt; files aren&rsquo;t mounted in /umbc/software, but I can write=  some scripts<br /> &gt;&gt; &gt; specific to this setup to this situation with a bit more info= rmation. Where<br /> &gt;&gt; &gt; are the script files located on the HPC? If you aren&rsquo;t = sure, you can get<br /> &gt;&gt; &gt; the full path by running &ldquo;realpath myfile.sh&rdquo; (re= placing myfile.sh with one<br /> &gt;&gt; &gt; of the cadence scripts, of course)<br /> &gt;&gt; &gt; ---<br /> &gt;&gt; &gt; Skye Jonke<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt; she/her<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt; Specialist, Linux System Administrator &amp; Lab Technical Su= pport<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt; On Oct 23, 2025, at 21:32, Naghmeh Karimi &lt;nkarimi@umbc.ed= u&nbsp;&lt;mailto:nkarimi@umbc.edu&gt;&gt; wrote:<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt; Hi Max, thanks for getting back to us.<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt; Hi Skye, we need your help here to find the file locations. C= ould you<br /> &gt;&gt; &gt; please help to resolve this issue?<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt; Thanks a lot<br /> &gt;&gt; &gt; Naghmeh<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt; Naghmeh Karimi, Ph.D.<br /> &gt;&gt; &gt; Associate Professor<br /> &gt;&gt; &gt; Department of Computer Science and Electrical Engineering<br = /> &gt;&gt; &gt; University of Maryland, Baltimore County<br /> &gt;&gt; &gt; Baltimore, MD 21250<br /> &gt;&gt; &gt; Tel: *410-455-3965* E-mail: *nkarimi@umbc.edu&nbsp;&lt;mailto= :nkarimi@umbc.edu&gt; &lt;nkarimi@umbc.edu&lt;mailto:nkarimi@umbc.edu&gt;&g= t;*<br /> &gt;&gt; &gt; Web: *http://www.csee.umbc.edu/~nkarimi/&nbsp;&lt;https://www= .google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-i= map&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7&gt;<b= r /> &gt;&gt; &gt; &lt;https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~= nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVa= w23Fk_KFBnlrXC_uhIbUFIW&nbsp;&lt;https://www.google.com/url?q=3Dhttps://www= .google.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmail-ima= p%26ust%3D1761874374000000%26usg%3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&amp;source= =3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw05OHRJ3ua_KXM9EHoV= YfgE&gt;&gt;*<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt; On Thu, Oct 23, 2025, 12:22=E2=80=AFPM Max Breitmeyer via RT = &lt;UMBCHelp@rt.umbc.edu&nbsp;&lt;mailto:UMBCHelp@rt.umbc.edu&gt;&gt;<br /> &gt;&gt; &gt; wrote:<br /> &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt; Ticket &lt;URL:&nbsp;https://rt.umbc.edu/Ticket/Display.h= tml?id=3D3296910&nbsp;&lt;https://www.google.com/url?q=3Dhttps://rt.umbc.ed= u/Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D176192= 1999000000&amp;usg=3DAOvVaw099WO1Ar-MqC9UTifVgSrZ&gt;<br /> &gt;&gt; &gt;&gt; &lt;https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ti= cket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761874374= 000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&nbsp;&lt;https://www.google.c= om/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.edu/Ticket/Displa= y.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg%= 3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&amp;source=3Dgmail-imap&amp;ust=3D1761921999= 000000&amp;usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; Last Update From Ticket:<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; Hi Mohammad,<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; According to the error message, it&#39;s having an issue = finding another file<br /> &gt;&gt; &gt;&gt; in<br /> &gt;&gt; &gt;&gt; the directory. This appears to be an issue due to the mou= nting locations<br /> &gt;&gt; &gt;&gt; being<br /> &gt;&gt; &gt;&gt; different. I would point you to the previous email where = I said that any<br /> &gt;&gt; &gt;&gt; software with hard-coded locations would not work, and th= at software<br /> &gt;&gt; &gt;&gt; compatibility is not guaranteed. I would recommend workin= g with Skye on<br /> &gt;&gt; &gt;&gt; porting<br /> &gt;&gt; &gt;&gt; what is in the directory to your local research directori= es on chip.<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; On Thu Oct 23 12:09:34 2025, NQ23652 wrote:<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt; Yes you are right, now I could see all shell files. = However still I have<br /> &gt;&gt; &gt;&gt; &gt; problem when I run the sh file.<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt; [e127@chip-login2 scripts]$ sh launch_synopsys_hspic= e.sh<br /> &gt;&gt; &gt;&gt; &gt; launch_synopsys_hspice.sh: line 3:<br /> &gt;&gt; &gt;&gt; &gt; /umbc/software/scripts/env_synopsys_hspice.sh: No su= ch file or directory<br /> &gt;&gt; &gt;&gt; &gt; [e127@chip-login2 scripts]$<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt; Would you please take a look to it.<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt; Regards,<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt; Mohammad<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt; On Thu Oct 23 11:59:17 2025, OL73413 wrote:<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt; Hi Mohammad,<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt; A good point. Chip uses a service called &quot;a= utofs&quot; to mount and unmount<br /> &gt;&gt; &gt;&gt; &gt;&gt; directories that aren&#39;t being actively used.=  This helps maintain the<br /> &gt;&gt; &gt;&gt; &gt;&gt; login nodes integrity. If you see in my screensh= ot below, the directory<br /> &gt;&gt; &gt;&gt; &gt;&gt; appears to be empty, but when I cd to the csee d= irectory where your<br /> &gt;&gt; &gt;&gt; &gt;&gt; software is mounted, it is all available. Backin= g out of the directory<br /> &gt;&gt; &gt;&gt; &gt;&gt; and ls&#39;ing the directory also shows that the=  directory is now mounted<br /> &gt;&gt; &gt;&gt; &gt;&gt; and available.<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt; On Thu, Oct 23, 2025 at 11:41 AM Mohammad Ebrahi= mabadi via RT<br /> &gt;&gt; &gt;&gt; &gt;&gt; &lt;UMBCHelp@rt.umbc.edu&nbsp;&lt;mailto:UMBCHel= p@rt.umbc.edu&gt;&gt; wrote:<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Ticket &lt;URL:&nbsp;https://rt.umbc.edu/Tic= ket/Display.html?id=3D3296910&nbsp;&lt;https://www.google.com/url?q=3Dhttps= ://rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp= ;ust=3D1761921999000000&amp;usg=3DAOvVaw099WO1Ar-MqC9UTifVgSrZ&gt;<br /> &gt;&gt; &gt;&gt; &lt;https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ti= cket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761874374= 000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&nbsp;&lt;https://www.google.c= om/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.edu/Ticket/Displa= y.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg%= 3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&amp;source=3Dgmail-imap&amp;ust=3D1761921999= 000000&amp;usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Comment From Ticket:<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Thank you Max!<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Actually we have still problem for accessing=  that directory. I<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; uploaded two<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; screenshots that shows what we can see in th= e department servers<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; and what we<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; can see in CHIP.<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; For example by running this shell file we ca= n launch HSPICE<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; software in the<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; department server:<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; /umbc/software/scripts/launch_synopsys_hspic= e.sh<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; But it is not still accessible on CHIP.<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Regards,<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Mohammad<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; On Thu Oct 23 10:54:59 2025, OL73413 wrote:<= br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Hi Nahmeh,<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Apologies &mdash; I meant to update you=  yesterday after completing<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; this, but I<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; was pulled into a long meeting. The sof= tware is now available on<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; chip at:<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; /umbc/software/csee<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Please note that if the software contai= ns any hardcoded file<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; paths, it may<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; not function correctly. Unfortunately, = we don&rsquo;t have control over<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; those<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; cases.<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Also, just to clarify, Skye and I work = in separate departments.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Similar to<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; how I don&rsquo;t have administrative a= ccess to the CSEE department<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; servers<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; (which include the software directory),=  Skye is not an<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; administrator on<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; chip.<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Please let me know if you encounter any=  issues accessing the<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; software on<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; the server. If there are problems runni= ng the software, I&rsquo;ll<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; coordinate<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; with Skye to troubleshoot, but since we=  don&rsquo;t maintain this<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; software, we<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; can&rsquo;t guarantee full compatibilit= y on our systems.<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; On Thu Oct 23 10:36:41 2025,&nbsp;naghm= eh.karimi@umbc.edu&nbsp;&lt;mailto:naghmeh.karimi@umbc.edu&gt; wrote:<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Hi Skye and Max, Could you please h= elp on mounting this today so<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; we can<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; use them as we have a deadline? Tha= nks,Naghmeh Karimi On Wed,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; Oct 22,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; 2025 at 4:22 PM Skye Jonke via RT &= lt;UMBCHelp@rt.umbc.edu&nbsp;&lt;mailto:UMBCHelp@rt.umbc.edu&gt;&gt; wrote:= <br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Ticket &lt;URL:&nbsp;https://rt= .umbc.edu/Ticket/Display.html?id=3D3296910&nbsp;&lt;https://www.google.com/= url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;source=3Dg= mail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw099WO1Ar-MqC9UTifVgSrZ= &gt;<br /> &gt;&gt; &gt;&gt; &lt;https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ti= cket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761874374= 000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&nbsp;&lt;https://www.google.c= om/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.edu/Ticket/Displa= y.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg%= 3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&amp;source=3Dgmail-imap&amp;ust=3D1761921999= 000000&amp;usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Last Update From Ticket:<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Mohammad,<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; So long as you continue to acce= ss them through<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; /umbc/software/scripts, they sh= ould remain up to date and you<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; will<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; see any new scripts. If you cop= y them to a local directory on<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; the<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; HPC cluster, they will not be u= pdated.<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; ---<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Skye Jonke<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; she/her<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Specialist, Linux System Admini= strator &amp; Lab Technical Support<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; On Oct 22, 2025, at 16:14,=  Naghmeh Karimi &lt;nkarimi@umbc.edu&nbsp;&lt;mailto:nkarimi@umbc.edu&gt;&g= t;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; wrote:<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Hi Max, Roy, Skye,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Thank you all for your hel= p.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; I added Skye here as she i= s our IT exert on adding the<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; capability<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; of running synopsis tools (Hspi= ce) in servers.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Skye we want to use the HP= C cluster to run hspice an other<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; synopsys tools. Could you pleas= e help on this?<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Thanks,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Naghmeh<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; On Wed, Oct 22, 2025 at 2:= 06 PM Mohammad Ebrahimabadi via RT<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &lt;UMBCHelp@rt.umbc.edu&nbsp;&= lt;mailto:UMBCHelp@rt.umbc.edu&gt; &lt;mailto:UMBCHelp@rt.umbc.edu&lt;mailt= o:UMBCHelp@rt.umbc.edu&gt;&gt;&gt; wrote:<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Ticket &lt;URL:<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt;&nbsp;https://rt.umbc.edu/Ticket/Display.html= ?id=3D3296910&nbsp;&lt;https://www.google.com/url?q=3Dhttps://rt.umbc.edu/T= icket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D176192199= 9000000&amp;usg=3DAOvVaw099WO1Ar-MqC9UTifVgSrZ&gt;<br /> &gt;&gt; &gt;&gt; &lt;https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ti= cket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1761874374= 000000&amp;usg=3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&nbsp;&lt;https://www.google.c= om/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.edu/Ticket/Displa= y.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg%= 3DAOvVaw3ImziGtKZ-9j9JFW8J65w7&amp;source=3Dgmail-imap&amp;ust=3D1761921999= 000000&amp;usg=3DAOvVaw3WPs04yO5gQkOFpjU6my-g&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &lt;<br /> &gt;&gt; &gt;&gt;&nbsp;https://www.google.com/url?q=3Dhttps://rt.umbc.edu/T= icket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D176176889= 1000000&amp;usg=3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW&nbsp;&lt;https://www.google.= com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://rt.umbc.edu/Ticket/Displ= ay.html?id%253D3296910%26source%3Dgmail-imap%26ust%3D1761768891000000%26usg= %3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW&amp;source=3Dgmail-imap&amp;ust=3D176192199= 9000000&amp;usg=3DAOvVaw04hQC2SRO0y4BoVR0LFNp_&gt;<br /> &gt;&gt; &gt;&gt; &lt;https://www.google.com/url?q=3Dhttps://www.google.com= /url?q%3Dhttps://rt.umbc.edu/Ticket/Display.html?id%253D3296910%26source%3D= gmail-imap%26ust%3D1761768891000000%26usg%3DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW&am= p;source=3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw0fLylAsBAo= Q6LgqY79I9-d&lt;https://www.google.com/url?q=3Dhttps://www.google.com/url?q= %3Dhttps://www.google.com/url?q%253Dhttps://rt.umbc.edu/Ticket/Display.html= ?id%25253D3296910%2526source%253Dgmail-imap%2526ust%253D1761768891000000%25= 26usg%253DAOvVaw2jxB3nArHBD7-ij9ZN8ZsW%26source%3Dgmail-imap%26ust%3D176187= 4374000000%26usg%3DAOvVaw0fLylAsBAoQ6LgqY79I9-d&amp;source=3Dgmail-imap&amp= ;ust=3D1761921999000000&amp;usg=3DAOvVaw3pQOXM-6FD6kt0kA6ujsVN&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Last Update From Ticke= t:<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Hi Max,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Thanks for your help! = Let&rsquo;s proceed based on your<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; suggestion,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; hopefully it<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; works as expected.<br = /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; I just have one quick = question: since the IT team in the<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; department can modify<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; or add new shell scrip= ts in that directory, will we also<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; have<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; access to those<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; updated or newly added=  scripts from the mounted directory on<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; HPCF?<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Thanks again,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Mohammad<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; On Wed Oct 22 14:01:32=  2025, OL73413 wrote:<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Hi Mohammad,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; We can get the so= ftware mounted for you on chip, but it<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; will<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; have to be<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; read-only as we w= on&#39;t be able to manage who can make<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; changes<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; to it<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; otherwise.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; As a reminder, th= e machines on that run on chip are not<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; the<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; same as those<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; run by the CSEE d= epartment, so we can&#39;t guarantee that<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; these<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; will run the<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; same way or as ef= fectively.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; If that&#39;s all=  ok with you, please give me some time to get<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; the<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; directory<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; mounted.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; On Wed Oct 22 13:= 26:05 2025, NQ23652 wrote:<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Thanks Tartel= a for your help to resolve my issue!<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Actually it s= eems all server in CSEE department have<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; access<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; to that<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; path. When I = ran the command that you gave me I got the<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; following<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; report:<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; mohammad@albo= rz:~$ df -h /umbc/software<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Filesystem Si= ze Used Avail Use% Mounted on<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&nbsp;nfs.iss.= rs.umbc.edu&nbsp;&lt;http://nfs.iss.rs.umbc.edu/&gt;:/ifs/data/csee/softwar= e 2.5T 2.2T<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; 326G<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; 88%<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; /umbc/softwar= e<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; mohammad@albo= rz:~$ mount | grep /umbc/software<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&nbsp;nfs.iss.= rs.umbc.edu&nbsp;&lt;http://nfs.iss.rs.umbc.edu/&gt;:/ifs/data/csee/softwar= e on<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; /umbc/software<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; type nfs<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt; (rw,relatime,vers=3D3,rsize=3D131072,wsize=3D524288,namle= n=3D255,acregmin=3D15,acregmax=3D15,acdirmin=3D15,acdirmax=3D15,hard,noacl,= proto=3Dtcp,timeo=3D600,retrans=3D2,sec=3Dsys,mountaddr=3D10.2.44.60,mountv= ers=3D3,mountport=3D300,mountproto=3Dtcp,local_lock=3Dnone,addr=3D10.2.44.6= 0)<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Please let me=  know if you need other information.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Regards,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; Mohammad<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt; On Wed Oct 22=  13:15:12 2025, HK41259 wrote:<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Hello,<br=  /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Thanks fo= r reaching out. Could you clarify which<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; specific<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; machine<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; or server=  you&rsquo;re referring to when you mention the<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &ldquo;departmental<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; server&rd= quo;?<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; The /umbc= /software/scripts/ directory isn&rsquo;t accessible<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; from<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; HPCF<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; (e.g., Ch= ip) by default &mdash; those systems have separate<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; storage and<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; don&rsquo= ;t automatically mount departmental shares.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Could you=  check where /umbc/software is mounted on your<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; departmen= tal system (e.g., by running df -h<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; /umbc/software<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; or mount<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; | grep /u= mbc/software)? That&rsquo;ll tell us which server<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; hosts<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; it. Once<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; we know t= hat, we can confirm whether it&rsquo;s possible or<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; appropriate<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; to make i= t visible from HPCF.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Best,<br = /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Tartela<b= r /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; On Tue Oc= t 21 15:59:49 2025, ZZ99999 wrote:<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; First=  Name: Mohammad<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Last = Name: Ebrahimabadi<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Email= :&nbsp;e127@umbc.edu&nbsp;&lt;mailto:e127@umbc.edu&gt; &lt;mailto:e127@umbc= .edu&lt;mailto:e127@umbc.edu&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Campu= s ID: NQ23652<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Reque= st Type: High Performance Cluster<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Dear = Team,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; I hop= e you&rsquo;re doing well.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; I&rsq= uo;m a Ph.D. student in Computer Engineering and have a<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; question regarding running a so= ftware on the HPCF. On the<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; department server, I usually ru= n software using shell scripts<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; located at /umbc/software/scrip= ts/ which is provided by the IT<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; group pf the department (Geoff = Weiss Team). However, in case of<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; HPCF, it seems that this path i= s not accessible from the HPCF<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; environment.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Could=  you please let me know if there is any specific<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; configuration or access permiss= ion required to reach this<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; directory<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; through the HPCF? or direct me = to a related person that can<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; help<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; me.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Best = regards,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&gt; Moham= mad<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; --<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Best,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; Max Breitmeyer<br=  /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; DOIT HPC System A= dministrator<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; --<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Naghmeh Karimi, Ph.D.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Associate Professor<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Department of Computer Sci= ence and Electrical Engineering<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; University of Maryland, Ba= ltimore County<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Baltimore, MD 21250<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Tel: 410-455-3965 E-mail:&= nbsp;nkarimi@umbc.edu&nbsp;&lt;mailto:nkarimi@umbc.edu&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &lt;mailto:nkarimi@umbc.edu&nbs= p;&lt;mailto:nkarimi@umbc.edu&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt; Web:&nbsp;http://www.csee.= umbc.edu/~nkarimi/&nbsp;&lt;https://www.google.com/url?q=3Dhttp://www.csee.= umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&amp;u= sg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7&gt;<br /> &gt;&gt; &gt;&gt; &lt;https://www.google.com/url?q=3Dhttp://www.csee.umbc.e= du/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg=3DA= OvVaw23Fk_KFBnlrXC_uhIbUFIW&nbsp;&lt;https://www.google.com/url?q=3Dhttps:/= /www.google.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmail= -imap%26ust%3D1761874374000000%26usg%3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&amp;sou= rce=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw05OHRJ3ua_KXM9E= HoVYfgE&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &lt;<br /> &gt;&gt; &gt;&gt;&nbsp;https://www.google.com/url?q=3Dhttp://www.csee.umbc.= edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761768891000000&amp;usg=3D= AOvVaw0xLE02sw5-ueWRHyHyCwkN&nbsp;&lt;https://www.google.com/url?q=3Dhttps:= //www.google.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmai= l-imap%26ust%3D1761768891000000%26usg%3DAOvVaw0xLE02sw5-ueWRHyHyCwkN&amp;so= urce=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw06Mkt_vSjpo8ln= qxEqL6dt&gt;<br /> &gt;&gt; &gt;&gt; &lt;https://www.google.com/url?q=3Dhttps://www.google.com= /url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmail-imap%26ust%3D1= 761768891000000%26usg%3DAOvVaw0xLE02sw5-ueWRHyHyCwkN&amp;source=3Dgmail-ima= p&amp;ust=3D1761874374000000&amp;usg=3DAOvVaw3g338pGQqShCgyX8cIt8tV&nbsp;&l= t;https://www.google.com/url?q=3Dhttps://www.google.com/url?q%3Dhttps://www= .google.com/url?q%253Dhttp://www.csee.umbc.edu/~nkarimi/%2526source%253Dgma= il-imap%2526ust%253D1761768891000000%2526usg%253DAOvVaw0xLE02sw5-ueWRHyHyCw= kN%26source%3Dgmail-imap%26ust%3D1761874374000000%26usg%3DAOvVaw3g338pGQqSh= CgyX8cIt8tV&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAO= vVaw3ktt5GEiV5miHD1TY9Dut7&gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; -- Naghmeh Karimi, Ph.D.<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Associate Professor<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Department of Computer Science and = Electrical Engineering<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; University of Maryland, Baltimore C= ounty<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Baltimore, MD 21250<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Tel: 410-455-3965 E-mail:&nbsp;nkar= imi@umbc.edu&nbsp;&lt;mailto:nkarimi@umbc.edu&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Web:&nbsp;http://www.csee.umbc.edu/= ~nkarimi/&nbsp;&lt;https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/= ~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvV= aw35D5VfZwU_gTyQhCtkbUZ7&gt;<br /> &gt;&gt; &gt;&gt; &lt;https://www.google.com/url?q=3Dhttp://www.csee.umbc.e= du/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1761874374000000&amp;usg=3DA= OvVaw23Fk_KFBnlrXC_uhIbUFIW&nbsp;&lt;https://www.google.com/url?q=3Dhttps:/= /www.google.com/url?q%3Dhttp://www.csee.umbc.edu/~nkarimi/%26source%3Dgmail= -imap%26ust%3D1761874374000000%26usg%3DAOvVaw23Fk_KFBnlrXC_uhIbUFIW&amp;sou= rce=3Dgmail-imap&amp;ust=3D1761921999000000&amp;usg=3DAOvVaw05OHRJ3ua_KXM9E= HoVYfgE&gt;&gt;<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; --<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Best,<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Max Breitmeyer<br /> &gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; DOIT HPC System Administrator<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; &gt;&gt; -- V/R,Maxwell BreitmeyerUMBC HPCF SpecialistUMB= C Observatory IT<br /> &gt;&gt; &gt;&gt; &gt;&gt; Manager Graduate Student(443) 835-8250<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; --<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt; Best,<br /> &gt;&gt; &gt;&gt; Max Breitmeyer<br /> &gt;&gt; &gt;&gt; DOIT HPC System Administrator<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;&gt;<br /> &gt;&gt; &gt;<br /> &gt;&gt;&nbsp;<br /> &gt;&gt; --&nbsp;<br /> &gt;&gt; Naghmeh Karimi, Ph.D.<br /> &gt;&gt; Associate Professor<br /> &gt;&gt; Department of Computer Science and Electrical Engineering<br /> &gt;&gt; University of Maryland, Baltimore County<br /> &gt;&gt; Baltimore, MD 21250<br /> &gt;&gt; Tel: *410-455-3965* E-mail: *nkarimi@umbc.edu&nbsp;&lt;mailto:nkar= imi@umbc.edu&gt; &lt;nkarimi@umbc.edu&lt;mailto:nkarimi@umbc.edu&gt;&gt;*<b= r /> &gt;&gt; Web: *http://www.csee.umbc.edu/~nkarimi/&nbsp;&lt;https://www.goog= le.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&a= mp;ust=3D1761921999000000&amp;usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7&gt;<br /> &gt;&gt; &lt;http://www.csee.umbc.edu/~nkarimi/&nbsp;&lt;https://www.google= .com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp= ;ust=3D1761921999000000&amp;usg=3DAOvVaw35D5VfZwU_gTyQhCtkbUZ7&gt;&gt;*</bl= ockquote> </div> </div> </blockquote> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3296910,72524480,Correspond,DoIT-Research-Computing,2025-10-28 13:42:45.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Skye Jonke,ii69854@umbc.edu,"<p>Got it, in that case, it might be as simple as sourcing the bashrc rather than executing a script which sources it. Is it possible for me to get access to the cluster so I can test it?&nbsp;<br /> <br /> The NFS share is on visiond1.cs.umbc.edu, and contains data stores /data1 and /data2 that are Dr. Karimi and her grad students use. Its set up to only share to certain IPs, in that case just the IP of the head cluster would probably be enough to share it.</p> "
3296910,72527434,Correspond,DoIT-Research-Computing,2025-10-28 14:42:27.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Roy Prouty,proutyr1@umbc.edu,"<div> <p>Hi all,<br /> <br /> I&#39;ll let Max respond RE linker paths for the CSEE Software, but I wanted to weigh-in on the potential NFS shares being discussed below.<br /> <br /> As a rule, DoIT does not allow for filesystem mounts to/from non-DoIT managed systems to/from DoIT-managed systems. This is to maintain a stability and security standard we&#39;ve worked to ensure with external research sponsors. For clarity, the CSEE Software share is &quot;living&quot; on a DoIT-managed Dell Isilon Storage System -- which is why we&#39;re OK with a read-only mount from that system into the cluster.<br /> <br /> Beyond these stability/security requirements directly related to our cluster hardware, the cluster compute nodes are only routable via an internal class A network. We are here again unwilling to expose paths into this network from external-to-DoIT devices. The cluster does have a more accessible gateway that will allow for non-persistent connections via file xfer (scp/rsync/similar) commands from a compute node or from the cluster login nodes.</p>  <p>I&#39;ll let Max answer any questions you might have about other modes of file transfer, either via scp/rsync/similar.</p>  <p>On Tue Oct 28 09:42:45 2025, II69854 wrote:</p>  <blockquote> <p>Got it, in that case, it might be as simple as sourcing the bashrc rather than executing a script which sources it. Is it possible for me to get access to the cluster so I can test it?&nbsp;<br /> <br /> The NFS share is on visiond1.cs.umbc.edu, and contains data stores /data1 and /data2 that are Dr. Karimi and her grad students use. Its set up to only share to certain IPs, in that case just the IP of the head cluster would probably be enough to share it.</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Roy Prouty<br /> DoIT Research Computing Team</p> "
3296910,72530115,Correspond,DoIT-Research-Computing,2025-10-28 15:42:53.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Skye,</p>  <p>You can get access by requesting an account here:&nbsp;https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research%20Computing to Naghmeh&#39;s PI group (assuming they are ok with that).&nbsp;</p>  <p>On Tue Oct 28 10:42:27 2025, WH39335 wrote:</p>  <blockquote> <div> <p>Hi all,<br /> <br /> I&#39;ll let Max respond RE linker paths for the CSEE Software, but I wanted to weigh-in on the potential NFS shares being discussed below.<br /> <br /> As a rule, DoIT does not allow for filesystem mounts to/from non-DoIT managed systems to/from DoIT-managed systems. This is to maintain a stability and security standard we&#39;ve worked to ensure with external research sponsors. For clarity, the CSEE Software share is &quot;living&quot; on a DoIT-managed Dell Isilon Storage System -- which is why we&#39;re OK with a read-only mount from that system into the cluster.<br /> <br /> Beyond these stability/security requirements directly related to our cluster hardware, the cluster compute nodes are only routable via an internal class A network. We are here again unwilling to expose paths into this network from external-to-DoIT devices. The cluster does have a more accessible gateway that will allow for non-persistent connections via file xfer (scp/rsync/similar) commands from a compute node or from the cluster login nodes.</p>  <p>I&#39;ll let Max answer any questions you might have about other modes of file transfer, either via scp/rsync/similar.</p>  <p>On Tue Oct 28 09:42:45 2025, II69854 wrote:</p>  <blockquote> <p>Got it, in that case, it might be as simple as sourcing the bashrc rather than executing a script which sources it. Is it possible for me to get access to the cluster so I can test it?&nbsp;<br /> <br /> The NFS share is on visiond1.cs.umbc.edu, and contains data stores /data1 and /data2 that are Dr. Karimi and her grad students use. Its set up to only share to certain IPs, in that case just the IP of the head cluster would probably be enough to share it.</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Roy Prouty<br /> DoIT Research Computing Team</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3296910,72535343,Correspond,DoIT-Research-Computing,2025-10-28 18:01:42.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Skye Jonke,ii69854@umbc.edu,"Dr. Karimi, is it alright for me to get an account in your group? It looks like I will need some info, like your PI email and group name.  --- Skye Jonke  she/her  Specialist, Linux System Administrator & Lab Technical Support  > On Oct 28, 2025, at 11:42, Max Breitmeyer via RT <UMBCHelp@rt.umbc.edu> wrote: >  > Ticket <URL: https://www.google.com/url?q=https://rt.umbc.edu/Ticket/Display.html?id%3D3296910&source=gmail-imap&ust=1762270980000000&usg=AOvVaw1slN5U7sA_6m11CLIWg_XB > >  > Last Update From Ticket: >  > Hi Skye, >  > You can get access by requesting an account here: > https://www.google.com/url?q=https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto%3DResearch%2520Computing&source=gmail-imap&ust=1762270980000000&usg=AOvVaw3lkeIx486anNYkX22uh_VO > to Naghmeh's PI group (assuming they are ok with that). >  > On Tue Oct 28 10:42:27 2025, WH39335 wrote: >  >> Hi all, >  >> I'll let Max respond RE linker paths for the CSEE Software, but I wanted to >> weigh-in on the potential NFS shares being discussed below. >  >> As a rule, DoIT does not allow for filesystem mounts to/from non-DoIT >> managed systems to/from DoIT-managed systems. This is to maintain a >> stability and security standard we've worked to ensure with external >> research sponsors. For clarity, the CSEE Software share is ""living"" on a >> DoIT-managed Dell Isilon Storage System -- which is why we're OK with a >> read-only mount from that system into the cluster. >  >> Beyond these stability/security requirements directly related to our >> cluster hardware, the cluster compute nodes are only routable via an >> internal class A network. We are here again unwilling to expose paths into >> this network from external-to-DoIT devices. The cluster does have a more >> accessible gateway that will allow for non-persistent connections via file >> xfer (scp/rsync/similar) commands from a compute node or from the cluster >> login nodes. >  >> I'll let Max answer any questions you might have about other modes of file >> transfer, either via scp/rsync/similar. >  >> On Tue Oct 28 09:42:45 2025, II69854 wrote: >  >>> Got it, in that case, it might be as simple as sourcing the bashrc >>> rather than executing a script which sources it. Is it possible for me >>> to get access to the cluster so I can test it? >  >>> The NFS share is on visiond1.cs.umbc.edu, and contains data stores >>> /data1 and /data2 that are Dr. Karimi and her grad students use. Its >>> set up to only share to certain IPs, in that case just the IP of the >>> head cluster would probably be enough to share it. >  >> -- >  >> Roy Prouty >> DoIT Research Computing Team >  > -- >  > Best, > Max Breitmeyer > DOIT HPC System Administrator >  >   "
3296910,72535343,Correspond,DoIT-Research-Computing,2025-10-28 18:01:42.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Skye Jonke,ii69854@umbc.edu,"<html aria-label=""message body""><head><meta http-equiv=""content-type"" content=""text/html; charset=us-ascii""></head><body style=""overflow-wrap: break-word; -webkit-nbsp-mode: space; line-break: after-white-space;"">Dr. Karimi, is it alright for me to get an account in your group? It looks like I will need some info, like your PI email and group name.&nbsp;<br id=""lineBreakAtBeginningOfMessage""><div> <meta charset=""UTF-8""><div dir=""auto"" style=""caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0); letter-spacing: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration: none; overflow-wrap: break-word; -webkit-nbsp-mode: space; line-break: after-white-space;"">---</div><div dir=""auto"" style=""caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0); letter-spacing: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration: none; overflow-wrap: break-word; -webkit-nbsp-mode: space; line-break: after-white-space;"">Skye Jonke<br><br>she/her<br><br>Specialist, Linux System Administrator &amp;<span class=""Apple-converted-space"">&nbsp;</span>Lab Technical Support<br></div> </div> <div><br><blockquote type=""cite""><div>On Oct 28, 2025, at 11:42, Max Breitmeyer via RT &lt;UMBCHelp@rt.umbc.edu&gt; wrote:</div><br class=""Apple-interchange-newline""><div><div>Ticket &lt;URL: https://www.google.com/url?q=https://rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;source=gmail-imap&amp;ust=1762270980000000&amp;usg=AOvVaw1slN5U7sA_6m11CLIWg_XB &gt;<br><br>Last Update From Ticket:<br><br>Hi Skye,<br><br>You can get access by requesting an account here:<br>https://www.google.com/url?q=https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto%3DResearch%2520Computing&amp;source=gmail-imap&amp;ust=1762270980000000&amp;usg=AOvVaw3lkeIx486anNYkX22uh_VO<br>to Naghmeh's PI group (assuming they are ok with that).<br><br>On Tue Oct 28 10:42:27 2025, WH39335 wrote:<br><br><blockquote type=""cite"">Hi all,<br></blockquote><br><blockquote type=""cite"">I'll let Max respond RE linker paths for the CSEE Software, but I wanted to<br>weigh-in on the potential NFS shares being discussed below.<br></blockquote><br><blockquote type=""cite"">As a rule, DoIT does not allow for filesystem mounts to/from non-DoIT<br>managed systems to/from DoIT-managed systems. This is to maintain a<br>stability and security standard we've worked to ensure with external<br>research sponsors. For clarity, the CSEE Software share is ""living"" on a<br>DoIT-managed Dell Isilon Storage System -- which is why we're OK with a<br>read-only mount from that system into the cluster.<br></blockquote><br><blockquote type=""cite"">Beyond these stability/security requirements directly related to our<br>cluster hardware, the cluster compute nodes are only routable via an<br>internal class A network. We are here again unwilling to expose paths into<br>this network from external-to-DoIT devices. The cluster does have a more<br>accessible gateway that will allow for non-persistent connections via file<br>xfer (scp/rsync/similar) commands from a compute node or from the cluster<br>login nodes.<br></blockquote><br><blockquote type=""cite"">I'll let Max answer any questions you might have about other modes of file<br>transfer, either via scp/rsync/similar.<br></blockquote><br><blockquote type=""cite"">On Tue Oct 28 09:42:45 2025, II69854 wrote:<br></blockquote><br><blockquote type=""cite""><blockquote type=""cite"">Got it, in that case, it might be as simple as sourcing the bashrc<br>rather than executing a script which sources it. Is it possible for me<br>to get access to the cluster so I can test it?<br></blockquote></blockquote><br><blockquote type=""cite""><blockquote type=""cite"">The NFS share is on visiond1.cs.umbc.edu, and contains data stores<br>/data1 and /data2 that are Dr. Karimi and her grad students use. Its<br>set up to only share to certain IPs, in that case just the IP of the<br>head cluster would probably be enough to share it.<br></blockquote></blockquote><br><blockquote type=""cite"">--<br></blockquote><br><blockquote type=""cite"">Roy Prouty<br>DoIT Research Computing Team<br></blockquote><br>--<br><br>Best,<br>Max Breitmeyer<br>DOIT HPC System Administrator<br><br><br></div></div></blockquote></div><br></body></html>"
3296910,72535686,Correspond,DoIT-Research-Computing,2025-10-28 18:09:27.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,NULL,naghmeh.karimi@umbc.edu,"Hi Skye,  Yes sure, please register under my group.  Regards, Naghmeh  On Tue, Oct 28, 2025 at 2:01=E2=80=AFPM Skye Jonke <ii69854@umbc.edu> wrote:  > Dr. Karimi, is it alright for me to get an account in your group? It looks > like I will need some info, like your PI email and group name. > --- > Skye Jonke > > she/her > > Specialist, Linux System Administrator & Lab Technical Support > > On Oct 28, 2025, at 11:42, Max Breitmeyer via RT <UMBCHelp@rt.umbc.edu> > wrote: > > Ticket <URL: > https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id= %3D3296910&source=3Dgmail-imap&ust=3D1762270980000000&usg=3DAOvVaw1slN5U7sA= _6m11CLIWg_XB > > > > Last Update From Ticket: > > Hi Skye, > > You can get access by requesting an account here: > > https://www.google.com/url?q=3Dhttps://rtforms.umbc.edu/rt_authenticated/= doit/DoIT-support.php?auto%3DResearch%2520Computing&source=3Dgmail-imap&ust= =3D1762270980000000&usg=3DAOvVaw3lkeIx486anNYkX22uh_VO > to Naghmeh's PI group (assuming they are ok with that). > > On Tue Oct 28 10:42:27 2025, WH39335 wrote: > > Hi all, > > > I'll let Max respond RE linker paths for the CSEE Software, but I wanted = to > weigh-in on the potential NFS shares being discussed below. > > > As a rule, DoIT does not allow for filesystem mounts to/from non-DoIT > managed systems to/from DoIT-managed systems. This is to maintain a > stability and security standard we've worked to ensure with external > research sponsors. For clarity, the CSEE Software share is ""living"" on a > DoIT-managed Dell Isilon Storage System -- which is why we're OK with a > read-only mount from that system into the cluster. > > > Beyond these stability/security requirements directly related to our > cluster hardware, the cluster compute nodes are only routable via an > internal class A network. We are here again unwilling to expose paths into > this network from external-to-DoIT devices. The cluster does have a more > accessible gateway that will allow for non-persistent connections via file > xfer (scp/rsync/similar) commands from a compute node or from the cluster > login nodes. > > > I'll let Max answer any questions you might have about other modes of file > transfer, either via scp/rsync/similar. > > > On Tue Oct 28 09:42:45 2025, II69854 wrote: > > > Got it, in that case, it might be as simple as sourcing the bashrc > rather than executing a script which sources it. Is it possible for me > to get access to the cluster so I can test it? > > > The NFS share is on visiond1.cs.umbc.edu, and contains data stores > /data1 and /data2 that are Dr. Karimi and her grad students use. Its > set up to only share to certain IPs, in that case just the IP of the > head cluster would probably be enough to share it. > > > -- > > > Roy Prouty > DoIT Research Computing Team > > > -- > > Best, > Max Breitmeyer > DOIT HPC System Administrator > > > >  --=20 Naghmeh Karimi, Ph.D. Associate Professor Department of Computer Science and Electrical Engineering University of Maryland, Baltimore County Baltimore, MD 21250 Tel: *410-455-3965* E-mail: *nkarimi@umbc.edu <nkarimi@umbc.edu>* Web: *http://www.csee.umbc.edu/~nkarimi/ <http://www.csee.umbc.edu/~nkarimi/>* "
3296910,72535686,Correspond,DoIT-Research-Computing,2025-10-28 18:09:27.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,NULL,naghmeh.karimi@umbc.edu,"<div dir=3D""ltr"">Hi Skye,<div><br></div><div>Yes sure, please register unde= r my group.</div><div><br></div><div>Regards,</div><div>Naghmeh</div></div>= <br><div class=3D""gmail_quote gmail_quote_container""><div dir=3D""ltr"" class= =3D""gmail_attr"">On Tue, Oct 28, 2025 at 2:01=E2=80=AFPM Skye Jonke &lt;<a h= ref=3D""mailto:ii69854@umbc.edu"">ii69854@umbc.edu</a>&gt; wrote:<br></div><b= lockquote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-le= ft:1px solid rgb(204,204,204);padding-left:1ex""><div>Dr. Karimi, is it alri= ght for me to get an account in your group? It looks like I will need some = info, like your PI email and group name.=C2=A0<br id=3D""m_-4778981646348501= lineBreakAtBeginningOfMessage""><div> <div dir=3D""auto"" style=3D""color:rgb(0,0,0);letter-spacing:normal;text-alig= n:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing= :0px;text-decoration:none"">---</div><div dir=3D""auto"" style=3D""color:rgb(0,= 0,0);letter-spacing:normal;text-align:start;text-indent:0px;text-transform:= none;white-space:normal;word-spacing:0px;text-decoration:none"">Skye Jonke<b= r><br>she/her<br><br>Specialist, Linux System Administrator &amp;<span>=C2= =A0</span>Lab Technical Support<br></div> </div> <div><br><blockquote type=3D""cite""><div>On Oct 28, 2025, at 11:42, Max Brei= tmeyer via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank""= >UMBCHelp@rt.umbc.edu</a>&gt; wrote:</div><br><div><div>Ticket &lt;URL: <a = href=3D""https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.h= tml?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D1762270980000000&amp;usg= =3DAOvVaw1slN5U7sA_6m11CLIWg_XB"" target=3D""_blank"">https://www.google.com/u= rl?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgm= ail-imap&amp;ust=3D1762270980000000&amp;usg=3DAOvVaw1slN5U7sA_6m11CLIWg_XB<= /a> &gt;<br><br>Last Update From Ticket:<br><br>Hi Skye,<br><br>You can get=  access by requesting an account here:<br><a href=3D""https://www.google.com= /url?q=3Dhttps://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?au= to%3DResearch%2520Computing&amp;source=3Dgmail-imap&amp;ust=3D1762270980000= 000&amp;usg=3DAOvVaw3lkeIx486anNYkX22uh_VO"" target=3D""_blank"">https://www.g= oogle.com/url?q=3Dhttps://rtforms.umbc.edu/rt_authenticated/doit/DoIT-suppo= rt.php?auto%3DResearch%2520Computing&amp;source=3Dgmail-imap&amp;ust=3D1762= 270980000000&amp;usg=3DAOvVaw3lkeIx486anNYkX22uh_VO</a><br>to Naghmeh&#39;s=  PI group (assuming they are ok with that).<br><br>On Tue Oct 28 10:42:27 2= 025, WH39335 wrote:<br><br><blockquote type=3D""cite"">Hi all,<br></blockquot= e><br><blockquote type=3D""cite"">I&#39;ll let Max respond RE linker paths fo= r the CSEE Software, but I wanted to<br>weigh-in on the potential NFS share= s being discussed below.<br></blockquote><br><blockquote type=3D""cite"">As a=  rule, DoIT does not allow for filesystem mounts to/from non-DoIT<br>manage= d systems to/from DoIT-managed systems. This is to maintain a<br>stability = and security standard we&#39;ve worked to ensure with external<br>research = sponsors. For clarity, the CSEE Software share is &quot;living&quot; on a<b= r>DoIT-managed Dell Isilon Storage System -- which is why we&#39;re OK with=  a<br>read-only mount from that system into the cluster.<br></blockquote><b= r><blockquote type=3D""cite"">Beyond these stability/security requirements di= rectly related to our<br>cluster hardware, the cluster compute nodes are on= ly routable via an<br>internal class A network. We are here again unwilling=  to expose paths into<br>this network from external-to-DoIT devices. The cl= uster does have a more<br>accessible gateway that will allow for non-persis= tent connections via file<br>xfer (scp/rsync/similar) commands from a compu= te node or from the cluster<br>login nodes.<br></blockquote><br><blockquote=  type=3D""cite"">I&#39;ll let Max answer any questions you might have about o= ther modes of file<br>transfer, either via scp/rsync/similar.<br></blockquo= te><br><blockquote type=3D""cite"">On Tue Oct 28 09:42:45 2025, II69854 wrote= :<br></blockquote><br><blockquote type=3D""cite""><blockquote type=3D""cite"">G= ot it, in that case, it might be as simple as sourcing the bashrc<br>rather=  than executing a script which sources it. Is it possible for me<br>to get = access to the cluster so I can test it?<br></blockquote></blockquote><br><b= lockquote type=3D""cite""><blockquote type=3D""cite"">The NFS share is on <a hr= ef=3D""http://visiond1.cs.umbc.edu"" target=3D""_blank"">visiond1.cs.umbc.edu</= a>, and contains data stores<br>/data1 and /data2 that are Dr. Karimi and h= er grad students use. Its<br>set up to only share to certain IPs, in that c= ase just the IP of the<br>head cluster would probably be enough to share it= .<br></blockquote></blockquote><br><blockquote type=3D""cite"">--<br></blockq= uote><br><blockquote type=3D""cite"">Roy Prouty<br>DoIT Research Computing Te= am<br></blockquote><br>--<br><br>Best,<br>Max Breitmeyer<br>DOIT HPC System=  Administrator<br><br><br></div></div></blockquote></div><br></div></blockq= uote></div><div><br clear=3D""all""></div><div><br></div><span class=3D""gmail= _signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_signature""= ><div dir=3D""ltr""><span style=3D""font-size:9.5pt;line-height:115%;font-fami= ly:Arial,sans-serif;color:rgb(136,136,136);background-image:initial;backgro= und-position:initial;background-repeat:initial"">Naghmeh=C2=A0Karimi, Ph.D.<= br> Associate Professor<br> Department of Computer Science and Electrical Engineering<br> University of Maryland, Baltimore County<br> Baltimore, MD 21250<br> Tel:</span><span style=3D""font-size:11pt;line-height:115%;font-family:Calib= ri,&quot;sans-serif&quot;;color:black"">=C2=A0</span><u><span style=3D""font-= size:11pt;line-height:115%;font-family:Calibri,&quot;sans-serif&quot;;color= :rgb(17,85,204)"">410-455-3965</span></u><span style=3D""font-size:11pt;line-= height:115%;font-family:Calibri,&quot;sans-serif&quot;;color:black""> </span= ><span style=3D""font-size:9.5pt;line-height:115%;font-family:Arial,sans-ser= if;color:rgb(136,136,136);background-image:initial;background-position:init= ial;background-repeat:initial"">E-mail:</span><span style=3D""font-size:11pt;= line-height:115%;font-family:Calibri,&quot;sans-serif&quot;;color:black"">= =C2=A0</span><u><span style=3D""font-size:11pt;line-height:115%;font-family:= Calibri,&quot;sans-serif&quot;;color:rgb(17,85,204)""><a href=3D""mailto:nkar= imi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a></span></u><span style= =3D""font-size:9.5pt;line-height:115%;font-family:Arial,sans-serif;color:rgb= (136,136,136);background-image:initial;background-position:initial;backgrou= nd-repeat:initial""><br> Web:</span><span style=3D""font-size:9.5pt;line-height:115%;font-family:Aria= l,sans-serif;color:black;background-image:initial;background-position:initi= al;background-repeat:initial"">=C2=A0</span><u><span style=3D""font-size:11pt= ;line-height:115%;font-family:Calibri,sans-serif;color:rgb(17,85,204);backg= round-image:initial;background-position:initial;background-repeat:initial"">= <a href=3D""http://www.csee.umbc.edu/~nkarimi/"" target=3D""_blank"">http://www= .csee.umbc.edu/~nkarimi/</a></span></u></div></div> "
3296910,72537652,Correspond,DoIT-Research-Computing,2025-10-28 18:58:32.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Skye Jonke,ii69854@umbc.edu,"<p>Okay, I created a request for an HPC account, RT#3301107</p> "
3296910,72564484,Correspond,DoIT-Research-Computing,2025-10-29 18:30:07.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Skye Jonke,ii69854@umbc.edu,"<p>On the HPC cluster, it should now be possible to run hspice using the command:<br /> &nbsp;</p>  <p>&nbsp;/umbc/software/csee/scripts/launch_synopsys_hspice.sh&nbsp;<br /> <br /> Please test it and let me know if it works. Also, let me know what other tools you need to use, if any, so I can adapt those scripts as well.</p> "
3296910,72569632,Correspond,DoIT-Research-Computing,2025-10-29 20:13:39.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,NULL,naghmeh.karimi@umbc.edu,"Hi Skye,  Thanks. Hspice works but we also need WV (waveviewer) to see Hspice results as well. Could you please add that also and let us know to check?  Thanks a lot, Naghmeh  On Wed, Oct 29, 2025 at 2:30=E2=80=AFPM Skye Jonke via RT <UMBCHelp@rt.umbc= .edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 > > > Last Update From Ticket: > > On the HPC cluster, it should now be possible to run hspice using the > command: > > /umbc/software/csee/scripts/launch_synopsys_hspice.sh > > Please test it and let me know if it works. Also, let me know what other > tools > you need to use, if any, so I can adapt those scripts as well. > > >  --=20 Naghmeh Karimi, Ph.D. Associate Professor Department of Computer Science and Electrical Engineering University of Maryland, Baltimore County Baltimore, MD 21250 Tel: *410-455-3965* E-mail: *nkarimi@umbc.edu <nkarimi@umbc.edu>* Web: *http://www.csee.umbc.edu/~nkarimi/ <http://www.csee.umbc.edu/~nkarimi/>* "
3296910,72569632,Correspond,DoIT-Research-Computing,2025-10-29 20:13:39.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,NULL,naghmeh.karimi@umbc.edu,"<div dir=3D""ltr"">Hi Skye,<div><br></div><div>Thanks. Hspice works but we al= so need WV (waveviewer) to see Hspice results as well. Could you please add=  that also and let us know to check?</div><div><br></div><div>Thanks=C2=A0a=  lot,</div><div>Naghmeh</div></div><br><div class=3D""gmail_quote gmail_quot= e_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Wed, Oct 29, 2025 at = 2:30=E2=80=AFPM Skye Jonke via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.ed= u"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_= quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,= 204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticke= t/Display.html?id=3D3296910"" rel=3D""noreferrer"" target=3D""_blank"">https://r= t.umbc.edu/Ticket/Display.html?id=3D3296910</a> &gt;<br> <br> Last Update From Ticket:<br> <br> On the HPC cluster, it should now be possible to run hspice using the comma= nd:<br> <br> /umbc/software/csee/scripts/launch_synopsys_hspice.sh<br> <br> Please test it and let me know if it works. Also, let me know what other to= ols<br> you need to use, if any, so I can adapt those scripts as well.<br> <br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><span style=3D""font-size:9.5pt;line-height:115%;= font-family:Arial,sans-serif;color:rgb(136,136,136);background-image:initia= l;background-position:initial;background-repeat:initial"">Naghmeh=C2=A0Karim= i, Ph.D.<br> Associate Professor<br> Department of Computer Science and Electrical Engineering<br> University of Maryland, Baltimore County<br> Baltimore, MD 21250<br> Tel:</span><span style=3D""font-size:11pt;line-height:115%;font-family:Calib= ri,&quot;sans-serif&quot;;color:black"">=C2=A0</span><u><span style=3D""font-= size:11pt;line-height:115%;font-family:Calibri,&quot;sans-serif&quot;;color= :rgb(17,85,204)"">410-455-3965</span></u><span style=3D""font-size:11pt;line-= height:115%;font-family:Calibri,&quot;sans-serif&quot;;color:black""> </span= ><span style=3D""font-size:9.5pt;line-height:115%;font-family:Arial,sans-ser= if;color:rgb(136,136,136);background-image:initial;background-position:init= ial;background-repeat:initial"">E-mail:</span><span style=3D""font-size:11pt;= line-height:115%;font-family:Calibri,&quot;sans-serif&quot;;color:black"">= =C2=A0</span><u><span style=3D""font-size:11pt;line-height:115%;font-family:= Calibri,&quot;sans-serif&quot;;color:rgb(17,85,204)""><a href=3D""mailto:nkar= imi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a></span></u><span style= =3D""font-size:9.5pt;line-height:115%;font-family:Arial,sans-serif;color:rgb= (136,136,136);background-image:initial;background-position:initial;backgrou= nd-repeat:initial""><br> Web:</span><span style=3D""font-size:9.5pt;line-height:115%;font-family:Aria= l,sans-serif;color:black;background-image:initial;background-position:initi= al;background-repeat:initial"">=C2=A0</span><u><span style=3D""font-size:11pt= ;line-height:115%;font-family:Calibri,sans-serif;color:rgb(17,85,204);backg= round-image:initial;background-position:initial;background-repeat:initial"">= <a href=3D""http://www.csee.umbc.edu/~nkarimi/"" target=3D""_blank"">http://www= .csee.umbc.edu/~nkarimi/</a></span></u></div></div> "
3296910,72588802,Correspond,DoIT-Research-Computing,2025-10-29 21:01:50.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Skye Jonke,ii69854@umbc.edu,"Wave view should also be working now, give it a try when you have a chance. --- Skye Jonke  she/her  Specialist, Linux System Administrator & Lab Technical Support  > On Oct 29, 2025, at 16:12, Naghmeh Karimi <nkarimi@umbc.edu> wrote: >=20 > Hi Skye, >=20 > Thanks. Hspice works but we also need WV (waveviewer) to see Hspice resul= ts as well. Could you please add that also and let us know to check? >=20 > Thanks a lot, > Naghmeh >=20 > On Wed, Oct 29, 2025 at 2:30=E2=80=AFPM Skye Jonke via RT <UMBCHelp@rt.um= bc.edu <mailto:UMBCHelp@rt.umbc.edu>> wrote: >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 <https= ://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3296= 910&source=3Dgmail-imap&ust=3D1762373620000000&usg=3DAOvVaw3sdxyNkvAgAa3D8C= kynvbF> > >>=20 >> Last Update From Ticket: >>=20 >> On the HPC cluster, it should now be possible to run hspice using the co= mmand: >>=20 >> /umbc/software/csee/scripts/launch_synopsys_hspice.sh >>=20 >> Please test it and let me know if it works. Also, let me know what other=  tools >> you need to use, if any, so I can adapt those scripts as well. >>=20 >>=20 >=20 >=20 >=20 > --=20 > Naghmeh Karimi, Ph.D. > Associate Professor > Department of Computer Science and Electrical Engineering > University of Maryland, Baltimore County > Baltimore, MD 21250 > Tel: 410-455-3965 E-mail: nkarimi@umbc.edu <mailto:nkarimi@umbc.edu> > Web: http://www.csee.umbc.edu/~nkarimi/ <https://www.google.com/url?q=3Dh= ttp://www.csee.umbc.edu/~nkarimi/&source=3Dgmail-imap&ust=3D176237362000000= 0&usg=3DAOvVaw1mTQPjV_EliH_BiHB0xweg> "
3296910,72588802,Correspond,DoIT-Research-Computing,2025-10-29 21:01:50.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Skye Jonke,ii69854@umbc.edu,"<html aria-label=3D""message body""><head><meta http-equiv=3D""content-type"" c= ontent=3D""text/html; charset=3Dutf-8""></head><body style=3D""overflow-wrap: = break-word; -webkit-nbsp-mode: space; line-break: after-white-space;"">Wave = view should also be working now, give it a try when you have a chance.<br i= d=3D""lineBreakAtBeginningOfMessage""><div> <meta charset=3D""UTF-8""><div dir=3D""auto"" style=3D""caret-color: rgb(0, 0, 0= ); color: rgb(0, 0, 0); letter-spacing: normal; text-align: start; text-ind= ent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; -we= bkit-text-stroke-width: 0px; text-decoration: none; overflow-wrap: break-wo= rd; -webkit-nbsp-mode: space; line-break: after-white-space;"">---</div><div=  dir=3D""auto"" style=3D""caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0); lett= er-spacing: normal; text-align: start; text-indent: 0px; text-transform: no= ne; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px;=  text-decoration: none; overflow-wrap: break-word; -webkit-nbsp-mode: space= ; line-break: after-white-space;"">Skye Jonke<br><br>she/her<br><br>Speciali= st, Linux System Administrator &amp;<span class=3D""Apple-converted-space"">&= nbsp;</span>Lab Technical Support<br></div> </div> <div><br><blockquote type=3D""cite""><div>On Oct 29, 2025, at 16:12, Naghmeh = Karimi &lt;nkarimi@umbc.edu&gt; wrote:</div><br class=3D""Apple-interchange-= newline""><div><meta charset=3D""UTF-8""><div dir=3D""ltr"" style=3D""caret-color= : rgb(0, 0, 0); font-family: Helvetica; font-size: 14px; font-style: normal= ; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; text= -align: start; text-indent: 0px; text-transform: none; white-space: normal;=  word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration: none;""= >Hi Skye,<div><br></div><div>Thanks. Hspice works but we also need WV (wave= viewer) to see Hspice results as well. Could you please add that also and l= et us know to check?</div><div><br></div><div>Thanks&nbsp;a lot,</div><div>= Naghmeh</div></div><br style=3D""caret-color: rgb(0, 0, 0); font-family: Hel= vetica; font-size: 14px; font-style: normal; font-variant-caps: normal; fon= t-weight: 400; letter-spacing: normal; text-align: start; text-indent: 0px;=  text-transform: none; white-space: normal; word-spacing: 0px; -webkit-text= -stroke-width: 0px; text-decoration: none;""><div class=3D""gmail_quote gmail= _quote_container"" style=3D""caret-color: rgb(0, 0, 0); font-family: Helvetic= a; font-size: 14px; font-style: normal; font-variant-caps: normal; font-wei= ght: 400; letter-spacing: normal; text-align: start; text-indent: 0px; text= -transform: none; white-space: normal; word-spacing: 0px; -webkit-text-stro= ke-width: 0px; text-decoration: none;""><div dir=3D""ltr"" class=3D""gmail_attr= "">On Wed, Oct 29, 2025 at 2:30=E2=80=AFPM Skye Jonke via RT &lt;<a href=3D""= mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><= blockquote class=3D""gmail_quote"" style=3D""margin: 0px 0px 0px 0.8ex; border= -left-width: 1px; border-left-style: solid; border-left-color: rgb(204, 204= , 204); padding-left: 1ex;"">Ticket &lt;URL:<span class=3D""Apple-converted-s= pace"">&nbsp;</span><a href=3D""https://www.google.com/url?q=3Dhttps://rt.umb= c.edu/Ticket/Display.html?id%3D3296910&amp;source=3Dgmail-imap&amp;ust=3D17= 62373620000000&amp;usg=3DAOvVaw3sdxyNkvAgAa3D8CkynvbF"" rel=3D""noreferrer"" t= arget=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3296910</a><s= pan class=3D""Apple-converted-space"">&nbsp;</span>&gt;<br><br>Last Update Fr= om Ticket:<br><br>On the HPC cluster, it should now be possible to run hspi= ce using the command:<br><br>/umbc/software/csee/scripts/launch_synopsys_hs= pice.sh<br><br>Please test it and let me know if it works. Also, let me kno= w what other tools<br>you need to use, if any, so I can adapt those scripts=  as well.<br><br><br></blockquote></div><div style=3D""caret-color: rgb(0, 0= , 0); font-family: Helvetica; font-size: 14px; font-style: normal; font-var= iant-caps: normal; font-weight: 400; letter-spacing: normal; text-align: st= art; text-indent: 0px; text-transform: none; white-space: normal; word-spac= ing: 0px; -webkit-text-stroke-width: 0px; text-decoration: none;""><br clear= =3D""all""></div><div style=3D""caret-color: rgb(0, 0, 0); font-family: Helvet= ica; font-size: 14px; font-style: normal; font-variant-caps: normal; font-w= eight: 400; letter-spacing: normal; text-align: start; text-indent: 0px; te= xt-transform: none; white-space: normal; word-spacing: 0px; -webkit-text-st= roke-width: 0px; text-decoration: none;""><br></div><span class=3D""gmail_sig= nature_prefix"" style=3D""caret-color: rgb(0, 0, 0); font-family: Helvetica; = font-size: 14px; font-style: normal; font-variant-caps: normal; font-weight= : 400; letter-spacing: normal; text-align: start; text-indent: 0px; text-tr= ansform: none; white-space: normal; word-spacing: 0px; -webkit-text-stroke-= width: 0px; text-decoration: none;"">--<span class=3D""Apple-converted-space""= >&nbsp;</span></span><br style=3D""caret-color: rgb(0, 0, 0); font-family: H= elvetica; font-size: 14px; font-style: normal; font-variant-caps: normal; f= ont-weight: 400; letter-spacing: normal; text-align: start; text-indent: 0p= x; text-transform: none; white-space: normal; word-spacing: 0px; -webkit-te= xt-stroke-width: 0px; text-decoration: none;""><div dir=3D""ltr"" class=3D""gma= il_signature"" style=3D""caret-color: rgb(0, 0, 0); font-family: Helvetica; f= ont-size: 14px; font-style: normal; font-variant-caps: normal; font-weight:=  400; letter-spacing: normal; text-align: start; text-indent: 0px; text-tra= nsform: none; white-space: normal; word-spacing: 0px; -webkit-text-stroke-w= idth: 0px; text-decoration: none;""><div dir=3D""ltr""><span style=3D""font-siz= e: 9.5pt; line-height: 14.566668px; font-family: Arial, sans-serif; color: = rgb(136, 136, 136); background-image: initial; background-position: initial= ; background-repeat: initial;"">Naghmeh&nbsp;Karimi, Ph.D.<br>Associate Prof= essor<br>Department of Computer Science and Electrical Engineering<br>Unive= rsity of Maryland, Baltimore County<br>Baltimore, MD 21250<br>Tel:</span><s= pan style=3D""font-size: 11pt; line-height: 16.866667px; font-family: Calibr= i, sans-serif;"">&nbsp;</span><u><span style=3D""font-size: 11pt; line-height= : 16.866667px; font-family: Calibri, sans-serif; color: rgb(17, 85, 204);"">= 410-455-3965</span></u><span style=3D""font-size: 11pt; line-height: 16.8666= 67px; font-family: Calibri, sans-serif;""><span class=3D""Apple-converted-spa= ce"">&nbsp;</span></span><span style=3D""font-size: 9.5pt; line-height: 14.56= 6668px; font-family: Arial, sans-serif; color: rgb(136, 136, 136); backgrou= nd-image: initial; background-position: initial; background-repeat: initial= ;"">E-mail:</span><span style=3D""font-size: 11pt; line-height: 16.866667px; = font-family: Calibri, sans-serif;"">&nbsp;</span><u><span style=3D""font-size= : 11pt; line-height: 16.866667px; font-family: Calibri, sans-serif; color: = rgb(17, 85, 204);""><a href=3D""mailto:nkarimi@umbc.edu"" target=3D""_blank"">nk= arimi@umbc.edu</a></span></u><span style=3D""font-size: 9.5pt; line-height: = 14.566668px; font-family: Arial, sans-serif; color: rgb(136, 136, 136); bac= kground-image: initial; background-position: initial; background-repeat: in= itial;""><br>Web:</span><span style=3D""font-size: 9.5pt; line-height: 14.566= 668px; font-family: Arial, sans-serif; background-image: initial; backgroun= d-position: initial; background-repeat: initial;"">&nbsp;</span><u><span sty= le=3D""font-size: 11pt; line-height: 16.866667px; font-family: Calibri, sans= -serif; color: rgb(17, 85, 204); background-image: initial; background-posi= tion: initial; background-repeat: initial;""><a href=3D""https://www.google.c= om/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;u= st=3D1762373620000000&amp;usg=3DAOvVaw1mTQPjV_EliH_BiHB0xweg"" target=3D""_bl= ank"">http://www.csee.umbc.edu/~nkarimi/</a></span></u></div></div></div></b= lockquote></div><br></body></html>= "
3296910,72603023,Correspond,DoIT-Research-Computing,2025-10-30 16:57:01.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,NULL,naghmeh.karimi@umbc.edu,"Thanks a lot Skye. Both hspice and waveviewer work.  Regards, Naghmeh  On Wed, Oct 29, 2025 at 5:01=E2=80=AFPM Skye Jonke <ii69854@umbc.edu> wrote:  > Wave view should also be working now, give it a try when you have a chanc= e. > --- > Skye Jonke > > she/her > > Specialist, Linux System Administrator & Lab Technical Support > > On Oct 29, 2025, at 16:12, Naghmeh Karimi <nkarimi@umbc.edu> wrote: > > Hi Skye, > > Thanks. Hspice works but we also need WV (waveviewer) to see Hspice > results as well. Could you please add that also and let us know to check? > > Thanks a lot, > Naghmeh > > On Wed, Oct 29, 2025 at 2:30=E2=80=AFPM Skye Jonke via RT <UMBCHelp@rt.um= bc.edu> > wrote: > >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 >> <https://www.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?= id%3D3296910&source=3Dgmail-imap&ust=3D1762373620000000&usg=3DAOvVaw3sdxyNk= vAgAa3D8CkynvbF> >>  > >> >> Last Update From Ticket: >> >> On the HPC cluster, it should now be possible to run hspice using the >> command: >> >> /umbc/software/csee/scripts/launch_synopsys_hspice.sh >> >> Please test it and let me know if it works. Also, let me know what other >> tools >> you need to use, if any, so I can adapt those scripts as well. >> >> >> > > -- > Naghmeh Karimi, Ph.D. > Associate Professor > Department of Computer Science and Electrical Engineering > University of Maryland, Baltimore County > Baltimore, MD 21250 > Tel: *410-455-3965* E-mail: *nkarimi@umbc.edu <nkarimi@umbc.edu>* > Web: *http://www.csee.umbc.edu/~nkarimi/ > <https://www.google.com/url?q=3Dhttp://www.csee.umbc.edu/~nkarimi/&source= =3Dgmail-imap&ust=3D1762373620000000&usg=3DAOvVaw1mTQPjV_EliH_BiHB0xweg>* > > >  --=20 Naghmeh Karimi, Ph.D. Associate Professor Department of Computer Science and Electrical Engineering University of Maryland, Baltimore County Baltimore, MD 21250 Tel: *410-455-3965* E-mail: *nkarimi@umbc.edu <nkarimi@umbc.edu>* Web: *http://www.csee.umbc.edu/~nkarimi/ <http://www.csee.umbc.edu/~nkarimi/>* "
3296910,72603023,Correspond,DoIT-Research-Computing,2025-10-30 16:57:01.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,NULL,naghmeh.karimi@umbc.edu,"<div dir=3D""ltr"">Thanks a lot Skye. Both hspice and waveviewer work.<div><b= r></div><div>Regards,</div><div>Naghmeh</div></div><br><div class=3D""gmail_= quote gmail_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Wed, = Oct 29, 2025 at 5:01=E2=80=AFPM Skye Jonke &lt;<a href=3D""mailto:ii69854@um= bc.edu"">ii69854@umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail= _quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204= ,204);padding-left:1ex""><div>Wave view should also be working now, give it = a try when you have a chance.<br id=3D""m_2244490437301877703lineBreakAtBegi= nningOfMessage""><div> <div dir=3D""auto"" style=3D""color:rgb(0,0,0);letter-spacing:normal;text-alig= n:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing= :0px;text-decoration:none"">---</div><div dir=3D""auto"" style=3D""color:rgb(0,= 0,0);letter-spacing:normal;text-align:start;text-indent:0px;text-transform:= none;white-space:normal;word-spacing:0px;text-decoration:none"">Skye Jonke<b= r><br>she/her<br><br>Specialist, Linux System Administrator &amp;<span>=C2= =A0</span>Lab Technical Support<br></div> </div> <div><br><blockquote type=3D""cite""><div>On Oct 29, 2025, at 16:12, Naghmeh = Karimi &lt;<a href=3D""mailto:nkarimi@umbc.edu"" target=3D""_blank"">nkarimi@um= bc.edu</a>&gt; wrote:</div><br><div><div dir=3D""ltr"" style=3D""font-family:H= elvetica;font-size:14px;font-style:normal;font-variant-caps:normal;font-wei= ght:400;letter-spacing:normal;text-align:start;text-indent:0px;text-transfo= rm:none;white-space:normal;word-spacing:0px;text-decoration:none"">Hi Skye,<= div><br></div><div>Thanks. Hspice works but we also need WV (waveviewer) to=  see Hspice results as well. Could you please add that also and let us know=  to check?</div><div><br></div><div>Thanks=C2=A0a lot,</div><div>Naghmeh</d= iv></div><br style=3D""font-family:Helvetica;font-size:14px;font-style:norma= l;font-variant-caps:normal;font-weight:400;letter-spacing:normal;text-align= :start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:= 0px;text-decoration:none""><div class=3D""gmail_quote"" style=3D""font-family:H= elvetica;font-size:14px;font-style:normal;font-variant-caps:normal;font-wei= ght:400;letter-spacing:normal;text-align:start;text-indent:0px;text-transfo= rm:none;white-space:normal;word-spacing:0px;text-decoration:none""><div dir= =3D""ltr"" class=3D""gmail_attr"">On Wed, Oct 29, 2025 at 2:30=E2=80=AFPM Skye = Jonke via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">= UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quo= te"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204= );padding-left:1ex"">Ticket &lt;URL:<span>=C2=A0</span><a href=3D""https://ww= w.google.com/url?q=3Dhttps://rt.umbc.edu/Ticket/Display.html?id%3D3296910&a= mp;source=3Dgmail-imap&amp;ust=3D1762373620000000&amp;usg=3DAOvVaw3sdxyNkvA= gAa3D8CkynvbF"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tic= ket/Display.html?id=3D3296910</a><span>=C2=A0</span>&gt;<br><br>Last Update=  From Ticket:<br><br>On the HPC cluster, it should now be possible to run h= spice using the command:<br><br>/umbc/software/csee/scripts/launch_synopsys= _hspice.sh<br><br>Please test it and let me know if it works. Also, let me = know what other tools<br>you need to use, if any, so I can adapt those scri= pts as well.<br><br><br></blockquote></div><div style=3D""font-family:Helvet= ica;font-size:14px;font-style:normal;font-variant-caps:normal;font-weight:4= 00;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:no= ne;white-space:normal;word-spacing:0px;text-decoration:none""><br clear=3D""a= ll""></div><div style=3D""font-family:Helvetica;font-size:14px;font-style:nor= mal;font-variant-caps:normal;font-weight:400;letter-spacing:normal;text-ali= gn:start;text-indent:0px;text-transform:none;white-space:normal;word-spacin= g:0px;text-decoration:none""><br></div><span class=3D""gmail_signature_prefix= "" style=3D""font-family:Helvetica;font-size:14px;font-style:normal;font-vari= ant-caps:normal;font-weight:400;letter-spacing:normal;text-align:start;text= -indent:0px;text-transform:none;white-space:normal;word-spacing:0px;text-de= coration:none"">--<span>=C2=A0</span></span><br style=3D""font-family:Helveti= ca;font-size:14px;font-style:normal;font-variant-caps:normal;font-weight:40= 0;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:non= e;white-space:normal;word-spacing:0px;text-decoration:none""><div dir=3D""ltr= "" class=3D""gmail_signature"" style=3D""font-family:Helvetica;font-size:14px;f= ont-style:normal;font-variant-caps:normal;font-weight:400;letter-spacing:no= rmal;text-align:start;text-indent:0px;text-transform:none;white-space:norma= l;word-spacing:0px;text-decoration:none""><div dir=3D""ltr""><span style=3D""fo= nt-size:9.5pt;line-height:14.5667px;font-family:Arial,sans-serif;color:rgb(= 136,136,136);background-image:initial;background-position:initial;backgroun= d-repeat:initial"">Naghmeh=C2=A0Karimi, Ph.D.<br>Associate Professor<br>Depa= rtment of Computer Science and Electrical Engineering<br>University of Mary= land, Baltimore County<br>Baltimore, MD 21250<br>Tel:</span><span style=3D""= font-size:11pt;line-height:16.8667px;font-family:Calibri,sans-serif"">=C2=A0= </span><u><span style=3D""font-size:11pt;line-height:16.8667px;font-family:C= alibri,sans-serif;color:rgb(17,85,204)"">410-455-3965</span></u><span style= =3D""font-size:11pt;line-height:16.8667px;font-family:Calibri,sans-serif""><s= pan>=C2=A0</span></span><span style=3D""font-size:9.5pt;line-height:14.5667p= x;font-family:Arial,sans-serif;color:rgb(136,136,136);background-image:init= ial;background-position:initial;background-repeat:initial"">E-mail:</span><s= pan style=3D""font-size:11pt;line-height:16.8667px;font-family:Calibri,sans-= serif"">=C2=A0</span><u><span style=3D""font-size:11pt;line-height:16.8667px;= font-family:Calibri,sans-serif;color:rgb(17,85,204)""><a href=3D""mailto:nkar= imi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a></span></u><span style= =3D""font-size:9.5pt;line-height:14.5667px;font-family:Arial,sans-serif;colo= r:rgb(136,136,136);background-image:initial;background-position:initial;bac= kground-repeat:initial""><br>Web:</span><span style=3D""font-size:9.5pt;line-= height:14.5667px;font-family:Arial,sans-serif;background-image:initial;back= ground-position:initial;background-repeat:initial"">=C2=A0</span><u><span st= yle=3D""font-size:11pt;line-height:16.8667px;font-family:Calibri,sans-serif;= color:rgb(17,85,204);background-image:initial;background-position:initial;b= ackground-repeat:initial""><a href=3D""https://www.google.com/url?q=3Dhttp://= www.csee.umbc.edu/~nkarimi/&amp;source=3Dgmail-imap&amp;ust=3D1762373620000= 000&amp;usg=3DAOvVaw1mTQPjV_EliH_BiHB0xweg"" target=3D""_blank"">http://www.cs= ee.umbc.edu/~nkarimi/</a></span></u></div></div></div></blockquote></div><b= r></div></blockquote></div><div><br clear=3D""all""></div><div><br></div><spa= n class=3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""= gmail_signature""><div dir=3D""ltr""><span style=3D""font-size:9.5pt;line-heigh= t:115%;font-family:Arial,sans-serif;color:rgb(136,136,136);background-image= :initial;background-position:initial;background-repeat:initial"">Naghmeh=C2= =A0Karimi, Ph.D.<br> Associate Professor<br> Department of Computer Science and Electrical Engineering<br> University of Maryland, Baltimore County<br> Baltimore, MD 21250<br> Tel:</span><span style=3D""font-size:11pt;line-height:115%;font-family:Calib= ri,&quot;sans-serif&quot;;color:black"">=C2=A0</span><u><span style=3D""font-= size:11pt;line-height:115%;font-family:Calibri,&quot;sans-serif&quot;;color= :rgb(17,85,204)"">410-455-3965</span></u><span style=3D""font-size:11pt;line-= height:115%;font-family:Calibri,&quot;sans-serif&quot;;color:black""> </span= ><span style=3D""font-size:9.5pt;line-height:115%;font-family:Arial,sans-ser= if;color:rgb(136,136,136);background-image:initial;background-position:init= ial;background-repeat:initial"">E-mail:</span><span style=3D""font-size:11pt;= line-height:115%;font-family:Calibri,&quot;sans-serif&quot;;color:black"">= =C2=A0</span><u><span style=3D""font-size:11pt;line-height:115%;font-family:= Calibri,&quot;sans-serif&quot;;color:rgb(17,85,204)""><a href=3D""mailto:nkar= imi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a></span></u><span style= =3D""font-size:9.5pt;line-height:115%;font-family:Arial,sans-serif;color:rgb= (136,136,136);background-image:initial;background-position:initial;backgrou= nd-repeat:initial""><br> Web:</span><span style=3D""font-size:9.5pt;line-height:115%;font-family:Aria= l,sans-serif;color:black;background-image:initial;background-position:initi= al;background-repeat:initial"">=C2=A0</span><u><span style=3D""font-size:11pt= ;line-height:115%;font-family:Calibri,sans-serif;color:rgb(17,85,204);backg= round-image:initial;background-position:initial;background-repeat:initial"">= <a href=3D""http://www.csee.umbc.edu/~nkarimi/"" target=3D""_blank"">http://www= .csee.umbc.edu/~nkarimi/</a></span></u></div></div> "
3296910,72605545,Correspond,DoIT-Research-Computing,2025-10-30 18:11:37.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi all,</p>  <p>Is it fair to say this ticket can now be closed?&nbsp;</p>  <p>On Thu Oct 30 12:57:01 2025, naghmeh.karimi@umbc.edu wrote:</p>  <blockquote> <div>Thanks a lot Skye. Both hspice and waveviewer work. <div>&nbsp;</div>  <div>Regards,</div>  <div>Naghmeh</div> </div> &nbsp;  <div> <div>On Wed, Oct 29, 2025 at 5:01=E2=80=AFPM Skye Jonke &lt;ii69854@umbc.ed= u&gt; wrote:</div>  <blockquote> <div>Wave view should also be working now, give it a try when you have a ch= ance. <div> <div>---</div>  <div>Skye Jonke<br /> <br /> she/her<br /> <br /> Specialist, Linux System Administrator &amp;&nbsp;Lab Technical Support</di= v> </div>  <div>&nbsp; <blockquote> <div>On Oct 29, 2025, at 16:12, Naghmeh Karimi &lt;nkarimi@umbc.edu&gt; wro= te:</div> &nbsp;  <div> <div>Hi Skye, <div>&nbsp;</div>  <div>Thanks. Hspice works but we also need WV (waveviewer) to see Hspice re= sults as well. Could you please add that also and let us know to check?</di= v>  <div>&nbsp;</div>  <div>Thanks&nbsp;a lot,</div>  <div>Naghmeh</div> </div> &nbsp;  <div> <div>On Wed, Oct 29, 2025 at 2:30=E2=80=AFPM Skye Jonke via RT &lt;UMBCHelp= @rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL:&nbsp;https://rt.umbc.edu/Ticket/Display.html?id= =3D3296910&nbsp;&gt;<br /> <br /> Last Update From Ticket:<br /> <br /> On the HPC cluster, it should now be possible to run hspice using the comma= nd:<br /> <br /> /umbc/software/csee/scripts/launch_synopsys_hspice.sh<br /> <br /> Please test it and let me know if it works. Also, let me know what other to= ols<br /> you need to use, if any, so I can adapt those scripts as well.<br /> <br /> &nbsp;</blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --&nbsp;  <div> <div><span style=3D""color:#888888"">Naghmeh&nbsp;Karimi, Ph.D.<br /> Associate Professor<br /> Department of Computer Science and Electrical Engineering<br /> University of Maryland, Baltimore County<br /> Baltimore, MD 21250<br /> Tel:</span>&nbsp;<span style=3D""color:#1155cc"">410-455-3965</span>&nbsp;<sp= an style=3D""color:#888888"">E-mail:</span>&nbsp;<span style=3D""color:#1155cc= "">nkarimi@umbc.edu</span><br /> <span style=3D""color:#888888"">Web:</span>&nbsp;<span style=3D""color:#1155cc= "">http://www.csee.umbc.edu/~nkarimi/</span></div> </div> </div> </blockquote> </div> </div> </blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --  <div> <div><span style=3D""color:#888888"">Naghmeh&nbsp;Karimi, Ph.D.<br /> Associate Professor<br /> Department of Computer Science and Electrical Engineering<br /> University of Maryland, Baltimore County<br /> Baltimore, MD 21250<br /> Tel:</span><span style=3D""color:black"">&nbsp;</span><span style=3D""color:#1= 155cc"">410-455-3965</span><span style=3D""color:black""> </span><span style= =3D""color:#888888"">E-mail:</span><span style=3D""color:black"">&nbsp;</span><= span style=3D""color:#1155cc"">nkarimi@umbc.edu</span><br /> <span style=3D""color:#888888"">Web:</span><span style=3D""color:black"">&nbsp;= </span><span style=3D""color:#1155cc"">http://www.csee.umbc.edu/~nkarimi/</sp= an></div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3296910,72606850,Correspond,DoIT-Research-Computing,2025-10-30 18:44:55.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,NULL,naghmeh.karimi@umbc.edu,"Yes, thanks for all help.   Regards, Naghmeh  On Thu, Oct 30, 2025 at 2:11=E2=80=AFPM Max Breitmeyer via RT <UMBCHelp@rt.= umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 > > > Last Update From Ticket: > > Hi all, > > Is it fair to say this ticket can now be closed? > > On Thu Oct 30 12:57:01 2025, naghmeh.karimi@umbc.edu wrote: > > > Thanks a lot Skye. Both hspice and waveviewer work. Regards,Naghmeh On > Wed, > > Oct 29, 2025 at 5:01 PM Skye Jonke <ii69854@umbc.edu> wrote: > > >> Wave view should also be working now, give it a try when you have a > >> chance. ---Skye Jonke > > >> she/her > > >> Specialist, Linux System Administrator & Lab Technical Support > > >>> On Oct 29, 2025, at 16:12, Naghmeh Karimi <nkarimi@umbc.edu> wrote: > >>> Hi Skye, Thanks. Hspice works but we also need WV (waveviewer) to > >>> see Hspice results as well. Could you please add that also and let > >>> us know to check? Thanks a lot,Naghmeh On Wed, Oct 29, 2025 at 2:30 > >>> PM Skye Jonke via RT <UMBCHelp@rt.umbc.edu> wrote: > > >>>> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3296910 > >>>> > > > >>>> Last Update From Ticket: > > >>>> On the HPC cluster, it should now be possible to run hspice > >>>> using the command: > > >>>> /umbc/software/csee/scripts/launch_synopsys_hspice.sh > > >>>> Please test it and let me know if it works. Also, let me know > >>>> what other tools > >>>> you need to use, if any, so I can adapt those scripts as well. > > >>> -- Naghmeh Karimi, Ph.D. > >>> Associate Professor > >>> Department of Computer Science and Electrical Engineering > >>> University of Maryland, Baltimore County > >>> Baltimore, MD 21250 > >>> Tel: 410-455-3965 E-mail: nkarimi@umbc.edu > >>> Web: http://www.csee.umbc.edu/~nkarimi/ > > > -- Naghmeh Karimi, Ph.D. > > Associate Professor > > Department of Computer Science and Electrical Engineering > > University of Maryland, Baltimore County > > Baltimore, MD 21250 > > Tel: 410-455-3965 E-mail: nkarimi@umbc.edu > > Web: http://www.csee.umbc.edu/~nkarimi/ > > -- > > Best, > Max Breitmeyer > DOIT HPC System Administrator > > >  --=20 Naghmeh Karimi, Ph.D. Associate Professor Department of Computer Science and Electrical Engineering University of Maryland, Baltimore County Baltimore, MD 21250 Tel: *410-455-3965* E-mail: *nkarimi@umbc.edu <nkarimi@umbc.edu>* Web: *http://www.csee.umbc.edu/~nkarimi/ <http://www.csee.umbc.edu/~nkarimi/>* "
3296910,72606850,Correspond,DoIT-Research-Computing,2025-10-30 18:44:55.0000000,HPC Other Issue: Running Hspice software on HPCF,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,NULL,naghmeh.karimi@umbc.edu,"<div dir=3D""ltr"">Yes, thanks for all help.<div><br></div><div><br></div><di= v>Regards,</div><div>Naghmeh</div></div><br><div class=3D""gmail_quote gmail= _quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Thu, Oct 30, 202= 5 at 2:11=E2=80=AFPM Max Breitmeyer via RT &lt;<a href=3D""mailto:UMBCHelp@r= t.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class= =3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rg= b(204,204,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc= .edu/Ticket/Display.html?id=3D3296910"" rel=3D""noreferrer"" target=3D""_blank""= >https://rt.umbc.edu/Ticket/Display.html?id=3D3296910</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi all,<br> <br> Is it fair to say this ticket can now be closed?<br> <br> On Thu Oct 30 12:57:01 2025, <a href=3D""mailto:naghmeh.karimi@umbc.edu"" tar= get=3D""_blank"">naghmeh.karimi@umbc.edu</a> wrote:<br> <br> &gt; Thanks a lot Skye. Both hspice and waveviewer work. Regards,Naghmeh On=  Wed,<br> &gt; Oct 29, 2025 at 5:01 PM Skye Jonke &lt;<a href=3D""mailto:ii69854@umbc.= edu"" target=3D""_blank"">ii69854@umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; Wave view should also be working now, give it a try when you have = a<br> &gt;&gt; chance. ---Skye Jonke<br> <br> &gt;&gt; she/her<br> <br> &gt;&gt; Specialist, Linux System Administrator &amp; Lab Technical Support= <br> <br> &gt;&gt;&gt; On Oct 29, 2025, at 16:12, Naghmeh Karimi &lt;<a href=3D""mailt= o:nkarimi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a>&gt; wrote:<br> &gt;&gt;&gt; Hi Skye, Thanks. Hspice works but we also need WV (waveviewer)=  to<br> &gt;&gt;&gt; see Hspice results as well. Could you please add that also and=  let<br> &gt;&gt;&gt; us know to check? Thanks a lot,Naghmeh On Wed, Oct 29, 2025 at=  2:30<br> &gt;&gt;&gt; PM Skye Jonke via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.ed= u"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt;&gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Disp= lay.html?id=3D3296910"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc= .edu/Ticket/Display.html?id=3D3296910</a><br> &gt;&gt;&gt;&gt; &gt;<br> <br> &gt;&gt;&gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt;&gt;&gt; On the HPC cluster, it should now be possible to run hspic= e<br> &gt;&gt;&gt;&gt; using the command:<br> <br> &gt;&gt;&gt;&gt; /umbc/software/csee/scripts/launch_synopsys_hspice.sh<br> <br> &gt;&gt;&gt;&gt; Please test it and let me know if it works. Also, let me k= now<br> &gt;&gt;&gt;&gt; what other tools<br> &gt;&gt;&gt;&gt; you need to use, if any, so I can adapt those scripts as w= ell.<br> <br> &gt;&gt;&gt; -- Naghmeh Karimi, Ph.D.<br> &gt;&gt;&gt; Associate Professor<br> &gt;&gt;&gt; Department of Computer Science and Electrical Engineering<br> &gt;&gt;&gt; University of Maryland, Baltimore County<br> &gt;&gt;&gt; Baltimore, MD 21250<br> &gt;&gt;&gt; Tel: 410-455-3965 E-mail: <a href=3D""mailto:nkarimi@umbc.edu"" = target=3D""_blank"">nkarimi@umbc.edu</a><br> &gt;&gt;&gt; Web: <a href=3D""http://www.csee.umbc.edu/~nkarimi/"" rel=3D""nor= eferrer"" target=3D""_blank"">http://www.csee.umbc.edu/~nkarimi/</a><br> <br> &gt; -- Naghmeh Karimi, Ph.D.<br> &gt; Associate Professor<br> &gt; Department of Computer Science and Electrical Engineering<br> &gt; University of Maryland, Baltimore County<br> &gt; Baltimore, MD 21250<br> &gt; Tel: 410-455-3965 E-mail: <a href=3D""mailto:nkarimi@umbc.edu"" target= =3D""_blank"">nkarimi@umbc.edu</a><br> &gt; Web: <a href=3D""http://www.csee.umbc.edu/~nkarimi/"" rel=3D""noreferrer""=  target=3D""_blank"">http://www.csee.umbc.edu/~nkarimi/</a><br> <br> --<br> <br> Best,<br> Max Breitmeyer<br> DOIT HPC System Administrator<br> <br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><span style=3D""font-size:9.5pt;line-height:115%;= font-family:Arial,sans-serif;color:rgb(136,136,136);background-image:initia= l;background-position:initial;background-repeat:initial"">Naghmeh=C2=A0Karim= i, Ph.D.<br> Associate Professor<br> Department of Computer Science and Electrical Engineering<br> University of Maryland, Baltimore County<br> Baltimore, MD 21250<br> Tel:</span><span style=3D""font-size:11pt;line-height:115%;font-family:Calib= ri,&quot;sans-serif&quot;;color:black"">=C2=A0</span><u><span style=3D""font-= size:11pt;line-height:115%;font-family:Calibri,&quot;sans-serif&quot;;color= :rgb(17,85,204)"">410-455-3965</span></u><span style=3D""font-size:11pt;line-= height:115%;font-family:Calibri,&quot;sans-serif&quot;;color:black""> </span= ><span style=3D""font-size:9.5pt;line-height:115%;font-family:Arial,sans-ser= if;color:rgb(136,136,136);background-image:initial;background-position:init= ial;background-repeat:initial"">E-mail:</span><span style=3D""font-size:11pt;= line-height:115%;font-family:Calibri,&quot;sans-serif&quot;;color:black"">= =C2=A0</span><u><span style=3D""font-size:11pt;line-height:115%;font-family:= Calibri,&quot;sans-serif&quot;;color:rgb(17,85,204)""><a href=3D""mailto:nkar= imi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a></span></u><span style= =3D""font-size:9.5pt;line-height:115%;font-family:Arial,sans-serif;color:rgb= (136,136,136);background-image:initial;background-position:initial;backgrou= nd-repeat:initial""><br> Web:</span><span style=3D""font-size:9.5pt;line-height:115%;font-family:Aria= l,sans-serif;color:black;background-image:initial;background-position:initi= al;background-repeat:initial"">=C2=A0</span><u><span style=3D""font-size:11pt= ;line-height:115%;font-family:Calibri,sans-serif;color:rgb(17,85,204);backg= round-image:initial;background-position:initial;background-repeat:initial"">= <a href=3D""http://www.csee.umbc.edu/~nkarimi/"" target=3D""_blank"">http://www= .csee.umbc.edu/~nkarimi/</a></span></u></div></div> "
3298342,72451845,Create,DoIT-Research-Computing,2025-10-23 14:09:11.0000000,HPC Other Issue: 5-days time limit not enough for jobs,resolved,Danielle Esposito,desposi1,Lais Grossel,laisg1,laisg1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Lais Last Name:                 Grossel Email:                     laisg1@umbc.edu Campus ID:                 VE32332  Request Type:              High Performance Cluster  Hello all! I'd like to ask your help with the jobs I'm submitting in chip cluster. A f= ew of them are pretty heavy, and the 5-days limit to run are not being enou= gh. I saw there's the possibility to run in the partition 'shared', with 14= -days limit, but I don't have access to that. Is it possible to give me acc= ess to that partition, or maybe to increase the time limit in the partition=  'general'? In general my samples run within the 5 days, I would require mo= re time only for samples whose jobs abort after 5 days. Thank you! Best, La=C3=ADs  "
3298342,72456450,Correspond,DoIT-Research-Computing,2025-10-23 15:52:23.0000000,HPC Other Issue: 5-days time limit not enough for jobs,resolved,Danielle Esposito,desposi1,Lais Grossel,laisg1,laisg1@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Lais,</p>  <p>Unfortunately, under the current cluster model only contributing groups have access to the &#39;shared&#39; QOS option. In your situation, I would recommend seeing if you can optimize your code to run faster, in parallel, or across multiple nodes. You can also attempt to break apart your one large job into two smaller sets of jobs, which would allow you to complete the run in less than 5 days.&nbsp;</p>  <p>Let me know if you have any additional questions! Have a nice day!</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Thu Oct 23 10:09:11 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Lais Last Name:                 Grossel Email:                     laisg1@umbc.edu Campus ID:                 VE32332  Request Type:              High Performance Cluster  Hello all! I&#39;d like to ask your help with the jobs I&#39;m submitting in chip cluster. A few of them are pretty heavy, and the 5-days limit to run are not being enough. I saw there&#39;s the possibility to run in the partition &#39;shared&#39;, with 14-days limit, but I don&#39;t have access to that. Is it possible to give me access to that partition, or maybe to increase the time limit in the partition &#39;general&#39;? In general my samples run within the 5 days, I would require more time only for samples whose jobs abort after 5 days. Thank you! Best, La&iacute;s  </pre> </blockquote> </div> "
3298471,72456901,Create,DoIT-Research-Computing,2025-10-23 16:01:48.0000000,HPC Other Issue: I cannot login to my ada account,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Mohammad Last Name:                 Ebrahimabadi Email:                     e127@umbc.edu Campus ID:                 NQ23652  Request Type:              High Performance Cluster  Hello,  I=E2=80=99m currently unable to access my ADA account. When I try to connec= t via SSH using the command below:  ssh e127@ada.rs.umbc.edu  I receive the following error message and do not even get the password prom= pt:  ssh: connect to host ada.rs.umbc.edu port 22: Connection timed out  Could you please help me resolve this issue?  Thank you, Mohammad  "
3298471,72457687,Correspond,DoIT-Research-Computing,2025-10-23 16:18:16.0000000,HPC Other Issue: I cannot login to my ada account,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Mohammad,</p>  <p>The hpc cluster &quot;ada&quot; has been completely absorbed by &quot;chip&quot; as of April 10th. Please see this myumbc posting detailing the rollout of chip and the deletion of ada.&nbsp;https://my3.my.umbc.edu/groups/hpcf/posts/147513&nbsp;</p>  <p>On Thu Oct 23 12:01:48 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Mohammad Last Name:                 Ebrahimabadi Email:                     e127@umbc.edu Campus ID:                 NQ23652  Request Type:              High Performance Cluster  Hello,  I&rsquo;m currently unable to access my ADA account. When I try to connect via SSH using the command below:  ssh e127@ada.rs.umbc.edu  I receive the following error message and do not even get the password prompt:  ssh: connect to host ada.rs.umbc.edu port 22: Connection timed out  Could you please help me resolve this issue?  Thank you, Mohammad  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3298471,72457824,Correspond,DoIT-Research-Computing,2025-10-23 16:21:05.0000000,HPC Other Issue: I cannot login to my ada account,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Mohammad Ebrahimabadi,e127@umbc.edu,"Many Thanks Max,  Regards, Mohammad  On Thu, Oct 23, 2025 at 12:18=E2=80=AFPM Max Breitmeyer via RT <UMBCHelp@rt= .umbc.edu> wrote:  > If you agree your issue is resolved, please give us feedback on your > experience by completing a brief satisfaction survey: > > > https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&customeremail= =3De127%40umbc.edu&groupid=3DEIS&ticketid=3D3298471&ticketowner=3Dmb17%40um= bc.edu&ticketsubject=3DHPC%20Other%20Issue%3A%20I%20cannot%20login%20to%20m= y%20ada%20account > > If you believe your issue has not been resolved, please respond to this > message, which will reopen your ticket. Note: A full record of your reque= st > can be found at: > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3298471 > > > Thank You > > _________________________________________ > > R e s o l u t i o n: > =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D = =3D > > Hi Mohammad, > > The hpc cluster ""ada"" has been completely absorbed by ""chip"" as of April > 10th. > Please see this myumbc posting detailing the rollout of chip and the > deletion > of ada. https://my3.my.umbc.edu/groups/hpcf/posts/147513 > > On Thu Oct 23 12:01:48 2025, ZZ99999 wrote: > > > First Name:                Mohammad > > Last Name:                 Ebrahimabadi > > Email:                     e127@umbc.edu > > Campus ID:                 NQ23652 > > > > Request Type:              High Performance Cluster > > > > Hello, > > > > I=E2=80=99m currently unable to access my ADA account. When I try to co= nnect via > SSH using the command below: > > > > ssh e127@ada.rs.umbc.edu > > > > I receive the following error message and do not even get the password > prompt: > > > > ssh: connect to host ada.rs.umbc.edu port 22: Connection timed out > > > > Could you please help me resolve this issue? > > > > Thank you, > > Mohammad > > > -- > > Best, > Max Breitmeyer > DOIT HPC System Administrator > > > > ______________________________________ > > Original Request: > > Requestors: Mohammad Ebrahimabadi > > First Name:                Mohammad > Last Name:                 Ebrahimabadi > Email:                     e127@umbc.edu > Campus ID:                 NQ23652 > > Request Type:              High Performance Cluster > > Hello, > > I=E2=80=99m currently unable to access my ADA account. When I try to conn= ect via > SSH using the command below: > > ssh e127@ada.rs.umbc.edu > > I receive the following error message and do not even get the password > prompt: > > ssh: connect to host ada.rs.umbc.edu port 22: Connection timed out > > Could you please help me resolve this issue? > > Thank you, > Mohammad > > > "
3298471,72457824,Correspond,DoIT-Research-Computing,2025-10-23 16:21:05.0000000,HPC Other Issue: I cannot login to my ada account,resolved,Max Breitmeyer,mb17,Mohammad Ebrahimabadi,e127,e127@umbc.edu,Mohammad Ebrahimabadi,e127@umbc.edu,"<div dir=3D""ltr"">Many Thanks Max,<div><br></div><div>Regards,</div><div>Moh= ammad</div></div><br><div class=3D""gmail_quote gmail_quote_container""><div = dir=3D""ltr"" class=3D""gmail_attr"">On Thu, Oct 23, 2025 at 12:18=E2=80=AFPM M= ax Breitmeyer via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@r= t.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" style= =3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding= -left:1ex"">If you agree your issue is resolved, please give us feedback on = your experience by completing a brief satisfaction survey: <br> <br> <a href=3D""https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&amp;= customeremail=3De127%40umbc.edu&amp;groupid=3DEIS&amp;ticketid=3D3298471&am= p;ticketowner=3Dmb17%40umbc.edu&amp;ticketsubject=3DHPC%20Other%20Issue%3A%= 20I%20cannot%20login%20to%20my%20ada%20account"" rel=3D""noreferrer"" target= =3D""_blank"">https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&amp= ;customeremail=3De127%40umbc.edu&amp;groupid=3DEIS&amp;ticketid=3D3298471&a= mp;ticketowner=3Dmb17%40umbc.edu&amp;ticketsubject=3DHPC%20Other%20Issue%3A= %20I%20cannot%20login%20to%20my%20ada%20account</a><br> <br> If you believe your issue has not been resolved, please respond to this mes= sage, which will reopen your ticket. Note: A full record of your request ca= n be found at:=C2=A0 <br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D329= 8471"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3298471</a> &gt;<br> <br> Thank You<br> <br> _________________________________________<br> <br> R e s o l u t i o n:<br> =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D= =C2=A0 <br> <br> Hi Mohammad,<br> <br> The hpc cluster &quot;ada&quot; has been completely absorbed by &quot;chip&= quot; as of April 10th.<br> Please see this myumbc posting detailing the rollout of chip and the deleti= on<br> of ada. <a href=3D""https://my3.my.umbc.edu/groups/hpcf/posts/147513"" rel=3D= ""noreferrer"" target=3D""_blank"">https://my3.my.umbc.edu/groups/hpcf/posts/14= 7513</a><br> <br> On Thu Oct 23 12:01:48 2025, ZZ99999 wrote:<br> <br> &gt; First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Moh= ammad<br> &gt; Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0Ebrahimabadi<br> &gt; Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 =C2=A0<a href=3D""mailto:e127@umbc.edu"" target=3D""_blank"">e127@umbc.e= du</a><br> &gt; Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0NQ23652<br> &gt; <br> &gt; Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Per= formance Cluster<br> &gt; <br> &gt; Hello,<br> &gt; <br> &gt; I=E2=80=99m currently unable to access my ADA account. When I try to c= onnect via SSH using the command below:<br> &gt; <br> &gt; ssh <a href=3D""mailto:e127@ada.rs.umbc.edu"" target=3D""_blank"">e127@ada= .rs.umbc.edu</a><br> &gt; <br> &gt; I receive the following error message and do not even get the password=  prompt:<br> &gt; <br> &gt; ssh: connect to host <a href=3D""http://ada.rs.umbc.edu"" rel=3D""norefer= rer"" target=3D""_blank"">ada.rs.umbc.edu</a> port 22: Connection timed out<br> &gt; <br> &gt; Could you please help me resolve this issue?<br> &gt; <br> &gt; Thank you,<br> &gt; Mohammad<br> <br> <br> --<br> <br> Best,<br> Max Breitmeyer<br> DOIT HPC System Administrator<br> <br> <br> <br> ______________________________________<br> <br> Original Request:<br> <br> Requestors: Mohammad Ebrahimabadi<br> <br> First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Mohammad= <br> Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0Ebr= ahimabadi<br> Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=  =C2=A0<a href=3D""mailto:e127@umbc.edu"" target=3D""_blank"">e127@umbc.edu</a>= <br> Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0NQ2= 3652<br> <br> Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Performa= nce Cluster<br> <br> Hello,<br> <br> I=E2=80=99m currently unable to access my ADA account. When I try to connec= t via SSH using the command below:<br> <br> ssh <a href=3D""mailto:e127@ada.rs.umbc.edu"" target=3D""_blank"">e127@ada.rs.u= mbc.edu</a><br> <br> I receive the following error message and do not even get the password prom= pt:<br> <br> ssh: connect to host <a href=3D""http://ada.rs.umbc.edu"" rel=3D""noreferrer"" = target=3D""_blank"">ada.rs.umbc.edu</a> port 22: Connection timed out<br> <br> Could you please help me resolve this issue?<br> <br> Thank you,<br> Mohammad<br> <br> <br> </blockquote></div> "
3298811,72467982,Create,DoIT-Research-Computing,2025-10-23 20:30:39.0000000,HPC User Account: asayyed2 in H.A.R.M.O.N.I. Lab,resolved,Danielle Esposito,desposi1,Adam Sayyed,asayyed2,asayyed2@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Adam<br /> Last Name:                 Sayyed<br /> Email:                     asayyed2@umbc.edu<br /> Campus ID:                 YX18091<br /> <br /> Request Type:              High Performance Cluster<br /> <br /> Create/Modify account in existing PI group<br /> Existing PI Email:    ksolaima@umbc.edu<br /> Existing Group:       H.A.R.M.O.N.I. Lab<br /> Project Title:        Multimodal Stock Price Direction Prediction<br /> Project Abstract:     This project, Multimodal Stock Price Direction Prediction Using Historical Prices, News, and Sentiment, aims to predict short-term stock movements up, down, or stable by integrating numerical and textual financial data. The approach combines historical stock price features with sentiment analysis of financial news to capture both market trends and investor sentiment. Historical price data are collected from Yahoo Finance, while relevant news articles and headlines are sourced from the News API or GDELT. Sentiment scores are derived using FinBERT, a transformer-based model optimized for financial text, and aggregated on a daily basis. These sentiment features are then merged with technical indicators such as moving averages, relative strength index (RSI), and multi-day returns. The resulting multimodal dataset is labeled according to future price direction and used to train classification models including Random Forest, XGBoost, and Multilayer Perceptrons. Model performance is evaluated using accuracy, F1 score, and confusion matrices to compare the predictive power of price-only, sentiment-only, and combined feature sets. The study explores both early and late fusion approaches to integrate modalities and applies feature importance analysis (e.g., SHAP) to interpret model behavior. The findings aim to demonstrate how combining quantitative and qualitative signals can improve stock trend forecasting and provide insights into the interplay between market sentiment and price dynamics.<br /> <br /> N/A<br /> <br /> Attachment 1: <a href=""https://umbc.box.com/s/ytqz3aly2wntqzby3e5d56xfjvackh7x"" target=""_blank"">Project Guide_ Multimodal Stock Price Direction Prediction.pdf</a><br /> "
3298811,72469090,Correspond,DoIT-Research-Computing,2025-10-23 21:30:44.0000000,HPC User Account: asayyed2 in H.A.R.M.O.N.I. Lab,resolved,Danielle Esposito,desposi1,Adam Sayyed,asayyed2,asayyed2@umbc.edu,Khaled Solaiman,ksolaima@umbc.edu,"Yes, I approve.  On Thu, Oct 23, 2025 at 4:30=E2=80=AFPM RT API via RT <UMBCHelp@rt.umbc.edu= > wrote:  > This e-mail is a notification that a UMBC user: Adam Sayyed < > asayyed2@umbc.edu> has requested an account within UMBC's HPC environment > in your group <H.A.R.M.O.N.I. Lab>. As the PI, we request that you > acknowledge and approve this account creation by replying to this message. > Alternatively you can go to this link and review the ticket and indicate > your decision here: > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3298811 > > > Once we have your approval, we will create the account and you and the new > user will receive another e-mail notifying you that the account has been > created. If you have any other questions or concerns please contact us. > > - UMBC DoIT Research Computing Support Staff >   --=20 Best Regards, KMA Solaiman, PhD Assistant Teaching Professor Department of Computer Science and Electrical Engineering University of Maryland, Baltimore County (765) 775-8230 "
3298811,72469090,Correspond,DoIT-Research-Computing,2025-10-23 21:30:44.0000000,HPC User Account: asayyed2 in H.A.R.M.O.N.I. Lab,resolved,Danielle Esposito,desposi1,Adam Sayyed,asayyed2,asayyed2@umbc.edu,Khaled Solaiman,ksolaima@umbc.edu,"<div dir=3D""ltr"">Yes, I approve.</div><br><div class=3D""gmail_quote gmail_q= uote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Thu, Oct 23, 2025 = at 4:30=E2=80=AFPM RT API via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu= "">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_q= uote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,2= 04);padding-left:1ex"">This e-mail is a notification that a UMBC user: Adam = Sayyed &lt;<a href=3D""mailto:asayyed2@umbc.edu"" target=3D""_blank"">asayyed2@= umbc.edu</a>&gt; has requested an account within UMBC&#39;s HPC environment=  in your group &lt;H.A.R.M.O.N.I. Lab&gt;. As the PI, we request that you a= cknowledge and approve this account creation by replying to this message. A= lternatively you can go to this link and review the ticket and indicate you= r decision here:<br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D329= 8811"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3298811</a> &gt;<br> <br> Once we have your approval, we will create the account and you and the new = user will receive another e-mail notifying you that the account has been cr= eated. If you have any other questions or concerns please contact us.<br> <br> - UMBC DoIT Research Computing Support Staff<br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><font color=3D""#000000"">Best Regards,<br>KMA Sol= aiman, PhD</font><div><div><font color=3D""#000000"">Assistant Teaching Profe= ssor</font></div><div><font color=3D""#000000"">Department of Computer Scienc= e and Electrical Engineering<br>University of Maryland, Baltimore County<br= >(765) 775-8230</font><br></div></div></div></div> "
3298811,72480009,Correspond,DoIT-Research-Computing,2025-10-24 15:50:45.0000000,HPC User Account: asayyed2 in H.A.R.M.O.N.I. Lab,resolved,Danielle Esposito,desposi1,Adam Sayyed,asayyed2,asayyed2@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Adam,</p>  <p>Your account (asayyed2) has been created on chip.rs.umbc.edu.<br /> Your primary group is pi_ksolaima.<br /> Your home directory has 500M of storage.<br /> You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip<br /> Please read through the documentation found at hpcf.umbc.edu &gt; User Support.<br /> All available modules can be viewed using the command &#39;module avail&#39;.<br /> Please submit additional questions or issues as separate tickets via the following link.<br /> (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/)</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Thu Oct 23 16:30:39 2025, ZZ99999 wrote: <blockquote>First Name: Adam<br /> Last Name: Sayyed<br /> Email: asayyed2@umbc.edu<br /> Campus ID: YX18091<br /> <br /> Request Type: High Performance Cluster<br /> <br /> Create/Modify account in existing PI group<br /> Existing PI Email: ksolaima@umbc.edu<br /> Existing Group: H.A.R.M.O.N.I. Lab<br /> Project Title: Multimodal Stock Price Direction Prediction<br /> Project Abstract: This project, Multimodal Stock Price Direction Prediction Using Historical Prices, News, and Sentiment, aims to predict short-term stock movements up, down, or stable by integrating numerical and textual financial data. The approach combines historical stock price features with sentiment analysis of financial news to capture both market trends and investor sentiment. Historical price data are collected from Yahoo Finance, while relevant news articles and headlines are sourced from the News API or GDELT. Sentiment scores are derived using FinBERT, a transformer-based model optimized for financial text, and aggregated on a daily basis. These sentiment features are then merged with technical indicators such as moving averages, relative strength index (RSI), and multi-day returns. The resulting multimodal dataset is labeled according to future price direction and used to train classification models including Random Forest, XGBoost, and Multilayer Perceptrons. Model performance is evaluated using accuracy, F1 score, and confusion matrices to compare the predictive power of price-only, sentiment-only, and combined feature sets. The study explores both early and late fusion approaches to integrate modalities and applies feature importance analysis (e.g., SHAP) to interpret model behavior. The findings aim to demonstrate how combining quantitative and qualitative signals can improve stock trend forecasting and provide insights into the interplay between market sentiment and price dynamics.<br /> <br /> N/A<br /> <br /> Attachment 1: Project Guide_ Multimodal Stock Price Direction Prediction.pdf</blockquote> </div> "
3299581,72491178,Create,DoIT-Research-Computing,2025-10-25 16:59:22.0000000,HPC User Account: nageshc1 in FSI,resolved,Beamlak Bekele,bbekele1,Nagesh Chennakeshava,nageshc1,nageshc1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Nagesh Last Name:                 Chennakeshava Email:                     nageshc1@umbc.edu Campus ID:                 PZ57812  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    dli@umbc.edu Existing Group:       FSI Project Title:        Sleep Disorder Monitoring Project Abstract:     This project aims to create a sleep stage monitoring system that utilizes contact-free multi-modal data. The long term goal is to extend the system to be able to detect and monitor sleep disorders. By using cutting-edge signal processing and machine learning methods, the system will be capable of providing precise and non-invasive sleep pattern analysis. In order to make the system scalable and capable of real-time execution, the ultimate solution will be refined for deployment on edge devices.  I am requesting access to a GPU cluster to help experiments on contact-free, multi-modal sleep stage monitoring and sleep disorder detection. This is a project to create and train machine learning models that will be used on edge devices.  "
3299581,72491279,Correspond,DoIT-Research-Computing,2025-10-25 18:07:33.0000000,HPC User Account: nageshc1 in FSI,resolved,Beamlak Bekele,bbekele1,Nagesh Chennakeshava,nageshc1,nageshc1@umbc.edu,Dong Li,dli@umbc.edu,"Yes, I approve.   On Sat, Oct 25, 2025 at 12:59=E2=80=AFPM RT API via RT <UMBCHelp@rt.umbc.ed= u> wrote:  > This e-mail is a notification that a UMBC user: Nagesh Chennakeshava < > nageshc1@umbc.edu> has requested an account within UMBC's HPC environment > in your group <FSI>. As the PI, we request that you acknowledge and appro= ve > this account creation by replying to this message. Alternatively you can = go > to this link and review the ticket and indicate your decision here: > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3299581 > > > Once we have your approval, we will create the account and you and the new > user will receive another e-mail notifying you that the account has been > created. If you have any other questions or concerns please contact us. > > - UMBC DoIT Research Computing Support Staff > "
3299581,72491279,Correspond,DoIT-Research-Computing,2025-10-25 18:07:33.0000000,HPC User Account: nageshc1 in FSI,resolved,Beamlak Bekele,bbekele1,Nagesh Chennakeshava,nageshc1,nageshc1@umbc.edu,Dong Li,dli@umbc.edu,"<div>Yes, I approve.</div><div><br></div><div><br><div class=3D""gmail_quote=  gmail_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Sat, Oct 2= 5, 2025 at 12:59=E2=80=AFPM RT API via RT &lt;<a href=3D""mailto:UMBCHelp@rt= .umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class= =3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left-width:1px;bo= rder-left-style:solid;padding-left:1ex;border-left-color:rgb(204,204,204)"">= This e-mail is a notification that a UMBC user: Nagesh Chennakeshava &lt;<a=  href=3D""mailto:nageshc1@umbc.edu"" target=3D""_blank"">nageshc1@umbc.edu</a>&= gt; has requested an account within UMBC&#39;s HPC environment in your grou= p &lt;FSI&gt;. As the PI, we request that you acknowledge and approve this = account creation by replying to this message. Alternatively you can go to t= his link and review the ticket and indicate your decision here:<br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D329= 9581"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3299581</a> &gt;<br> <br> Once we have your approval, we will create the account and you and the new = user will receive another e-mail notifying you that the account has been cr= eated. If you have any other questions or concerns please contact us.<br> <br> - UMBC DoIT Research Computing Support Staff<br> </blockquote></div></div> "
3299581,72523983,Correspond,DoIT-Research-Computing,2025-10-28 13:33:04.0000000,HPC User Account: nageshc1 in FSI,resolved,Beamlak Bekele,bbekele1,Nagesh Chennakeshava,nageshc1,nageshc1@umbc.edu,Beamlak Bekele,bbekele1@umbc.edu,"<div> <p>Hi Nagesh,</p>  <p>Your account (nageshc1) has been created on chip.rs.umbc.edu.<br /> Your primary group is pi_dli.<br /> Your home directory has 500M of storage.<br /> You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip<br /> Please read through the documentation found at hpcf.umbc.edu &gt; User Support.<br /> All available modules can be viewed using the command &#39;module avail&#39;.<br /> Please submit additional questions or issues as separate tickets via the following link.<br /> (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/)</p>  <p>On Sat Oct 25 12:59:22 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Nagesh Last Name:                 Chennakeshava Email:                     nageshc1@umbc.edu Campus ID:                 PZ57812  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    dli@umbc.edu Existing Group:       FSI Project Title:        Sleep Disorder Monitoring Project Abstract:     This project aims to create a sleep stage monitoring system that utilizes contact-free multi-modal data. The long term goal is to extend the system to be able to detect and monitor sleep disorders. By using cutting-edge signal processing and machine learning methods, the system will be capable of providing precise and non-invasive sleep pattern analysis. In order to make the system scalable and capable of real-time execution, the ultimate solution will be refined for deployment on edge devices.  I am requesting access to a GPU cluster to help experiments on contact-free, multi-modal sleep stage monitoring and sleep disorder detection. This is a project to create and train machine learning models that will be used on edge devices.  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> "
3299984,72501851,Create,DoIT-Research-Computing,2025-10-27 14:31:08.0000000,HPC User Account: rrobert2 in Quantum Thermodynamics Group,resolved,Beamlak Bekele,bbekele1,Reece Robertson,rrobert2,rrobert2@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Reece Last Name:                 Robertson Email:                     rrobert2@umbc.edu Campus ID:                 YB60896  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    deffner@umbc.edu Existing Group:       Quantum Thermodynamics Group Project Title:        Noise-Aware Quantum Dynamics Compilation Via Tensor Networks Project Abstract:     Quantum system simulation is quantum native, however compilation of variational simulation circuits is a difficult task. Efficency of the compiled algorithm is paramount to successful simulation, yet algorithmic methods such as Trotterization result in suboptimal circuits. Prior work has identified machine learning techniques which effeciently improve on Trotterization by several orders of magnitude. This work advances these results by factoring noise into the compilation process, allowing for the generation of circuits which will perform optimally when executed on real, noisy quantum hardware.  Dear HPC team,  I would like to request HPC access to execute quantum simulation algorithms. My advisor, Dr. Deffner, already has group access on the system, and I would like to be added to the group. Please let me know if you need anything else from me to complete my application.  Thank you, Reece Robertson  "
3299984,72502020,Correspond,DoIT-Research-Computing,2025-10-27 14:33:51.0000000,HPC User Account: rrobert2 in Quantum Thermodynamics Group,resolved,Beamlak Bekele,bbekele1,Reece Robertson,rrobert2,rrobert2@umbc.edu,Sebastian Deffner,deffner@umbc.edu,"<!DOCTYPE html> <html>   <head>     <meta http-equiv=3D""Content-Type"" content=3D""text/html; charset=3DUTF-8= "">   </head>   <body>     <p>Acknowledged and approved!</p>     <p>-Sebastian=C2=A0</p>     <div class=3D""moz-signature"">------------------------------------------= ----------------------------------------------<br>       Dr. rer. nat. Sebastian Deffner<br>       Associate Professor<br>       Chair, Graduate Admissions Committee<br>       Department of Physics<br>       Quantum Science Institute<br>       UMBC (University of Maryland, Baltimore County)<br>       1000 Hilltop Circle, Baltimore, MD 21250, USA<br>       <br>       Fellow<br>       National Quantum Laboratory<br>       4505 Campus Dr, College Park, MD 20740, USA<br>       <br>       Chair-Elect and Program Chair<br>       Division of Statistical and Nonlinear Physics (DSNP)<br>       American Physical Society<br>       <br>       Tel.: +1-410-455-1972<br>       <a href=3D""https://quthermo.umbc.edu/"">quthermo.umbc.edu</a><br>       <br>       <i>UMBC was established upon the land of the Piscataway and         Susquehannock peoples. Over time, citizens of many more         Indigenous nations have come to reside in this region. We humbly         offer our respect to all past, present, and future Indigenous         people connected to this place.</i>       <br>       <p style=3D""text-align: left"">         <img src=3D""cid:part1.h7sZY1fp.e67teuqf@umbc.edu"" alt=3D""""           width=3D""300"" height=3D""96"">       </p>     </div>     <div class=3D""moz-cite-prefix"">On 10/27/25 10:31, RT API via RT wrote:<= br>     </div>     <blockquote type=3D""cite"" cite=3D""mid:rt-5.0.5-27464-1761575468-382.3299984-7560-0@rt.umbc.edu"">       <pre wrap=3D"""" class=3D""moz-quote-pre"">This e-mail is a notification = that a UMBC user: Reece Robertson <a class=3D""moz-txt-link-rfc2396E"" href= =3D""mailto:rrobert2@umbc.edu"">&lt;rrobert2@umbc.edu&gt;</a> has requested a= n account within UMBC's HPC environment in your group &lt;Quantum Thermodyn= amics Group&gt;. As the PI, we request that you acknowledge and approve thi= s account creation by replying to this message. Alternatively you can go to=  this link and review the ticket and indicate your decision here:  Ticket &lt;URL: <a class=3D""moz-txt-link-freetext"" href=3D""https://rt.umbc.= edu/Ticket/Display.html?id=3D3299984"">https://rt.umbc.edu/Ticket/Display.ht= ml?id=3D3299984</a> &gt;  Once we have your approval, we will create the account and you and the new = user will receive another e-mail notifying you that the account has been cr= eated. If you have any other questions or concerns please contact us.  - UMBC DoIT Research Computing Support Staff </pre>     </blockquote>   </body> </html>= "
3299984,72502020,Correspond,DoIT-Research-Computing,2025-10-27 14:33:51.0000000,HPC User Account: rrobert2 in Quantum Thermodynamics Group,resolved,Beamlak Bekele,bbekele1,Reece Robertson,rrobert2,rrobert2@umbc.edu,Sebastian Deffner,deffner@umbc.edu,"Acknowledged and approved!  -Sebastian  ---------------------------------------------------------------------------------------- Dr. rer. nat. Sebastian Deffner Associate Professor Chair, Graduate Admissions Committee Department of Physics Quantum Science Institute UMBC (University of Maryland, Baltimore County) 1000 Hilltop Circle, Baltimore, MD 21250, USA  Fellow National Quantum Laboratory 4505 Campus Dr, College Park, MD 20740, USA  Chair-Elect and Program Chair Division of Statistical and Nonlinear Physics (DSNP) American Physical Society  Tel.: +1-410-455-1972 quthermo.umbc.edu <https://quthermo.umbc.edu/>  /UMBC was established upon the land of the Piscataway and Susquehannock  peoples. Over time, citizens of many more Indigenous nations have come  to reside in this region. We humbly offer our respect to all past,  present, and future Indigenous people connected to this place./  On 10/27/25 10:31, RT API via RT wrote: > This e-mail is a notification that a UMBC user: Reece Robertson<rrobert2@umbc.edu> has requested an account within UMBC's HPC environment in your group <Quantum Thermodynamics Group>. As the PI, we request that you acknowledge and approve this account creation by replying to this message. Alternatively you can go to this link and review the ticket and indicate your decision here: > > Ticket <URL:https://rt.umbc.edu/Ticket/Display.html?id=3299984 > > > Once we have your approval, we will create the account and you and the new user will receive another e-mail notifying you that the account has been created. If you have any other questions or concerns please contact us. > > - UMBC DoIT Research Computing Support Staff"
3299984,72524540,Correspond,DoIT-Research-Computing,2025-10-28 13:43:27.0000000,HPC User Account: rrobert2 in Quantum Thermodynamics Group,resolved,Beamlak Bekele,bbekele1,Reece Robertson,rrobert2,rrobert2@umbc.edu,Beamlak Bekele,bbekele1@umbc.edu,"<div> <p>Hi Reece,</p>  <p>Your account (rrobert2) has been created on chip.rs.umbc.edu.<br /> Your primary group is pi_deffner.<br /> Your home directory has 500M of storage.<br /> You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip<br /> Please read through the documentation found at hpcf.umbc.edu &gt; User Support.<br /> All available modules can be viewed using the command &#39;module avail&#39;.<br /> Please submit additional questions or issues as separate tickets via the following link.<br /> (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/)</p>  <p>On Mon Oct 27 10:31:08 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Reece Last Name:                 Robertson Email:                     rrobert2@umbc.edu Campus ID:                 YB60896  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    deffner@umbc.edu Existing Group:       Quantum Thermodynamics Group Project Title:        Noise-Aware Quantum Dynamics Compilation Via Tensor Networks Project Abstract:     Quantum system simulation is quantum native, however compilation of variational simulation circuits is a difficult task. Efficency of the compiled algorithm is paramount to successful simulation, yet algorithmic methods such as Trotterization result in suboptimal circuits. Prior work has identified machine learning techniques which effeciently improve on Trotterization by several orders of magnitude. This work advances these results by factoring noise into the compilation process, allowing for the generation of circuits which will perform optimally when executed on real, noisy quantum hardware.  Dear HPC team,  I would like to request HPC access to execute quantum simulation algorithms. My advisor, Dr. Deffner, already has group access on the system, and I would like to be added to the group. Please let me know if you need anything else from me to complete my application.  Thank you, Reece Robertson  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> "
3300174,72508193,Create,DoIT-Research-Computing,2025-10-27 17:02:13.0000000,HPC User Account: josey1 in Deffner,resolved,Beamlak Bekele,bbekele1,Josey Stevens,josey1,josey1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Josey Last Name:                 Stevens Email:                     josey1@umbc.edu Campus ID:                 CZ12050  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    deffner@umbc.edu Existing Group:       Deffner Project Title:        Thermodynamics of Quantum Computation Project Abstract:     N/A  I am unable to logon to Chip notes.  Following the access instructions, I get to the poin where I am prompted for my password, but it does not accept the password  "
3300174,72516902,Correspond,DoIT-Research-Computing,2025-10-27 20:56:34.0000000,HPC User Account: josey1 in Deffner,resolved,Beamlak Bekele,bbekele1,Josey Stevens,josey1,josey1@umbc.edu,Sebastian Deffner,deffner@umbc.edu,"Acknowledged and approved.  -Sebastian  ---------------------------------------------------------------------------------------- Dr. rer. nat. Sebastian Deffner Associate Professor Chair, Graduate Admissions Committee Department of Physics Quantum Science Institute UMBC (University of Maryland, Baltimore County) 1000 Hilltop Circle, Baltimore, MD 21250, USA  Fellow National Quantum Laboratory 4505 Campus Dr, College Park, MD 20740, USA  Chair-Elect and Program Chair Division of Statistical and Nonlinear Physics (DSNP) American Physical Society  Tel.: +1-410-455-1972 quthermo.umbc.edu <https://quthermo.umbc.edu/>  /UMBC was established upon the land of the Piscataway and Susquehannock  peoples. Over time, citizens of many more Indigenous nations have come  to reside in this region. We humbly offer our respect to all past,  present, and future Indigenous people connected to this place./  On 10/27/25 13:02, RT API via RT wrote: > This e-mail is a notification that a UMBC user: Josey Stevens<josey1@umbc.edu> has requested an account within UMBC's HPC environment in your group <Deffner>. As the PI, we request that you acknowledge and approve this account creation by replying to this message. Alternatively you can go to this link and review the ticket and indicate your decision here: > > Ticket <URL:https://rt.umbc.edu/Ticket/Display.html?id=3300174 > > > Once we have your approval, we will create the account and you and the new user will receive another e-mail notifying you that the account has been created. If you have any other questions or concerns please contact us. > > - UMBC DoIT Research Computing Support Staff"
3300174,72516902,Correspond,DoIT-Research-Computing,2025-10-27 20:56:34.0000000,HPC User Account: josey1 in Deffner,resolved,Beamlak Bekele,bbekele1,Josey Stevens,josey1,josey1@umbc.edu,Sebastian Deffner,deffner@umbc.edu,"<!DOCTYPE html> <html>   <head>     <meta http-equiv=3D""Content-Type"" content=3D""text/html; charset=3DUTF-8= "">   </head>   <body>     <p>Acknowledged and approved.</p>     <p>-Sebastian=C2=A0</p>     <div class=3D""moz-signature"">------------------------------------------= ----------------------------------------------<br>       Dr. rer. nat. Sebastian Deffner<br>       Associate Professor<br>       Chair, Graduate Admissions Committee<br>       Department of Physics<br>       Quantum Science Institute<br>       UMBC (University of Maryland, Baltimore County)<br>       1000 Hilltop Circle, Baltimore, MD 21250, USA<br>       <br>       Fellow<br>       National Quantum Laboratory<br>       4505 Campus Dr, College Park, MD 20740, USA<br>       <br>       Chair-Elect and Program Chair<br>       Division of Statistical and Nonlinear Physics (DSNP)<br>       American Physical Society<br>       <br>       Tel.: +1-410-455-1972<br>       <a href=3D""https://quthermo.umbc.edu/"">quthermo.umbc.edu</a><br>       <br>       <i>UMBC was established upon the land of the Piscataway and         Susquehannock peoples. Over time, citizens of many more         Indigenous nations have come to reside in this region. We humbly         offer our respect to all past, present, and future Indigenous         people connected to this place.</i>       <br>       <p style=3D""text-align: left"">         <img src=3D""cid:part1.36J17qkc.XtKjQSA8@umbc.edu"" alt=3D""""           width=3D""300"" height=3D""96"">       </p>     </div>     <div class=3D""moz-cite-prefix"">On 10/27/25 13:02, RT API via RT wrote:<= br>     </div>     <blockquote type=3D""cite"" cite=3D""mid:rt-5.0.5-13046-1761584533-236.3300174-7560-0@rt.umbc.edu"">       <pre wrap=3D"""" class=3D""moz-quote-pre"">This e-mail is a notification = that a UMBC user: Josey Stevens <a class=3D""moz-txt-link-rfc2396E"" href=3D""= mailto:josey1@umbc.edu"">&lt;josey1@umbc.edu&gt;</a> has requested an accoun= t within UMBC's HPC environment in your group &lt;Deffner&gt;. As the PI, w= e request that you acknowledge and approve this account creation by replyin= g to this message. Alternatively you can go to this link and review the tic= ket and indicate your decision here:  Ticket &lt;URL: <a class=3D""moz-txt-link-freetext"" href=3D""https://rt.umbc.= edu/Ticket/Display.html?id=3D3300174"">https://rt.umbc.edu/Ticket/Display.ht= ml?id=3D3300174</a> &gt;  Once we have your approval, we will create the account and you and the new = user will receive another e-mail notifying you that the account has been cr= eated. If you have any other questions or concerns please contact us.  - UMBC DoIT Research Computing Support Staff </pre>     </blockquote>   </body> </html>= "
3300174,72523771,Correspond,DoIT-Research-Computing,2025-10-28 13:29:02.0000000,HPC User Account: josey1 in Deffner,resolved,Beamlak Bekele,bbekele1,Josey Stevens,josey1,josey1@umbc.edu,Beamlak Bekele,bbekele1@umbc.edu,"<div> <p>Hi Josey,</p>  <p>You weren&#39;t able to log in because you didn&#39;t have a chip account. Your account (josey1) has been created on chip.rs.umbc.edu.<br /> Your primary group is pi_deffner.<br /> Your home directory has 500M of storage.<br /> You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip<br /> Please read through the documentation found at hpcf.umbc.edu &gt; User Support.<br /> All available modules can be viewed using the command &#39;module avail&#39;.<br /> Please submit additional questions or issues as separate tickets via the following link.<br /> (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/)</p>  <p>On Mon Oct 27 13:02:13 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Josey Last Name:                 Stevens Email:                     josey1@umbc.edu Campus ID:                 CZ12050  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    deffner@umbc.edu Existing Group:       Deffner Project Title:        Thermodynamics of Quantum Computation Project Abstract:     N/A  I am unable to logon to Chip notes.  Following the access instructions, I get to the poin where I am prompted for my password, but it does not accept the password  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> "
3300690,72523025,Create,DoIT-Research-Computing,2025-10-28 13:13:49.0000000,Data upload and server processing,stalled,Danielle Esposito,desposi1,Max Breitmeyer,mb17,mb17@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<p>From Sayantan:<br /> &quot;</p>  <p>We would like to set up&nbsp;data&nbsp;transfer&nbsp;to storage and&nbsp;processing&nbsp;using Chip.</p>  <p>&nbsp;</p>  <p>Can you please share some instructions or links on the following topics -</p>  <p>1. Storage access for the students - Emily would&nbsp;upload&nbsp;data&nbsp;to USDA storage that we purchased and Nadeem will store&nbsp;data&nbsp;on the storage allotted&nbsp;to me as PI.</p>  <p>&nbsp;</p>  <p>2.&nbsp;Server&nbsp;access for MATLAB&nbsp;processing&nbsp;- We would do ssh login to the&nbsp;server&nbsp;and storage both, for&nbsp;server&nbsp;SSH with X11 forwarding such that matlab GUI opens up. Also do we need to mount the storage in the&nbsp;server&nbsp;machine to access&nbsp;data&nbsp;or is it automatically connected?</p>  <p>&nbsp;</p>  <p>3. Some instructions on general things like - ssh access, mounting&nbsp;server, accessing storage from&nbsp;server&nbsp;for&nbsp;processing, checking the directory structure, file size, file moving using rsync etc. - standard things (students will also Google, but if there is any standard documentation on DOIT end, we will appreciate that).</p>  <p>&nbsp;</p>  <p>Thank you,</p>  <p>Sayantan&quot;</p>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3300690,72528981,Correspond,DoIT-Research-Computing,2025-10-28 15:15:44.0000000,Data upload and server processing,stalled,Danielle Esposito,desposi1,Max Breitmeyer,mb17,mb17@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hello there,</p>  <p>Of course, we&#39;ve recently made large improvements to our wiki, so here&#39;s the top-level link to that, which should answer most of your common questions. For questions that you can&#39;t find a page for, feel free to submit a ticket and ask one of us directly; we&#39;ll answer it and probably add a new page to the wiki for future similar questions.</p>  <p>Wiki Link:&nbsp;https://umbc.atlassian.net/wiki/spaces/faq/pages/1082589207/UMBC+HPCF+-+chip</p>  <p>Link to submit a ticket:&nbsp;https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research%20Computing</p>  <p>&nbsp;</p>  <p>1. Here&#39;s a link to the general storage wiki page, and a page on how to upload data to the cluster:</p>  <p>General Storage wiki page:&nbsp;https://umbc.atlassian.net/wiki/spaces/faq/pages/1072267344/Storage</p>  <p>How to upload data to the cluster: https://umbc.atlassian.net/wiki/spaces/faq/pages/1576534018/Uploading+Data+to+the+Cluster</p>  <p>Note about the &quot;How to upload data to the cluster&quot;, it&#39;s very beginner-friendly, so it currently only includes simple scp uploading; there are, of course, other ways to upload data to the cluster.</p>  <p>2. Here&#39;s a link to the wiki page on X11 forwarding and GUI applications:</p>  <p>https://umbc.atlassian.net/wiki/spaces/faq/pages/1202749443/Accessing+GUI+Applications+on+Chip+Using+X11+Forwarding</p>  <p>3. We should have pages for all those general questions on the wiki, but a great place for users to start is on our &quot;Tutorials&quot; wiki page, and specifically &quot;Getting Started on Chip&quot;:</p>  <p>Tutorials wiki page:&nbsp;https://umbc.atlassian.net/wiki/spaces/faq/pages/1239908360/Tutorials</p>  <p>Getting started on chip:&nbsp;https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip</p>  <p>&nbsp;</p>  <p>Let me know if you have any more questions!</p>  <p>Best,</p>  <p>Elliot</p>  <p>&nbsp;</p> "
3300690,72529470,Comment,DoIT-Research-Computing,2025-10-28 15:25:14.0000000,Data upload and server processing,stalled,Danielle Esposito,desposi1,Max Breitmeyer,mb17,mb17@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Sorry but leave this one open until they say it&#39;s resolved. Like I said, they paid us money so they get a little bit of special treatment.&nbsp;</p>  <p>On Tue Oct 28 11:15:44 2025, IO12693 wrote:</p>  <blockquote> <p>Hello there,</p>  <p>Of course, we&#39;ve recently made large improvements to our wiki, so here&#39;s the top-level link to that, which should answer most of your common questions. For questions that you can&#39;t find a page for, feel free to submit a ticket and ask one of us directly; we&#39;ll answer it and probably add a new page to the wiki for future similar questions.</p>  <p>Wiki Link:&nbsp;https://umbc.atlassian.net/wiki/spaces/faq/pages/1082589207/UMBC+HPCF+-+chip</p>  <p>Link to submit a ticket:&nbsp;https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research%20Computing</p>  <p>&nbsp;</p>  <p>1. Here&#39;s a link to the general storage wiki page, and a page on how to upload data to the cluster:</p>  <p>General Storage wiki page:&nbsp;https://umbc.atlassian.net/wiki/spaces/faq/pages/1072267344/Storage</p>  <p>How to upload data to the cluster: https://umbc.atlassian.net/wiki/spaces/faq/pages/1576534018/Uploading+Data+to+the+Cluster</p>  <p>Note about the &quot;How to upload data to the cluster&quot;, it&#39;s very beginner-friendly, so it currently only includes simple scp uploading; there are, of course, other ways to upload data to the cluster.</p>  <p>2. Here&#39;s a link to the wiki page on X11 forwarding and GUI applications:</p>  <p>https://umbc.atlassian.net/wiki/spaces/faq/pages/1202749443/Accessing+GUI+Applications+on+Chip+Using+X11+Forwarding</p>  <p>3. We should have pages for all those general questions on the wiki, but a great place for users to start is on our &quot;Tutorials&quot; wiki page, and specifically &quot;Getting Started on Chip&quot;:</p>  <p>Tutorials wiki page:&nbsp;https://umbc.atlassian.net/wiki/spaces/faq/pages/1239908360/Tutorials</p>  <p>Getting started on chip:&nbsp;https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip</p>  <p>&nbsp;</p>  <p>Let me know if you have any more questions!</p>  <p>Best,</p>  <p>Elliot</p>  <p>&nbsp;</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3300690,72529471,CommentEmailRecord,DoIT-Research-Computing,2025-10-28 15:25:15.0000000,Data upload and server processing,stalled,Danielle Esposito,desposi1,Max Breitmeyer,mb17,mb17@umbc.edu,The RT System itself,NULL,"Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3300690 >  Comment just added.    Sorry but leave this one open until they say it's resolved. Like I said, they paid us money so they get a little bit of special treatment.  On Tue Oct 28 11:15:44 2025, IO12693 wrote:  > Hello there,  > Of course, we've recently made large improvements to our wiki, so here's > the top-level link to that, which should answer most of your common > questions. For questions that you can't find a page for, feel free to > submit a ticket and ask one of us directly; we'll answer it and probably > add a new page to the wiki for future similar questions.  > Wiki Link: > https://umbc.atlassian.net/wiki/spaces/faq/pages/1082589207/UMBC+HPCF+-+chip  > Link to submit a ticket: > https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research%20Computing  > 1. Here's a link to the general storage wiki page, and a page on how to > upload data to the cluster:  > General Storage wiki page: > https://umbc.atlassian.net/wiki/spaces/faq/pages/1072267344/Storage  > How to upload data to the cluster: > https://umbc.atlassian.net/wiki/spaces/faq/pages/1576534018/Uploading+Data+to+the+Cluster  > Note about the ""How to upload data to the cluster"", it's very > beginner-friendly, so it currently only includes simple scp uploading; > there are, of course, other ways to upload data to the cluster.  > 2. Here's a link to the wiki page on X11 forwarding and GUI applications:  > https://umbc.atlassian.net/wiki/spaces/faq/pages/1202749443/Accessing+GUI+Applications+on+Chip+Using+X11+Forwarding  > 3. We should have pages for all those general questions on the wiki, but a > great place for users to start is on our ""Tutorials"" wiki page, and > specifically ""Getting Started on Chip"":  > Tutorials wiki page: > https://umbc.atlassian.net/wiki/spaces/faq/pages/1239908360/Tutorials  > Getting started on chip: > https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip  > Let me know if you have any more questions!  > Best,  > Elliot  --  Best, Max Breitmeyer DOIT HPC System Administrator  "
3300690,72529502,Comment,DoIT-Research-Computing,2025-10-28 15:26:22.0000000,Data upload and server processing,stalled,Danielle Esposito,desposi1,Max Breitmeyer,mb17,mb17@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Sounds good, will do</p> "
3300690,72529799,Correspond,DoIT-Research-Computing,2025-10-28 15:33:46.0000000,Data upload and server processing,stalled,Danielle Esposito,desposi1,Max Breitmeyer,mb17,mb17@umbc.edu,Sayantan Bhattacharya,sbhattac@umbc.edu,"Thank you for the response and links. Nadeem and Emily, please go through them.  I have an additional question about the USDA storage (which we invested in). Is that research volume setup? What would be the login name for that?   Thanks, Sayantan  On Tue, Oct 28, 2025 at 11:15=E2=80=AFAM Elliot Gobbert via RT <UMBCHelp@rt= .umbc.edu> wrote:  > If you agree your issue is resolved, please give us feedback on your > experience by completing a brief satisfaction survey: > > > https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&customeremail= =3Dmb17%40umbc.edu&groupid=3DEIS&ticketid=3D3300690&ticketowner=3Delliotg2%= 40umbc.edu&ticketsubject=3DData%20upload%20and%20server%20processing > > If you believe your issue has not been resolved, please respond to this > message, which will reopen your ticket. Note: A full record of your reque= st > can be found at: > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3300690 > > > Thank You > > _________________________________________ > > R e s o l u t i o n: > =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D = =3D > > Hello there, > > Of course, we've recently made large improvements to our wiki, so here's > the > top-level link to that, which should answer most of your common questions. > For > questions that you can't find a page for, feel free to submit a ticket and > ask > one of us directly; we'll answer it and probably add a new page to the > wiki for > future similar questions. > > Wiki Link: > > https://umbc.atlassian.net/wiki/spaces/faq/pages/1082589207/UMBC+HPCF+-+c= hip > > Link to submit a ticket: > > https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=3DRe= search%20Computing > > 1. Here's a link to the general storage wiki page, and a page on how to > upload > data to the cluster: > > General Storage wiki page: > https://umbc.atlassian.net/wiki/spaces/faq/pages/1072267344/Storage > > How to upload data to the cluster: > > https://umbc.atlassian.net/wiki/spaces/faq/pages/1576534018/Uploading+Dat= a+to+the+Cluster > > Note about the ""How to upload data to the cluster"", it's very > beginner-friendly, so it currently only includes simple scp uploading; > there > are, of course, other ways to upload data to the cluster. > > 2. Here's a link to the wiki page on X11 forwarding and GUI applications: > > > https://umbc.atlassian.net/wiki/spaces/faq/pages/1202749443/Accessing+GUI= +Applications+on+Chip+Using+X11+Forwarding > > 3. We should have pages for all those general questions on the wiki, but a > great place for users to start is on our ""Tutorials"" wiki page, and > specifically ""Getting Started on Chip"": > > Tutorials wiki page: > https://umbc.atlassian.net/wiki/spaces/faq/pages/1239908360/Tutorials > > Getting started on chip: > > https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Start= ed+on+chip > > Let me know if you have any more questions! > > Best, > > Elliot > > > > ______________________________________ > > Original Request: > > Requestors: Max Breitmeyer > > From Sayantan: > "" > > We would like to set up data transfer to storage and processing using Chi= p. > > Can you please share some instructions or links on the following topics - > > 1. Storage access for the students - Emily would upload data to USDA > storage > that we purchased and Nadeem will store data on the storage allotted to me > as > PI. > > 2. Server access for MATLAB processing - We would do ssh login to the > server > and storage both, for server SSH with X11 forwarding such that matlab GUI > opens > up. Also do we need to mount the storage in the server machine to access > data > or is it automatically connected? > > 3. Some instructions on general things like - ssh access, mounting server, > accessing storage from server for processing, checking the directory > structure, > file size, file moving using rsync etc. - standard things (students will > also > Google, but if there is any standard documentation on DOIT end, we will > appreciate that). > > Thank you, > > Sayantan"" > > -- > > Best, > Max Breitmeyer > DOIT HPC System Administrator > > > "
3300690,72529799,Correspond,DoIT-Research-Computing,2025-10-28 15:33:46.0000000,Data upload and server processing,stalled,Danielle Esposito,desposi1,Max Breitmeyer,mb17,mb17@umbc.edu,Sayantan Bhattacharya,sbhattac@umbc.edu,"<div dir=3D""ltr"">Thank you for the response and links. Nadeem and Emily, pl= ease go through them.<div><br></div><div>I have an additional question abou= t the USDA storage (which we invested in). Is that research volume setup? W= hat would be the login name for that?</div><div><br></div><div><br></div><d= iv>Thanks,</div><div>Sayantan</div></div><br><div class=3D""gmail_quote gmai= l_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Tue, Oct 28, 20= 25 at 11:15=E2=80=AFAM Elliot Gobbert via RT &lt;<a href=3D""mailto:UMBCHelp= @rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote clas= s=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid r= gb(204,204,204);padding-left:1ex"">If you agree your issue is resolved, plea= se give us feedback on your experience by completing a brief satisfaction s= urvey: <br> <br> <a href=3D""https://umbc.us2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&amp;= customeremail=3Dmb17%40umbc.edu&amp;groupid=3DEIS&amp;ticketid=3D3300690&am= p;ticketowner=3Delliotg2%40umbc.edu&amp;ticketsubject=3DData%20upload%20and= %20server%20processing"" rel=3D""noreferrer"" target=3D""_blank"">https://umbc.u= s2.qualtrics.com/SE/?SID=3DSV_etfDUq3MTISF6Ly&amp;customeremail=3Dmb17%40um= bc.edu&amp;groupid=3DEIS&amp;ticketid=3D3300690&amp;ticketowner=3Delliotg2%= 40umbc.edu&amp;ticketsubject=3DData%20upload%20and%20server%20processing</a= ><br> <br> If you believe your issue has not been resolved, please respond to this mes= sage, which will reopen your ticket. Note: A full record of your request ca= n be found at:=C2=A0 <br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D330= 0690"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3300690</a> &gt;<br> <br> Thank You<br> <br> _________________________________________<br> <br> R e s o l u t i o n:<br> =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D =3D= =C2=A0 <br> <br> Hello there,<br> <br> Of course, we&#39;ve recently made large improvements to our wiki, so here&= #39;s the<br> top-level link to that, which should answer most of your common questions. = For<br> questions that you can&#39;t find a page for, feel free to submit a ticket = and ask<br> one of us directly; we&#39;ll answer it and probably add a new page to the = wiki for<br> future similar questions.<br> <br> Wiki Link:<br> <a href=3D""https://umbc.atlassian.net/wiki/spaces/faq/pages/1082589207/UMBC= +HPCF+-+chip"" rel=3D""noreferrer"" target=3D""_blank"">https://umbc.atlassian.n= et/wiki/spaces/faq/pages/1082589207/UMBC+HPCF+-+chip</a><br> <br> Link to submit a ticket:<br> <a href=3D""https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?= auto=3DResearch%20Computing"" rel=3D""noreferrer"" target=3D""_blank"">https://r= tforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=3DResearch%20Co= mputing</a><br> <br> 1. Here&#39;s a link to the general storage wiki page, and a page on how to=  upload<br> data to the cluster:<br> <br> General Storage wiki page:<br> <a href=3D""https://umbc.atlassian.net/wiki/spaces/faq/pages/1072267344/Stor= age"" rel=3D""noreferrer"" target=3D""_blank"">https://umbc.atlassian.net/wiki/s= paces/faq/pages/1072267344/Storage</a><br> <br> How to upload data to the cluster:<br> <a href=3D""https://umbc.atlassian.net/wiki/spaces/faq/pages/1576534018/Uplo= ading+Data+to+the+Cluster"" rel=3D""noreferrer"" target=3D""_blank"">https://umb= c.atlassian.net/wiki/spaces/faq/pages/1576534018/Uploading+Data+to+the+Clus= ter</a><br> <br> Note about the &quot;How to upload data to the cluster&quot;, it&#39;s very= <br> beginner-friendly, so it currently only includes simple scp uploading; ther= e<br> are, of course, other ways to upload data to the cluster.<br> <br> 2. Here&#39;s a link to the wiki page on X11 forwarding and GUI application= s:<br> <br> <a href=3D""https://umbc.atlassian.net/wiki/spaces/faq/pages/1202749443/Acce= ssing+GUI+Applications+on+Chip+Using+X11+Forwarding"" rel=3D""noreferrer"" tar= get=3D""_blank"">https://umbc.atlassian.net/wiki/spaces/faq/pages/1202749443/= Accessing+GUI+Applications+on+Chip+Using+X11+Forwarding</a><br> <br> 3. We should have pages for all those general questions on the wiki, but a<= br> great place for users to start is on our &quot;Tutorials&quot; wiki page, a= nd<br> specifically &quot;Getting Started on Chip&quot;:<br> <br> Tutorials wiki page:<br> <a href=3D""https://umbc.atlassian.net/wiki/spaces/faq/pages/1239908360/Tuto= rials"" rel=3D""noreferrer"" target=3D""_blank"">https://umbc.atlassian.net/wiki= /spaces/faq/pages/1239908360/Tutorials</a><br> <br> Getting started on chip:<br> <a href=3D""https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Gett= ing+Started+on+chip"" rel=3D""noreferrer"" target=3D""_blank"">https://umbc.atla= ssian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip</a><br> <br> Let me know if you have any more questions!<br> <br> Best,<br> <br> Elliot<br> <br> <br> <br> ______________________________________<br> <br> Original Request:<br> <br> Requestors: Max Breitmeyer<br> <br> From Sayantan:<br> &quot;<br> <br> We would like to set up data transfer to storage and processing using Chip.= <br> <br> Can you please share some instructions or links on the following topics -<b= r> <br> 1. Storage access for the students - Emily would upload data to USDA storag= e<br> that we purchased and Nadeem will store data on the storage allotted to me = as<br> PI.<br> <br> 2. Server access for MATLAB processing - We would do ssh login to the serve= r<br> and storage both, for server SSH with X11 forwarding such that matlab GUI o= pens<br> up. Also do we need to mount the storage in the server machine to access da= ta<br> or is it automatically connected?<br> <br> 3. Some instructions on general things like - ssh access, mounting server,<= br> accessing storage from server for processing, checking the directory struct= ure,<br> file size, file moving using rsync etc. - standard things (students will al= so<br> Google, but if there is any standard documentation on DOIT end, we will<br> appreciate that).<br> <br> Thank you,<br> <br> Sayantan&quot;<br> <br> --<br> <br> Best,<br> Max Breitmeyer<br> DOIT HPC System Administrator<br> <br> <br> </blockquote></div> "
3300690,72601144,Comment,DoIT-Research-Computing,2025-10-30 16:02:21.0000000,Data upload and server processing,stalled,Danielle Esposito,desposi1,Max Breitmeyer,mb17,mb17@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hi,</p>  <p>No, I have not set up the research volume yet. Before I do, could I know the&nbsp;specific name or grant award number that we should use for the name of this Center on the cluster?</p>  <p>Best,</p>  <p>Elliot</p> "
3300690,72705428,Correspond,DoIT-Research-Computing,2025-11-04 14:26:24.0000000,Data upload and server processing,stalled,Danielle Esposito,desposi1,Max Breitmeyer,mb17,mb17@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hi,</p>  <p>Just checking back in with you guys. Any specific name for the Center that we should use for the cluster?</p>  <p>Best,</p>  <p>Elliot</p> "
3300690,72705798,Correspond,DoIT-Research-Computing,2025-11-04 14:32:16.0000000,Data upload and server processing,stalled,Danielle Esposito,desposi1,Max Breitmeyer,mb17,mb17@umbc.edu,Sayantan Bhattacharya,sbhattac@umbc.edu,"Maybe we can use the following name - ""USDA-EB"" for the cluster.  On Tue, Nov 4, 2025 at 9:26=E2=80=AFAM Elliot Gobbert via RT <UMBCHelp@rt.u= mbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3300690 > > > Last Update From Ticket: > > Hi, > > Just checking back in with you guys. Any specific name for the Center that > we > should use for the cluster? > > Best, > > Elliot > > > "
3300690,72705798,Correspond,DoIT-Research-Computing,2025-11-04 14:32:16.0000000,Data upload and server processing,stalled,Danielle Esposito,desposi1,Max Breitmeyer,mb17,mb17@umbc.edu,Sayantan Bhattacharya,sbhattac@umbc.edu,"<div dir=3D""ltr"">Maybe we can use the following name - &quot;USDA-EB&quot; = for the cluster.</div><br><div class=3D""gmail_quote gmail_quote_container"">= <div dir=3D""ltr"" class=3D""gmail_attr"">On Tue, Nov 4, 2025 at 9:26=E2=80=AFA= M Elliot Gobbert via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHel= p@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" sty= le=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);paddi= ng-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.= html?id=3D3300690"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu= /Ticket/Display.html?id=3D3300690</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi,<br> <br> Just checking back in with you guys. Any specific name for the Center that = we<br> should use for the cluster?<br> <br> Best,<br> <br> Elliot<br> <br> <br> </blockquote></div> "
3300690,72713194,Comment,DoIT-Research-Computing,2025-11-04 16:46:57.0000000,Data upload and server processing,stalled,Danielle Esposito,desposi1,Max Breitmeyer,mb17,mb17@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Double check that this name is ok with Roy. Imo, this is too vague of a = name, and doesn&#39;t easily help us find the owners of the group in the fu= ture, but if Roy is fine with it then c&#39;est la vie&nbsp;</p>  <p>On Tue Nov 04 09:32:16 2025, NJ87978 wrote:</p>  <blockquote> <div>Maybe we can use the following name - &quot;USDA-EB&quot; for the clus= ter.</div> &nbsp;  <div> <div>On Tue, Nov 4, 2025 at 9:26=E2=80=AFAM Elliot Gobbert via RT &lt;UMBCH= elp@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D33= 00690 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hi,<br /> <br /> Just checking back in with you guys. Any specific name for the Center that = we<br /> should use for the cluster?<br /> <br /> Best,<br /> <br /> Elliot<br /> <br /> &nbsp;</blockquote> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3300690,72713197,CommentEmailRecord,DoIT-Research-Computing,2025-11-04 16:46:59.0000000,Data upload and server processing,stalled,Danielle Esposito,desposi1,Max Breitmeyer,mb17,mb17@umbc.edu,The RT System itself,NULL,"Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3300690 >  Comment just added.    Double check that this name is ok with Roy. Imo, this is too vague of a name, and doesn't easily help us find the owners of the group in the future, but if Roy is fine with it then c'est la vie  On Tue Nov 04 09:32:16 2025, NJ87978 wrote:  > Maybe we can use the following name - ""USDA-EB"" for the cluster. On Tue, > Nov 4, 2025 at 9:26 AM Elliot Gobbert via RT <UMBCHelp@rt.umbc.edu> wrote:  >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3300690 >  >> Last Update From Ticket:  >> Hi,  >> Just checking back in with you guys. Any specific name for the Center >> that we >> should use for the cluster?  >> Best,  >> Elliot  --  Best, Max Breitmeyer DOIT HPC System Administrator  "
3300690,72718294,Comment,DoIT-Research-Computing,2025-11-04 17:06:33.0000000,Data upload and server processing,stalled,Danielle Esposito,desposi1,Max Breitmeyer,mb17,mb17@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Roy: &quot;usda-eb is fine with me. Get an award number, title, abstract from them, too. Don&#39;t let that hold up the group provisioning though&quot;<br /> <br /> Danielle has a script she wants to use, she&#39;ll probably be creating the center</p> "
3300690,72727340,Correspond,DoIT-Research-Computing,2025-11-04 18:52:46.0000000,Data upload and server processing,stalled,Danielle Esposito,desposi1,Max Breitmeyer,mb17,mb17@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Sayantan,</p>  <p>Welcome to chip, UMBC&#39;s High Performance Computing Cluster!</p>  <p>The group, usda-eb, now exists on the chip cluster. Members of this grou= p can access and contribute to the research storage space allocated to the = group.</p>  <p>This storage space is located at /umbc/rs/usda-eb, and currently has a q= uota of 50T.</p>  <p>For information on accessing the cluster, adding accounts to your group,=  and getting started using the cluster, check out the tutorial on our wiki:=  https://umbc.atlassian.net/wiki/x/R4BPQg</p>  <p>Additional documentation is also available here: https://umbc.atlassian.= net/wiki/x/FwCHQ</p>  <p>If you have any questions or issues, please submit a new RT ticket at: h= ttps://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=3DResea= rch%20Computing</p>  <p>Currently, sbhattac is the &#39;owner&#39; of the group, and is the only=  member of the group. To request users to be added to your group, please su= bmit an RT ticket from the following link:&nbsp;https://rtforms.umbc.edu/rt= _authenticated/doit/DoIT-support.php?auto=3DResearch%20Computing</p>  <p><strong>Additionally, I will need a little bit more information regardin= g your grant. Could you please provide an award number, title, and abstract= ?&nbsp;</strong></p>  <p>If you have any additional concerns, questions, or run into any problems= , please feel free to submit a new ticket! Have a great day!</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Tue Nov 04 09:32:16 2025, NJ87978 wrote: <blockquote> <div>Maybe we can use the following name - &quot;USDA-EB&quot; for the clus= ter.</div> &nbsp;  <div> <div>On Tue, Nov 4, 2025 at 9:26=E2=80=AFAM Elliot Gobbert via RT &lt;UMBCH= elp@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D33= 00690 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hi,<br /> <br /> Just checking back in with you guys. Any specific name for the Center that = we<br /> should use for the cluster?<br /> <br /> Best,<br /> <br /> Elliot<br /> <br /> &nbsp;</blockquote> </div> </blockquote> </div> "
3301107,72537577,Create,DoIT-Research-Computing,2025-10-28 18:57:40.0000000,HPC User Account: ii69854 in nkarimi,resolved,Hakim Fessuh,hfessuh1,Skye Jonke,ii69854,ii69854@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Skye Last Name:                 Jonke Email:                     ii69854@umbc.edu Campus ID:                 II69854  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    nkarimi@umbc.edu Existing Group:       nkarimi Project Title:        Synopsys Testing Project Abstract:     Testing Synopsys scripts to adapt them for HPC computing  I need access so I can adapt the synopsys scripts so they will run on the cluster, see RT 3296910  "
3301107,72537715,Correspond,DoIT-Research-Computing,2025-10-28 18:59:24.0000000,HPC User Account: ii69854 in nkarimi,resolved,Hakim Fessuh,hfessuh1,Skye Jonke,ii69854,ii69854@umbc.edu,NULL,naghmeh.karimi@umbc.edu,"Approved.  Regards, Naghmeh  On Tue, Oct 28, 2025 at 2:57=E2=80=AFPM RT API via RT <UMBCHelp@rt.umbc.edu= > wrote:  > This e-mail is a notification that a UMBC user: Skye Jonke < > ii69854@umbc.edu> has requested an account within UMBC's HPC environment > in your group <nkarimi>. As the PI, we request that you acknowledge and > approve this account creation by replying to this message. Alternatively > you can go to this link and review the ticket and indicate your decision > here: > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3301107 > > > Once we have your approval, we will create the account and you and the new > user will receive another e-mail notifying you that the account has been > created. If you have any other questions or concerns please contact us. > > - UMBC DoIT Research Computing Support Staff >   --=20 Naghmeh Karimi, Ph.D. Associate Professor Department of Computer Science and Electrical Engineering University of Maryland, Baltimore County Baltimore, MD 21250 Tel: *410-455-3965* E-mail: *nkarimi@umbc.edu <nkarimi@umbc.edu>* Web: *http://www.csee.umbc.edu/~nkarimi/ <http://www.csee.umbc.edu/~nkarimi/>* "
3301107,72537715,Correspond,DoIT-Research-Computing,2025-10-28 18:59:24.0000000,HPC User Account: ii69854 in nkarimi,resolved,Hakim Fessuh,hfessuh1,Skye Jonke,ii69854,ii69854@umbc.edu,NULL,naghmeh.karimi@umbc.edu,"<div dir=3D""ltr"">Approved.<div><br></div><div>Regards,</div><div>Naghmeh</d= iv></div><br><div class=3D""gmail_quote gmail_quote_container""><div dir=3D""l= tr"" class=3D""gmail_attr"">On Tue, Oct 28, 2025 at 2:57=E2=80=AFPM RT API via=  RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt= ; wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""margin:0px 0px=  0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"">This e-= mail is a notification that a UMBC user: Skye Jonke &lt;<a href=3D""mailto:i= i69854@umbc.edu"" target=3D""_blank"">ii69854@umbc.edu</a>&gt; has requested a= n account within UMBC&#39;s HPC environment in your group &lt;nkarimi&gt;. = As the PI, we request that you acknowledge and approve this account creatio= n by replying to this message. Alternatively you can go to this link and re= view the ticket and indicate your decision here:<br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D330= 1107"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3301107</a> &gt;<br> <br> Once we have your approval, we will create the account and you and the new = user will receive another e-mail notifying you that the account has been cr= eated. If you have any other questions or concerns please contact us.<br> <br> - UMBC DoIT Research Computing Support Staff<br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><span style=3D""font-size:9.5pt;line-height:115%;= font-family:Arial,sans-serif;color:rgb(136,136,136);background-image:initia= l;background-position:initial;background-repeat:initial"">Naghmeh=C2=A0Karim= i, Ph.D.<br> Associate Professor<br> Department of Computer Science and Electrical Engineering<br> University of Maryland, Baltimore County<br> Baltimore, MD 21250<br> Tel:</span><span style=3D""font-size:11pt;line-height:115%;font-family:Calib= ri,&quot;sans-serif&quot;;color:black"">=C2=A0</span><u><span style=3D""font-= size:11pt;line-height:115%;font-family:Calibri,&quot;sans-serif&quot;;color= :rgb(17,85,204)"">410-455-3965</span></u><span style=3D""font-size:11pt;line-= height:115%;font-family:Calibri,&quot;sans-serif&quot;;color:black""> </span= ><span style=3D""font-size:9.5pt;line-height:115%;font-family:Arial,sans-ser= if;color:rgb(136,136,136);background-image:initial;background-position:init= ial;background-repeat:initial"">E-mail:</span><span style=3D""font-size:11pt;= line-height:115%;font-family:Calibri,&quot;sans-serif&quot;;color:black"">= =C2=A0</span><u><span style=3D""font-size:11pt;line-height:115%;font-family:= Calibri,&quot;sans-serif&quot;;color:rgb(17,85,204)""><a href=3D""mailto:nkar= imi@umbc.edu"" target=3D""_blank"">nkarimi@umbc.edu</a></span></u><span style= =3D""font-size:9.5pt;line-height:115%;font-family:Arial,sans-serif;color:rgb= (136,136,136);background-image:initial;background-position:initial;backgrou= nd-repeat:initial""><br> Web:</span><span style=3D""font-size:9.5pt;line-height:115%;font-family:Aria= l,sans-serif;color:black;background-image:initial;background-position:initi= al;background-repeat:initial"">=C2=A0</span><u><span style=3D""font-size:11pt= ;line-height:115%;font-family:Calibri,sans-serif;color:rgb(17,85,204);backg= round-image:initial;background-position:initial;background-repeat:initial"">= <a href=3D""http://www.csee.umbc.edu/~nkarimi/"" target=3D""_blank"">http://www= .csee.umbc.edu/~nkarimi/</a></span></u></div></div> "
3301107,72562517,Correspond,DoIT-Research-Computing,2025-10-29 17:44:30.0000000,HPC User Account: ii69854 in nkarimi,resolved,Hakim Fessuh,hfessuh1,Skye Jonke,ii69854,ii69854@umbc.edu,Hakim Fessuh,hfessuh1@umbc.edu,"<p>Hi Skye,</p>  <p>Your account (ii69854) has been created on chip.rs.umbc.edu.<br /> Your primary group is pi_nkarimi.<br /> Your home directory has 500M of storage.<br /> You can find a short tutorial on how to use chip here: https://umbc.atlassian.net/wiki/spaces/faq/pages/1112506439/Getting+Started+on+chip<br /> Please read through the documentation found at hpcf.umbc.edu &gt; User Support.<br /> All available modules can be viewed using the command &#39;module avail&#39;.<br /> Please submit additional questions or issues as separate tickets via the following link.<br /> (https://doit.umbc.edu/request-tracker-rt/doit-research-computing/)</p>  <p>You can also view your project information here on our wiki (https://hpcf.umbc.edu/libraries/research-projects-hpcf/?preview_id=76&amp;preview_nonce=ee7c2f7bd1&amp;preview=true)<br /> &nbsp;</p>  <p>Best regards,</p>  <p>Hakim Fessuh</p> "
3301187,72540375,Create,DoIT-Research-Computing,2025-10-28 20:05:31.0000000,HPC New Group: pi_kotturi,resolved,Danielle Esposito,desposi1,Yasmine Kotturi,kotturi,kotturi@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Yasmine Last Name:                 Kotturi Email:                     kotturi@umbc.edu Campus ID:                 DV36177  Request Type:              High Performance Cluster  Group Type:=20=20=20=20=20=20=20=20=20=20=20 Project Title:        Towards Designing for Resilience: Community-Centered = Deployment of an AI Business Planning Tool in a Feminist Makerspace Project Abstract:     Entrepreneurs in resource-constrained communities oft= en lack the time and support to translate ideas into actionable business plans. While generative AI promises assistance, most systems assume high digital literacy and overlook community infrastructures that shape adoption. We report on the community-centered design and deployment of BizChat, an LLM-powered tool for business plan development, introduced across four workshops at a feminist busi- ness incubator and makerspace. BizChat was designed to center entrepreneurs=E2=80=99 knowledge and workflows while providing just-= in-time micro-learning and low-floor-high-ceiling accessibility. Through system log data (N=3D30) and semi-structured interviews (N=3D10) with entrepreneurs, we show how the design and deploy- ment of BizChat with existing community contexts lowered bar- riers to accessing capital, encouraged reflection, and empowered entrepreneurs to support AI-literacy within their own communities. We contribute insights into how AI tools can be deployed within local support networks, and implications for design that strengthen community resilience amid rapid technological change.  N/A  "
3301187,72636518,Correspond,DoIT-Research-Computing,2025-10-31 17:12:23.0000000,HPC New Group: pi_kotturi,resolved,Danielle Esposito,desposi1,Yasmine Kotturi,kotturi,kotturi@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Yasmine,</p>  <p>Welcome to chip, UMBC&#39;s High Performance Computing Cluster!</p>  <p>&nbsp; &nbsp; The group, pi_kotturi, now exists on the chip cluster. Members of this group can access and contribute to the research storage space allocated to the group.</p>  <p>&nbsp; &nbsp; This storage space is located at /umbc/rs/pi_kotturi, and currently has a quota of 25T.</p>  <p>&nbsp; &nbsp; For information on accessing the cluster, adding accounts to your group, and getting started using the cluster, check out the tutorial on our wiki: https://umbc.atlassian.net/wiki/x/R4BPQg</p>  <p>&nbsp; &nbsp; Additional documentation is also available here: https://umbc.atlassian.net/wiki/x/FwCHQ</p>  <p>&nbsp; &nbsp; If you have any questions or issues, please submit a new RT ticket at: https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research%20Computing</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Tue Oct 28 16:05:31 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Yasmine Last Name:                 Kotturi Email:                     kotturi@umbc.edu Campus ID:                 DV36177  Request Type:              High Performance Cluster  Group Type:            Project Title:        Towards Designing for Resilience: Community-Centered Deployment of an AI Business Planning Tool in a Feminist Makerspace Project Abstract:     Entrepreneurs in resource-constrained communities often lack the time and support to translate ideas into actionable business plans. While generative AI promises assistance, most systems assume high digital literacy and overlook community infrastructures that shape adoption. We report on the community-centered design and deployment of BizChat, an LLM-powered tool for business plan development, introduced across four workshops at a feminist busi- ness incubator and makerspace. BizChat was designed to center entrepreneurs&rsquo; knowledge and workflows while providing just-in-time micro-learning and low-floor-high-ceiling accessibility. Through system log data (N=30) and semi-structured interviews (N=10) with entrepreneurs, we show how the design and deploy- ment of BizChat with existing community contexts lowered bar- riers to accessing capital, encouraged reflection, and empowered entrepreneurs to support AI-literacy within their own communities. We contribute insights into how AI tools can be deployed within local support networks, and implications for design that strengthen community resilience amid rapid technological change.  N/A  </pre> </blockquote> </div> "
3301564,72549955,Create,DoIT-Research-Computing,2025-10-29 14:00:13.0000000,HPC Other Issue: very slow editing on /home/sergio on c24-52,open,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Sergio Last Name:                 De souza-machad Email:                     sergio@umbc.edu Campus ID:                 VR64161  Request Type:              High Performance Cluster  Takes sooo long for making changes/loading/saving files using emacs ... can you look into it? I am currently editing a file on /umbc/xfs2/strow/asl/s1/sergio/home/git/era5_pipeline  This has been happening for at least last two days  -Sergio  "
3301564,72550350,Correspond,DoIT-Research-Computing,2025-10-29 14:08:05.0000000,HPC Other Issue: very slow editing on /home/sergio on c24-52,open,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"Plus any commands associated with git (status, add, commit etc) are very slow  -Sergio  On Wed, Oct 29, 2025 at 10:00=E2=80=AFAM via RT <UMBCHelp@rt.umbc.edu> wrot= e:  > Greetings, > > This message has been automatically generated in response to the > creation of a ticket regarding: > > ------------------------------------------------------------------------- > Subject: ""HPC Other Issue: very slow editing on /home/sergio on c24-52"" > > Message: > > First Name:                Sergio > Last Name:                 De souza-machad > Email:                     sergio@umbc.edu > Campus ID:                 VR64161 > > Request Type:              High Performance Cluster > > Takes sooo long for making changes/loading/saving files using emacs ... > can you look into it? I am currently editing a file on > /umbc/xfs2/strow/asl/s1/sergio/home/git/era5_pipeline > > This has been happening for at least last two days > > -Sergio > > > ------------------------------------------------------------------------- > > There is no need to reply to this message right now. > > Your ticket has been assigned an ID of [Research Computing #3301564] or > you can go there directly by clicking the link below. > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3301564 > > > You can login to view your open tickets at any time by visiting > http://my.umbc.edu and clicking on ""Help"" and ""Request Help"". > > Alternately you can click on http://my.umbc.edu/help > >                         Thank you > >  --=20 ---------------------------------------------------------------------------= ------------------------------------------ Sergio DeSouza-Machado sergio@umbc.edu Research Assoc. Professor,                                              (W) 410-455-1944 JCET/Dept of Physics (F) 410-455-1072 UMBC, Baltimore MD 21250 "
3301564,72550350,Correspond,DoIT-Research-Computing,2025-10-29 14:08:05.0000000,HPC Other Issue: very slow editing on /home/sergio on c24-52,open,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"<div dir=3D""ltr"">Plus any commands associated with git (status, add, commit=  etc) are very slow<div><br></div><div>-Sergio</div></div><br><div class=3D= ""gmail_quote gmail_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">O= n Wed, Oct 29, 2025 at 10:00=E2=80=AFAM via RT &lt;<a href=3D""mailto:UMBCHe= lp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote cl= ass=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid=  rgb(204,204,204);padding-left:1ex"">Greetings,<br> <br> This message has been automatically generated in response to the<br> creation of a ticket regarding:<br> <br> -------------------------------------------------------------------------<b= r> Subject: &quot;HPC Other Issue: very slow editing on /home/sergio on c24-52= &quot;<br> <br> Message: <br> <br> First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Sergio<b= r> Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0De = souza-machad<br> Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=  =C2=A0<a href=3D""mailto:sergio@umbc.edu"" target=3D""_blank"">sergio@umbc.edu= </a><br> Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0VR6= 4161<br> <br> Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Performa= nce Cluster<br> <br> Takes sooo long for making changes/loading/saving files using emacs ... can=  you look into it? I am currently editing a file on /umbc/xfs2/strow/asl/s1= /sergio/home/git/era5_pipeline<br> <br> This has been happening for at least last two days<br> <br> -Sergio<br> <br> <br> -------------------------------------------------------------------------<b= r> <br> There is no need to reply to this message right now.=C2=A0 <br> <br> Your ticket has been assigned an ID of [Research Computing #3301564] or you=  can go there directly by clicking the link below.<br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D330= 1564"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3301564</a> &gt;<br> <br> You can login to view your open tickets at any time by visiting <a href=3D""= http://my.umbc.edu"" rel=3D""noreferrer"" target=3D""_blank"">http://my.umbc.edu= </a> and clicking on &quot;Help&quot; and &quot;Request Help&quot;. <br> <br> Alternately you can click on <a href=3D""http://my.umbc.edu/help"" rel=3D""nor= eferrer"" target=3D""_blank"">http://my.umbc.edu/help</a><br> <br> =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 Thank you<br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><div><div dir=3D""ltr""><div><div dir=3D""ltr""><div= >--------------------------------------------------------------------------= -------------------------------------------<br>Sergio DeSouza-Machado=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 <a href=3D""mailto:sergio@umbc.edu"" target= =3D""_blank"">sergio@umbc.edu</a><br>Research Assoc. Professor,=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 (W) 410-455-1944<br>JCET/Dept of Physics= =C2=A0=C2=A0 =C2=A0=C2=A0 =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0 (F) 410-455-1072<br></div><div>UMBC, Baltimore MD 21250<br>= </div></div></div></div></div></div></div> "
3301564,72550843,Correspond,DoIT-Research-Computing,2025-10-29 14:17:36.0000000,HPC Other Issue: very slow editing on /home/sergio on c24-52,open,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"Actually now I cannot save anything at all, on any directory  Can you let me know which disk is filled?  -Sergio   On Wed, Oct 29, 2025 at 10:00=E2=80=AFAM via RT <UMBCHelp@rt.umbc.edu> wrot= e:  > Greetings, > > This message has been automatically generated in response to the > creation of a ticket regarding: > > ------------------------------------------------------------------------- > Subject: ""HPC Other Issue: very slow editing on /home/sergio on c24-52"" > > Message: > > First Name:                Sergio > Last Name:                 De souza-machad > Email:                     sergio@umbc.edu > Campus ID:                 VR64161 > > Request Type:              High Performance Cluster > > Takes sooo long for making changes/loading/saving files using emacs ... > can you look into it? I am currently editing a file on > /umbc/xfs2/strow/asl/s1/sergio/home/git/era5_pipeline > > This has been happening for at least last two days > > -Sergio > > > ------------------------------------------------------------------------- > > There is no need to reply to this message right now. > > Your ticket has been assigned an ID of [Research Computing #3301564] or > you can go there directly by clicking the link below. > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3301564 > > > You can login to view your open tickets at any time by visiting > http://my.umbc.edu and clicking on ""Help"" and ""Request Help"". > > Alternately you can click on http://my.umbc.edu/help > >                         Thank you > >  --=20 ---------------------------------------------------------------------------= ------------------------------------------ Sergio DeSouza-Machado sergio@umbc.edu Research Assoc. Professor,                                              (W) 410-455-1944 JCET/Dept of Physics (F) 410-455-1072 UMBC, Baltimore MD 21250 "
3301564,72550843,Correspond,DoIT-Research-Computing,2025-10-29 14:17:36.0000000,HPC Other Issue: very slow editing on /home/sergio on c24-52,open,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"<div dir=3D""ltr"">Actually now I cannot save anything at all, on any directo= ry<div><br></div><div>Can you let me know which disk is filled?</div><div><= br></div><div>-Sergio</div><div><br></div></div><br><div class=3D""gmail_quo= te gmail_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Wed, Oct=  29, 2025 at 10:00=E2=80=AFAM via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc= .edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gma= il_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,2= 04,204);padding-left:1ex"">Greetings,<br> <br> This message has been automatically generated in response to the<br> creation of a ticket regarding:<br> <br> -------------------------------------------------------------------------<b= r> Subject: &quot;HPC Other Issue: very slow editing on /home/sergio on c24-52= &quot;<br> <br> Message: <br> <br> First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Sergio<b= r> Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0De = souza-machad<br> Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=  =C2=A0<a href=3D""mailto:sergio@umbc.edu"" target=3D""_blank"">sergio@umbc.edu= </a><br> Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0VR6= 4161<br> <br> Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Performa= nce Cluster<br> <br> Takes sooo long for making changes/loading/saving files using emacs ... can=  you look into it? I am currently editing a file on /umbc/xfs2/strow/asl/s1= /sergio/home/git/era5_pipeline<br> <br> This has been happening for at least last two days<br> <br> -Sergio<br> <br> <br> -------------------------------------------------------------------------<b= r> <br> There is no need to reply to this message right now.=C2=A0 <br> <br> Your ticket has been assigned an ID of [Research Computing #3301564] or you=  can go there directly by clicking the link below.<br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D330= 1564"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3301564</a> &gt;<br> <br> You can login to view your open tickets at any time by visiting <a href=3D""= http://my.umbc.edu"" rel=3D""noreferrer"" target=3D""_blank"">http://my.umbc.edu= </a> and clicking on &quot;Help&quot; and &quot;Request Help&quot;. <br> <br> Alternately you can click on <a href=3D""http://my.umbc.edu/help"" rel=3D""nor= eferrer"" target=3D""_blank"">http://my.umbc.edu/help</a><br> <br> =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 Thank you<br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><div><div dir=3D""ltr""><div><div dir=3D""ltr""><div= >--------------------------------------------------------------------------= -------------------------------------------<br>Sergio DeSouza-Machado=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 <a href=3D""mailto:sergio@umbc.edu"" target= =3D""_blank"">sergio@umbc.edu</a><br>Research Assoc. Professor,=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 (W) 410-455-1944<br>JCET/Dept of Physics= =C2=A0=C2=A0 =C2=A0=C2=A0 =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0 (F) 410-455-1072<br></div><div>UMBC, Baltimore MD 21250<br>= </div></div></div></div></div></div></div> "
3301564,72550925,Correspond,DoIT-Research-Computing,2025-10-29 14:19:16.0000000,HPC Other Issue: very slow editing on /home/sergio on c24-52,open,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"[sergio@c24-52 merra2_pipeline]$ xnw boo.   -------------------->>>>>> could not save this [sergio@c24-52 merra2_pipeline]$ /bin/pwd /umbc/xfs2/strow/asl/s1/sergio/home/git/merra2_pipeline  /xfs2 shows it is 88% full  [sergio@c24-52 merra2_pipeline]$ df Filesystem                                   1K-blocks         Used Available Use% Mounted on devtmpfs                                          4096            0  4096   0% /dev tmpfs                                        263927132         2280 263924852   1% /run /dev/sda1                                     83845120      9880128  73964992  12% / tmpfs                                        263927132         2864 263924268   1% /dev/shm tmpfs                                             4096            0  4096   0% /sys/fs/cgroup efivarfs                                           304          141 158  48% /sys/firmware/efi/efivars /dev/nvme0n1p1                              1874455300     13102052  1861353248   1% /scratch /dev/sda3                                     12572672       120876  12451796   1% /tmp /dev/sda2                                     12572672      2533484  10039188  21% /var nfs.iss.rs.umbc.edu:/ifs/data/chip/shared    262144000     65342464 196801536  25% /cm/shared nfs.iss.rs.umbc.edu:/ifs/data/chip/eb-sft   1610612736    814780928 795831808  51% /usr/ebuild/software nfs.iss.rs.umbc.edu:/ifs/data/eb-src         524288000    465326080  58961920  89% /usr/ebuild/source 10.2.47.2:/mnt/xfs2/strow                 371512584192 326419324032 45093260160  88% /umbc/xfs2/strow nfs.iss:/ifs/data/chip/home/sergio              512000       228096  283904  45% /home/sergio 10.2.47.3:/mnt/xfs3/strow                 374517966848 303396888064 71121078784  82% /umbc/xfs3/strow   On Wed, Oct 29, 2025 at 10:00=E2=80=AFAM via RT <UMBCHelp@rt.umbc.edu> wrot= e:  > Greetings, > > This message has been automatically generated in response to the > creation of a ticket regarding: > > ------------------------------------------------------------------------- > Subject: ""HPC Other Issue: very slow editing on /home/sergio on c24-52"" > > Message: > > First Name:                Sergio > Last Name:                 De souza-machad > Email:                     sergio@umbc.edu > Campus ID:                 VR64161 > > Request Type:              High Performance Cluster > > Takes sooo long for making changes/loading/saving files using emacs ... > can you look into it? I am currently editing a file on > /umbc/xfs2/strow/asl/s1/sergio/home/git/era5_pipeline > > This has been happening for at least last two days > > -Sergio > > > ------------------------------------------------------------------------- > > There is no need to reply to this message right now. > > Your ticket has been assigned an ID of [Research Computing #3301564] or > you can go there directly by clicking the link below. > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3301564 > > > You can login to view your open tickets at any time by visiting > http://my.umbc.edu and clicking on ""Help"" and ""Request Help"". > > Alternately you can click on http://my.umbc.edu/help > >                         Thank you > >  --=20 ---------------------------------------------------------------------------= ------------------------------------------ Sergio DeSouza-Machado sergio@umbc.edu Research Assoc. Professor,                                              (W) 410-455-1944 JCET/Dept of Physics (F) 410-455-1072 UMBC, Baltimore MD 21250 "
3301564,72550925,Correspond,DoIT-Research-Computing,2025-10-29 14:19:16.0000000,HPC Other Issue: very slow editing on /home/sergio on c24-52,open,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"<div dir=3D""ltr"">[sergio@c24-52 merra2_pipeline]$ xnw boo.=C2=A0 =C2=A0----= ----------------&gt;&gt;&gt;&gt;&gt;&gt; could not save this<br>[sergio@c24= -52 merra2_pipeline]$ /bin/pwd<br>/umbc/xfs2/strow/asl/s1/sergio/home/git/m= erra2_pipeline<br><br>/xfs2 shows it is 88% full<br><br>[sergio@c24-52 merr= a2_pipeline]$ df<br>Filesystem =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 1K-blocks =C2=A0 =C2=A0 =C2=A0 =C2=A0 Used =C2=A0 Available Use% Mounte= d on<br>devtmpfs =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 =C2=A04096 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A00 =C2=A0 =C2= =A0 =C2=A0 =C2=A04096 =C2=A0 0% /dev<br>tmpfs =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0263927132 =C2=A0 =C2=A0 =C2=A0 =C2=A0=  2280 =C2=A0 263924852 =C2=A0 1% /run<br>/dev/sda1 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 =C2=A0 =C2=A0 =C2=A0 83845120 =C2=A0 =C2=A0 =C2=A09880128 =C2=A0 =C2= =A073964992 =C2=A012% /<br>tmpfs =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 =C2=A0 =C2=A0263927132 =C2=A0 =C2=A0 =C2=A0 =C2=A0 2864 =C2=A0 2= 63924268 =C2=A0 1% /dev/shm<br>tmpfs =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 4096 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 =C2=A00 =C2=A0 =C2=A0 =C2=A0 =C2=A04096 =C2=A0 0% /sys/fs/cgroup<br>= efivarfs =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 304 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0141 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 158 =C2=A048% /sys/firmware/efi/efivars<br>/dev/nvme0n1p1 =C2=A0 =C2=A0=  =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 =C2=A01874455300 =C2=A0 =C2=A0 13102052 =C2=A01861353248 =C2=A0 = 1% /scratch<br>/dev/sda3 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 12572672 =C2=A0 =C2=A0 =C2=A0 120876 =C2=A0 =C2=A012451796 =C2=A0 1% /t= mp<br>/dev/sda2 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 12572672=  =C2=A0 =C2=A0 =C2=A02533484 =C2=A0 =C2=A010039188 =C2=A021% /var<br>nfs.is= s.rs.umbc.edu:/ifs/data/chip/shared =C2=A0 =C2=A0262144000 =C2=A0 =C2=A0 65= 342464 =C2=A0 196801536 =C2=A025% /cm/shared<br>nfs.iss.rs.umbc.edu:/ifs/da= ta/chip/eb-sft =C2=A0 1610612736 =C2=A0 =C2=A0814780928 =C2=A0 795831808 = =C2=A051% /usr/ebuild/software<br>nfs.iss.rs.umbc.edu:/ifs/data/eb-src =C2= =A0 =C2=A0 =C2=A0 =C2=A0 524288000 =C2=A0 =C2=A0465326080 =C2=A0 =C2=A05896= 1920 =C2=A089% /usr/ebuild/source<br>10.2.47.2:/mnt/xfs2/strow =C2=A0 =C2= =A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 371512584192 326419324032 450= 93260160 =C2=A088% /umbc/xfs2/strow<br>nfs.iss:/ifs/data/chip/home/sergio = =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0512000 =C2=A0 =C2=A0 =C2=A0=  228096 =C2=A0 =C2=A0 =C2=A0283904 =C2=A045% /home/sergio<br>10.2.47.3:/mnt= /xfs3/strow =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 3745179= 66848 303396888064 71121078784 =C2=A082% /umbc/xfs3/strow<br><br></div><br>= <div class=3D""gmail_quote gmail_quote_container""><div dir=3D""ltr"" class=3D""= gmail_attr"">On Wed, Oct 29, 2025 at 10:00=E2=80=AFAM via RT &lt;<a href=3D""= mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><= blockquote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-l= eft:1px solid rgb(204,204,204);padding-left:1ex"">Greetings,<br>=0D <br>=0D This message has been automatically generated in response to the<br>=0D creation of a ticket regarding:<br>=0D <br>=0D -------------------------------------------------------------------------<b= r>=0D Subject: &quot;HPC Other Issue: very slow editing on /home/sergio on c24-52= &quot;<br>=0D <br>=0D Message: <br>=0D <br>=0D First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Sergio<b= r>=0D Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0De = souza-machad<br>=0D Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=  =C2=A0<a href=3D""mailto:sergio@umbc.edu"" target=3D""_blank"">sergio@umbc.edu= </a><br>=0D Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0VR6= 4161<br>=0D <br>=0D Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Performa= nce Cluster<br>=0D <br>=0D Takes sooo long for making changes/loading/saving files using emacs ... can=  you look into it? I am currently editing a file on /umbc/xfs2/strow/asl/s1= /sergio/home/git/era5_pipeline<br>=0D <br>=0D This has been happening for at least last two days<br>=0D <br>=0D -Sergio<br>=0D <br>=0D <br>=0D -------------------------------------------------------------------------<b= r>=0D <br>=0D There is no need to reply to this message right now.=C2=A0 <br>=0D <br>=0D Your ticket has been assigned an ID of [Research Computing #3301564] or you=  can go there directly by clicking the link below.<br>=0D <br>=0D Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D330= 1564"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3301564</a> &gt;<br>=0D <br>=0D You can login to view your open tickets at any time by visiting <a href=3D""= http://my.umbc.edu"" rel=3D""noreferrer"" target=3D""_blank"">http://my.umbc.edu= </a> and clicking on &quot;Help&quot; and &quot;Request Help&quot;. <br>=0D <br>=0D Alternately you can click on <a href=3D""http://my.umbc.edu/help"" rel=3D""nor= eferrer"" target=3D""_blank"">http://my.umbc.edu/help</a><br>=0D <br>=0D =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 Thank you<br>=0D <br>=0D </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><div><div dir=3D""ltr""><div><div dir=3D""ltr""><div= >--------------------------------------------------------------------------= -------------------------------------------<br>Sergio DeSouza-Machado=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 <a href=3D""mailto:sergio@umbc.edu"" target= =3D""_blank"">sergio@umbc.edu</a><br>Research Assoc. Professor,=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 (W) 410-455-1944<br>JCET/Dept of Physics= =C2=A0=C2=A0 =C2=A0=C2=A0 =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0 (F) 410-455-1072<br></div><div>UMBC, Baltimore MD 21250<br>= </div></div></div></div></div></div></div>=0D "
3301564,72562122,Correspond,DoIT-Research-Computing,2025-10-29 17:33:04.0000000,HPC Other Issue: very slow editing on /home/sergio on c24-52,open,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Tartela Tabassum,tartelt1@umbc.edu,"<div> <p><tt>Hi Sergio,</tt></p>  <p><tt>Thanks for reaching out. Can you please clarify a few points so we c= an better diagnose the issue?</tt></p>  <p><tt>&bull; Are the slowdowns happening only inside <strong>Emacs</strong= >, or do you also see delays when running <strong>other commands</strong> (= e.g., <code>ls</code>, <code>cat</code>, or editing with <code>vi</code> or=  <code>nano</code>)?<br /> &bull; Are you currently working in an <strong>interactive session</strong>=  or through a <strong>regular SSH login</strong>?<br /> &bull; When you say you can&rsquo;t save files, do you get a specific error=  message in Emacs or in the terminal</tt>?</p>  <p>Could you please check whether you have any large or temporary files tha= t could be cleaned up or moved to<code>/scratch</code> instead? You can use=  the command below to list directory sizes:</p>  <p><strong><code>du -h --max-depth=3D1 /umbc/xfs2/strow/asl/s1/sergio/home/= git | sort -h</code></strong></p>  <p>This will help us see if large files in your directories are contributin= g to the issue.</p>  <p>Once you confirm the points above, we&rsquo;ll determine if it&rsquo;s a=  general filesystem performance issue or specific to your environment.</p>  <p>Best,<br /> UMBC HPCF Support</p>  <p>On Wed Oct 29 10:08:05 2025, VR64161 wrote:</p>  <blockquote> <div>Plus any commands associated with git (status, add, commit etc) are ve= ry slow <div>&nbsp;</div>  <div>-Sergio</div> </div> &nbsp;  <div> <div>On Wed, Oct 29, 2025 at 10:00=E2=80=AFAM via RT &lt;UMBCHelp@rt.umbc.e= du&gt; wrote:</div>  <blockquote>Greetings,<br /> <br /> This message has been automatically generated in response to the<br /> creation of a ticket regarding:<br /> <br /> -------------------------------------------------------------------------<b= r /> Subject: &quot;HPC Other Issue: very slow editing on /home/sergio on c24-52= &quot;<br /> <br /> Message:<br /> <br /> First Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Sergio<b= r /> Last Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;De = souza-machad<br /> Email:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;=  &nbsp;sergio@umbc.edu<br /> Campus ID:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;VR6= 4161<br /> <br /> Request Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; High Performa= nce Cluster<br /> <br /> Takes sooo long for making changes/loading/saving files using emacs ... can=  you look into it? I am currently editing a file on /umbc/xfs2/strow/asl/s1= /sergio/home/git/era5_pipeline<br /> <br /> This has been happening for at least last two days<br /> <br /> -Sergio<br /> <br /> <br /> -------------------------------------------------------------------------<b= r /> <br /> There is no need to reply to this message right now.&nbsp;<br /> <br /> Your ticket has been assigned an ID of [Research Computing #3301564] or you=  can go there directly by clicking the link below.<br /> <br /> Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3301564 &gt;<b= r /> <br /> You can login to view your open tickets at any time by visiting http://my.u= mbc.edu and clicking on &quot;Help&quot; and &quot;Request Help&quot;.<br /> <br /> Alternately you can click on http://my.umbc.edu/help<br /> <br /> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp= ; &nbsp; Thank you<br /> &nbsp;</blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --  <div> <div> <div> <div> <div> <div> <div>----------------------------------------------------------------------= -----------------------------------------------<br /> Sergio DeSouza-Machado&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sergio@umbc.e= du<br /> Research Assoc. Professor,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&= nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbs= p;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&= nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (W)=  410-455-1944<br /> JCET/Dept of Physics&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (F) 410-455-1072</div>  <div>UMBC, Baltimore MD 21250</div> </div> </div> </div> </div> </div> </div> </blockquote> </div> "
3301564,72562957,Correspond,DoIT-Research-Computing,2025-10-29 17:57:53.0000000,HPC Other Issue: very slow editing on /home/sergio on c24-52,open,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"<div dir=3D""ltr"">I laccess our machine using<br>[sergio@chip-login2 nesdis]= $ alias xschip<br>alias xschip=3D&#39;srun =C2=A0 =C2=A0--cluster=3Dchip-cp= u --account=3Dpi_strow --partition=3Dpi_strow --qos=3Dpi_strow --time=3D30-= 00:00:00 --x11 --mem=3D10G --pty $SHELL&#39;<br><br>But the errors happen w= hether I am on chip or on our machine, as soon as i move away from /home/se= rgio<div>ie if I am at /home/sergio, I can edit and save /home/sergio/bo<br= ><div><br>So eg after I get into /home/sergio/git I start getting=C2=A0inpu= t/output errors<br>[sergio@c24-52 git]$ pwd<br>/home/sergio/git<br>[sergio@= c24-52 git]$ /bin/pwd<br>/umbc/xfs2/strow/asl/s1/sergio/home/git<br><div><b= r></div><div>Here I am in /home/sergio/git/merra2_pipeline</div>It start up=  emacs, it takes a second to even wake up, then I type=C2=A0garbage and try=  to save it; here is the screenshot<br><br><img src=3D""cid:ii_mhcako8s0"" al= t=3D""Screenshot 2025-10-29 at 1.49.14=E2=80=AFPM.jpg"" width=3D""550"" height= =3D""348""><div><br></div><div>Your suggestion=C2=A0gives this (I hit Ctrl C = to stop it)<br><br>[sergio@c24-52 merra2_pipeline]$ du -h --max-depth=3D1 /= umbc/xfs2/strow/asl/s1/sergio/home/git | sort -h<br>du: cannot access &#39;= /umbc/xfs2/strow/asl/s1/sergio/home/git/rtp/.git/objects/09&#39;: Input/out= put error<br>du: cannot access &#39;/umbc/xfs2/strow/asl/s1/sergio/home/git= /rtp/.git/objects/e6&#39;: Input/output error<br>du: cannot access &#39;/um= bc/xfs2/strow/asl/s1/sergio/home/git/rtp/.git/objects/f4&#39;: Input/output=  error<br>du: cannot access &#39;/umbc/xfs2/strow/asl/s1/sergio/home/git/rt= p/.git/objects/47&#39;: Input/output error<br>du: cannot access &#39;/umbc/= xfs2/strow/asl/s1/sergio/home/git/rtp/.git/objects/52&#39;: Input/output er= ror<br><br>Hope this helps, thanks<br><br></div></div><div>Sergio</div></di= v></div><br><div class=3D""gmail_quote gmail_quote_container""><div dir=3D""lt= r"" class=3D""gmail_attr"">On Wed, Oct 29, 2025 at 1:33=E2=80=AFPM Tartela Tab= assum via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.e= du</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""margi= n:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex= "">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D3= 301564"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Dis= play.html?id=3D3301564</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Sergio,<br> <br> Thanks for reaching out. Can you please clarify a few points so we can bett= er<br> diagnose the issue?<br> <br> =E2=80=A2 Are the slowdowns happening only inside Emacs, or do you also see=  delays when<br> running other commands (e.g., ls, cat, or editing with vi or nano)?<br> =E2=80=A2 Are you currently working in an interactive session or through a = regular SSH<br> login?<br> =E2=80=A2 When you say you can=E2=80=99t save files, do you get a specific = error message in<br> Emacs or in the terminal?<br> <br> Could you please check whether you have any large or temporary files that c= ould<br> be cleaned up or moved to/scratch instead? You can use the command below to= <br> list directory sizes:<br> <br> du -h --max-depth=3D1 /umbc/xfs2/strow/asl/s1/sergio/home/git | sort -h<br> <br> This will help us see if large files in your directories are contributing t= o<br> the issue.<br> <br> Once you confirm the points above, we=E2=80=99ll determine if it=E2=80=99s = a general filesystem<br> performance issue or specific to your environment.<br> <br> Best,<br> UMBC HPCF Support<br> <br> On Wed Oct 29 10:08:05 2025, VR64161 wrote:<br> <br> &gt; Plus any commands associated with git (status, add, commit etc) are ve= ry<br> &gt; slow -Sergio On Wed, Oct 29, 2025 at 10:00 AM via RT &lt;<a href=3D""ma= ilto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt;<b= r> &gt; wrote:<br> <br> &gt;&gt; Greetings,<br> <br> &gt;&gt; This message has been automatically generated in response to the<b= r> &gt;&gt; creation of a ticket regarding:<br> <br> &gt;&gt; ------------------------------------------------------------------= -------<br> &gt;&gt; Subject: &quot;HPC Other Issue: very slow editing on /home/sergio = on c24-52&quot;<br> <br> &gt;&gt; Message:<br> <br> &gt;&gt; First Name: Sergio<br> &gt;&gt; Last Name: De souza-machad<br> &gt;&gt; Email: <a href=3D""mailto:sergio@umbc.edu"" target=3D""_blank"">sergio= @umbc.edu</a><br> &gt;&gt; Campus ID: VR64161<br> <br> &gt;&gt; Request Type: High Performance Cluster<br> <br> &gt;&gt; Takes sooo long for making changes/loading/saving files using emac= s ...<br> &gt;&gt; can you look into it? I am currently editing a file on<br> &gt;&gt; /umbc/xfs2/strow/asl/s1/sergio/home/git/era5_pipeline<br> <br> &gt;&gt; This has been happening for at least last two days<br> <br> &gt;&gt; -Sergio<br> <br> <br> &gt;&gt; ------------------------------------------------------------------= -------<br> <br> &gt;&gt; There is no need to reply to this message right now.<br> <br> &gt;&gt; Your ticket has been assigned an ID of [Research Computing #330156= 4] or<br> &gt;&gt; you can go there directly by clicking the link below.<br> <br> &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html= ?id=3D3301564"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tic= ket/Display.html?id=3D3301564</a> &gt;<br> <br> &gt;&gt; You can login to view your open tickets at any time by visiting<br> &gt;&gt; <a href=3D""http://my.umbc.edu"" rel=3D""noreferrer"" target=3D""_blank= "">http://my.umbc.edu</a> and clicking on &quot;Help&quot; and &quot;Request=  Help&quot;.<br> <br> &gt;&gt; Alternately you can click on <a href=3D""http://my.umbc.edu/help"" r= el=3D""noreferrer"" target=3D""_blank"">http://my.umbc.edu/help</a><br> <br> &gt;&gt; Thank you<br> <br> &gt; --<br> &gt; ----------------------------------------------------------------------= -----------------------------------------------<br> &gt; Sergio DeSouza-Machado <a href=3D""mailto:sergio@umbc.edu"" target=3D""_b= lank"">sergio@umbc.edu</a><br> &gt; Research Assoc. Professor, (W) 410-455-1944<br> &gt; JCET/Dept of Physics (F) 410-455-1072UMBC, Baltimore MD 21250<br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><div><div dir=3D""ltr""><div><div dir=3D""ltr""><div= >--------------------------------------------------------------------------= -------------------------------------------<br>Sergio DeSouza-Machado=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 <a href=3D""mailto:sergio@umbc.edu"" target= =3D""_blank"">sergio@umbc.edu</a><br>Research Assoc. Professor,=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 (W) 410-455-1944<br>JCET/Dept of Physics= =C2=A0=C2=A0 =C2=A0=C2=A0 =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0 (F) 410-455-1072<br></div><div>UMBC, Baltimore MD 21250<br>= </div></div></div></div></div></div></div> "
3301564,72562957,Correspond,DoIT-Research-Computing,2025-10-29 17:57:53.0000000,HPC Other Issue: very slow editing on /home/sergio on c24-52,open,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"I laccess our machine using [sergio@chip-login2 nesdis]$ alias xschip alias xschip=3D'srun    --cluster=3Dchip-cpu --account=3Dpi_strow --partition=3Dpi_strow --qos=3Dpi_strow --time=3D30-00:00:00 --x11 --mem=3D= 10G --pty $SHELL'  But the errors happen whether I am on chip or on our machine, as soon as i move away from /home/sergio ie if I am at /home/sergio, I can edit and save /home/sergio/bo  So eg after I get into /home/sergio/git I start getting input/output errors [sergio@c24-52 git]$ pwd /home/sergio/git [sergio@c24-52 git]$ /bin/pwd /umbc/xfs2/strow/asl/s1/sergio/home/git  Here I am in /home/sergio/git/merra2_pipeline It start up emacs, it takes a second to even wake up, then I type garbage and try to save it; here is the screenshot  [image: Screenshot 2025-10-29 at 1.49.14=E2=80=AFPM.jpg]  Your suggestion gives this (I hit Ctrl C to stop it)  [sergio@c24-52 merra2_pipeline]$ du -h --max-depth=3D1 /umbc/xfs2/strow/asl/s1/sergio/home/git | sort -h du: cannot access '/umbc/xfs2/strow/asl/s1/sergio/home/git/rtp/.git/objects/09': Input/output error du: cannot access '/umbc/xfs2/strow/asl/s1/sergio/home/git/rtp/.git/objects/e6': Input/output error du: cannot access '/umbc/xfs2/strow/asl/s1/sergio/home/git/rtp/.git/objects/f4': Input/output error du: cannot access '/umbc/xfs2/strow/asl/s1/sergio/home/git/rtp/.git/objects/47': Input/output error du: cannot access '/umbc/xfs2/strow/asl/s1/sergio/home/git/rtp/.git/objects/52': Input/output error  Hope this helps, thanks  Sergio  On Wed, Oct 29, 2025 at 1:33=E2=80=AFPM Tartela Tabassum via RT < UMBCHelp@rt.umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3301564 > > > Last Update From Ticket: > > Hi Sergio, > > Thanks for reaching out. Can you please clarify a few points so we can > better > diagnose the issue? > > =E2=80=A2 Are the slowdowns happening only inside Emacs, or do you also s= ee delays > when > running other commands (e.g., ls, cat, or editing with vi or nano)? > =E2=80=A2 Are you currently working in an interactive session or through = a regular > SSH > login? > =E2=80=A2 When you say you can=E2=80=99t save files, do you get a specifi= c error message in > Emacs or in the terminal? > > Could you please check whether you have any large or temporary files that > could > be cleaned up or moved to/scratch instead? You can use the command below = to > list directory sizes: > > du -h --max-depth=3D1 /umbc/xfs2/strow/asl/s1/sergio/home/git | sort -h > > This will help us see if large files in your directories are contributing > to > the issue. > > Once you confirm the points above, we=E2=80=99ll determine if it=E2=80=99= s a general > filesystem > performance issue or specific to your environment. > > Best, > UMBC HPCF Support > > On Wed Oct 29 10:08:05 2025, VR64161 wrote: > > > Plus any commands associated with git (status, add, commit etc) are very > > slow -Sergio On Wed, Oct 29, 2025 at 10:00 AM via RT < > UMBCHelp@rt.umbc.edu> > > wrote: > > >> Greetings, > > >> This message has been automatically generated in response to the > >> creation of a ticket regarding: > > >> > ------------------------------------------------------------------------- > >> Subject: ""HPC Other Issue: very slow editing on /home/sergio on c24-52"" > > >> Message: > > >> First Name: Sergio > >> Last Name: De souza-machad > >> Email: sergio@umbc.edu > >> Campus ID: VR64161 > > >> Request Type: High Performance Cluster > > >> Takes sooo long for making changes/loading/saving files using emacs ... > >> can you look into it? I am currently editing a file on > >> /umbc/xfs2/strow/asl/s1/sergio/home/git/era5_pipeline > > >> This has been happening for at least last two days > > >> -Sergio > > > >> > ------------------------------------------------------------------------- > > >> There is no need to reply to this message right now. > > >> Your ticket has been assigned an ID of [Research Computing #3301564] or > >> you can go there directly by clicking the link below. > > >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3301564 > > > >> You can login to view your open tickets at any time by visiting > >> http://my.umbc.edu and clicking on ""Help"" and ""Request Help"". > > >> Alternately you can click on http://my.umbc.edu/help > > >> Thank you > > > -- > > > -------------------------------------------------------------------------= -------------------------------------------- > > Sergio DeSouza-Machado sergio@umbc.edu > > Research Assoc. Professor, (W) 410-455-1944 > > JCET/Dept of Physics (F) 410-455-1072UMBC, Baltimore MD 21250 > >  --=20 ---------------------------------------------------------------------------= ------------------------------------------ Sergio DeSouza-Machado sergio@umbc.edu Research Assoc. Professor,                                              (W) 410-455-1944 JCET/Dept of Physics (F) 410-455-1072 UMBC, Baltimore MD 21250 "
3301564,72565453,Correspond,DoIT-Research-Computing,2025-10-29 18:53:16.0000000,HPC Other Issue: very slow editing on /home/sergio on c24-52,open,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"Again, from /home/sergio/git,  [sergio@chip-login2 git]$ pwd /home/sergio/git [sergio@chip-login2 git]$ /bin/pwd /umbc/xfs2/strow/asl/s1/sergio/home/git  a cut and paste of some of the output from ""du""  shows errors at random spots  du: cannot access './MATFILES_for_JGR_trends_paper/.git/objects/2c': Input/output error du: cannot access './MATFILES_for_JGR_trends_paper/.git/objects/6b': Input/output error du: cannot access './MATFILES_for_JGR_trends_paper/.git/objects/59': Input/output error 4 ./MATFILES_for_JGR_trends_paper/.git/objects/8e 4 ./MATFILES_for_JGR_trends_paper/.git/objects/be 4 ./MATFILES_for_JGR_trends_paper/.git/objects/34 du: cannot read directory './MATFILES_for_JGR_trends_paper/.git/objects/86': Input/output error 4 ./MATFILES_for_JGR_trends_paper/.git/objects/86 du: cannot access './MATFILES_for_JGR_trends_paper/.git/objects/22': Input/output error du: cannot access './MATFILES_for_JGR_trends_paper/.git/objects/98': Input/output error 8 ./MATFILES_for_JGR_trends_paper/.git/objects/f5 du: cannot access './MATFILES_for_JGR_trends_paper/.git/objects/54': Input/output error du: cannot read directory './MATFILES_for_JGR_trends_paper/.git/objects/e8': Input/output error 4 ./MATFILES_for_JGR_trends_paper/.git/objects/e8 du: cannot access './MATFILES_for_JGR_trends_paper/.git/objects/c6': Input/output error du: cannot access './MATFILES_for_JGR_trends_paper/.git/objects/53': Input/output error 8 ./MATFILES_for_JGR_trends_paper/.git/objects/65 du: cannot access './MATFILES_for_JGR_trends_paper/.git/objects/20': Input/output error du: cannot read directory './MATFILES_for_JGR_trends_paper/.git/objects/e0': Input/output error 4 ./MATFILES_for_JGR_trends_paper/.git/objects/e0 4 ./MATFILES_for_JGR_trends_paper/.git/objects/2b 180 ./MATFILES_for_JGR_trends_paper/.git/objects/b3  -Sergio    On Wed, Oct 29, 2025 at 1:33=E2=80=AFPM Tartela Tabassum via RT < UMBCHelp@rt.umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3301564 > > > Last Update From Ticket: > > Hi Sergio, > > Thanks for reaching out. Can you please clarify a few points so we can > better > diagnose the issue? > > =E2=80=A2 Are the slowdowns happening only inside Emacs, or do you also s= ee delays > when > running other commands (e.g., ls, cat, or editing with vi or nano)? > =E2=80=A2 Are you currently working in an interactive session or through = a regular > SSH > login? > =E2=80=A2 When you say you can=E2=80=99t save files, do you get a specifi= c error message in > Emacs or in the terminal? > > Could you please check whether you have any large or temporary files that > could > be cleaned up or moved to/scratch instead? You can use the command below = to > list directory sizes: > > du -h --max-depth=3D1 /umbc/xfs2/strow/asl/s1/sergio/home/git | sort -h > > This will help us see if large files in your directories are contributing > to > the issue. > > Once you confirm the points above, we=E2=80=99ll determine if it=E2=80=99= s a general > filesystem > performance issue or specific to your environment. > > Best, > UMBC HPCF Support > > On Wed Oct 29 10:08:05 2025, VR64161 wrote: > > > Plus any commands associated with git (status, add, commit etc) are very > > slow -Sergio On Wed, Oct 29, 2025 at 10:00 AM via RT < > UMBCHelp@rt.umbc.edu> > > wrote: > > >> Greetings, > > >> This message has been automatically generated in response to the > >> creation of a ticket regarding: > > >> > ------------------------------------------------------------------------- > >> Subject: ""HPC Other Issue: very slow editing on /home/sergio on c24-52"" > > >> Message: > > >> First Name: Sergio > >> Last Name: De souza-machad > >> Email: sergio@umbc.edu > >> Campus ID: VR64161 > > >> Request Type: High Performance Cluster > > >> Takes sooo long for making changes/loading/saving files using emacs ... > >> can you look into it? I am currently editing a file on > >> /umbc/xfs2/strow/asl/s1/sergio/home/git/era5_pipeline > > >> This has been happening for at least last two days > > >> -Sergio > > > >> > ------------------------------------------------------------------------- > > >> There is no need to reply to this message right now. > > >> Your ticket has been assigned an ID of [Research Computing #3301564] or > >> you can go there directly by clicking the link below. > > >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3301564 > > > >> You can login to view your open tickets at any time by visiting > >> http://my.umbc.edu and clicking on ""Help"" and ""Request Help"". > > >> Alternately you can click on http://my.umbc.edu/help > > >> Thank you > > > -- > > > -------------------------------------------------------------------------= -------------------------------------------- > > Sergio DeSouza-Machado sergio@umbc.edu > > Research Assoc. Professor, (W) 410-455-1944 > > JCET/Dept of Physics (F) 410-455-1072UMBC, Baltimore MD 21250 > >  --=20 ---------------------------------------------------------------------------= ------------------------------------------ Sergio DeSouza-Machado sergio@umbc.edu Research Assoc. Professor,                                              (W) 410-455-1944 JCET/Dept of Physics (F) 410-455-1072 UMBC, Baltimore MD 21250 "
3301564,72565453,Correspond,DoIT-Research-Computing,2025-10-29 18:53:16.0000000,HPC Other Issue: very slow editing on /home/sergio on c24-52,open,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"<div dir=3D""ltr"">Again, from /home/sergio/git,=C2=A0<br><br>[sergio@chip-lo= gin2 git]$ pwd<br>/home/sergio/git<br>[sergio@chip-login2 git]$ /bin/pwd<br= >/umbc/xfs2/strow/asl/s1/sergio/home/git<br><br>a cut and paste of some of = the output from &quot;du&quot;<br><br>shows errors at random spots<br><br>d= u: cannot access &#39;./MATFILES_for_JGR_trends_paper/.git/objects/2c&#39;:=  Input/output error<br>du: cannot access &#39;./MATFILES_for_JGR_trends_pap= er/.git/objects/6b&#39;: Input/output error<br>du: cannot access &#39;./MAT= FILES_for_JGR_trends_paper/.git/objects/59&#39;: Input/output error<br>4	./= MATFILES_for_JGR_trends_paper/.git/objects/8e<br>4	./MATFILES_for_JGR_trend= s_paper/.git/objects/be<br>4	./MATFILES_for_JGR_trends_paper/.git/objects/3= 4<br>du: cannot read directory &#39;./MATFILES_for_JGR_trends_paper/.git/ob= jects/86&#39;: Input/output error<br>4	./MATFILES_for_JGR_trends_paper/.git= /objects/86<br>du: cannot access &#39;./MATFILES_for_JGR_trends_paper/.git/= objects/22&#39;: Input/output error<br>du: cannot access &#39;./MATFILES_fo= r_JGR_trends_paper/.git/objects/98&#39;: Input/output error<br>8	./MATFILES= _for_JGR_trends_paper/.git/objects/f5<br>du: cannot access &#39;./MATFILES_= for_JGR_trends_paper/.git/objects/54&#39;: Input/output error<br>du: cannot=  read directory &#39;./MATFILES_for_JGR_trends_paper/.git/objects/e8&#39;: = Input/output error<br>4	./MATFILES_for_JGR_trends_paper/.git/objects/e8<br>= du: cannot access &#39;./MATFILES_for_JGR_trends_paper/.git/objects/c6&#39;= : Input/output error<br>du: cannot access &#39;./MATFILES_for_JGR_trends_pa= per/.git/objects/53&#39;: Input/output error<br>8	./MATFILES_for_JGR_trends= _paper/.git/objects/65<br>du: cannot access &#39;./MATFILES_for_JGR_trends_= paper/.git/objects/20&#39;: Input/output error<br>du: cannot read directory=  &#39;./MATFILES_for_JGR_trends_paper/.git/objects/e0&#39;: Input/output er= ror<br>4	./MATFILES_for_JGR_trends_paper/.git/objects/e0<br>4	./MATFILES_fo= r_JGR_trends_paper/.git/objects/2b<br>180	./MATFILES_for_JGR_trends_paper/.= git/objects/b3<br><br>-Sergio<div><br><br></div></div><br><div class=3D""gma= il_quote gmail_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On We= d, Oct 29, 2025 at 1:33=E2=80=AFPM Tartela Tabassum via RT &lt;<a href=3D""m= ailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><b= lockquote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-le= ft:1px solid rgb(204,204,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""= https://rt.umbc.edu/Ticket/Display.html?id=3D3301564"" rel=3D""noreferrer"" ta= rget=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3301564</a> &g= t;<br> <br> Last Update From Ticket:<br> <br> Hi Sergio,<br> <br> Thanks for reaching out. Can you please clarify a few points so we can bett= er<br> diagnose the issue?<br> <br> =E2=80=A2 Are the slowdowns happening only inside Emacs, or do you also see=  delays when<br> running other commands (e.g., ls, cat, or editing with vi or nano)?<br> =E2=80=A2 Are you currently working in an interactive session or through a = regular SSH<br> login?<br> =E2=80=A2 When you say you can=E2=80=99t save files, do you get a specific = error message in<br> Emacs or in the terminal?<br> <br> Could you please check whether you have any large or temporary files that c= ould<br> be cleaned up or moved to/scratch instead? You can use the command below to= <br> list directory sizes:<br> <br> du -h --max-depth=3D1 /umbc/xfs2/strow/asl/s1/sergio/home/git | sort -h<br> <br> This will help us see if large files in your directories are contributing t= o<br> the issue.<br> <br> Once you confirm the points above, we=E2=80=99ll determine if it=E2=80=99s = a general filesystem<br> performance issue or specific to your environment.<br> <br> Best,<br> UMBC HPCF Support<br> <br> On Wed Oct 29 10:08:05 2025, VR64161 wrote:<br> <br> &gt; Plus any commands associated with git (status, add, commit etc) are ve= ry<br> &gt; slow -Sergio On Wed, Oct 29, 2025 at 10:00 AM via RT &lt;<a href=3D""ma= ilto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt;<b= r> &gt; wrote:<br> <br> &gt;&gt; Greetings,<br> <br> &gt;&gt; This message has been automatically generated in response to the<b= r> &gt;&gt; creation of a ticket regarding:<br> <br> &gt;&gt; ------------------------------------------------------------------= -------<br> &gt;&gt; Subject: &quot;HPC Other Issue: very slow editing on /home/sergio = on c24-52&quot;<br> <br> &gt;&gt; Message:<br> <br> &gt;&gt; First Name: Sergio<br> &gt;&gt; Last Name: De souza-machad<br> &gt;&gt; Email: <a href=3D""mailto:sergio@umbc.edu"" target=3D""_blank"">sergio= @umbc.edu</a><br> &gt;&gt; Campus ID: VR64161<br> <br> &gt;&gt; Request Type: High Performance Cluster<br> <br> &gt;&gt; Takes sooo long for making changes/loading/saving files using emac= s ...<br> &gt;&gt; can you look into it? I am currently editing a file on<br> &gt;&gt; /umbc/xfs2/strow/asl/s1/sergio/home/git/era5_pipeline<br> <br> &gt;&gt; This has been happening for at least last two days<br> <br> &gt;&gt; -Sergio<br> <br> <br> &gt;&gt; ------------------------------------------------------------------= -------<br> <br> &gt;&gt; There is no need to reply to this message right now.<br> <br> &gt;&gt; Your ticket has been assigned an ID of [Research Computing #330156= 4] or<br> &gt;&gt; you can go there directly by clicking the link below.<br> <br> &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html= ?id=3D3301564"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tic= ket/Display.html?id=3D3301564</a> &gt;<br> <br> &gt;&gt; You can login to view your open tickets at any time by visiting<br> &gt;&gt; <a href=3D""http://my.umbc.edu"" rel=3D""noreferrer"" target=3D""_blank= "">http://my.umbc.edu</a> and clicking on &quot;Help&quot; and &quot;Request=  Help&quot;.<br> <br> &gt;&gt; Alternately you can click on <a href=3D""http://my.umbc.edu/help"" r= el=3D""noreferrer"" target=3D""_blank"">http://my.umbc.edu/help</a><br> <br> &gt;&gt; Thank you<br> <br> &gt; --<br> &gt; ----------------------------------------------------------------------= -----------------------------------------------<br> &gt; Sergio DeSouza-Machado <a href=3D""mailto:sergio@umbc.edu"" target=3D""_b= lank"">sergio@umbc.edu</a><br> &gt; Research Assoc. Professor, (W) 410-455-1944<br> &gt; JCET/Dept of Physics (F) 410-455-1072UMBC, Baltimore MD 21250<br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><div><div dir=3D""ltr""><div><div dir=3D""ltr""><div= >--------------------------------------------------------------------------= -------------------------------------------<br>Sergio DeSouza-Machado=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 <a href=3D""mailto:sergio@umbc.edu"" target= =3D""_blank"">sergio@umbc.edu</a><br>Research Assoc. Professor,=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 (W) 410-455-1944<br>JCET/Dept of Physics= =C2=A0=C2=A0 =C2=A0=C2=A0 =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0 (F) 410-455-1072<br></div><div>UMBC, Baltimore MD 21250<br>= </div></div></div></div></div></div></div> "
3301564,72568252,Correspond,DoIT-Research-Computing,2025-10-29 20:03:46.0000000,HPC Other Issue: very slow editing on /home/sergio on c24-52,open,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"BTW this is the tail end of the command you asked to run ... I'm not sure if it followed symbolic links to (large) data files, as I doubt this is source code only  744G /umbc/xfs2/strow/asl/s1/sergio/home/git/MATLABCODE_Git  Thanks  Sergio  >>> du: cannot access '/umbc/xfs2/strow/asl/s1/sergio/home/git/ftc_dev/NeuralNet/fit_nonLTE': Input/output error du: cannot access '/umbc/xfs2/strow/asl/s1/sergio/home/git/ftc_dev/Refs': Input/output error du: cannot access '/umbc/xfs2/strow/asl/s1/sergio/home/git/ftc_dev/fit_ch4': Input/output error du: cannot access '/umbc/xfs2/strow/asl/s1/sergio/home/git/ftc_dev/run': Input/output error du: cannot read directory '/umbc/xfs2/strow/asl/s1/sergio/home/git/sarta': Input/output error du: cannot access '/umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_JGR_trends_paper/.git= /objects/c0': Input/output error du: cannot access '/umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_JGR_trends_paper/.git= /objects/b1': Input/output error du: cannot read directory '/umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_JGR_trends_paper/.git= /objects/16': Input/output error du: cannot access '/umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_JGR_trends_paper/.git= /objects/2f': Input/output error du: cannot read directory '/umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_JGR_trends_paper/.git= /objects/55': Input/output error du: cannot access '/umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_JGR_trends_paper/.git= /objects/5a': Input/output error du: cannot access '/umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_JGR_trends_paper/.git= /objects/e6': Input/output error du: cannot access '/umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_JGR_trends_paper/.git= /objects/2c': Input/output error du: cannot access '/umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_JGR_trends_paper/.git= /objects/6b': Input/output error du: cannot access '/umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_JGR_trends_paper/.git= /objects/59': Input/output error du: cannot read directory '/umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_JGR_trends_paper/.git= /objects/86': Input/output error du: cannot access '/umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_JGR_trends_paper/.git= /objects/22': Input/output error du: cannot access '/umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_JGR_trends_paper/.git= /objects/98': Input/output error du: cannot access '/umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_JGR_trends_paper/.git= /objects/54': Input/output error du: cannot read directory '/umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_JGR_trends_paper/.git= /objects/e8': Input/output error du: cannot access '/umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_JGR_trends_paper/.git= /objects/c6': Input/output error du: cannot access '/umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_JGR_trends_paper/.git= /objects/53': Input/output error du: cannot access '/umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_JGR_trends_paper/.git= /objects/20': Input/output error du: cannot read directory '/umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_JGR_trends_paper/.git= /objects/e0': Input/output error 4.0K /umbc/xfs2/strow/asl/s1/sergio/home/git/oem_pkg 4.0K /umbc/xfs2/strow/asl/s1/sergio/home/git/oem_pkg_run 4.0K /umbc/xfs2/strow/asl/s1/sergio/home/git/Readme_RTP_PIPELINE 4.0K /umbc/xfs2/strow/asl/s1/sergio/home/git/RTPMAKE 4.0K /umbc/xfs2/strow/asl/s1/sergio/home/git/sarta 20K /umbc/xfs2/strow/asl/s1/sergio/home/git/cfg_for_rtp_prod2 136K /umbc/xfs2/strow/asl/s1/sergio/home/git/merra2_pipeline 2.6M /umbc/xfs2/strow/asl/s1/sergio/home/git/era5_pipeline 4.4M /umbc/xfs2/strow/asl/s1/sergio/home/git/rtp_prod2 16M /umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_JGR_trends_paper 18M /umbc/xfs2/strow/asl/s1/sergio/home/git/clear_anomaly_retrievals 25M /umbc/xfs2/strow/asl/s1/sergio/home/git/get_ecmwf 54M /umbc/xfs2/strow/asl/s1/sergio/home/git/rtp 80M /umbc/xfs2/strow/asl/s1/sergio/home/git/l1c 81M /umbc/xfs2/strow/asl/s1/sergio/home/git/kcarta 102M /umbc/xfs2/strow/asl/s1/sergio/home/git/matlib 107M /umbc/xfs2/strow/asl/s1/sergio/home/git/sarta_utilities 183M /umbc/xfs2/strow/asl/s1/sergio/home/git/ftc_dev 254M /umbc/xfs2/strow/asl/s1/sergio/home/git/kcarta-matlab 1.1G /umbc/xfs2/strow/asl/s1/sergio/home/git/SPECTRA 2.6G /umbc/xfs2/strow/asl/s1/sergio/home/git/UM 5.8G /umbc/xfs2/strow/asl/s1/sergio/home/git/HITRAN2UMBCLBL 9.9G /umbc/xfs2/strow/asl/s1/sergio/home/git/SARTA_CLOUDY_RTP_KLAYERS_NLEVELS 14G /umbc/xfs2/strow/asl/s1/sergio/home/git/KCARTA 16G /umbc/xfs2/strow/asl/s1/sergio/home/git/CRODGERS_FAST_CLOUD 16G /umbc/xfs2/strow/asl/s1/sergio/home/git/oem_pkg_run_sergio_AuxJacs 31G /umbc/xfs2/strow/asl/s1/sergio/home/git/IR_NIR_VIS_UV_RTcodes 56G /umbc/xfs2/strow/asl/s1/sergio/home/git/FastModelDevelopment 744G /umbc/xfs2/strow/asl/s1/sergio/home/git/MATLABCODE_Git 895G /umbc/xfs2/strow/asl/s1/sergio/home/git >>>  On Wed, Oct 29, 2025 at 1:33=E2=80=AFPM Tartela Tabassum via RT < UMBCHelp@rt.umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3301564 > > > Last Update From Ticket: > > Hi Sergio, > > Thanks for reaching out. Can you please clarify a few points so we can > better > diagnose the issue? > > =E2=80=A2 Are the slowdowns happening only inside Emacs, or do you also s= ee delays > when > running other commands (e.g., ls, cat, or editing with vi or nano)? > =E2=80=A2 Are you currently working in an interactive session or through = a regular > SSH > login? > =E2=80=A2 When you say you can=E2=80=99t save files, do you get a specifi= c error message in > Emacs or in the terminal? > > Could you please check whether you have any large or temporary files that > could > be cleaned up or moved to/scratch instead? You can use the command below = to > list directory sizes: > > du -h --max-depth=3D1 /umbc/xfs2/strow/asl/s1/sergio/home/git | sort -h > > This will help us see if large files in your directories are contributing > to > the issue. > > Once you confirm the points above, we=E2=80=99ll determine if it=E2=80=99= s a general > filesystem > performance issue or specific to your environment. > > Best, > UMBC HPCF Support > > On Wed Oct 29 10:08:05 2025, VR64161 wrote: > > > Plus any commands associated with git (status, add, commit etc) are very > > slow -Sergio On Wed, Oct 29, 2025 at 10:00 AM via RT < > UMBCHelp@rt.umbc.edu> > > wrote: > > >> Greetings, > > >> This message has been automatically generated in response to the > >> creation of a ticket regarding: > > >> > ------------------------------------------------------------------------- > >> Subject: ""HPC Other Issue: very slow editing on /home/sergio on c24-52"" > > >> Message: > > >> First Name: Sergio > >> Last Name: De souza-machad > >> Email: sergio@umbc.edu > >> Campus ID: VR64161 > > >> Request Type: High Performance Cluster > > >> Takes sooo long for making changes/loading/saving files using emacs ... > >> can you look into it? I am currently editing a file on > >> /umbc/xfs2/strow/asl/s1/sergio/home/git/era5_pipeline > > >> This has been happening for at least last two days > > >> -Sergio > > > >> > ------------------------------------------------------------------------- > > >> There is no need to reply to this message right now. > > >> Your ticket has been assigned an ID of [Research Computing #3301564] or > >> you can go there directly by clicking the link below. > > >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3301564 > > > >> You can login to view your open tickets at any time by visiting > >> http://my.umbc.edu and clicking on ""Help"" and ""Request Help"". > > >> Alternately you can click on http://my.umbc.edu/help > > >> Thank you > > > -- > > > -------------------------------------------------------------------------= -------------------------------------------- > > Sergio DeSouza-Machado sergio@umbc.edu > > Research Assoc. Professor, (W) 410-455-1944 > > JCET/Dept of Physics (F) 410-455-1072UMBC, Baltimore MD 21250 > >  --=20 ---------------------------------------------------------------------------= ------------------------------------------ Sergio DeSouza-Machado sergio@umbc.edu Research Assoc. Professor,                                              (W) 410-455-1944 JCET/Dept of Physics (F) 410-455-1072 UMBC, Baltimore MD 21250 "
3301564,72568252,Correspond,DoIT-Research-Computing,2025-10-29 20:03:46.0000000,HPC Other Issue: very slow editing on /home/sergio on c24-52,open,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"<div dir=3D""ltr"">BTW this is the tail end of the command you asked to run .= .. I&#39;m not sure if it followed symbolic links to (large) data files, as=  I doubt this is source code only<br><br>744G /umbc/xfs2/strow/asl/s1/sergi= o/home/git/MATLABCODE_Git<br><br>Thanks<div><br></div><div>Sergio</div><div= ><br>&gt;&gt;&gt;<br>du: cannot access &#39;/umbc/xfs2/strow/asl/s1/sergio/= home/git/ftc_dev/NeuralNet/fit_nonLTE&#39;: Input/output error<br>du: canno= t access &#39;/umbc/xfs2/strow/asl/s1/sergio/home/git/ftc_dev/Refs&#39;: In= put/output error<br>du: cannot access &#39;/umbc/xfs2/strow/asl/s1/sergio/h= ome/git/ftc_dev/fit_ch4&#39;: Input/output error<br>du: cannot access &#39;= /umbc/xfs2/strow/asl/s1/sergio/home/git/ftc_dev/run&#39;: Input/output erro= r<br>du: cannot read directory &#39;/umbc/xfs2/strow/asl/s1/sergio/home/git= /sarta&#39;: Input/output error<br>du: cannot access &#39;/umbc/xfs2/strow/= asl/s1/sergio/home/git/MATFILES_for_JGR_trends_paper/.git/objects/c0&#39;: = Input/output error<br>du: cannot access &#39;/umbc/xfs2/strow/asl/s1/sergio= /home/git/MATFILES_for_JGR_trends_paper/.git/objects/b1&#39;: Input/output = error<br>du: cannot read directory &#39;/umbc/xfs2/strow/asl/s1/sergio/home= /git/MATFILES_for_JGR_trends_paper/.git/objects/16&#39;: Input/output error= <br>du: cannot access &#39;/umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES= _for_JGR_trends_paper/.git/objects/2f&#39;: Input/output error<br>du: canno= t read directory &#39;/umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_= JGR_trends_paper/.git/objects/55&#39;: Input/output error<br>du: cannot acc= ess &#39;/umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_JGR_trends_pa= per/.git/objects/5a&#39;: Input/output error<br>du: cannot access &#39;/umb= c/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_JGR_trends_paper/.git/obje= cts/e6&#39;: Input/output error<br>du: cannot access &#39;/umbc/xfs2/strow/= asl/s1/sergio/home/git/MATFILES_for_JGR_trends_paper/.git/objects/2c&#39;: = Input/output error<br>du: cannot access &#39;/umbc/xfs2/strow/asl/s1/sergio= /home/git/MATFILES_for_JGR_trends_paper/.git/objects/6b&#39;: Input/output = error<br>du: cannot access &#39;/umbc/xfs2/strow/asl/s1/sergio/home/git/MAT= FILES_for_JGR_trends_paper/.git/objects/59&#39;: Input/output error<br>du: = cannot read directory &#39;/umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES= _for_JGR_trends_paper/.git/objects/86&#39;: Input/output error<br>du: canno= t access &#39;/umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_JGR_tren= ds_paper/.git/objects/22&#39;: Input/output error<br>du: cannot access &#39= ;/umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_JGR_trends_paper/.git= /objects/98&#39;: Input/output error<br>du: cannot access &#39;/umbc/xfs2/s= trow/asl/s1/sergio/home/git/MATFILES_for_JGR_trends_paper/.git/objects/54&#= 39;: Input/output error<br>du: cannot read directory &#39;/umbc/xfs2/strow/= asl/s1/sergio/home/git/MATFILES_for_JGR_trends_paper/.git/objects/e8&#39;: = Input/output error<br>du: cannot access &#39;/umbc/xfs2/strow/asl/s1/sergio= /home/git/MATFILES_for_JGR_trends_paper/.git/objects/c6&#39;: Input/output = error<br>du: cannot access &#39;/umbc/xfs2/strow/asl/s1/sergio/home/git/MAT= FILES_for_JGR_trends_paper/.git/objects/53&#39;: Input/output error<br>du: = cannot access &#39;/umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_JGR= _trends_paper/.git/objects/20&#39;: Input/output error<br>du: cannot read d= irectory &#39;/umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_JGR_tren= ds_paper/.git/objects/e0&#39;: Input/output error<br>4.0K	/umbc/xfs2/strow/= asl/s1/sergio/home/git/oem_pkg<br>4.0K	/umbc/xfs2/strow/asl/s1/sergio/home/= git/oem_pkg_run<br>4.0K	/umbc/xfs2/strow/asl/s1/sergio/home/git/Readme_RTP_= PIPELINE<br>4.0K	/umbc/xfs2/strow/asl/s1/sergio/home/git/RTPMAKE<br>4.0K	/u= mbc/xfs2/strow/asl/s1/sergio/home/git/sarta<br>20K	/umbc/xfs2/strow/asl/s1/= sergio/home/git/cfg_for_rtp_prod2<br>136K	/umbc/xfs2/strow/asl/s1/sergio/ho= me/git/merra2_pipeline<br>2.6M	/umbc/xfs2/strow/asl/s1/sergio/home/git/era5= _pipeline<br>4.4M	/umbc/xfs2/strow/asl/s1/sergio/home/git/rtp_prod2<br>16M	= /umbc/xfs2/strow/asl/s1/sergio/home/git/MATFILES_for_JGR_trends_paper<br>18= M	/umbc/xfs2/strow/asl/s1/sergio/home/git/clear_anomaly_retrievals<br>25M	/= umbc/xfs2/strow/asl/s1/sergio/home/git/get_ecmwf<br>54M	/umbc/xfs2/strow/as= l/s1/sergio/home/git/rtp<br>80M	/umbc/xfs2/strow/asl/s1/sergio/home/git/l1c= <br>81M	/umbc/xfs2/strow/asl/s1/sergio/home/git/kcarta<br>102M	/umbc/xfs2/s= trow/asl/s1/sergio/home/git/matlib<br>107M	/umbc/xfs2/strow/asl/s1/sergio/h= ome/git/sarta_utilities<br>183M	/umbc/xfs2/strow/asl/s1/sergio/home/git/ftc= _dev<br>254M	/umbc/xfs2/strow/asl/s1/sergio/home/git/kcarta-matlab<br>1.1G	= /umbc/xfs2/strow/asl/s1/sergio/home/git/SPECTRA<br>2.6G	/umbc/xfs2/strow/as= l/s1/sergio/home/git/UM<br>5.8G	/umbc/xfs2/strow/asl/s1/sergio/home/git/HIT= RAN2UMBCLBL<br>9.9G	/umbc/xfs2/strow/asl/s1/sergio/home/git/SARTA_CLOUDY_RT= P_KLAYERS_NLEVELS<br>14G	/umbc/xfs2/strow/asl/s1/sergio/home/git/KCARTA<br>= 16G	/umbc/xfs2/strow/asl/s1/sergio/home/git/CRODGERS_FAST_CLOUD<br>16G	/umb= c/xfs2/strow/asl/s1/sergio/home/git/oem_pkg_run_sergio_AuxJacs<br>31G	/umbc= /xfs2/strow/asl/s1/sergio/home/git/IR_NIR_VIS_UV_RTcodes<br>56G	/umbc/xfs2/= strow/asl/s1/sergio/home/git/FastModelDevelopment<br>744G	/umbc/xfs2/strow/= asl/s1/sergio/home/git/MATLABCODE_Git<br>895G	/umbc/xfs2/strow/asl/s1/sergi= o/home/git<br><div>&gt;&gt;&gt;</div></div></div><br><div class=3D""gmail_qu= ote gmail_quote_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Wed, Oc= t 29, 2025 at 1:33=E2=80=AFPM Tartela Tabassum via RT &lt;<a href=3D""mailto= :UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockq= uote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1p= x solid rgb(204,204,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https= ://rt.umbc.edu/Ticket/Display.html?id=3D3301564"" rel=3D""noreferrer"" target= =3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3301564</a> &gt;<b= r> <br> Last Update From Ticket:<br> <br> Hi Sergio,<br> <br> Thanks for reaching out. Can you please clarify a few points so we can bett= er<br> diagnose the issue?<br> <br> =E2=80=A2 Are the slowdowns happening only inside Emacs, or do you also see=  delays when<br> running other commands (e.g., ls, cat, or editing with vi or nano)?<br> =E2=80=A2 Are you currently working in an interactive session or through a = regular SSH<br> login?<br> =E2=80=A2 When you say you can=E2=80=99t save files, do you get a specific = error message in<br> Emacs or in the terminal?<br> <br> Could you please check whether you have any large or temporary files that c= ould<br> be cleaned up or moved to/scratch instead? You can use the command below to= <br> list directory sizes:<br> <br> du -h --max-depth=3D1 /umbc/xfs2/strow/asl/s1/sergio/home/git | sort -h<br> <br> This will help us see if large files in your directories are contributing t= o<br> the issue.<br> <br> Once you confirm the points above, we=E2=80=99ll determine if it=E2=80=99s = a general filesystem<br> performance issue or specific to your environment.<br> <br> Best,<br> UMBC HPCF Support<br> <br> On Wed Oct 29 10:08:05 2025, VR64161 wrote:<br> <br> &gt; Plus any commands associated with git (status, add, commit etc) are ve= ry<br> &gt; slow -Sergio On Wed, Oct 29, 2025 at 10:00 AM via RT &lt;<a href=3D""ma= ilto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt;<b= r> &gt; wrote:<br> <br> &gt;&gt; Greetings,<br> <br> &gt;&gt; This message has been automatically generated in response to the<b= r> &gt;&gt; creation of a ticket regarding:<br> <br> &gt;&gt; ------------------------------------------------------------------= -------<br> &gt;&gt; Subject: &quot;HPC Other Issue: very slow editing on /home/sergio = on c24-52&quot;<br> <br> &gt;&gt; Message:<br> <br> &gt;&gt; First Name: Sergio<br> &gt;&gt; Last Name: De souza-machad<br> &gt;&gt; Email: <a href=3D""mailto:sergio@umbc.edu"" target=3D""_blank"">sergio= @umbc.edu</a><br> &gt;&gt; Campus ID: VR64161<br> <br> &gt;&gt; Request Type: High Performance Cluster<br> <br> &gt;&gt; Takes sooo long for making changes/loading/saving files using emac= s ...<br> &gt;&gt; can you look into it? I am currently editing a file on<br> &gt;&gt; /umbc/xfs2/strow/asl/s1/sergio/home/git/era5_pipeline<br> <br> &gt;&gt; This has been happening for at least last two days<br> <br> &gt;&gt; -Sergio<br> <br> <br> &gt;&gt; ------------------------------------------------------------------= -------<br> <br> &gt;&gt; There is no need to reply to this message right now.<br> <br> &gt;&gt; Your ticket has been assigned an ID of [Research Computing #330156= 4] or<br> &gt;&gt; you can go there directly by clicking the link below.<br> <br> &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html= ?id=3D3301564"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tic= ket/Display.html?id=3D3301564</a> &gt;<br> <br> &gt;&gt; You can login to view your open tickets at any time by visiting<br> &gt;&gt; <a href=3D""http://my.umbc.edu"" rel=3D""noreferrer"" target=3D""_blank= "">http://my.umbc.edu</a> and clicking on &quot;Help&quot; and &quot;Request=  Help&quot;.<br> <br> &gt;&gt; Alternately you can click on <a href=3D""http://my.umbc.edu/help"" r= el=3D""noreferrer"" target=3D""_blank"">http://my.umbc.edu/help</a><br> <br> &gt;&gt; Thank you<br> <br> &gt; --<br> &gt; ----------------------------------------------------------------------= -----------------------------------------------<br> &gt; Sergio DeSouza-Machado <a href=3D""mailto:sergio@umbc.edu"" target=3D""_b= lank"">sergio@umbc.edu</a><br> &gt; Research Assoc. Professor, (W) 410-455-1944<br> &gt; JCET/Dept of Physics (F) 410-455-1072UMBC, Baltimore MD 21250<br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><div><div dir=3D""ltr""><div><div dir=3D""ltr""><div= >--------------------------------------------------------------------------= -------------------------------------------<br>Sergio DeSouza-Machado=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 <a href=3D""mailto:sergio@umbc.edu"" target= =3D""_blank"">sergio@umbc.edu</a><br>Research Assoc. Professor,=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 (W) 410-455-1944<br>JCET/Dept of Physics= =C2=A0=C2=A0 =C2=A0=C2=A0 =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0 (F) 410-455-1072<br></div><div>UMBC, Baltimore MD 21250<br>= </div></div></div></div></div></div></div> "
3301564,72594281,Correspond,DoIT-Research-Computing,2025-10-30 13:09:53.0000000,HPC Other Issue: very slow editing on /home/sergio on c24-52,open,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"I ran a script to list file sizes on   /home/sergio/git/MATLABCODE_Git since that is a large directory (the .gitignore scratches out all .mat files from git commits so there are no large file issues on github)    du -h >& ugh;sort -g -r ugh > ughsort; grep  -in 'G    ' ughsort  and attached the following file to show those which are Gb in size; I can move the first few (REGR_PROFILES_SARTA/RUN_KCARTA) to a JUNK directory but they would still effectively reside on /asl/s1/sergio  -Sergio   On Wed, Oct 29, 2025 at 1:33=E2=80=AFPM Tartela Tabassum via RT < UMBCHelp@rt.umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3301564 > > > Last Update From Ticket: > > Hi Sergio, > > Thanks for reaching out. Can you please clarify a few points so we can > better > diagnose the issue? > > =E2=80=A2 Are the slowdowns happening only inside Emacs, or do you also s= ee delays > when > running other commands (e.g., ls, cat, or editing with vi or nano)? > =E2=80=A2 Are you currently working in an interactive session or through = a regular > SSH > login? > =E2=80=A2 When you say you can=E2=80=99t save files, do you get a specifi= c error message in > Emacs or in the terminal? > > Could you please check whether you have any large or temporary files that > could > be cleaned up or moved to/scratch instead? You can use the command below = to > list directory sizes: > > du -h --max-depth=3D1 /umbc/xfs2/strow/asl/s1/sergio/home/git | sort -h > > This will help us see if large files in your directories are contributing > to > the issue. > > Once you confirm the points above, we=E2=80=99ll determine if it=E2=80=99= s a general > filesystem > performance issue or specific to your environment. > > Best, > UMBC HPCF Support > > On Wed Oct 29 10:08:05 2025, VR64161 wrote: > > > Plus any commands associated with git (status, add, commit etc) are very > > slow -Sergio On Wed, Oct 29, 2025 at 10:00 AM via RT < > UMBCHelp@rt.umbc.edu> > > wrote: > > >> Greetings, > > >> This message has been automatically generated in response to the > >> creation of a ticket regarding: > > >> > ------------------------------------------------------------------------- > >> Subject: ""HPC Other Issue: very slow editing on /home/sergio on c24-52"" > > >> Message: > > >> First Name: Sergio > >> Last Name: De souza-machad > >> Email: sergio@umbc.edu > >> Campus ID: VR64161 > > >> Request Type: High Performance Cluster > > >> Takes sooo long for making changes/loading/saving files using emacs ... > >> can you look into it? I am currently editing a file on > >> /umbc/xfs2/strow/asl/s1/sergio/home/git/era5_pipeline > > >> This has been happening for at least last two days > > >> -Sergio > > > >> > ------------------------------------------------------------------------- > > >> There is no need to reply to this message right now. > > >> Your ticket has been assigned an ID of [Research Computing #3301564] or > >> you can go there directly by clicking the link below. > > >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3301564 > > > >> You can login to view your open tickets at any time by visiting > >> http://my.umbc.edu and clicking on ""Help"" and ""Request Help"". > > >> Alternately you can click on http://my.umbc.edu/help > > >> Thank you > > > -- > > > -------------------------------------------------------------------------= -------------------------------------------- > > Sergio DeSouza-Machado sergio@umbc.edu > > Research Assoc. Professor, (W) 410-455-1944 > > JCET/Dept of Physics (F) 410-455-1072UMBC, Baltimore MD 21250 > >  --=20 ---------------------------------------------------------------------------= ------------------------------------------ Sergio DeSouza-Machado sergio@umbc.edu Research Assoc. Professor,                                              (W) 410-455-1944 JCET/Dept of Physics (F) 410-455-1072 UMBC, Baltimore MD 21250 "
3301564,72594281,Correspond,DoIT-Research-Computing,2025-10-30 13:09:53.0000000,HPC Other Issue: very slow editing on /home/sergio on c24-52,open,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"<div dir=3D""ltr"">I ran a script to list file sizes on=C2=A0<br>=C2=A0=C2=A0= /home/sergio/git/MATLABCODE_Git<br>since that is a large directory (the .gi= tignore scratches out all .mat files from git commits so there are no large=  file issues on github)<br><br>=C2=A0=C2=A0du -h &gt;&amp; ugh;sort -g -r u= gh &gt; ughsort;=C2=A0grep =C2=A0-in &#39;G =C2=A0 =C2=A0&#39; ughsort<br><= br>and attached the following file to show those which are Gb in size; I ca= n move the first few (REGR_PROFILES_SARTA/RUN_KCARTA) to a JUNK directory b= ut they would still effectively reside on /asl/s1/sergio<div><br></div><div= >-Sergio</div><div><br></div></div><br><div class=3D""gmail_quote gmail_quot= e_container""><div dir=3D""ltr"" class=3D""gmail_attr"">On Wed, Oct 29, 2025 at = 1:33=E2=80=AFPM Tartela Tabassum via RT &lt;<a href=3D""mailto:UMBCHelp@rt.u= mbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""= gmail_quote"" style=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(20= 4,204,204);padding-left:1ex"">Ticket &lt;URL: <a href=3D""https://rt.umbc.edu= /Ticket/Display.html?id=3D3301564"" rel=3D""noreferrer"" target=3D""_blank"">htt= ps://rt.umbc.edu/Ticket/Display.html?id=3D3301564</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Sergio,<br> <br> Thanks for reaching out. Can you please clarify a few points so we can bett= er<br> diagnose the issue?<br> <br> =E2=80=A2 Are the slowdowns happening only inside Emacs, or do you also see=  delays when<br> running other commands (e.g., ls, cat, or editing with vi or nano)?<br> =E2=80=A2 Are you currently working in an interactive session or through a = regular SSH<br> login?<br> =E2=80=A2 When you say you can=E2=80=99t save files, do you get a specific = error message in<br> Emacs or in the terminal?<br> <br> Could you please check whether you have any large or temporary files that c= ould<br> be cleaned up or moved to/scratch instead? You can use the command below to= <br> list directory sizes:<br> <br> du -h --max-depth=3D1 /umbc/xfs2/strow/asl/s1/sergio/home/git | sort -h<br> <br> This will help us see if large files in your directories are contributing t= o<br> the issue.<br> <br> Once you confirm the points above, we=E2=80=99ll determine if it=E2=80=99s = a general filesystem<br> performance issue or specific to your environment.<br> <br> Best,<br> UMBC HPCF Support<br> <br> On Wed Oct 29 10:08:05 2025, VR64161 wrote:<br> <br> &gt; Plus any commands associated with git (status, add, commit etc) are ve= ry<br> &gt; slow -Sergio On Wed, Oct 29, 2025 at 10:00 AM via RT &lt;<a href=3D""ma= ilto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">UMBCHelp@rt.umbc.edu</a>&gt;<b= r> &gt; wrote:<br> <br> &gt;&gt; Greetings,<br> <br> &gt;&gt; This message has been automatically generated in response to the<b= r> &gt;&gt; creation of a ticket regarding:<br> <br> &gt;&gt; ------------------------------------------------------------------= -------<br> &gt;&gt; Subject: &quot;HPC Other Issue: very slow editing on /home/sergio = on c24-52&quot;<br> <br> &gt;&gt; Message:<br> <br> &gt;&gt; First Name: Sergio<br> &gt;&gt; Last Name: De souza-machad<br> &gt;&gt; Email: <a href=3D""mailto:sergio@umbc.edu"" target=3D""_blank"">sergio= @umbc.edu</a><br> &gt;&gt; Campus ID: VR64161<br> <br> &gt;&gt; Request Type: High Performance Cluster<br> <br> &gt;&gt; Takes sooo long for making changes/loading/saving files using emac= s ...<br> &gt;&gt; can you look into it? I am currently editing a file on<br> &gt;&gt; /umbc/xfs2/strow/asl/s1/sergio/home/git/era5_pipeline<br> <br> &gt;&gt; This has been happening for at least last two days<br> <br> &gt;&gt; -Sergio<br> <br> <br> &gt;&gt; ------------------------------------------------------------------= -------<br> <br> &gt;&gt; There is no need to reply to this message right now.<br> <br> &gt;&gt; Your ticket has been assigned an ID of [Research Computing #330156= 4] or<br> &gt;&gt; you can go there directly by clicking the link below.<br> <br> &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html= ?id=3D3301564"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tic= ket/Display.html?id=3D3301564</a> &gt;<br> <br> &gt;&gt; You can login to view your open tickets at any time by visiting<br> &gt;&gt; <a href=3D""http://my.umbc.edu"" rel=3D""noreferrer"" target=3D""_blank= "">http://my.umbc.edu</a> and clicking on &quot;Help&quot; and &quot;Request=  Help&quot;.<br> <br> &gt;&gt; Alternately you can click on <a href=3D""http://my.umbc.edu/help"" r= el=3D""noreferrer"" target=3D""_blank"">http://my.umbc.edu/help</a><br> <br> &gt;&gt; Thank you<br> <br> &gt; --<br> &gt; ----------------------------------------------------------------------= -----------------------------------------------<br> &gt; Sergio DeSouza-Machado <a href=3D""mailto:sergio@umbc.edu"" target=3D""_b= lank"">sergio@umbc.edu</a><br> &gt; Research Assoc. Professor, (W) 410-455-1944<br> &gt; JCET/Dept of Physics (F) 410-455-1072UMBC, Baltimore MD 21250<br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><div><div dir=3D""ltr""><div><div dir=3D""ltr""><div= >--------------------------------------------------------------------------= -------------------------------------------<br>Sergio DeSouza-Machado=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 <a href=3D""mailto:sergio@umbc.edu"" target= =3D""_blank"">sergio@umbc.edu</a><br>Research Assoc. Professor,=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 (W) 410-455-1944<br>JCET/Dept of Physics= =C2=A0=C2=A0 =C2=A0=C2=A0 =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0 (F) 410-455-1072<br></div><div>UMBC, Baltimore MD 21250<br>= </div></div></div></div></div></div></div> "
3301564,72594281,Correspond,DoIT-Research-Computing,2025-10-30 13:09:53.0000000,HPC Other Issue: very slow editing on /home/sergio on c24-52,open,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"[sergio@c24-52 MATLABCODE_Git]$ du -h >& ugh;sort -g -r ugh > ughsort; /home/sergio/tera_gig_sort.sc; more ughsort [sergio@c24-52 MATLABCODE_Git]$ grep  -in 'G    ' ughsort 40:745G	. 58:647G	./REGR_PROFILES_SARTA 60:639G	./REGR_PROFILES_SARTA/RUN_KCARTA 242:192G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2025_ECMWF83Profiles_AIRS2834_3CrIS_IASI 353:114G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_Jan2025_PBL_AIRS2834_3CrIS_IASI 364:112G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2022_AIRS2834_3CrIS_IASI 365:112G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_May2021_AIRS2834_3CrIS_IASI 366:111G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_Feb2020_AIRS2834_CHIRP 813:31G	./PBL_Retrievals/HALO_BdryLayer/Proposal2024 814:31G	./PBL_Retrievals/HALO_BdryLayer 815:31G	./PBL_Retrievals 830:28G	./PBL_Retrievals/HALO_BdryLayer/Proposal2024/DATA/AMDAR_YZhang_JGR2020 831:28G	./PBL_Retrievals/HALO_BdryLayer/Proposal2024/DATA 884:21G	./QUICKTASKS_TELECON 894:20G	./ESRL_TRACE_GAS/CarbonTracker 895:20G	./ESRL_TRACE_GAS 929:15G	./BRDF_EMISSIVITY_NALLI/TestNalliEMiss 930:15G	./BRDF_EMISSIVITY_NALLI 945:13G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2025_ECMWF83Profiles_AIRS2834_3CrIS_IASI/wvbandF 946:13G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2025_ECMWF83Profiles_AIRS2834_3CrIS_IASI/n2ohno3bandN2O 947:13G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2025_ECMWF83Profiles_AIRS2834_3CrIS_IASI/n2ohno3bandHNO3 948:13G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2025_ECMWF83Profiles_AIRS2834_3CrIS_IASI/FWxDyO 949:13G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2025_ECMWF83Profiles_AIRS2834_3CrIS_IASI/FWOP 950:13G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2025_ECMWF83Profiles_AIRS2834_3CrIS_IASI/FWO_dHDO 951:13G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2025_ECMWF83Profiles_AIRS2834_3CrIS_IASI/FWO 952:13G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2025_ECMWF83Profiles_AIRS2834_3CrIS_IASI/FW 953:13G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2025_ECMWF83Profiles_AIRS2834_3CrIS_IASI/FO 954:13G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2025_ECMWF83Profiles_AIRS2834_3CrIS_IASI/FDO 955:13G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2025_ECMWF83Profiles_AIRS2834_3CrIS_IASI/FD 956:13G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2025_ECMWF83Profiles_AIRS2834_3CrIS_IASI/F 957:13G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2025_ECMWF83Profiles_AIRS2834_3CrIS_IASI/cobandF 986:12G	./QUICKTASKS_TELECON/PBL_BillIrion_klayers 994:11G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2025_ECMWF83Profiles_AIRS2834_3CrIS_IASI/ch4bandCH4 1004:8.1G	./REGR_PROFILES_SARTA/Git_ECMWF_SAF_137Profiles 1039:7.7G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2025_ECMWF83Profiles_AIRS2834_3CrIS_IASI/so2bandS 1040:7.7G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_Jan2025_PBL_AIRS2834_3CrIS_IASI/FWxDyO 1041:7.7G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_Jan2025_PBL_AIRS2834_3CrIS_IASI/FWOP 1042:7.7G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_Jan2025_PBL_AIRS2834_3CrIS_IASI/FWO_dHDO 1043:7.7G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_Jan2025_PBL_AIRS2834_3CrIS_IASI/FWO 1044:7.7G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_Jan2025_PBL_AIRS2834_3CrIS_IASI/FDO 1046:7.6G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2022_AIRS2834_3CrIS_IASI/FWxDyO 1047:7.6G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2022_AIRS2834_3CrIS_IASI/FWOP 1048:7.6G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2022_AIRS2834_3CrIS_IASI/FWO_dHDO 1049:7.6G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2022_AIRS2834_3CrIS_IASI/FWO 1050:7.6G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2022_AIRS2834_3CrIS_IASI/FDO 1051:7.6G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_Jan2025_PBL_AIRS2834_3CrIS_IASI/wvbandF 1052:7.6G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_Jan2025_PBL_AIRS2834_3CrIS_IASI/FW 1053:7.6G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_Jan2025_PBL_AIRS2834_3CrIS_IASI/FO 1054:7.6G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_Jan2025_PBL_AIRS2834_3CrIS_IASI/FD 1055:7.6G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_May2021_AIRS2834_3CrIS_IASI/FWOP 1056:7.6G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_May2021_AIRS2834_3CrIS_IASI/FWO_dHDO 1057:7.6G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_May2021_AIRS2834_3CrIS_IASI/FWO 1058:7.6G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_May2021_AIRS2834_3CrIS_IASI/FDO 1061:7.5G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2022_AIRS2834_3CrIS_IASI/wvbandF 1062:7.5G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2022_AIRS2834_3CrIS_IASI/FW 1063:7.5G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2022_AIRS2834_3CrIS_IASI/FO 1064:7.5G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2022_AIRS2834_3CrIS_IASI/FD 1065:7.5G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_Jan2025_PBL_AIRS2834_3CrIS_IASI/n2ohno3bandN2O 1066:7.5G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_Jan2025_PBL_AIRS2834_3CrIS_IASI/n2ohno3bandHNO3 1067:7.5G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_Jan2025_PBL_AIRS2834_3CrIS_IASI/F 1068:7.5G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_Jan2025_PBL_AIRS2834_3CrIS_IASI/cobandF 1069:7.5G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_May2021_AIRS2834_3CrIS_IASI/wvbandF 1070:7.5G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_May2021_AIRS2834_3CrIS_IASI/FWxDyO 1071:7.5G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_May2021_AIRS2834_3CrIS_IASI/FW 1072:7.5G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_May2021_AIRS2834_3CrIS_IASI/FO 1073:7.5G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_May2021_AIRS2834_3CrIS_IASI/FD 1074:7.4G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2022_AIRS2834_3CrIS_IASI/n2ohno3bandN2O 1075:7.4G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2022_AIRS2834_3CrIS_IASI/n2ohno3bandHNO3 1076:7.4G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2022_AIRS2834_3CrIS_IASI/F 1077:7.4G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2022_AIRS2834_3CrIS_IASI/cobandF 1078:7.4G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_May2021_AIRS2834_3CrIS_IASI/n2ohno3bandN2O 1079:7.4G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_May2021_AIRS2834_3CrIS_IASI/n2ohno3bandHNO3 1080:7.4G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_May2021_AIRS2834_3CrIS_IASI/F 1081:7.4G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_May2021_AIRS2834_3CrIS_IASI/cobandF 1085:7.0G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_Feb2020_AIRS2834_CHIRP/FWxDyO 1086:7.0G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_Feb2020_AIRS2834_CHIRP/FWOP_1.05 1087:7.0G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_Feb2020_AIRS2834_CHIRP/FWOP 1088:7.0G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_Feb2020_AIRS2834_CHIRP/FWO_dHDO 1089:7.0G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_Feb2020_AIRS2834_CHIRP/FWO 1090:7.0G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_Feb2020_AIRS2834_CHIRP/FDO 1092:6.9G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_Feb2020_AIRS2834_CHIRP/wvbandF 1093:6.9G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_Feb2020_AIRS2834_CHIRP/FW 1094:6.9G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_Feb2020_AIRS2834_CHIRP/FO 1095:6.9G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_Feb2020_AIRS2834_CHIRP/FD 1097:6.8G	./SONDES 1098:6.8G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2025_ECMWF83Profiles_AIRS2834_3CrIS_IASI/nh3bandNH3 1099:6.8G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_Feb2020_AIRS2834_CHIRP/n2ohno3bandN2O 1100:6.8G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_Feb2020_AIRS2834_CHIRP/n2ohno3bandHNO3 1101:6.8G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_Feb2020_AIRS2834_CHIRP/F 1102:6.8G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_Feb2020_AIRS2834_CHIRP/cobandF 1104:6.6G	./QUICKTASKS_TELECON/Pengwang_SOS 1110:6.2G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_Jan2025_PBL_AIRS2834_3CrIS_IASI/ch4bandCH4 1112:6.0G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2022_AIRS2834_3CrIS_IASI/ch4bandCH4 1113:6.0G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_May2021_AIRS2834_3CrIS_IASI/ch4bandCH4 1114:5.9G	./SONDES/MAGIC 1122:5.6G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_Feb2020_AIRS2834_CHIRP/ch4bandCH4 1135:4.7G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_Jan2025_PBL_AIRS2834_3CrIS_IASI/so2bandS 1136:4.5G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2022_AIRS2834_3CrIS_IASI/so2bandS 1137:4.5G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_Jan2025_PBL_AIRS2834_3CrIS_IASI/nh3bandNH3 1143:4.1G	./SONDES/MAGIC/Sergio2016 1167:4.0G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2020_July2022_AIRS2834_3CrIS_IASI/nh3bandNH3 1168:4.0G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_May2021_AIRS2834_3CrIS_IASI/so2bandS 1169:4.0G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_May2021_AIRS2834_3CrIS_IASI/nh3bandNH3 1170:4.0G	./REGR_PROFILES_SARTA/Git_ECMWF_SAF_137Profiles/WORKS_Dec5_2018 1176:3.7G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_Feb2020_AIRS2834_CHIRP/so2bandS 1177:3.6G	./REGR_PROFILES_SARTA/RUN_KCARTA/REGR49_400ppm_H2016_Feb2020_AIRS2834_CHIRP/nh3bandNH3 1186:3.0G	./QUICKTASKS_TELECON/Pengwang_SOS/TRP_Apr8_2024 1198:2.7G	./PBL_Retrievals/HALO_BdryLayer/Proposal2024/PBLH_Retr 1202:2.5G	./QUICKTASKS_TELECON/PBL_BillIrion_klayers/PROFILES 1203:2.5G	./PBL_Retrievals/HALO_BdryLayer/Proposal2024/PBLH_Retr/2019/04 1204:2.5G	./PBL_Retrievals/HALO_BdryLayer/Proposal2024/PBLH_Retr/2019 1215:2.3G	./QUICKTASKS_TELECON/Pengwang_SOS/KCARTA 1220:2.1G	./QUICKTASKS_TELECON/Pengwang_SOS/KCARTA/KCARTA_Results 1221:2.1G	./PBL_Retrievals/HALO_BdryLayer/Proposal2024/PBLH_Retr/2019/04/26 1240:1.4G	./SONDES/MAGIC/Strow2015 1245:1.3G	./AI_cloud_retrievals 1248:1.2G	./CAMEL_emissivity/Trends_camelV003_emis4608tiles 1249:1.2G	./CAMEL_emissivity 1260:1.1G	./AI_cloud_retrievals/Climatology "
3301564,72604120,Correspond,DoIT-Research-Computing,2025-10-30 17:30:56.0000000,HPC Other Issue: very slow editing on /home/sergio on c24-52,open,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Sergio,</p>  <p>There is currently some degradation on the file system that sits on xfs2. We&#39;re investigating into how to resolve, but out of an abundance of caution, we are going to unmount xfs2 so that the degradation doesn&#39;t spread. We&#39;ll let you know when we have more information for you, but expect to not have access to your mount for a while.&nbsp;</p>  <p>On Thu Oct 30 13:23:27 2025, OL73413 wrote:</p>  <blockquote> <div> <p>Hi Sergio,</p>  <p>We are aware of the issue and are currently investigating. I&#39;m going to merge this with the other ticket since they are related.&nbsp;</p>  <p>On Thu Oct 30 12:53:30 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Sergio Last Name:                 De souza-machad Email:                     sergio@umbc.edu Campus ID:                 VR64161  Request Type:              High Performance Cluster  Have the disk issues been worked on yet (ticket #3301564)? I still can&#39;t edit files  And now Matlab took about 3 minutes to start up on c24-52  There is something seriously wrong!  Thanks  Sergio  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3301564,72604483,Correspond,DoIT-Research-Computing,2025-10-30 17:44:40.0000000,HPC Other Issue: very slow editing on /home/sergio on c24-52,open,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"Hi Max  Thanks for the reply. Maybe you should also cc Larrabee as you all work on this (strow@umbc.edu)  Sergio   On Thu, Oct 30, 2025 at 1:30=E2=80=AFPM Max Breitmeyer via RT <UMBCHelp@rt.= umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3301564 > > > Last Update From Ticket: > > Sergio, > > There is currently some degradation on the file system that sits on xfs2. > We're > investigating into how to resolve, but out of an abundance of caution, we > are > going to unmount xfs2 so that the degradation doesn't spread. We'll let y= ou > know when we have more information for you, but expect to not have access > to > your mount for a while. > > On Thu Oct 30 13:23:27 2025, OL73413 wrote: > > > Hi Sergio, > > > We are aware of the issue and are currently investigating. I'm going to > > merge this with the other ticket since they are related. > > > On Thu Oct 30 12:53:30 2025, ZZ99999 wrote: > > >> First Name:                Sergio > >> Last Name:                 De souza-machad > >> Email:                     sergio@umbc.edu > >> Campus ID:                 VR64161 > >> > >> Request Type:              High Performance Cluster > >> > >> Have the disk issues been worked on yet (ticket #3301564)? I still > can't edit files > >> > >> And now Matlab took about 3 minutes to start up on c24-52 > >> > >> There is something seriously wrong! > >> > >> Thanks > >> > >> Sergio > > > > > -- > > > Best, > > Max Breitmeyer > > DOIT HPC System Administrator > > -- > > Best, > Max Breitmeyer > DOIT HPC System Administrator > >  --=20 ---------------------------------------------------------------------------= ------------------------------------------ Sergio DeSouza-Machado sergio@umbc.edu Research Assoc. Professor,                                              (W) 410-455-1944 JCET/Dept of Physics (F) 410-455-1072 UMBC, Baltimore MD 21250 "
3301564,72604483,Correspond,DoIT-Research-Computing,2025-10-30 17:44:40.0000000,HPC Other Issue: very slow editing on /home/sergio on c24-52,open,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"<div dir=3D""ltr"">Hi Max<br><br>Thanks for the reply. Maybe you should also = cc Larrabee as you all work on this (<a href=3D""mailto:strow@umbc.edu"">stro= w@umbc.edu</a>)<div><br></div><div>Sergio</div><div><br></div></div><br><di= v class=3D""gmail_quote gmail_quote_container""><div dir=3D""ltr"" class=3D""gma= il_attr"">On Thu, Oct 30, 2025 at 1:30=E2=80=AFPM Max Breitmeyer via RT &lt;= <a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:= <br></div><blockquote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8= ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"">Ticket &lt;URL:=  <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D3301564"" rel=3D""no= referrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D33= 01564</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Sergio,<br> <br> There is currently some degradation on the file system that sits on xfs2. W= e&#39;re<br> investigating into how to resolve, but out of an abundance of caution, we a= re<br> going to unmount xfs2 so that the degradation doesn&#39;t spread. We&#39;ll=  let you<br> know when we have more information for you, but expect to not have access t= o<br> your mount for a while.<br> <br> On Thu Oct 30 13:23:27 2025, OL73413 wrote:<br> <br> &gt; Hi Sergio,<br> <br> &gt; We are aware of the issue and are currently investigating. I&#39;m goi= ng to<br> &gt; merge this with the other ticket since they are related.<br> <br> &gt; On Thu Oct 30 12:53:30 2025, ZZ99999 wrote:<br> <br> &gt;&gt; First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=  Sergio<br> &gt;&gt; Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0De souza-machad<br> &gt;&gt; Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 =C2=A0<a href=3D""mailto:sergio@umbc.edu"" target=3D""_blank"">sergi= o@umbc.edu</a><br> &gt;&gt; Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0VR64161<br> &gt;&gt; <br> &gt;&gt; Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High=  Performance Cluster<br> &gt;&gt; <br> &gt;&gt; Have the disk issues been worked on yet (ticket #3301564)? I still=  can&#39;t edit files<br> &gt;&gt; <br> &gt;&gt; And now Matlab took about 3 minutes to start up on c24-52<br> &gt;&gt; <br> &gt;&gt; There is something seriously wrong!<br> &gt;&gt; <br> &gt;&gt; Thanks<br> &gt;&gt; <br> &gt;&gt; Sergio<br> &gt; <br> <br> &gt; --<br> <br> &gt; Best,<br> &gt; Max Breitmeyer<br> &gt; DOIT HPC System Administrator<br> <br> --<br> <br> Best,<br> Max Breitmeyer<br> DOIT HPC System Administrator<br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><div><div dir=3D""ltr""><div><div dir=3D""ltr""><div= >--------------------------------------------------------------------------= -------------------------------------------<br>Sergio DeSouza-Machado=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 <a href=3D""mailto:sergio@umbc.edu"" target= =3D""_blank"">sergio@umbc.edu</a><br>Research Assoc. Professor,=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 (W) 410-455-1944<br>JCET/Dept of Physics= =C2=A0=C2=A0 =C2=A0=C2=A0 =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0 (F) 410-455-1072<br></div><div>UMBC, Baltimore MD 21250<br>= </div></div></div></div></div></div></div> "
3301564,72605508,Correspond,DoIT-Research-Computing,2025-10-30 18:10:41.0000000,HPC Other Issue: very slow editing on /home/sergio on c24-52,open,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Sergio,</p>  <p>Please exit out of /umbc/xfs2/strow on all devices where you may be logg= ed in. We can&#39;t unmount the device until it&#39;s no longer active.&nbs= p;</p>  <p>On Thu Oct 30 13:44:40 2025, VR64161 wrote:</p>  <blockquote> <div>Hi Max<br /> <br /> Thanks for the reply. Maybe you should also cc Larrabee as you all work on = this (strow@umbc.edu) <div>&nbsp;</div>  <div>Sergio</div>  <div>&nbsp;</div> </div> &nbsp;  <div> <div>On Thu, Oct 30, 2025 at 1:30=E2=80=AFPM Max Breitmeyer via RT &lt;UMBC= Help@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D33= 01564 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Sergio,<br /> <br /> There is currently some degradation on the file system that sits on xfs2. W= e&#39;re<br /> investigating into how to resolve, but out of an abundance of caution, we a= re<br /> going to unmount xfs2 so that the degradation doesn&#39;t spread. We&#39;ll=  let you<br /> know when we have more information for you, but expect to not have access t= o<br /> your mount for a while.<br /> <br /> On Thu Oct 30 13:23:27 2025, OL73413 wrote:<br /> <br /> &gt; Hi Sergio,<br /> <br /> &gt; We are aware of the issue and are currently investigating. I&#39;m goi= ng to<br /> &gt; merge this with the other ticket since they are related.<br /> <br /> &gt; On Thu Oct 30 12:53:30 2025, ZZ99999 wrote:<br /> <br /> &gt;&gt; First Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;=  Sergio<br /> &gt;&gt; Last Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = &nbsp;De souza-machad<br /> &gt;&gt; Email:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p; &nbsp; &nbsp;sergio@umbc.edu<br /> &gt;&gt; Campus ID:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = &nbsp;VR64161<br /> &gt;&gt;<br /> &gt;&gt; Request Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; High=  Performance Cluster<br /> &gt;&gt;<br /> &gt;&gt; Have the disk issues been worked on yet (ticket #3301564)? I still=  can&#39;t edit files<br /> &gt;&gt;<br /> &gt;&gt; And now Matlab took about 3 minutes to start up on c24-52<br /> &gt;&gt;<br /> &gt;&gt; There is something seriously wrong!<br /> &gt;&gt;<br /> &gt;&gt; Thanks<br /> &gt;&gt;<br /> &gt;&gt; Sergio<br /> &gt;<br /> <br /> &gt; --<br /> <br /> &gt; Best,<br /> &gt; Max Breitmeyer<br /> &gt; DOIT HPC System Administrator<br /> <br /> --<br /> <br /> Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator<br /> &nbsp;</blockquote> </div>  <div>&nbsp;</div>  <div>&nbsp;</div> --  <div> <div> <div> <div> <div> <div> <div>----------------------------------------------------------------------= -----------------------------------------------<br /> Sergio DeSouza-Machado&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sergio@umbc.e= du<br /> Research Assoc. Professor,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&= nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbs= p;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&= nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (W)=  410-455-1944<br /> JCET/Dept of Physics&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp= ;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n= bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (F) 410-455-1072</div>  <div>UMBC, Baltimore MD 21250</div> </div> </div> </div> </div> </div> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3301564,72605706,Correspond,DoIT-Research-Computing,2025-10-30 18:16:49.0000000,HPC Other Issue: very slow editing on /home/sergio on c24-52,open,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"Hi Max,  OK al logged outta chip and our machine  -Sergio   On Thu, Oct 30, 2025 at 2:10=E2=80=AFPM Max Breitmeyer via RT <UMBCHelp@rt.= umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3301564 > > > Last Update From Ticket: > > Hi Sergio, > > Please exit out of /umbc/xfs2/strow on all devices where you may be logged > in. > We can't unmount the device until it's no longer active. > > On Thu Oct 30 13:44:40 2025, VR64161 wrote: > > > Hi Max > > > Thanks for the reply. Maybe you should also cc Larrabee as you all work > on > > this (strow@umbc.edu) Sergio On Thu, Oct 30, 2025 at 1:30 PM Max > Breitmeyer > > via RT <UMBCHelp@rt.umbc.edu> wrote: > > >> Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3301564 > > > >> Last Update From Ticket: > > >> Sergio, > > >> There is currently some degradation on the file system that sits on > >> xfs2. We're > >> investigating into how to resolve, but out of an abundance of caution, > >> we are > >> going to unmount xfs2 so that the degradation doesn't spread. We'll let > >> you > >> know when we have more information for you, but expect to not have > >> access to > >> your mount for a while. > > >> On Thu Oct 30 13:23:27 2025, OL73413 wrote: > > >> > Hi Sergio, > > >> > We are aware of the issue and are currently investigating. I'm going > >> to > >> > merge this with the other ticket since they are related. > > >> > On Thu Oct 30 12:53:30 2025, ZZ99999 wrote: > > >> >> First Name: Sergio > >> >> Last Name: De souza-machad > >> >> Email: sergio@umbc.edu > >> >> Campus ID: VR64161 > >> >> > >> >> Request Type: High Performance Cluster > >> >> > >> >> Have the disk issues been worked on yet (ticket #3301564)? I still > >> can't edit files > >> >> > >> >> And now Matlab took about 3 minutes to start up on c24-52 > >> >> > >> >> There is something seriously wrong! > >> >> > >> >> Thanks > >> >> > >> >> Sergio > >> > > > >> > -- > > >> > Best, > >> > Max Breitmeyer > >> > DOIT HPC System Administrator > > >> -- > > >> Best, > >> Max Breitmeyer > >> DOIT HPC System Administrator > > > -- > > > -------------------------------------------------------------------------= -------------------------------------------- > > Sergio DeSouza-Machado sergio@umbc.edu > > Research Assoc. Professor, (W) 410-455-1944 > > JCET/Dept of Physics (F) 410-455-1072UMBC, Baltimore MD 21250 > > -- > > Best, > Max Breitmeyer > DOIT HPC System Administrator > >  --=20 ---------------------------------------------------------------------------= ------------------------------------------ Sergio DeSouza-Machado sergio@umbc.edu Research Assoc. Professor,                                              (W) 410-455-1944 JCET/Dept of Physics (F) 410-455-1072 UMBC, Baltimore MD 21250 "
3301564,72605706,Correspond,DoIT-Research-Computing,2025-10-30 18:16:49.0000000,HPC Other Issue: very slow editing on /home/sergio on c24-52,open,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Sergio De souza-machad,sergio@umbc.edu,"<div dir=3D""ltr"">Hi Max,<div><br></div><div>OK al logged outta chip and our=  machine</div><div><br></div><div>-Sergio</div><div><br></div></div><br><di= v class=3D""gmail_quote gmail_quote_container""><div dir=3D""ltr"" class=3D""gma= il_attr"">On Thu, Oct 30, 2025 at 2:10=E2=80=AFPM Max Breitmeyer via RT &lt;= <a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt; wrote:= <br></div><blockquote class=3D""gmail_quote"" style=3D""margin:0px 0px 0px 0.8= ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"">Ticket &lt;URL:=  <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D3301564"" rel=3D""no= referrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D33= 01564</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Sergio,<br> <br> Please exit out of /umbc/xfs2/strow on all devices where you may be logged = in.<br> We can&#39;t unmount the device until it&#39;s no longer active.<br> <br> On Thu Oct 30 13:44:40 2025, VR64161 wrote:<br> <br> &gt; Hi Max<br> <br> &gt; Thanks for the reply. Maybe you should also cc Larrabee as you all wor= k on<br> &gt; this (<a href=3D""mailto:strow@umbc.edu"" target=3D""_blank"">strow@umbc.e= du</a>) Sergio On Thu, Oct 30, 2025 at 1:30 PM Max Breitmeyer<br> &gt; via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"" target=3D""_blank"">U= MBCHelp@rt.umbc.edu</a>&gt; wrote:<br> <br> &gt;&gt; Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html= ?id=3D3301564"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Tic= ket/Display.html?id=3D3301564</a> &gt;<br> <br> &gt;&gt; Last Update From Ticket:<br> <br> &gt;&gt; Sergio,<br> <br> &gt;&gt; There is currently some degradation on the file system that sits o= n<br> &gt;&gt; xfs2. We&#39;re<br> &gt;&gt; investigating into how to resolve, but out of an abundance of caut= ion,<br> &gt;&gt; we are<br> &gt;&gt; going to unmount xfs2 so that the degradation doesn&#39;t spread. = We&#39;ll let<br> &gt;&gt; you<br> &gt;&gt; know when we have more information for you, but expect to not have= <br> &gt;&gt; access to<br> &gt;&gt; your mount for a while.<br> <br> &gt;&gt; On Thu Oct 30 13:23:27 2025, OL73413 wrote:<br> <br> &gt;&gt; &gt; Hi Sergio,<br> <br> &gt;&gt; &gt; We are aware of the issue and are currently investigating. I&= #39;m going<br> &gt;&gt; to<br> &gt;&gt; &gt; merge this with the other ticket since they are related.<br> <br> &gt;&gt; &gt; On Thu Oct 30 12:53:30 2025, ZZ99999 wrote:<br> <br> &gt;&gt; &gt;&gt; First Name: Sergio<br> &gt;&gt; &gt;&gt; Last Name: De souza-machad<br> &gt;&gt; &gt;&gt; Email: <a href=3D""mailto:sergio@umbc.edu"" target=3D""_blan= k"">sergio@umbc.edu</a><br> &gt;&gt; &gt;&gt; Campus ID: VR64161<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; Request Type: High Performance Cluster<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; Have the disk issues been worked on yet (ticket #3301564)= ? I still<br> &gt;&gt; can&#39;t edit files<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; And now Matlab took about 3 minutes to start up on c24-52= <br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; There is something seriously wrong!<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; Thanks<br> &gt;&gt; &gt;&gt;<br> &gt;&gt; &gt;&gt; Sergio<br> &gt;&gt; &gt;<br> <br> &gt;&gt; &gt; --<br> <br> &gt;&gt; &gt; Best,<br> &gt;&gt; &gt; Max Breitmeyer<br> &gt;&gt; &gt; DOIT HPC System Administrator<br> <br> &gt;&gt; --<br> <br> &gt;&gt; Best,<br> &gt;&gt; Max Breitmeyer<br> &gt;&gt; DOIT HPC System Administrator<br> <br> &gt; --<br> &gt; ----------------------------------------------------------------------= -----------------------------------------------<br> &gt; Sergio DeSouza-Machado <a href=3D""mailto:sergio@umbc.edu"" target=3D""_b= lank"">sergio@umbc.edu</a><br> &gt; Research Assoc. Professor, (W) 410-455-1944<br> &gt; JCET/Dept of Physics (F) 410-455-1072UMBC, Baltimore MD 21250<br> <br> --<br> <br> Best,<br> Max Breitmeyer<br> DOIT HPC System Administrator<br> <br> </blockquote></div><div><br clear=3D""all""></div><div><br></div><span class= =3D""gmail_signature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_s= ignature""><div dir=3D""ltr""><div><div dir=3D""ltr""><div><div dir=3D""ltr""><div= >--------------------------------------------------------------------------= -------------------------------------------<br>Sergio DeSouza-Machado=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 <a href=3D""mailto:sergio@umbc.edu"" target= =3D""_blank"">sergio@umbc.edu</a><br>Research Assoc. Professor,=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 (W) 410-455-1944<br>JCET/Dept of Physics= =C2=A0=C2=A0 =C2=A0=C2=A0 =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0= =C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2= =A0=C2=A0=C2=A0 (F) 410-455-1072<br></div><div>UMBC, Baltimore MD 21250<br>= </div></div></div></div></div></div></div> "
3302104,72564173,Create,DoIT-Research-Computing,2025-10-29 18:22:27.0000000,HPC Other Issue: Need cnDNN 8.4.1.50 module on CHIP,resolved,Beamlak Bekele,bbekele1,Quang Dang,qdang1,qdang1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Quang<br /> Last Name:                 Dang<br /> Email:                     qdang1@umbc.edu<br /> Campus ID:                 OG93134<br /> <br /> Request Type:              High Performance Cluster<br /> <br /> Hi, <br /> I need to work on TensorFlow 2.11 which using CUDA 11.7 version and cuDNN 8.4. I saw CUDA 11.7, however there is no cuDNN 8.4. The previous cluster (ADA) have module 'cuDNN/8.4.1.50-CUDA-11.7.0'. Can you install that module on CHIP also? <br />  <br /> Thank you so much, <br /> Quang Dang<br /> <br /> Attachment 1: <a href=""https://umbc.box.com/s/yo8dsku43hd0lobsci6e2a0i3rqvddjs"" target=""_blank"">Screenshot 2025-10-29 141531.png</a><br /> "
3302104,72622232,Correspond,DoIT-Research-Computing,2025-10-31 15:07:37.0000000,HPC Other Issue: Need cnDNN 8.4.1.50 module on CHIP,resolved,Beamlak Bekele,bbekele1,Quang Dang,qdang1,qdang1@umbc.edu,Beamlak Bekele,bbekele1@umbc.edu,"<div> <p>Hello&nbsp;</p>  <p>I have install the module cuDNN/8.4.1.50-CUDA-11.7.0,&nbsp; let us know if you still unable to find the module.&nbsp;</p>  <p>&nbsp;</p>  <p>&nbsp;</p>  <p>On Wed Oct 29 14:22:27 2025, ZZ99999 wrote:</p>  <blockquote>First Name: Quang<br /> Last Name: Dang<br /> Email: qdang1@umbc.edu<br /> Campus ID: OG93134<br /> <br /> Request Type: High Performance Cluster<br /> <br /> Hi,<br /> I need to work on TensorFlow 2.11 which using CUDA 11.7 version and cuDNN 8.4. I saw CUDA 11.7, however there is no cuDNN 8.4. The previous cluster (ADA) have module &#39;cuDNN/8.4.1.50-CUDA-11.7.0&#39;. Can you install that module on CHIP also?<br /> <br /> Thank you so much,<br /> Quang Dang<br /> <br /> Attachment 1: Screenshot 2025-10-29 141531.png</blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p><br /> Best,<br /> Beamlak Bekele<br /> DOIT Unix infra, Graduate Assistant</p> "
3304171,72602851,Create,DoIT-Research-Computing,2025-10-30 16:53:30.0000000,"HPC Other Issue: so slooooooooow, can't get anything done",open,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Sergio Last Name:                 De souza-machad Email:                     sergio@umbc.edu Campus ID:                 VR64161  Request Type:              High Performance Cluster  Have the disk issues been worked on yet (ticket #3301564)? I still can't edit files  And now Matlab took about 3 minutes to start up on c24-52  There is something seriously wrong!  Thanks  Sergio  "
3304171,72603896,Correspond,DoIT-Research-Computing,2025-10-30 17:23:27.0000000,"HPC Other Issue: so slooooooooow, can't get anything done",open,Max Breitmeyer,mb17,Sergio De souza-machad,sergio,sergio@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Sergio,</p>  <p>We are aware of the issue and are currently investigating. I&#39;m going to merge this with the other ticket since they are related.&nbsp;</p>  <p>On Thu Oct 30 12:53:30 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Sergio Last Name:                 De souza-machad Email:                     sergio@umbc.edu Campus ID:                 VR64161  Request Type:              High Performance Cluster  Have the disk issues been worked on yet (ticket #3301564)? I still can&#39;t edit files  And now Matlab took about 3 minutes to start up on c24-52  There is something seriously wrong!  Thanks  Sergio  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3304395,72608784,Create,DoIT-Research-Computing,2025-10-30 19:40:39.0000000,HPC New Group: pi_tatsuya,resolved,Danielle Esposito,desposi1,Tatsuya Ogura,tatsuya,tatsuya@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Tatsuya Last Name:                 Ogura Email:                     tatsuya@umbc.edu Campus ID:                 RX81928  Request Type:              High Performance Cluster  Group Type:            Project Title:        Multiscale Behavior Mapping to Decode Neuronal Health and Diseases in Animal Models Project Abstract:     Automated, high-fidelity behavioral phenotyping is essential for uncover the underlying neural circuits and functional perturbations in vivo. Yet current computer-vision tools often miss the subtle, ethologically relevant kinematic motifs that report changes in internal state. In this project, we will build a neuroscience-focused computational pipeline that couples the broad foundation model with the efficient and specialized detector to identify the animal behavior responded to sensory signal inputs. Specifically, we will distill knowledge from Vision Transformers into customized single-state Convolutional Neural Networks (CNNs) and pretrain these networks on a large, unlabeled corpus of murine behavioral videos spanning diverse common behavior experimental assays. The resulting models will be fine-tuned for high-throughput quantification of classified behavior phenotypes/signatures that serve as proxies for cognitive and affective states.  This pipeline will enable experimenters to (i) stratify behavior with ethological precision, (ii) align these phenotypes with neural activities or perturbations by endogenous and environmental factors, and (iii) standardize assays across labs for reproducible neuroscience. Training and optimization demand large-scale data parallelism and extensive hyperparameter searches; thus, access to the UMBC HPC cluster is indispensable. This work will yield a biologically grounded, scalable toolset for objective behavior measurement in animal models, such as mice.     The PI of this project is Dr. Weihong Lin, a Professor at Department of Biological Sciences (weihong@umbc.edu). We would like to setup a new group to use HPC and data storage space.  The Lin lab recently published an analysis pipeline for animal behavior and benchmark comparison with other available tools (IntegraPose: A unified framework for simultaneous pose estimation and behavior classification.  https://doi.org/10.1016/j.neuroscience.2025.10.020).  We need to scale up the model by training with a larger set of image data using the HPC.  Our project members include UMBC students who are familiar with script coding.  The project title and the abstract are included in this request.  Please let us know if you have any questions. Thank you.  "
3304395,72643338,Correspond,DoIT-Research-Computing,2025-10-31 19:53:50.0000000,HPC New Group: pi_tatsuya,resolved,Danielle Esposito,desposi1,Tatsuya Ogura,tatsuya,tatsuya@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Weihong,</p>  <p>Welcome to chip, UMBC&#39;s High Performance Computing Cluster!</p>  <p>&nbsp; &nbsp; The group, pi_tatsuya, now exists on the chip cluster. Members of this group can access and contribute to the research storage space allocated to the group.</p>  <p>&nbsp; &nbsp; This storage space is located at /umbc/rs/pi_tatsuya, and currently has a quota of 10T.</p>  <p>&nbsp; &nbsp; For information on accessing the cluster, adding accounts to your group, and getting started using the cluster, check out the tutorial on our wiki: https://umbc.atlassian.net/wiki/x/R4BPQg</p>  <p>An account for the PI (weihong) has been created. To add additional users to the group, please submit a new add user ticket.</p>  <p>&nbsp; &nbsp; Additional documentation is also available here: https://umbc.atlassian.net/wiki/x/FwCHQ</p>  <p>&nbsp; &nbsp; If you have any questions or issues, please submit a new RT ticket at: https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research%20Computing</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Thu Oct 30 15:40:39 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Tatsuya Last Name:                 Ogura Email:                     tatsuya@umbc.edu Campus ID:                 RX81928  Request Type:              High Performance Cluster  Group Type:            Project Title:        Multiscale Behavior Mapping to Decode Neuronal Health and Diseases in Animal Models Project Abstract:     Automated, high-fidelity behavioral phenotyping is essential for uncover the underlying neural circuits and functional perturbations in vivo. Yet current computer-vision tools often miss the subtle, ethologically relevant kinematic motifs that report changes in internal state. In this project, we will build a neuroscience-focused computational pipeline that couples the broad foundation model with the efficient and specialized detector to identify the animal behavior responded to sensory signal inputs. Specifically, we will distill knowledge from Vision Transformers into customized single-state Convolutional Neural Networks (CNNs) and pretrain these networks on a large, unlabeled corpus of murine behavioral videos spanning diverse common behavior experimental assays. The resulting models will be fine-tuned for high-throughput quantification of classified behavior phenotypes/signatures that serve as proxies for cognitive and affective states.  This pipeline will enable experimenters to (i) stratify behavior with ethological precision, (ii) align these phenotypes with neural activities or perturbations by endogenous and environmental factors, and (iii) standardize assays across labs for reproducible neuroscience. Training and optimization demand large-scale data parallelism and extensive hyperparameter searches; thus, access to the UMBC HPC cluster is indispensable. This work will yield a biologically grounded, scalable toolset for objective behavior measurement in animal models, such as mice.     The PI of this project is Dr. Weihong Lin, a Professor at Department of Biological Sciences (weihong@umbc.edu). We would like to setup a new group to use HPC and data storage space.  The Lin lab recently published an analysis pipeline for animal behavior and benchmark comparison with other available tools (IntegraPose: A unified framework for simultaneous pose estimation and behavior classification.  https://doi.org/10.1016/j.neuroscience.2025.10.020).  We need to scale up the model by training with a larger set of image data using the HPC.  Our project members include UMBC students who are familiar with script coding.  The project title and the abstract are included in this request.  Please let us know if you have any questions. Thank you.  </pre> </blockquote> </div> "
3304442,72610023,Create,DoIT-Research-Computing,2025-10-30 20:07:02.0000000,HPC Other Issue: more hardware details,stalled,Max Breitmeyer,mb17,Matthias Gobbert,gobbert,gobbert@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Matthias Last Name:                 Gobbert Email:                     gobbert@umbc.edu Campus ID:                 AX68683  Request Type:              High Performance Cluster  Hi,  I would like some more fine points of hardware information. I am referring to the 2024 portion of chip.  - CPU: what is the model number of the node from Dell for the 2024 compute nodes?  - The 2024 CPU nodes are connected by which InfiniBand? EDR? Some specs available?  Matthias  "
3304442,72612608,Correspond,DoIT-Research-Computing,2025-10-30 23:03:15.0000000,HPC Other Issue: more hardware details,stalled,Max Breitmeyer,mb17,Matthias Gobbert,gobbert,gobbert@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Matthias,</p>  <p>The model for the nodes used in the 2024 are PowerEdge R660.</p>  <p>Specifications on the network abilities and cpus themselves can be found here:&nbsp;</p>  <p>https://umbc.atlassian.net/wiki/spaces/faq/pages/1289486353/Cluster+Specifications</p>  <p>On Thu Oct 30 16:07:02 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Matthias Last Name:                 Gobbert Email:                     gobbert@umbc.edu Campus ID:                 AX68683  Request Type:              High Performance Cluster  Hi,  I would like some more fine points of hardware information. I am referring to the 2024 portion of chip.  - CPU: what is the model number of the node from Dell for the 2024 compute nodes?  - The 2024 CPU nodes are connected by which InfiniBand? EDR? Some specs available?  Matthias  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3304442,72613557,Correspond,DoIT-Research-Computing,2025-10-31 01:44:22.0000000,HPC Other Issue: more hardware details,stalled,Max Breitmeyer,mb17,Matthias Gobbert,gobbert,gobbert@umbc.edu,Matthias Gobbert,gobbert@umbc.edu,"Hi, Max,  Thanks for the Dell model.  Yes, I read that Wiki page. My request is to know more precisely than ""high speed backend Infiniband network supporting 100 Gbps"". That phrase is well-written to cover all nodes from 2018 to 2024. But is the 2024 network not a newer one? Purchased in 2024? Either way, I am asking for a technical term like DDR =3D dual data rate, QDR =3D quad data rate, EDR =3D extended = data rate, like I am recalling from the past. Does the 2024 portion's network not have a phrase like that with it?  Matthias  Matthias K. Gobbert, Ph.D., Professor of Mathematics Department of Mathematics and Statistics Center for Interdisciplinary Research and Consulting (circ.umbc.edu) UMBC High Performance Computing Facility (hpcf.umbc.edu) REU Site: Online Interdisciplinary Big Data Analytics (BigDataREU.umbc.edu <http://bigdatareu.umbc.edu>) University of Maryland, Baltimore County 1000 Hilltop Circle, Baltimore, MD 21250 http://www.umbc.edu/~gobbert   On Thu, Oct 30, 2025 at 7:03=E2=80=AFPM Max Breitmeyer via RT <UMBCHelp@rt.= umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3304442 > > > Last Update From Ticket: > > Hi Matthias, > > The model for the nodes used in the 2024 are PowerEdge R660. > > Specifications on the network abilities and cpus themselves can be found > here: > > > https://umbc.atlassian.net/wiki/spaces/faq/pages/1289486353/Cluster+Speci= fications > > On Thu Oct 30 16:07:02 2025, ZZ99999 wrote: > > > First Name:                Matthias > > Last Name:                 Gobbert > > Email:                     gobbert@umbc.edu > > Campus ID:                 AX68683 > > > > Request Type:              High Performance Cluster > > > > Hi, > > > > I would like some more fine points of hardware information. > > I am referring to the 2024 portion of chip. > > > > - CPU: what is the model number of the node from Dell for the 2024 > compute nodes? > > > > - The 2024 CPU nodes are connected by which InfiniBand? EDR? Some specs > available? > > > > Matthias > > > -- > > Best, > Max Breitmeyer > DOIT HPC System Administrator > > "
3304442,72613557,Correspond,DoIT-Research-Computing,2025-10-31 01:44:22.0000000,HPC Other Issue: more hardware details,stalled,Max Breitmeyer,mb17,Matthias Gobbert,gobbert,gobbert@umbc.edu,Matthias Gobbert,gobbert@umbc.edu,"<div dir=3D""ltr""><div>Hi, Max,</div><div><br></div><div>Thanks for the Dell=  model.</div><div><br></div><div>Yes, I read that Wiki page. My request is = to know more precisely than &quot;high speed backend Infiniband network sup= porting 100 Gbps&quot;. That phrase is well-written to cover all nodes from=  2018 to 2024. But is the 2024 network not a newer one? Purchased in 2024? = Either way, I am asking for a technical term like DDR =3D dual data rate, Q= DR =3D quad data rate, EDR =3D extended data rate, like I am recalling from=  the past. Does the 2024 portion&#39;s=C2=A0network not have a phrase like = that with it?<br><br>Matthias</div><div><div dir=3D""ltr"" class=3D""gmail_sig= nature"" data-smartmail=3D""gmail_signature""><div dir=3D""ltr""><div><br></div>= <div>Matthias K. Gobbert, Ph.D., Professor of Mathematics</div><div>Departm= ent of Mathematics and Statistics</div><div>Center for Interdisciplinary Re= search and Consulting (<a href=3D""http://circ.umbc.edu"" target=3D""_blank"">c= irc.umbc.edu</a>)</div><div>UMBC High Performance Computing Facility (<a hr= ef=3D""http://hpcf.umbc.edu"" target=3D""_blank"">hpcf.umbc.edu</a>)</div><div>= REU Site: Online Interdisciplinary Big Data Analytics (<a href=3D""http://bi= gdatareu.umbc.edu"" target=3D""_blank"">BigDataREU.umbc.edu</a>)</div><div>Uni= versity of Maryland, Baltimore County</div><div>1000 Hilltop Circle, Baltim= ore, MD 21250</div><div><a href=3D""http://www.umbc.edu/~gobbert"" target=3D""= _blank"">http://www.umbc.edu/~gobbert</a></div></div></div></div><br></div><= br><div class=3D""gmail_quote gmail_quote_container""><div dir=3D""ltr"" class= =3D""gmail_attr"">On Thu, Oct 30, 2025 at 7:03=E2=80=AFPM Max Breitmeyer via = RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.umbc.edu</a>&gt;=  wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D""margin:0px 0px = 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"">Ticket &= lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D3304442"" re= l=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?= id=3D3304442</a> &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Matthias,<br> <br> The model for the nodes used in the 2024 are PowerEdge R660.<br> <br> Specifications on the network abilities and cpus themselves can be found he= re:<br> <br> <a href=3D""https://umbc.atlassian.net/wiki/spaces/faq/pages/1289486353/Clus= ter+Specifications"" rel=3D""noreferrer"" target=3D""_blank"">https://umbc.atlas= sian.net/wiki/spaces/faq/pages/1289486353/Cluster+Specifications</a><br> <br> On Thu Oct 30 16:07:02 2025, ZZ99999 wrote:<br> <br> &gt; First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Mat= thias<br> &gt; Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0Gobbert<br> &gt; Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 = =C2=A0 =C2=A0<a href=3D""mailto:gobbert@umbc.edu"" target=3D""_blank"">gobbert@= umbc.edu</a><br> &gt; Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0AX68683<br> &gt; <br> &gt; Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Per= formance Cluster<br> &gt; <br> &gt; Hi,<br> &gt; <br> &gt; I would like some more fine points of hardware information.<br> &gt; I am referring to the 2024 portion of chip.<br> &gt; <br> &gt; - CPU: what is the model number of the node from Dell for the 2024 com= pute nodes?<br> &gt; <br> &gt; - The 2024 CPU nodes are connected by which InfiniBand? EDR? Some spec= s available?<br> &gt; <br> &gt; Matthias<br> <br> <br> --<br> <br> Best,<br> Max Breitmeyer<br> DOIT HPC System Administrator<br> <br> </blockquote></div> "
3304442,72705445,Correspond,DoIT-Research-Computing,2025-11-04 14:26:41.0000000,HPC Other Issue: more hardware details,stalled,Max Breitmeyer,mb17,Matthias Gobbert,gobbert,gobbert@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Everything is EDR.</p>  <p>On Thu Oct 30 21:44:22 2025, AX68683 wrote:</p>  <blockquote> <div> <div>Hi, Max,</div>  <div>&nbsp;</div>  <div>Thanks for the Dell model.</div>  <div>&nbsp;</div>  <div>Yes, I read that Wiki page. My request is to know more precisely than = &quot;high speed backend Infiniband network supporting 100 Gbps&quot;. That=  phrase is well-written to cover all nodes from 2018 to 2024. But is the 20= 24 network not a newer one? Purchased in 2024? Either way, I am asking for = a technical term like DDR =3D dual data rate, QDR =3D quad data rate, EDR = =3D extended data rate, like I am recalling from the past. Does the 2024 po= rtion&#39;s&nbsp;network not have a phrase like that with it?<br /> <br /> Matthias</div>  <div> <div> <div> <div>&nbsp;</div>  <div>Matthias K. Gobbert, Ph.D., Professor of Mathematics</div>  <div>Department of Mathematics and Statistics</div>  <div>Center for Interdisciplinary Research and Consulting (circ.umbc.edu)</= div>  <div>UMBC High Performance Computing Facility (hpcf.umbc.edu)</div>  <div>REU Site: Online Interdisciplinary Big Data Analytics (BigDataREU.umbc= .edu)</div>  <div>University of Maryland, Baltimore County</div>  <div>1000 Hilltop Circle, Baltimore, MD 21250</div>  <div>http://www.umbc.edu/~gobbert</div> </div> </div> </div> </div> &nbsp;  <div> <div>On Thu, Oct 30, 2025 at 7:03=E2=80=AFPM Max Breitmeyer via RT &lt;UMBC= Help@rt.umbc.edu&gt; wrote:</div>  <blockquote>Ticket &lt;URL: https://rt.umbc.edu/Ticket/Display.html?id=3D33= 04442 &gt;<br /> <br /> Last Update From Ticket:<br /> <br /> Hi Matthias,<br /> <br /> The model for the nodes used in the 2024 are PowerEdge R660.<br /> <br /> Specifications on the network abilities and cpus themselves can be found he= re:<br /> <br /> https://umbc.atlassian.net/wiki/spaces/faq/pages/1289486353/Cluster+Specifi= cations<br /> <br /> On Thu Oct 30 16:07:02 2025, ZZ99999 wrote:<br /> <br /> &gt; First Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Mat= thias<br /> &gt; Last Name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p;Gobbert<br /> &gt; Email:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &= nbsp; &nbsp;gobbert@umbc.edu<br /> &gt; Campus ID:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbs= p;AX68683<br /> &gt;<br /> &gt; Request Type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; High Per= formance Cluster<br /> &gt;<br /> &gt; Hi,<br /> &gt;<br /> &gt; I would like some more fine points of hardware information.<br /> &gt; I am referring to the 2024 portion of chip.<br /> &gt;<br /> &gt; - CPU: what is the model number of the node from Dell for the 2024 com= pute nodes?<br /> &gt;<br /> &gt; - The 2024 CPU nodes are connected by which InfiniBand? EDR? Some spec= s available?<br /> &gt;<br /> &gt; Matthias<br /> <br /> <br /> --<br /> <br /> Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator<br /> &nbsp;</blockquote> </div> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3304534,72612198,Create,DoIT-Research-Computing,2025-10-30 21:52:57.0000000,HPC Other Issue: Reset My Password,resolved,Max Breitmeyer,mb17,Hossein Pourmehrani,hosseip1,hosseip1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Hossein Last Name:                 Pourmehrani Email:                     hosseip1@umbc.edu Campus ID:                 DI13855  Request Type:              High Performance Cluster  Dear Sir/Madam,  I would like to access the chip server, but after logging in with my ID =E2= =80=9Chosseip1=E2=80=9D, my password does not work. I am not sure of the re= ason. Could you please provide me with a link or instructions to reset my p= assword?  Thank you very much for your assistance.  Regards, Hossein Pourmehrani ID: DI13855=20  "
3304534,72612575,Correspond,DoIT-Research-Computing,2025-10-30 22:55:41.0000000,HPC Other Issue: Reset My Password,resolved,Max Breitmeyer,mb17,Hossein Pourmehrani,hosseip1,hosseip1@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Hossein,</p>  <p>Your password is linked to your umbc account. So you would just need to use the myumbc reset password.&nbsp;https://umbc.atlassian.net/wiki/spaces/faq/pages/30736467/I+have+forgotten+my+myUMBC+password.+What+should+I+do</p>  <p>Note that it may take a minute to update on chip.&nbsp;</p>  <p>On Thu Oct 30 17:52:57 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Hossein Last Name:                 Pourmehrani Email:                     hosseip1@umbc.edu Campus ID:                 DI13855  Request Type:              High Performance Cluster  Dear Sir/Madam,  I would like to access the chip server, but after logging in with my ID &ldquo;hosseip1&rdquo;, my password does not work. I am not sure of the reason. Could you please provide me with a link or instructions to reset my password?  Thank you very much for your assistance.  Regards, Hossein Pourmehrani ID: DI13855   </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3304703,72614466,Create,DoIT-Research-Computing,2025-10-31 09:37:55.0000000,HPC Other Issue: Request for sample SLURM job script to run WRF on CHIP,resolved,Danielle Esposito,desposi1,Nathaniel Nwoke,md43972,md43972@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Nathaniel Last Name:                 Nwoke Email:                     md43972@umbc.edu Campus ID:                 MD43972  Request Type:              High Performance Cluster  Hi HPC Team,  I=E2=80=99m a graduate student working on the WRF model on the UMBC CHIP cl= uster. I was advised that running ./wrf.exe interactively is not permitted = and that I should submit the run via SLURM because of the job=E2=80=99s siz= e and runtime.  Could you please provide (or point me to) a sample SLURM job script for a p= arallel WRF run on CHIP? A template with the recommended directives and mod= ule loads would be very helpful. Specifically, I=E2=80=99m looking for guid= ance on:For context (in case it helps tailor the template):  Executable(s): real.exe and wrf.exe (WRF-ARW)  Input: single domain (d01) for a short 48-hour case; netCDF met_em and boun= dary files are ready  Code location: /umbc/rs/pi_cichoku/users/md43972/model2/sources/WRF/test/em= _real/  I can rebuild with the cluster=E2=80=99s preferred MPI stack if needed.  If there=E2=80=99s existing documentation or example scripts for CHIP users=  running WRF (or other large MPI jobs), a link would be great.  Thank you very much for your help!  Best regards, Nathan Nwoke Graduate Student, GES UMBC  "
3304703,72621395,Correspond,DoIT-Research-Computing,2025-10-31 14:44:35.0000000,HPC Other Issue: Request for sample SLURM job script to run WRF on CHIP,resolved,Danielle Esposito,desposi1,Nathaniel Nwoke,md43972,md43972@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Nathan,</p>  <p>First off, I wanted to clarify a couple points.&nbsp;</p>  <p>1. &quot;I was advised that running ./wrf.exe interactively is not permitted&quot;: This is untrue. You are able to run interactive jobs, where you create a slurm allocation on a node, then connect to it and run your code directly. You are not permitted to run jobs on the&nbsp;<strong>login node,&nbsp;</strong>which I believe is what you are thinking of. If you would like some documentation on how to run an interactive job, ill include a link to a page with the commands required to do that.&nbsp;</p>  <p>How to run interactive job:&nbsp;https://umbc.atlassian.net/wiki/x/CYCbQw</p>  <p>2. There is also a WRF module available to be loaded on chip. You can load WRF with: &#39;module load WRF/4.4-foss-2022a-dmpar&#39;. WPS is also available, and can be loaded with: &#39;module load&nbsp;WPS/4.4-foss-2022a-dmpar&#39;. These installations should work, however it is a slightly older version (4.4), whereas you compiled a newer one (4.5).</p>  <p>3. For your locally installed and compiled copy of WRF, if you desire to use it you must use the compiled binaries located under&nbsp;/umbc/rs/pi_cichoku/users/md43972/model2/sources/WRF/main. This is where the actual WRF binaries are stored, the directory you provided was a test directory for testing the software. You will want to add this location to your PATH environment variable to utilize the binaries. It is also possible, depending on how your install was compiled, that it would need to be recompiled for MPI support. For more info, check out the Building WRF section of this page:&nbsp;https://www2.mmm.ucar.edu/wrf/OnLineTutorial/compilation_tutorial.php</p>  <p>We do not have any existing documentation on running WRF/WPS explicitly, however we have documentation on running MPI jobs, including some example SBATCH scripts:&nbsp;https://umbc.atlassian.net/wiki/x/AQCKV</p>  <p>There is documentation on WRF&#39;s website for running WRF using mpi, take a look here:&nbsp;https://www2.mmm.ucar.edu/wrf/users/wrf_users_guide/build/html/running_wrf.html</p>  <p>If you have any additional questions or need clarification, please feel free to let me know. For now I will mark this as resolved. Have a good day!</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Fri Oct 31 05:37:55 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Nathaniel Last Name:                 Nwoke Email:                     md43972@umbc.edu Campus ID:                 MD43972  Request Type:              High Performance Cluster  Hi HPC Team,  I&rsquo;m a graduate student working on the WRF model on the UMBC CHIP cluster. I was advised that running ./wrf.exe interactively is not permitted and that I should submit the run via SLURM because of the job&rsquo;s size and runtime.  Could you please provide (or point me to) a sample SLURM job script for a parallel WRF run on CHIP? A template with the recommended directives and module loads would be very helpful. Specifically, I&rsquo;m looking for guidance on:For context (in case it helps tailor the template):  Executable(s): real.exe and wrf.exe (WRF-ARW)  Input: single domain (d01) for a short 48-hour case; netCDF met_em and boundary files are ready  Code location: /umbc/rs/pi_cichoku/users/md43972/model2/sources/WRF/test/em_real/  I can rebuild with the cluster&rsquo;s preferred MPI stack if needed.  If there&rsquo;s existing documentation or example scripts for CHIP users running WRF (or other large MPI jobs), a link would be great.  Thank you very much for your help!  Best regards, Nathan Nwoke Graduate Student, GES UMBC  </pre> </blockquote> </div> "
3305879,72641289,Create,DoIT-Research-Computing,2025-10-31 19:04:48.0000000,HPC Other Issue: Not able to login to the cluster (chip) through terminal,open,Max Breitmeyer,mb17,Keerthana Kanike,kkanike1,kkanike1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Keerthana<br /> Last Name:                 Kanike<br /> Email:                     kkanike1@umbc.edu<br /> Campus ID:                 YS76658<br /> <br /> Request Type:              High Performance Cluster<br /> <br /> Previously, I was able to login through the cluster (chip) but suddenly the login to cluster is not working. <br /> <br /> Attachment 1: <a href=""https://umbc.box.com/s/rizemfd2xxfhqz6zpc2bym8www52ylyf"" target=""_blank"">Screenshot 2025-10-31 at 3.03.21PM.png</a><br /> "
3305879,72670792,Correspond,DoIT-Research-Computing,2025-11-03 16:00:58.0000000,HPC Other Issue: Not able to login to the cluster (chip) through terminal,open,Max Breitmeyer,mb17,Keerthana Kanike,kkanike1,kkanike1@umbc.edu,Max Breitmeyer,mb17@umbc.edu,"<div> <p>Hi Keerthana,</p>  <p>Is this still an issue? We haven&#39;t seen any other reports of people being unable to login. Are you on campus vpn? What sort of internet connection do you have (wifi or wired)?</p>  <p>On Fri Oct 31 15:04:48 2025, ZZ99999 wrote:</p>  <blockquote>First Name: Keerthana<br /> Last Name: Kanike<br /> Email: kkanike1@umbc.edu<br /> Campus ID: YS76658<br /> <br /> Request Type: High Performance Cluster<br /> <br /> Previously, I was able to login through the cluster (chip) but suddenly the login to cluster is not working.<br /> <br /> Attachment 1: Screenshot 2025-10-31 at 3.03.21PM.png</blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Best,<br /> Max Breitmeyer<br /> DOIT HPC System Administrator</p> "
3305879,72731618,Correspond,DoIT-Research-Computing,2025-11-04 20:24:14.0000000,HPC Other Issue: Not able to login to the cluster (chip) through terminal,open,Max Breitmeyer,mb17,Keerthana Kanike,kkanike1,kkanike1@umbc.edu,Keerthana Kanike,kkanike1@umbc.edu,"Hi Max, Apologies for late reply Yes, still I face the issue and I tried with campus vpn and also my personal wifi. Both are giving the same problem. Could you help me in checking the issue. Thankyou for your help.  Regards, Keerthana Kanike. Campus ID: YS76658  On Mon, Nov 3, 2025 at 11:01=E2=80=AFAM Max Breitmeyer via RT <UMBCHelp@rt.= umbc.edu> wrote:  > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3305879 > > > Last Update From Ticket: > > Hi Keerthana, > > Is this still an issue? We haven't seen any other reports of people being > unable to login. Are you on campus vpn? What sort of internet connection > do you > have (wifi or wired)? > > On Fri Oct 31 15:04:48 2025, ZZ99999 wrote: > > > First Name: Keerthana > > Last Name: Kanike > > Email: kkanike1@umbc.edu > > Campus ID: YS76658 > > > Request Type: High Performance Cluster > > > Previously, I was able to login through the cluster (chip) but suddenly > the > > login to cluster is not working. > > > Attachment 1: Screenshot 2025-10-31 at 3.03.21PM.png > > -- > > Best, > Max Breitmeyer > DOIT HPC System Administrator > > "
3305879,72731618,Correspond,DoIT-Research-Computing,2025-11-04 20:24:14.0000000,HPC Other Issue: Not able to login to the cluster (chip) through terminal,open,Max Breitmeyer,mb17,Keerthana Kanike,kkanike1,kkanike1@umbc.edu,Keerthana Kanike,kkanike1@umbc.edu,"<div dir=3D""ltr"">Hi Max,=C2=A0<div>Apologies for late reply<br><div>Yes, st= ill I face the issue and I tried with campus vpn and also my personal wifi.=  Both are giving the same problem. Could you help me in checking the issue.= =C2=A0</div></div><div>Thankyou for=C2=A0your help.=C2=A0</div><div><br></d= iv><div>Regards,=C2=A0</div><div>Keerthana Kanike.</div><div>Campus ID: YS7= 6658</div></div><br><div class=3D""gmail_quote gmail_quote_container""><div d= ir=3D""ltr"" class=3D""gmail_attr"">On Mon, Nov 3, 2025 at 11:01=E2=80=AFAM Max=  Breitmeyer via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCHelp@rt.= umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" style=3D= ""margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-style:solid;bor= der-left-color:rgb(204,204,204);padding-left:1ex"">Ticket &lt;URL: <a href= =3D""https://rt.umbc.edu/Ticket/Display.html?id=3D3305879"" rel=3D""noreferrer= "" target=3D""_blank"">https://rt.umbc.edu/Ticket/Display.html?id=3D3305879</a= > &gt;<br> <br> Last Update From Ticket:<br> <br> Hi Keerthana,<br> <br> Is this still an issue? We haven&#39;t seen any other reports of people bei= ng<br> unable to login. Are you on campus vpn? What sort of internet connection do=  you<br> have (wifi or wired)?<br> <br> On Fri Oct 31 15:04:48 2025, ZZ99999 wrote:<br> <br> &gt; First Name: Keerthana<br> &gt; Last Name: Kanike<br> &gt; Email: <a href=3D""mailto:kkanike1@umbc.edu"" target=3D""_blank"">kkanike1= @umbc.edu</a><br> &gt; Campus ID: YS76658<br> <br> &gt; Request Type: High Performance Cluster<br> <br> &gt; Previously, I was able to login through the cluster (chip) but suddenl= y the<br> &gt; login to cluster is not working.<br> <br> &gt; Attachment 1: Screenshot 2025-10-31 at 3.03.21PM.png<br> <br> --<br> <br> Best,<br> Max Breitmeyer<br> DOIT HPC System Administrator<br> <br> </blockquote></div> "
3305960,72644247,Create,DoIT-Research-Computing,2025-10-31 20:25:15.0000000,HPC User Account: qiz1 in pi_kotturi,resolved,Danielle Esposito,desposi1,Qi Zhao,qiz1,qiz1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Qi Last Name:                 Zhao Email:                     qiz1@umbc.edu Campus ID:                 QQ45731  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    kotturi@umbc.edu Existing Group:       pi_kotturi Project Title:        A Human-Centered Approach to Building Generative AI S= ystem for Small Business Owners Project Abstract:     Entrepreneurs in resource-constrained communities oft= en lack the time and support to translate ideas into actionable business pl= ans.  While generative AI promises assistance, most systems assume high dig= ital literacy and overlook community infrastructures that shape adoption. W= e report on the community-centered design and deployment of BizChat, an LLM= -powered tool for business plan  development, introduced across four worksh= ops at a feminist business incubator and makerspace in a city. BizChat was = designed to center entrepreneurs=E2=80=99 knowledge and workflows while pro= viding just-in-time micro-learning and low-floor-high-ceiling accessibility= .  Through system log data (N=3D30) and semi-structured interviews (N=3D10)=  with entrepreneurs, we show how the design and deployment of  BizChat with=  existing community contexts lowered barriers to accessing capital, encoura= ged reflection, and empowered entrepreneurs  to support AI-literacy within = their own communities. We contribute insights into how AI tools can be depl= oyed within local support  networks, and implications for design that stren= gthen community resilience amid rapid technological change.  I am a phd student and my advisor is Dr. Yasmine Kotturi. I am requesting a= n account in order to join this exisiting group.  "
3305960,72711783,Correspond,DoIT-Research-Computing,2025-11-04 16:23:09.0000000,HPC User Account: qiz1 in pi_kotturi,resolved,Danielle Esposito,desposi1,Qi Zhao,qiz1,qiz1@umbc.edu,Yasmine Kotturi,kotturi@umbc.edu,"approved  On Fri, Oct 31, 2025 at 4:25=E2=80=AFPM RT API via RT <UMBCHelp@rt.umbc.edu= > wrote:  > Greetings, > > This message has been automatically generated in response to the > creation of a ticket regarding you were Cc'd on: > > ------------------------------------------------------------------------- > Subject: ""HPC User Account: qiz1 in pi_kotturi"" > > Message: > > First Name:                Qi > Last Name:                 Zhao > Email:                     qiz1@umbc.edu > Campus ID:                 QQ45731 > > Request Type:              High Performance Cluster > > Create/Modify account in existing PI group > Existing PI Email:    kotturi@umbc.edu > Existing Group:       pi_kotturi > Project Title:        A Human-Centered Approach to Building Generative AI > System for Small Business Owners > Project Abstract:     Entrepreneurs in resource-constrained communities > often lack the time and support to translate ideas into actionable busine= ss > plans.  While generative AI promises assistance, most systems assume high > digital literacy and overlook community infrastructures that shape > adoption. We report on the community-centered design and deployment of > BizChat, an LLM-powered tool for business plan  development, introduced > across four workshops at a feminist business incubator and makerspace in a > city. BizChat was designed to center entrepreneurs=E2=80=99 knowledge and=  workflows > while providing just-in-time micro-learning and low-floor-high-ceiling > accessibility.  Through system log data (N=3D30) and semi-structured > interviews (N=3D10) with entrepreneurs, we show how the design and deploy= ment > of  BizChat with existing community contexts lowered barriers to accessing > capital, encouraged reflection, and empowered entrepreneurs  to support > AI-literacy within their own communities. We contribute insights into how > AI tools can be deployed within local support  networks, and implications > for design that strengthen community resilience amid rapid technological > change. > > I am a phd student and my advisor is Dr. Yasmine Kotturi. I am requesting > an account in order to join this exisiting group. > > > ------------------------------------------------------------------------- > > There is no need to reply to this message right now. > > The ticket has been assigned an ID of [Research Computing #3305960] or you > can go there directly by clicking the link below. > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3305960 > > > You can login with your UMBC credentials to view open tickets at any time > by visiting http://my.umbc.edu and clicking on ""Help"" and ""Request Help"". > > >                         Thank you > >  --=20 Yasmine Kotturi, Ph.D. Assistant Professor, Information Systems Affiliate Faculty, Computer Science and Electrical Engineering University of Maryland, Baltimore County *CRA Trustworthy AI Research Fellow 2025-2026 <https://cra.org/cra-and-microsoft-announce-inaugural-cohort-of-cra-trustwo= rthy-ai-research-fellows/>* USM Generative AI Pedagogy Fellow 2025-2026 <https://www.usmd.edu/cai/inaugural-cohort-faculty-named-generative-ai-peda= gogy-fellows> FASPE Fellow 2025 <https://www.faspe-ethics.org/2025-fellows/> ykotturi.github.io she/her "
3305960,72711783,Correspond,DoIT-Research-Computing,2025-11-04 16:23:09.0000000,HPC User Account: qiz1 in pi_kotturi,resolved,Danielle Esposito,desposi1,Qi Zhao,qiz1,qiz1@umbc.edu,Yasmine Kotturi,kotturi@umbc.edu,"<div dir=3D""ltr"">approved</div><br><div class=3D""gmail_quote gmail_quote_co= ntainer""><div dir=3D""ltr"" class=3D""gmail_attr"">On Fri, Oct 31, 2025 at 4:25= =E2=80=AFPM RT API via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCH= elp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" s= tyle=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);pad= ding-left:1ex"">Greetings,<br> <br> This message has been automatically generated in response to the<br> creation of a ticket regarding you were Cc&#39;d on:<br> <br> -------------------------------------------------------------------------<b= r> Subject: &quot;HPC User Account: qiz1 in pi_kotturi&quot;<br> <br> Message: <br> <br> First Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Qi<br> Last Name:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0Zha= o<br> Email:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=  =C2=A0<a href=3D""mailto:qiz1@umbc.edu"" target=3D""_blank"">qiz1@umbc.edu</a>= <br> Campus ID:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0QQ4= 5731<br> <br> Request Type:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 High Performa= nce Cluster<br> <br> Create/Modify account in existing PI group<br> Existing PI Email:=C2=A0 =C2=A0 <a href=3D""mailto:kotturi@umbc.edu"" target= =3D""_blank"">kotturi@umbc.edu</a><br> Existing Group:=C2=A0 =C2=A0 =C2=A0 =C2=A0pi_kotturi<br> Project Title:=C2=A0 =C2=A0 =C2=A0 =C2=A0 A Human-Centered Approach to Buil= ding Generative AI System for Small Business Owners<br> Project Abstract:=C2=A0 =C2=A0 =C2=A0Entrepreneurs in resource-constrained = communities often lack the time and support to translate ideas into actiona= ble business plans.=C2=A0 While generative AI promises assistance, most sys= tems assume high digital literacy and overlook community infrastructures th= at shape adoption. We report on the community-centered design and deploymen= t of BizChat, an LLM-powered tool for business plan=C2=A0 development, intr= oduced across four workshops at a feminist business incubator and makerspac= e in a city. BizChat was designed to center entrepreneurs=E2=80=99 knowledg= e and workflows while providing just-in-time micro-learning and low-floor-h= igh-ceiling accessibility.=C2=A0 Through system log data (N=3D30) and semi-= structured interviews (N=3D10) with entrepreneurs, we show how the design a= nd deployment of=C2=A0 BizChat with existing community contexts lowered bar= riers to accessing capital, encouraged reflection, and empowered entreprene= urs=C2=A0 to support AI-literacy within their own communities. We contribut= e insights into how AI tools can be deployed within local support=C2=A0 net= works, and implications for design that strengthen community resilience ami= d rapid technological change.<br> <br> I am a phd student and my advisor is Dr. Yasmine Kotturi. I am requesting a= n account in order to join this exisiting group.<br> <br> <br> -------------------------------------------------------------------------<b= r> <br> There is no need to reply to this message right now.=C2=A0 <br> <br> The ticket has been assigned an ID of [Research Computing #3305960] or you = can go there directly by clicking the link below.<br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D330= 5960"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3305960</a> &gt;<br> <br> You can login with your UMBC credentials to view open tickets at any time b= y visiting <a href=3D""http://my.umbc.edu"" rel=3D""noreferrer"" target=3D""_bla= nk"">http://my.umbc.edu</a> and clicking on &quot;Help&quot; and &quot;Reque= st Help&quot;. <br> <br> <br> =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2= =A0 =C2=A0 Thank you<br> <br> </blockquote></div><div><br clear=3D""all""></div><br><span class=3D""gmail_si= gnature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_signature""><d= iv dir=3D""ltr""><div><span style=3D""color:rgb(0,0,0)"">Yasmine Kotturi, Ph.D.= </span></div><div><span style=3D""color:rgb(0,0,0)"">Assistant Professor, Inf= ormation Systems</span></div><div><span style=3D""color:rgb(0,0,0)"">Affiliat= e Faculty, Computer Science and Electrical Engineering</span></div><div><sp= an style=3D""color:rgb(0,0,0)"">University of Maryland, Baltimore County</spa= n></div><div><span style=3D""color:rgb(0,0,0)""><u><a href=3D""https://cra.org= /cra-and-microsoft-announce-inaugural-cohort-of-cra-trustworthy-ai-research= -fellows/"" target=3D""_blank"">CRA Trustworthy AI Research Fellow 2025-2026</= a></u></span></div><div><span style=3D""color:rgb(0,0,0)""><a href=3D""https:/= /www.usmd.edu/cai/inaugural-cohort-faculty-named-generative-ai-pedagogy-fel= lows"" target=3D""_blank"">USM Generative AI Pedagogy Fellow 2025-2026</a></sp= an></div><div><span style=3D""color:rgb(0,0,0)""><a href=3D""https://www.faspe= -ethics.org/2025-fellows/"" target=3D""_blank"">FASPE Fellow 2025</a></span></= div><div><a href=3D""http://ykotturi.github.io"" target=3D""_blank"">ykotturi.g= ithub.io</a></div><div><span style=3D""color:rgb(0,0,0)"">she/her</span></div= ><div><br></div></div></div> "
3305960,72712232,Correspond,DoIT-Research-Computing,2025-11-04 16:29:06.0000000,HPC User Account: qiz1 in pi_kotturi,resolved,Danielle Esposito,desposi1,Qi Zhao,qiz1,qiz1@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Qi,</p>  <p>Your account (qiz1) has been created on chip.rs.umbc.edu.<br /> Your primary group is pi_kotturi.<br /> Your home directory has 500M of storage.<br /> Please read through the documentation found at hpcf.umbc.edu &gt; User Support.<br /> All available modules can be viewed using the command &#39;module avail&#39;.<br /> Please submit additional questions or issues as separate tickets via the following link.<br /> (https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research%20Computing)</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Fri Oct 31 16:25:15 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Qi Last Name:                 Zhao Email:                     qiz1@umbc.edu Campus ID:                 QQ45731  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    kotturi@umbc.edu Existing Group:       pi_kotturi Project Title:        A Human-Centered Approach to Building Generative AI System for Small Business Owners Project Abstract:     Entrepreneurs in resource-constrained communities often lack the time and support to translate ideas into actionable business plans.  While generative AI promises assistance, most systems assume high digital literacy and overlook community infrastructures that shape adoption. We report on the community-centered design and deployment of BizChat, an LLM-powered tool for business plan  development, introduced across four workshops at a feminist business incubator and makerspace in a city. BizChat was designed to center entrepreneurs&rsquo; knowledge and workflows while providing just-in-time micro-learning and low-floor-high-ceiling accessibility.  Through system log data (N=30) and semi-structured interviews (N=10) with entrepreneurs, we show how the design and deployment of  BizChat with existing community contexts lowered barriers to accessing capital, encouraged reflection, and empowered entrepreneurs  to support AI-literacy within their own communities. We contribute insights into how AI tools can be deployed within local support  networks, and implications for design that strengthen community resilience amid rapid technological change.  I am a phd student and my advisor is Dr. Yasmine Kotturi. I am requesting an account in order to join this exisiting group.  </pre> </blockquote> </div> "
3306147,72647363,Create,DoIT-Research-Computing,2025-11-01 16:18:16.0000000,HPC Slurm/Software Issue: Cannot Cancel My Jobs,resolved,Roy Prouty,proutyr1,Justin Collier,justinc4,justinc4@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Justin Last Name:                 Collier Email:                     justinc4@umbc.edu Campus ID:                 WB55131  Request Type:              High Performance Cluster   Hello, I am having an issue with chip where I cannot cancel a number of jobs that are in queue. I used a script to create and submit the jobs last night but realized that I had a small mistake. I have tried to cancel all jobs under my username, justinc4, and cancel specific jobs by id but they remain in the queue. The jobs have remained in the queue for more than 12 hours since I first tried to cancel them. All pending jobs on the match partition under my username name need to be cancelled.  "
3306147,72647496,Correspond,DoIT-Research-Computing,2025-11-01 17:20:44.0000000,HPC Slurm/Software Issue: Cannot Cancel My Jobs,resolved,Roy Prouty,proutyr1,Justin Collier,justinc4,justinc4@umbc.edu,Roy Prouty,proutyr1@umbc.edu,"<div> <p>Understood and no problem. I&#39;ve cancelled all of your jobs submitted to the match partition that were pending. When canceling your own jobs, be sure to specify the cluster with something like `scancel -M chip-cpu JOBID`.</p>  <p>&nbsp;</p>  <p>On Sat Nov 01 12:18:16 2025, ZZ99999 wrote:</p>  <blockquote> <pre> First Name:                Justin Last Name:                 Collier Email:                     justinc4@umbc.edu Campus ID:                 WB55131  Request Type:              High Performance Cluster   Hello, I am having an issue with chip where I cannot cancel a number of jobs that are in queue. I used a script to create and submit the jobs last night but realized that I had a small mistake. I have tried to cancel all jobs under my username, justinc4, and cancel specific jobs by id but they remain in the queue. The jobs have remained in the queue for more than 12 hours since I first tried to cancel them. All pending jobs on the match partition under my username name need to be cancelled.  </pre> </blockquote> </div>  <p>&nbsp;</p>  <p>--&nbsp;</p>  <p>Roy Prouty<br /> DoIT Research Computing Team</p> "
3306171,72647607,Create,DoIT-Research-Computing,2025-11-01 17:50:40.0000000,HPC Other Issue: software request,resolved,Danielle Esposito,desposi1,James Williams,james36,james36@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                James Last Name:                 Williams Email:                     james36@umbc.edu Campus ID:                 BN46293  Request Type:              High Performance Cluster  Is it possible to request modules/packages to be added to the cluster? I would like to be able to use RSEM and BOWTIE2. Thanks!  "
3306171,72712777,Correspond,DoIT-Research-Computing,2025-11-04 16:38:35.0000000,HPC Other Issue: software request,resolved,Danielle Esposito,desposi1,James Williams,james36,james36@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi James,</p>  <p>The modules, RSEM and BOWTIE2 have been installed on chip!&nbsp;</p>  <p>I installed&nbsp;RSEM/1.3.3-foss-2022a, and&nbsp;Bowtie2/2.4.5-GCC-11.3.0.&nbsp;</p>  <p>These can be loaded with &#39;module load&nbsp;RSEM/1.3.3-foss-2022a&#39; (note, RSEM depends on BOWTIE2, so the correct version of BOWTIE2 is loaded when you load RSEM).</p>  <p>If you encounter any issues using the modules, or need any additional modules, feel free to submit a new ticket! Have a great day!</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Sat Nov 01 13:50:40 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                James Last Name:                 Williams Email:                     james36@umbc.edu Campus ID:                 BN46293  Request Type:              High Performance Cluster  Is it possible to request modules/packages to be added to the cluster? I would like to be able to use RSEM and BOWTIE2. Thanks!  </pre> </blockquote> </div> "
3306427,72650903,Create,DoIT-Research-Computing,2025-11-03 00:04:14.0000000,HPC Slurm/Software Issue: SSH connection failing,stalled,Danielle Esposito,desposi1,James Baker,jbaker15,jbaker15@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                James Last Name:                 Baker Email:                     jbaker15@umbc.edu Campus ID:                 VM40694  Request Type:              High Performance Cluster   I have a few ssh sessions open connecting to chip and they keep disconnecting every few minutes. Previously, this had been a problem and it was resolved (something to do with a load balancer) but its back now.  "
3306427,72684558,Correspond,DoIT-Research-Computing,2025-11-03 16:57:43.0000000,HPC Slurm/Software Issue: SSH connection failing,stalled,Danielle Esposito,desposi1,James Baker,jbaker15,jbaker15@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi James,</p>  <p>Since the load balancer issue was fixed, we have not seen any other reports of this issue. I am currently testing myself, and have not encountered any disconnects. Would you mind sharing a bit more about your setup? How are you sshing (ie, a dedicated client like putty, or just through your terminal)? Do you have any local SSH configuration? Are you logged in using GlobalProtect VPN?&nbsp;</p>  <p>Let me know and I can look into this further for you! Have a nice day!</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Sun Nov 02 19:04:14 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                James Last Name:                 Baker Email:                     jbaker15@umbc.edu Campus ID:                 VM40694  Request Type:              High Performance Cluster   I have a few ssh sessions open connecting to chip and they keep disconnecting every few minutes. Previously, this had been a problem and it was resolved (something to do with a load balancer) but its back now.  </pre> </blockquote> </div> "
3306608,72658513,Create,DoIT-Research-Computing,2025-11-03 14:47:01.0000000,HPC Slurm/Software Issue: Slurm Not Loading Anaconda Module - Job ID 113232-113234,resolved,Danielle Esposito,desposi1,John Vorhies,johnv3,johnv3@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                John Last Name:                 Vorhies Email:                     johnv3@umbc.edu Campus ID:                 HY99570  Request Type:              High Performance Cluster   I went to run a job this morning that I have run quite often. The first request for this job (ID 113231) started just fine. Subsequent requests error out in my Python script indicating issues loading modules from my Conda environment. The job log shows that ""conda"" is not a known command, indicating an issue loading the Anaconda module.  "
3306608,72660450,Correspond,DoIT-Research-Computing,2025-11-03 15:16:42.0000000,HPC Slurm/Software Issue: Slurm Not Loading Anaconda Module - Job ID 113232-113234,resolved,Danielle Esposito,desposi1,John Vorhies,johnv3,johnv3@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi John,</p>  <p>There appears to have been a minor configuration error with the node that job was running on (g24-11). This has now been resolved. If you continue to experience issues, please feel free to submit a new ticket! Have a nice day!</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Mon Nov 03 09:47:01 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                John Last Name:                 Vorhies Email:                     johnv3@umbc.edu Campus ID:                 HY99570  Request Type:              High Performance Cluster   I went to run a job this morning that I have run quite often. The first request for this job (ID 113231) started just fine. Subsequent requests error out in my Python script indicating issues loading modules from my Conda environment. The job log shows that &quot;conda&quot; is not a known command, indicating an issue loading the Anaconda module.  </pre> </blockquote> </div> "
3308276,72685803,Create,DoIT-Research-Computing,2025-11-03 17:23:11.0000000,"HPC Other Issue: Data missing from ""umbc/rs/zzbatmos/users/ztushar1""",resolved,Danielle Esposito,desposi1,Zahid Hassan Tushar,ztushar1,ztushar1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Zahid Hassan Last Name:                 Tushar Email:                     ztushar1@umbc.edu Campus ID:                 TS41250  Request Type:              High Performance Cluster  Hi, I hope this email finds you well. I noticed that my data from ""umbc/rs/zzbatmos/users/ztushar1"" is missing. I found that there is another directory ""umbc/rs/pi_zzbatmos/users/ztushar1"" in a similar path but this new directory contains partial data.   Could you please let me know what happened to my original data or if it was moved somewhere else?  thank you.  "
3308276,72687865,Correspond,DoIT-Research-Computing,2025-11-03 18:12:12.0000000,"HPC Other Issue: Data missing from ""umbc/rs/zzbatmos/users/ztushar1""",resolved,Danielle Esposito,desposi1,Zahid Hassan Tushar,ztushar1,ztushar1@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Zahid,&nbsp;</p>  <p>Currently, the research volume for pi_zzbatmos is undergoing migration to a new storage server (ceph). This date was scheduled with your PI in advance, and during the time of migration we request that all users in the group do not use chip to avoid disturbing your groups migration.</p>  <p>Your PI should have more information regarding the specifics. However, since pi_zzbatmos has a very large research volume, the migration is taking a long time (it was started on Friday, and is still going). After the migration properly completes, you should have access to your data.</p>  <p>Something that you already noticed, the new Ceph research volumes start with &#39;pi_&#39;. For example, the old volume was mounted at &#39;/umbc/rs/zzbatmos&#39;, where the new volume (the one you noticed) is located at &#39;/umbc/rs/pi_zzbatmos&#39;.&nbsp;</p>  <p>Your PI should let the group know when the migration is finished. But if you have any additional questions in the meantime, feel free to let us know! Have a good day!</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Mon Nov 03 12:23:11 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Zahid Hassan Last Name:                 Tushar Email:                     ztushar1@umbc.edu Campus ID:                 TS41250  Request Type:              High Performance Cluster  Hi, I hope this email finds you well. I noticed that my data from &quot;umbc/rs/zzbatmos/users/ztushar1&quot; is missing. I found that there is another directory &quot;umbc/rs/pi_zzbatmos/users/ztushar1&quot; in a similar path but this new directory contains partial data.   Could you please let me know what happened to my original data or if it was moved somewhere else?  thank you.  </pre> </blockquote> </div> "
3308743,72699324,Create,DoIT-Research-Computing,2025-11-04 00:07:52.0000000,HPC Slurm/Software Issue: Pending Jobs Not Cancelling,resolved,Elliot Gobbert,elliotg2,Benjamin Kale,bkale1,bkale1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Benjamin Last Name:                 Kale Email:                     bkale1@umbc.edu Campus ID:                 VE50939  Request Type:              High Performance Cluster   I have 6 jobs that have been pending for about 24 hours. I recently tried to cancel them with scancel, but it doesn't seem to be working. This has happened before when the queue was long. The job ids are 593665, 593672, 593679, 593686, 593693, 593700, 593707.  "
3308743,72705040,Correspond,DoIT-Research-Computing,2025-11-04 14:20:34.0000000,HPC Slurm/Software Issue: Pending Jobs Not Cancelling,resolved,Elliot Gobbert,elliotg2,Benjamin Kale,bkale1,bkale1@umbc.edu,Elliot Gobbert,elliotg2@umbc.edu,"<p>Hi,</p>  <p>Well, when I squeue&#39;d for your jobs just now, none of them came up, meaning I suppose they finished. If you could give me the exact path to the jobs you were trying to cancel, I could test some things myself. Without more information, I can&#39;t test much myself.&nbsp;</p>  <p>&nbsp;</p>  <p>However, looking at the history of your &quot;scancel&quot; commands, I do notice a mistake you made, which could very well be the reason it didn&#39;t work. You ran:</p>  <p>scancel -u bkale1<br /> <br /> This is not correct on the Chip cluster; the correct command would&#39;ve been:</p>  <pre> <code>scancel --cluster=chip-cpu -u </code>bkale1  or  <code>scancel --cluster=chip-gpu -u </code>bkale1</pre>  <p>&nbsp;</p>  <p>Yes, I know it&#39;s a bit confusing, but you have to specify the cluster CPU/GPU to cancel all jobs for your user. Here&#39;s some wiki documentation about that:</p>  <p>https://umbc.atlassian.net/wiki/spaces/faq/pages/1335951387/Basic+Slurm+Commands#Managing-and-Controlling-Jobs</p>  <p>&nbsp;</p>  <p>Let me know if that was the issue.</p>  <p>Best,</p>  <p>Elliot Gobbert</p> "
3308993,72706534,Create,DoIT-Research-Computing,2025-11-04 14:43:25.0000000,HPC User Account: kchhaya1 in pi_kotturi,resolved,Danielle Esposito,desposi1,Ketul Kishorbhai Chhaya,kchhaya1,kchhaya1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Ketul Kishorbhai Last Name:                 Chhaya Email:                     kchhaya1@umbc.edu Campus ID:                 IX96931  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    kotturi@umbc.edu Existing Group:       pi_kotturi Project Title:        Towards Designing for Resilience: Community-Centered = Deployment of an AI Business Planning Tool in a Feminist Makerspace Project Abstract:     Entrepreneurs in resource-constrained communities oft= en lack the time and support to translate ideas into actionable business plans. While generative AI promises assistance, most systems assume high digital literacy and overlook community infrastructures that shape adoption. We report on the community-centered design and deployment of BizChat, an LLM-powered tool for business plan development, introduced across four workshops at a feminist busi- ness incubator and makerspace. BizChat was designed to center entrepreneurs=E2=80=99 knowledge and workflows while providing just-= in-time micro-learning and low-floor-high-ceiling accessibility. Through system log data (N=3D30) and semi-structured interviews (N=3D10) with entrepreneurs, we show how the design and deploy- ment of BizChat with existing community contexts lowered bar- riers to accessing capital, encouraged reflection, and empowered entrepreneurs to support AI-literacy within their own communities. We contribute insights into how AI tools can be deployed within local support networks, and implications for design that strengthen community resilience amid rapid technological change.  I am a master's student and requesting this account to join in existing gro= up. I am working under Kotturi's Lab.  "
3308993,72711693,Correspond,DoIT-Research-Computing,2025-11-04 16:22:13.0000000,HPC User Account: kchhaya1 in pi_kotturi,resolved,Danielle Esposito,desposi1,Ketul Kishorbhai Chhaya,kchhaya1,kchhaya1@umbc.edu,Yasmine Kotturi,kotturi@umbc.edu,"approved  On Tue, Nov 4, 2025 at 9:43=E2=80=AFAM RT API via RT <UMBCHelp@rt.umbc.edu>=  wrote:  > This e-mail is a notification that a UMBC user: Ketul Kishorbhai Chhaya < > kchhaya1@umbc.edu> has requested an account within UMBC's HPC environment > in your group <pi_kotturi>. As the PI, we request that you acknowledge and > approve this account creation by replying to this message. Alternatively > you can go to this link and review the ticket and indicate your decision > here: > > Ticket <URL: https://rt.umbc.edu/Ticket/Display.html?id=3D3308993 > > > Once we have your approval, we will create the account and you and the new > user will receive another e-mail notifying you that the account has been > created. If you have any other questions or concerns please contact us. > > - UMBC DoIT Research Computing Support Staff >   --=20 Yasmine Kotturi, Ph.D. Assistant Professor, Information Systems Affiliate Faculty, Computer Science and Electrical Engineering University of Maryland, Baltimore County *CRA Trustworthy AI Research Fellow 2025-2026 <https://cra.org/cra-and-microsoft-announce-inaugural-cohort-of-cra-trustwo= rthy-ai-research-fellows/>* USM Generative AI Pedagogy Fellow 2025-2026 <https://www.usmd.edu/cai/inaugural-cohort-faculty-named-generative-ai-peda= gogy-fellows> FASPE Fellow 2025 <https://www.faspe-ethics.org/2025-fellows/> ykotturi.github.io she/her "
3308993,72711693,Correspond,DoIT-Research-Computing,2025-11-04 16:22:13.0000000,HPC User Account: kchhaya1 in pi_kotturi,resolved,Danielle Esposito,desposi1,Ketul Kishorbhai Chhaya,kchhaya1,kchhaya1@umbc.edu,Yasmine Kotturi,kotturi@umbc.edu,"<div dir=3D""ltr"">approved</div><br><div class=3D""gmail_quote gmail_quote_co= ntainer""><div dir=3D""ltr"" class=3D""gmail_attr"">On Tue, Nov 4, 2025 at 9:43= =E2=80=AFAM RT API via RT &lt;<a href=3D""mailto:UMBCHelp@rt.umbc.edu"">UMBCH= elp@rt.umbc.edu</a>&gt; wrote:<br></div><blockquote class=3D""gmail_quote"" s= tyle=3D""margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);pad= ding-left:1ex"">This e-mail is a notification that a UMBC user: Ketul Kishor= bhai Chhaya &lt;<a href=3D""mailto:kchhaya1@umbc.edu"" target=3D""_blank"">kchh= aya1@umbc.edu</a>&gt; has requested an account within UMBC&#39;s HPC enviro= nment in your group &lt;pi_kotturi&gt;. As the PI, we request that you ackn= owledge and approve this account creation by replying to this message. Alte= rnatively you can go to this link and review the ticket and indicate your d= ecision here:<br> <br> Ticket &lt;URL: <a href=3D""https://rt.umbc.edu/Ticket/Display.html?id=3D330= 8993"" rel=3D""noreferrer"" target=3D""_blank"">https://rt.umbc.edu/Ticket/Displ= ay.html?id=3D3308993</a> &gt;<br> <br> Once we have your approval, we will create the account and you and the new = user will receive another e-mail notifying you that the account has been cr= eated. If you have any other questions or concerns please contact us.<br> <br> - UMBC DoIT Research Computing Support Staff<br> </blockquote></div><div><br clear=3D""all""></div><br><span class=3D""gmail_si= gnature_prefix"">-- </span><br><div dir=3D""ltr"" class=3D""gmail_signature""><d= iv dir=3D""ltr""><div><span style=3D""color:rgb(0,0,0)"">Yasmine Kotturi, Ph.D.= </span></div><div><span style=3D""color:rgb(0,0,0)"">Assistant Professor, Inf= ormation Systems</span></div><div><span style=3D""color:rgb(0,0,0)"">Affiliat= e Faculty, Computer Science and Electrical Engineering</span></div><div><sp= an style=3D""color:rgb(0,0,0)"">University of Maryland, Baltimore County</spa= n></div><div><span style=3D""color:rgb(0,0,0)""><u><a href=3D""https://cra.org= /cra-and-microsoft-announce-inaugural-cohort-of-cra-trustworthy-ai-research= -fellows/"" target=3D""_blank"">CRA Trustworthy AI Research Fellow 2025-2026</= a></u></span></div><div><span style=3D""color:rgb(0,0,0)""><a href=3D""https:/= /www.usmd.edu/cai/inaugural-cohort-faculty-named-generative-ai-pedagogy-fel= lows"" target=3D""_blank"">USM Generative AI Pedagogy Fellow 2025-2026</a></sp= an></div><div><span style=3D""color:rgb(0,0,0)""><a href=3D""https://www.faspe= -ethics.org/2025-fellows/"" target=3D""_blank"">FASPE Fellow 2025</a></span></= div><div><a href=3D""http://ykotturi.github.io"" target=3D""_blank"">ykotturi.g= ithub.io</a></div><div><span style=3D""color:rgb(0,0,0)"">she/her</span></div= ><div><br></div></div></div> "
3308993,72712610,Correspond,DoIT-Research-Computing,2025-11-04 16:34:11.0000000,HPC User Account: kchhaya1 in pi_kotturi,resolved,Danielle Esposito,desposi1,Ketul Kishorbhai Chhaya,kchhaya1,kchhaya1@umbc.edu,Danielle Esposito,desposi1@umbc.edu,"<p>Hi Ketul,</p>  <p>Your account (kchhaya1) has been created on chip.rs.umbc.edu.<br /> Your primary group is pi_kotturi.<br /> Your home directory has 500M of storage.<br /> Please read through the documentation found at hpcf.umbc.edu &gt; User Support.<br /> All available modules can be viewed using the command &#39;module avail&#39;.<br /> Please submit additional questions or issues as separate tickets via the following link.<br /> (https://rtforms.umbc.edu/rt_authenticated/doit/DoIT-support.php?auto=Research%20Computing)</p>  <p>--&nbsp;</p>  <p>Kind regards,<br /> Danielle Esposito (she/her/hers)<br /> DoIT Unix Infra Student Worker</p>  <p>&nbsp;</p>  <div>On Tue Nov 04 09:43:25 2025, ZZ99999 wrote: <blockquote> <pre> First Name:                Ketul Kishorbhai Last Name:                 Chhaya Email:                     kchhaya1@umbc.edu Campus ID:                 IX96931  Request Type:              High Performance Cluster  Create/Modify account in existing PI group Existing PI Email:    kotturi@umbc.edu Existing Group:       pi_kotturi Project Title:        Towards Designing for Resilience: Community-Centered Deployment of an AI Business Planning Tool in a Feminist Makerspace Project Abstract:     Entrepreneurs in resource-constrained communities often lack the time and support to translate ideas into actionable business plans. While generative AI promises assistance, most systems assume high digital literacy and overlook community infrastructures that shape adoption. We report on the community-centered design and deployment of BizChat, an LLM-powered tool for business plan development, introduced across four workshops at a feminist busi- ness incubator and makerspace. BizChat was designed to center entrepreneurs&rsquo; knowledge and workflows while providing just-in-time micro-learning and low-floor-high-ceiling accessibility. Through system log data (N=30) and semi-structured interviews (N=10) with entrepreneurs, we show how the design and deploy- ment of BizChat with existing community contexts lowered bar- riers to accessing capital, encouraged reflection, and empowered entrepreneurs to support AI-literacy within their own communities. We contribute insights into how AI tools can be deployed within local support networks, and implications for design that strengthen community resilience amid rapid technological change.  I am a master&#39;s student and requesting this account to join in existing group. I am working under Kotturi&#39;s Lab.  </pre> </blockquote> </div> "
3309956,72724591,Create,DoIT-Research-Computing,2025-11-04 17:57:29.0000000,HPC Slurm/Software Issue: Request to reserve GPU space for a deadline in December ACL 2026,new,Max Breitmeyer,mb17,Roy Prouty,proutyr1,proutyr1@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Roy Last Name:                 Prouty Email:                     proutyr1@umbc.edu Campus ID:                 WH39335  Request Type:              High Performance Cluster   My student, Nilanjana Das (in CC), would like to reserve GPU nodes (preferably H100) for 2 weeks to complete her experiments for the ACL 2026 deadline.   Can you please assist her with this?   "
3310226,72733753,Create,DoIT-Research-Computing,2025-11-04 20:59:07.0000000,N-Mode cavity laser simulation - In need of computing power,new,Nobody in particular," ",Amber Manspeaker,amberm4,amberm4@umbc.edu,RT API,api-noreply@umbc.edu,"First Name:                Amber Last Name:                 Manspeaker Email:                     amberm4@umbc.edu Campus ID:                 JZ72012  Request Type:              Help with something else  I am in need of additional computer power as well as computational memory for simulations I am making and running for Dr. Shih in the Physics Department.   "
3310226,72733891,Correspond,DoIT-Research-Computing,2025-11-04 21:01:54.0000000,N-Mode cavity laser simulation - In need of computing power,new,Nobody in particular," ",Amber Manspeaker,amberm4,amberm4@umbc.edu,Amber Manspeaker,amberm4@umbc.edu,"<div> <blockquote> <pre> Please disregard this ticket request.   </pre> </blockquote> </div> "
